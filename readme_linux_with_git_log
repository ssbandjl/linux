https://docs.kernel.org/driver-api/scsi.html
https://github.com/ssbandjl/linux.git
bio下发流程: https://blog.csdn.net/flyingnosky/article/details/121362813
io路径: https://zhuanlan.zhihu.com/p/545906763
用户态与内核态通信netlink: https://gist.github.com/lflish/15e85da8bb9200794255439d0563b195
实现rfc3720: https://github.com/ssbandjl/linux/commit/39e84790d3b65a4af1ea1fb0d8f06c3ad75304b3
管理内核模块,红帽: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/managing-kernel-modules_managing-monitoring-and-updating-the-kernel
存储技术原理: file:///D:/xb/project/c/%E5%AD%98%E5%82%A8%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90_%E5%9F%BA%E4%BA%8ELinux%202.6%E5%86%85%E6%A0%B8%E6%BA%90%E4%BB%A3%E7%A0%81.pdf
内核定时器: 



dmesg -w

git tag -d v5.10

nvme
static struct blk_mq_ops nvme_mq_ops -> struct blk_mq_ops 
static const struct blk_mq_ops bsg_mq_ops = {
	.queue_rq		= bsg_queue_rq,
	.init_request		= bsg_init_rq,
	.exit_request		= bsg_exit_rq,
	.complete		= bsg_complete,
	.timeout		= bsg_timeout,
};

驱动目录: drivers/nvme/Makefile
参考: https://mp.weixin.qq.com/s/GKVIY_NOXDfUCOho4zBMzw
Kconfig 文件的作用是：
1.控制make menuconfig时， 出现的配置选项；
2.根据用户配置界面的选择，将配置结果保存在.config 配置文件（该文件将提供给Makefile使用，用以决定要编译的内核组件以及如何编译）

drivers/nvme/host/core.c
module_init(nvme_core_init) -> static int __init nvme_core_init(void)
alloc_chrdev_region nvme
class_create("nvme")

drivers/nvme/target/rdma.c
static int __init nvmet_rdma_init(void)
ib_register_client -> drivers/infiniband/core/device.c -> int ib_register_client -> IB 驱动程序的上层用户可以使用 ib_register_client() 来注册 IB 设备添加和删除的回调。 添加 IB 设备时，将调用每个已注册客户端的 add 方法（按照客户端注册的顺序），而当删除设备时，将调用每个客户端的 remove 方法（按照客户端注册的相反顺序）。 另外，调用ib_register_client()时，客户端会收到所有已经注册的设备的add回调
nvmet_register_transport -> int nvmet_register_transport
	down_write
	up_write

EXPORT_SYMBOL(ib_register_client) 导出符号表

multi_path:
drivers/md/md.c
drivers/md/md-multipath.c


.map = multipath_map_bio -> static int multipath_map_bio

static void process_queued_bios

dm-mpath.c -> static int fail_path #路径故障
	pgpath->pg->ps.type->fail_path(&pgpath->pg->ps, &pgpath->path)
	dm_path_uevent(DM_UEVENT_PATH_FAILED -> 此补丁添加了对失败路径和恢复路径的 dm_path_event 的调用 -> void dm_path_uevent
		dm_build_path_uevent
		dm_uevent_add -> void dm_uevent_add -> list_add(elist, &md->uevent_list) -> 挂链表
		dm_send_uevents -> void dm_send_uevents
			dm_copy_name_and_uuid
			kobject_uevent_env 发送uevent, kobject_uevent这个函数原型如下，就是向用户空间发送uevent，可以理解为驱动（内核态）向用户（用户态）发送了一个KOBJ_ADD
				kobject_uevent_net_broadcast -> 参考热拔插原理
	queue_work
	schedule_work
static int __multipath_map_bio


热拔插: 热插拔：在不重启系统的情况下，增减硬件设备。本文主要介绍linux下的热插拔 https://www.cnblogs.com/tzj-kernel/p/15307231.html
热插拔：实现了驱动向用户态通知设备插拔
（1）外设插入，硬件中断响应
（2）总线发现新的设备，驱动probe 再调用device_add(设备驱动？？)
（3）device_add调用kobject_uevent(, KOBJ_ADD)，向用户空间广播新设备加入事件通知；这里发出通知的方式，就是netlink；
（4）用户空间运行的daemon(udev)收到event事件广播；
（5）udev根据消息和环境变量，查询sysfs中的/sys的变化，按照规则(/etc/udev/rules.d/*)，在/dev目录下自动创建设备节点；
（6）运行 /sbin/hotplug 脚本
kobject_uevent_env--》kobject_uevent_net_broadcast--》uevent_net_broadcast_untagged或者uevent_net_broadcast_tagged--》netlink_broadcast，实际上就是将buf中的内容发送到用户空间，用户空间udev监听到此消息，则解析
/sys/kernel/uevent_helper

内核 uevents 和 udev 
必需的设备信息由 sysfs 文件系统导出。对于内核检测到并已初始化的设备，将创建一个带有该设备名称的目录。它包含带有特定于设备属性的属性文件。
每次添加或删除设备时，内核都会发送 uevent 来向 udev 通知更改。一旦启动，udev 守护程序便会读取并分析 /usr/lib/udev/rules.d/*.rules 和 /etc/udev/rules.d/*.rules 文件中的所有规则，并将它们保留在内存中。如果更改、添加或去除了规则文件，守护程序可以使用 udevadm control --reload 命令重新装载这些规则在内存中的表示。有关 udev 规则及其语法的更多细节，请参见第 22.6 节 “使用 udev 规则影响内核设备事件处理”。每个接收到的事件都根据所提供的规则集进行匹配。这些规则可以增加或更改事件环境键、为要创建的设备节点请求特定名称、添加指向该节点的符号链接或者添加设备节点创建后运行的程序。从内核 netlink 套接字接收驱动程序核心 uevent
事件处理: https://documentation.suse.com/zh-cn/sles/15-SP1/html/SLES-all/cha-udev.html
udev rule: ls -alh /usr/lib/udev/rules.d/
cat /usr/lib/udev/rules.d/62-multipath.rules




uevent设计: https://github.com/ssbandjl/linux/commit/7a8c3d3b92883798e4ead21dd48c16db0ec0ff6f
device-mapper uevent代码为device-mapper增加了创建和发送kobject uevents（uevents）的能力。 以前的设备映射器事件只能通过 ioctl 接口获得。 uevents 接口的优点是事件包含环境属性，为事件提供增加的上下文，避免在收到事件后查询设备映射器设备的状态
由 udevmonitor 捕获的生成的 uevent 示例


insert_work(pwq, work, worklist, work_flags)

dm_init_init
	dm_early_create
__bind
dm_table_event_callback(t, event_callback, md)
event_fn
dm_table_event
trigger_event

__map_bio

io_path, io路径
static const struct block_device_operations dm_blk_dops = {
	.submit_bio = dm_submit_bio,  -> static void dm_submit_bio

dm_split_and_process_bio
	__split_and_process_bio 选择正确的策略来处理非flush bio
		dm_table_find_target
		__map_bio
			ti->type->map(ti, clone) -> .map = multipath_map_bio -> static int __multipath_map_bio -> bio_set_dev(bio, pgpath->path.dev->bdev) -> pgpath->pg->ps.type->start_io
	bio_trim
	trace_block_split
	bio_inc_remaining
	submit_bio_noacct -> void submit_bio_noacct(struct bio *bio) ->  为 I/O 重新提交 bio 到块设备层 bio 描述内存和设备上的位置。 这是 submit_bio() 的一个版本，只能用于通过堆叠块驱动程序重新提交给较低级别驱动程序的 I/O。 块层的所有文件系统和其他上层用户应该使用 submit_bio() 代替, bio 在节流之前已经被检查过，所以在从节流队列中调度它之前不需要再次检查它。 为此目的添加 submit_bio_noacct_nocheck() 的助手
		__submit_bio_noacct_mq
		submit_bio_noacct_nocheck -> submit_bio() 确实可以通过递归调用 submit_bio_noacct 添加更多的 bios
			__submit_bio
				blk_mq_submit_bio -> void blk_mq_submit_bio -> 向块设备提交bio, 会执行调度,合并等操作
					blk_mq_bio_to_request
					blk_queue_bounce -> 弹跳
					__bio_split_to_limits
					blk_mq_bio_to_request -> request
					blk_mq_insert_request
						blk_mq_request_bypass_insert
					blk_mq_run_hw_queue

blk_mq_plug_issue_direct
blk_add_rq_to_plug

blk_mq_requeue_request
	blk_mq_sched_requeue_request
		e->type->ops.requeue_request(rq) -> requeue_request = 

blk_mq_submit_bio
blk_mq_run_hw_queue
__blk_mq_sched_dispatch_requests

static const struct blk_mq_ops nvme_rdma_mq_ops = {
	.queue_rq	= nvme_rdma_queue_rq,
	.complete	= nvme_rdma_complete_rq,
	.init_request	= nvme_rdma_init_request,
	.exit_request	= nvme_rdma_exit_request,
	.init_hctx	= nvme_rdma_init_hctx,
	.timeout	= nvme_rdma_timeout,
	.map_queues	= nvme_rdma_map_queues,
	.poll		= nvme_rdma_poll,
};
nvme_rdma_queue_rq
drivers/nvme/host/rdma.c -> static int nvme_rdma_post_send
	ib_post_send


三条链：current->bio_list存储在当前线程的所有bio; plug->mq_list使能plug/unplug机制时存放在缓存池的bio；若定义IO调度层，IO请求会发送到scheduler list中；若没有定义IO调度层，IO请求会发送到ctx->rq_lists
每个线程若已经在执行blk_mq_submit_bio()，将新下发BIO链入到线程current->bio_list;
依次处理current->list中的每个bio；
若bio中存在数据在高端内存区，在低端内存区分配内存，将高端内存区数据拷贝到新分配的内存区，称为bounce过程，后面单独一节介绍；
检查请求队列中的bio，若过大进行切分，称BIO的切分；
尝试将bio合并到plug->mq_list中，然后尝试合并到IO调度层链表或ctx->rq_lists中；
若没有合并，分配新的request；
若定义plug，且没达到冲刷数目，加入到plug->mq_list；若达到冲刷数目，将冲刷下发（plug/unplug机制）；
若定义IO调度器，往IO调度器中插入新的request（对于机械硬盘，通过IO调度层座合并和排序，有利于提高性能）;
若 没有定义IO调度器，可以直接下发（对于较快的硬盘如nvme盘，进入调度层可能会浪费时间，跳过IO调度层有利于性能提升）
（1）bounce过程
（2）bio的切分和合并
（3）IO请求和tag的分配
（4）plug/unplug机制
（5）IO调度器
（4）其他


app读写io:
int main()
{
       char buff[128] = {0};
       int fd = open("/var/pilgrimtao.txt", O_CREAT|O_RDWR);
​
       write(fd, "pilgrimtao is cool", 18);
       pread(fd, buff, 128, 0);
       printf("%s\n", buff);
​
       close(fd);
       return 0;
}


文件系统:
SYSCALL_DEFINE3(read, ...) -> ksys_read -> vfs_read -> read_iter -> xfs_file_read_iter
SYSCALL_DEFINE3(write, ...) -> ksys_write -> vfs_write -> new_sync_write -> call_write_iter ->write_iter -> xfs_file_write_iter

磁盘(disk)的访问模式有三种 BUFFERED、DIRECT、DAX。前面提到的由于page cache存在可以避免耗时的磁盘通信就是BUFFERED访问模式的集中体现；但是如果我要求用户的write请求要实时存储到磁盘里，不能只在内存中更新，那么此时我便需要DIRECT模式；大家可能听说过flash分为两种nand flash和nor flash，nor flash可以像ram一样直接通过地址线和数据线访问，不需要整块整块的刷，对于这种场景我们采用DAX模式。所以file_operations的read_iter和write_iter回调函数首先就需要根据不同的标志判断采用哪种访问模式
kernel在2020年12月的patch中提出了folio的概念，我们可以把folio简单理解为一段连续内存，一个或多个page的集合，他和page的关系如图
代码参考：xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __filemap_get_folio -> filemap_alloc_folio

读取磁盘inode代码参考：iomap_file_buffered_write -> iomap_iter -> .iomap_begin -> xfs_buffered_write_iomap_begin -> xfs_iread_extents -> xfs_btree_visit_blocks -> xfs_btree_readahead_ptr -> xfs_buf_readahead -> xfs_buf_readahead_map -> xfs_buf_read_map -> xfs_buf_read -> xfs_buf_submit -> __xfs_buf_submit -> xfs_buf_ioapply_map -> submit_bio

代码参考： xfs_file_read_iter -> xfs_file_buffered_read -> generic_file_read_iter -> filemap_read -> filemap_get_pages -> filemap_create_folio -> filemap_alloc_folio -> folio_alloc
filemap_get_pages -> filemap_readahead -> page_cache_async_ra -> ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> filemap_alloc_folio/filemap_add_folio

xfs_file_read_iter -> xfs_file_buffered_read -> generic_file_read_iter -> filemap_read -> copy_folio_to_iter(offset)

filemap_get_pages -> filemap_readahead -> page_cache_async_ra -> ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> read_pages -> aops.readahead -> xfs_vm_readahead -> iomap_readahead -> iomap_iter -> ops.iomap_begin（xfs文件系统维护的回调函数）

iomap_readahead -> iomap_readahead_iter -> iomap_readpage_iter -> bio_alloc/bio_add_folio

代码参考：iomap_readahead -> iomap_iter -> ops.iomap_begin（xfs文件系统维护的回调函数）

代码参考：
sys_read：ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> xa_load（在sys_read流程中，因为一开始就会把所有的folio都拿到，不是一个一个拿的）
iomap_readahead_iter -> readahead_folio
sys_write：
xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __filemap_get_folio -> mapping_get_entry/filemap_add_folio （在sys_write流程，是用完一个folio，再申请新的folio）


代码参考：
sys_read：iomap_readpage_iter -> bio_add_folio -> __bio_try_merge_page
sys_write：xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __iomap_write_begin -> iomap_read_folio_sync -> bio_init/bio_add_folio （一个bio只有一个bio_vec）


bpf: bpf.h

syscall 系统调用: linux/syscalls.h


iscsi:
ko:
[root@n73 linux-5.10.182]# find . -name "*.ko"|grep scsi
./drivers/firmware/iscsi_ibft.ko
./drivers/message/fusion/mptscsih.ko
./drivers/scsi/aacraid/aacraid.ko
./drivers/scsi/aic7xxx/aic79xx.ko
./drivers/scsi/arcmsr/arcmsr.ko
./drivers/scsi/be2iscsi/be2iscsi.ko
./drivers/scsi/bfa/bfa.ko
./drivers/scsi/bnx2fc/bnx2fc.ko
./drivers/scsi/bnx2i/bnx2i.ko
./drivers/scsi/csiostor/csiostor.ko
./drivers/scsi/cxgbi/cxgb3i/cxgb3i.ko
./drivers/scsi/cxgbi/cxgb4i/cxgb4i.ko
./drivers/scsi/cxgbi/libcxgbi.ko
./drivers/scsi/fcoe/fcoe.ko
./drivers/scsi/fcoe/libfcoe.ko
./drivers/scsi/fnic/fnic.ko
./drivers/scsi/isci/isci.ko
./drivers/scsi/libfc/libfc.ko
./drivers/scsi/libsas/libsas.ko
./drivers/scsi/lpfc/lpfc.ko
./drivers/scsi/megaraid/megaraid_sas.ko
./drivers/scsi/mpt3sas/mpt3sas.ko
./drivers/scsi/mvsas/mvsas.ko
./drivers/scsi/pm8001/pm80xx.ko
./drivers/scsi/qedf/qedf.ko
./drivers/scsi/qedi/qedi.ko
./drivers/scsi/qla2xxx/qla2xxx.ko
./drivers/scsi/qla2xxx/tcm_qla2xxx.ko
./drivers/scsi/smartpqi/smartpqi.ko
./drivers/scsi/ufs/ufshcd-core.ko
./drivers/scsi/ufs/ufshcd-pci.ko
./drivers/scsi/3w-9xxx.ko
./drivers/scsi/3w-sas.ko
./drivers/scsi/ch.ko
./drivers/scsi/hpsa.ko
./drivers/scsi/hv_storvsc.ko
./drivers/scsi/mvumi.ko
./drivers/scsi/pmcraid.ko
./drivers/scsi/libiscsi_tcp.ko
./drivers/scsi/scsi_debug.ko
./drivers/scsi/raid_class.ko
./drivers/scsi/scsi_transport_fc.ko
./drivers/scsi/scsi_transport_spi.ko
./drivers/scsi/scsi_transport_sas.ko
./drivers/scsi/hptiop.ko
./drivers/scsi/initio.ko
./drivers/scsi/iscsi_tcp.ko
./drivers/scsi/scsi_transport_srp.ko
./drivers/scsi/sd_mod.ko
./drivers/scsi/ses.ko
./drivers/scsi/sg.ko
./drivers/scsi/sr_mod.ko
./drivers/scsi/st.ko
./drivers/scsi/stex.ko
./drivers/scsi/virtio_scsi.ko
./drivers/scsi/vmw_pvscsi.ko
./drivers/target/iscsi/cxgbit/cxgbit.ko
./drivers/target/iscsi/iscsi_target_mod.ko
./drivers/target/target_core_pscsi.ko

磁盘驱动sd: ./drivers/scsi/sd_mod.ko
通用驱动sg: ./drivers/scsi/sg.ko

drivers/scsi/sd.h
include/scsi/scsi_host.h
struct scsi_host_template, Let it rip 分叉, 
drivers/scsi/scsi.c -> subsys_initcall(init_scsi) -> init -> static int __init init_scsi -> 驱动加载
	scsi_init_procfs -> int __init scsi_init_procfs -> proc_mkdir("scsi", NULL) -> struct proc_dir_entry *proc_mkdir -> /proc/scsi -> device_info  scsi  sg -> 创建proc目录或文件: https://www.cnblogs.com/lialong1st/p/8317143.html
		proc_create("scsi/scsi", 0, NULL, &scsi_scsi_proc_ops) -> static const struct proc_ops scsi_scsi_proc_ops -> struct proc_ops {
	scsi_init_devinfo 动态设备信息列表 -> int __init scsi_init_devinfo -> SCSI_DEVINFO_GLOBAL = 0 -> 增强多个表的设备信息匹配 当前的 scsi_devinfo.c 匹配例程对全局黑名单使用单个表。 但是，我们也需要将特定传输列入黑名单（特别是一些使用 SPI 的磁带驱动器，它们对高速协议的响应不佳）。 不是为每个需要它的传输类开发单独的黑名单匹配，而是增强当前的列表匹配以允许多个列表
		scsi_dev_info_add_list -> int scsi_dev_info_add_list -> scsi_devinfo_lookup_by_key(key) -> static LIST_HEAD(scsi_dev_info_list);
			devinfo_table = kmalloc(sizeof(*devinfo_table), GFP_KERNEL) -> GFP_KERNEL - 允许后台和直接回收，并使用默认的页面分配器行为。这意味着廉价的分配请求基本上是不会失败的，但不能保证这种行为, get_free_page -> 
			list_add_tail(&devinfo_table->node, &scsi_dev_info_list) -> 初始化,并加链表
		scsi_dev_info_list_add_str
			scsi_dev_info_list_add -> 
		scsi_dev_info_list_add -> scsi_static_device_list[] __initdata -> 废弃表
		proc_create("scsi/device_info", 0, NULL, &scsi_devinfo_proc_ops)
	scsi_init_hosts 注册shost_class
		int class_register(const struct class *cls) -> static struct class shost_class -> /sys/class/iscsi_host -> ls -alh /sys/class/scsi_* -> scsi_device/  scsi_disk/    scsi_generic/ scsi_host/
			klist_init
			kset_init
			lockdep_register_key(key) -> class_create() 用作围绕核心函数调用的宏包装器的一部分的静态分配的堆栈变量
			kobject_set_name
			cp->class = cls
			kset_register
			sysfs_create_groups
				internal_create_group
	scsi_sysfs_register -> static struct class sdev_class
		bus_register(&scsi_bus_type)
		class_register(&sdev_class)
	scsi_netlink_init -> scsi_netlink_init(void)
		struct netlink_kernel_cfg cfg
		.input	= scsi_nl_rcv_msg
		netlink_kernel_create(&init_net, NETLINK_SCSITRANSPORT




static struct pci_driver isci_pci_driver = {
	.name		= DRV_NAME,
	.id_table	= isci_id_table,
	.probe		= isci_pci_probe,
	.remove		= isci_pci_remove,
	.driver.pm      = &isci_pm_ops,
};
isci_pci_probe
isci_host_alloc
scsi_host_alloc -> bpftrace -> struct Scsi_Host *scsi_host_alloc
	shost->ehandler = kthread_run(scsi_error_handler -> scsi_error_handler -> int scsi_error_handler -> 错误恢复, 内核线程,所有主机(host) -> drivers/scsi/scsi_error.c
		scsi_restart_operations(shost) -> static void scsi_restart_operations
		shost->transportt->eh_strategy_handler(shost)
			

scsi_remove_host -> 模块卸载

Linux打印内核函数调用栈 dump_stack, krpobe, systemtap, ftrace, qemu等等, bpftrace, perf调性能


void blk_finish_plug
bpftrace -e 'tracepoint:block:block_rq_insert { printf("Block I/O by %s\n", kstack); }'
Block I/O by 
        __elv_add_request+259
        blk_flush_plug_list+320
        blk_finish_plug+20
        _xfs_buf_ioapply+820
        __xfs_buf_submit+114
        xlog_bdstrat+55
        xlog_sync+742
        xlog_state_release_iclog+123
        xfs_log_force_lsn+497
        xfs_file_fsync+253
        do_fsync+85
        sys_fsync+16
        system_call_fastpath+37

lsmod|grep sd_mod
modinfo sd_mod
make clean; make; rmmod sd_mod.ko; insmod sd_mod.ko

bpftrace -l | more

#!/usr/local/bin/bpftrace
tracepoint:syscalls:sys_enter_nanosleep
{
  printf("%s is sleeping.\n", comm);
}

bpftrace -e 'kprobe:do_sys_open { printf("opening: %s\n", str(arg1)); }'
bpftrace -e 'kprobe:ip_output { @[kstack] = count(); }'

bpftrace -e 'kprobe:scsi_error_handler { printf("bt:%s\n", kstack); }'

动态跟踪事件: bpftrace -e 'kprobe:scsi_host_set_state { printf("bt:%s\n", kstack); }'
bt:
        scsi_host_set_state+1
        scsi_remove_host+155
        iscsi_host_remove+120
        iscsi_sw_tcp_session_destroy+85 -> .destroy_session	= iscsi_sw_tcp_session_destroy
        iscsi_if_recv_msg+3382 -> ISCSI_UEVENT_DESTROY_SESSION
        iscsi_if_rx+202  调用 netlink_unicast 是将消息发送到内核 netlink 套接字的唯一路径。 但是，不幸的是，它也用于向用户发送数据
        netlink_unicast+421
        netlink_sendmsg+539
        sock_sendmsg+91
        ____sys_sendmsg+451
        ___sys_sendmsg+124
        __sys_sendmsg+87
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68
				
断开会话:
iscsiadm -m node --logoutall=all
ISCSI_UEVENT_DESTROY_SESSION
transport->destroy_session(session) -> iscsi_sw_tcp_session_destroy
	if (scsi_host_set_state(shost, SHOST_CANCEL))

rfc:
7. iSCSI Error Handling and Recovery ..............................92
bpftrace -e 't:syscalls:sys_enter_execve { printf("%s called %s\n", comm, str(args->filename)); }'

查看所有事件: bpftrace -l|grep scsi

static int isci_pci_probe
isci_pci_init
isci_host_alloc
scsi_scan_host -> void scsi_scan_host

sd_spinup_disk
	scsi_execute_cmd TEST_UNIT_READY START_UNIT

scsi_execute_req 
blk_execute_rq -> blk_status_t blk_execute_rq -> 底层调用高层(scci -> blk)


错误恢复:
● int (* eh_abort_handler) (struct scsi_cmnd *);
● int (* eh_device_reset_handler) (struct scsi_cmnd *);
● int (* eh_target_reset_handler) (struct scsi_cmnd *);
● int (* eh_bus_reset_handler) (struct scsi_cmnd *);
● int (* eh_host_reset_handler) (struct scsi_cmnd *);

scsi_eh_scmd_add
scsi_eh_abort_cmds
scsi_eh_flush_done_q

__blk_run_queue
scsi_send_eh_cmnd

查看会话: iscsiadm --mode session
连接: iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:622a4b9193bb4ee68cdb6cae6b76cc74 -p 172.16.156.3:3260 -l
bt:
        scsi_host_set_state+1
        scsi_add_host_with_dma.cold.9+294
        iscsi_sw_tcp_session_create+130
        iscsi_if_create_session+48 -> iscsi_if_create_session(struct iscsi_internal *priv  -> ISCSI_UEVENT_CREATE_SESSION
        iscsi_if_recv_msg+1109 -> iscsi_if_recv_msg(struct sk_buff *skb
        iscsi_if_rx+202 -> .input	= iscsi_if_rx
        netlink_unicast+421 -> int netlink_unicast
        netlink_sendmsg+539
        sock_sendmsg+91
        ____sys_sendmsg+451
        ___sys_sendmsg+124
        __sys_sendmsg+87
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68


netlink_kernel_create(&init_net, NETLINK_ISCSI, &cfg) -> netlink_kernel_create(struct net *net, int unit, struct netlink_kernel_cfg *cfg) ->input：为内核模块定义的netlink消息处理函数，当有消 息到达这个netlink socket时，该input函数指针就会被引用，且只有此函数返回时，调用者的sendmsg才能返回
	nlk_sk(sk)->netlink_rcv = cfg->input -> nlk->netlink_rcv(skb) <- static int netlink_unicast_kernel


netlink_unicast
	netlink_unicast_kernel


Attaching 1 probe...
bt:
        fail_path+1
        multipath_message+342 -> .message = multipath_message
        target_message+627
        ctl_ioctl+431
        dm_ctl_ioctl+10
        __x64_sys_ioctl+132
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68

int dm_register_target(struct target_type *tt)

static ioctl_fn lookup_ioctl
{DM_TARGET_MSG_CMD, 0, target_message} -> static int target_message
	ti->type->message(ti, argc, argv, result, maxlen) -> static int multipath_message
		action = fail_path;
		action_dev(m, dev, action) -> static int action_dev
			action(pgpath) -> static int fail_path


Documentation/kbuild/modules.rst

cp /lib/modules/`uname -r`/build/Makefile ./Makefile_current

CONFIG_PREEMPT_BUILD=y
此选项通过使所有内核代码（不在关键部分中执行）可抢占来减少内核的延迟。 这允许通过允许低优先级进程被非自愿地抢占来对交互事件做出反应，即使它在内核模式下执行系统调用，否则不会到达自然抢占点。 这允许应用程序即使在系统处于负载下时也能更“流畅”地运行，但代价是吞吐量略低，内核代码的运行时开销也很小。
如果您正在为延迟要求在毫秒范围内的桌面或嵌入式系统构建内核，请选择此选项


动态跟踪事件: bpftrace -e 'kprobe:scsi_host_set_state { printf("bt:%s\n", kstack); }'
bt:
        scsi_host_set_state+1
        scsi_remove_host+155
        iscsi_host_remove+120
        iscsi_sw_tcp_session_destroy+85 -> .destroy_session	= iscsi_sw_tcp_session_destroy
        iscsi_if_recv_msg+3382 -> ISCSI_UEVENT_DESTROY_SESSION
        iscsi_if_rx+202
        netlink_unicast+421
        netlink_sendmsg+539
        sock_sendmsg+91
        ____sys_sendmsg+451
        ___sys_sendmsg+124
        __sys_sendmsg+87
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68
				
断开会话:
iscsiadm -m node --logoutall=all
ISCSI_UEVENT_DESTROY_SESSION
transport->destroy_session(session) -> iscsi_sw_tcp_session_destroy

rfc:
7. iSCSI Error Handling and Recovery ..............................92
bpftrace -e 't:syscalls:sys_enter_execve { printf("%s called %s\n", comm, str(args->filename)); }'

查看所有事件: bpftrace -l|grep scsi





故障处理
ip set link xxx down (close 1 path) -> iscsi_check_transport_timeouts <- iscsi_conn_setup
[12291.650194]  connection3:0: ping timeout of 5 secs expired, recv timeout 5, last rx 4306948309, last ping 4306953344, now 4306958848
[12291.650313]  connection3:0: detected conn error (1022) <- void iscsi_conn_error_event
[12296.770199]  session3: session recovery timed out after 5 secs
[12297.554572] device-mapper: multipath: 253:3: Failing path 8:16. <- static int fail_path
static int fail_path



static void iscsi_check_transport_timeouts
	iscsi_has_ping_timed_out
	iscsi_conn_failure(conn, ISCSI_ERR_NOP_TIMEDOUT)
		iscsi_conn_error_event(conn->cls_conn, err)
		iscsi_if_transport_lookup(conn->transport)
		alloc_skb
		__nlmsg_put
		ev->type = ISCSI_KEVENT_CONN_ERROR -> 给用户态发事件
		iscsi_multicast_skb
	iscsi_send_nopout



iscsi_if_recv_msg -> struct iscsi_uevent *ev = nlmsg_data(nlh) -> (unsigned char *) nlh + NLMSG_HDRLEN
	case ISCSI_UEVENT_SEND_PDU

iscsi_if_recv_msg
iscsi_if_transport_conn
	switch (nlh->nlmsg_type)
	case ISCSI_UEVENT_CREATE_CONN
		iscsi_if_create_conn, 创建连接的时候设置的故障处理函数
			iscsi_session_lookup
			conn = transport->create_conn(session, ev->u.c_conn.cid)
static struct iscsi_transport iscsi_sw_tcp_transport
.create_conn		= iscsi_sw_tcp_conn_create, <- conn = transport->create_conn(session, ev->u.c_conn.cid)
iscsi_sw_tcp_conn_create
	iscsi_tcp_conn_setup
		iscsi_conn_setup(struct iscsi_cls_session *cls_session
			iscsi_alloc_conn
			timer_setup(&conn->transport_timer, iscsi_check_transport_timeouts, 0) -> 作为 timer_setup 的 callback, 超时的时候执行回调
				void iscsi_conn_error_event
				...
			INIT_LIST_HEAD(&conn->mgmtqueue)
			...
			INIT_WORK(&conn->xmitwork, iscsi_xmitworker) -> 处理iscsi发送任务
			__get_free_pages
			err = iscsi_add_conn(cls_conn)
				iscsi_dev_to_session
				device_add
				transport_register_device
				list_add(&conn->conn_list, &connlist)
	INIT_WORK(&conn->recvwork, iscsi_sw_tcp_recv_data_work)
	tcp_sw_conn->queue_recv = iscsi_recv_from_iscsi_q
	crypto_alloc_ahash
	tcp_sw_conn->tx_hash = ahash_request_alloc
	ahash_request_set_callback
	tcp_sw_conn->rx_hash = ahash_request_alloc
	return cls_conn


超时处理:
static int iscsi_nop_out_rsp -> 处理来自使用或用户空间的 nop 响应。 在 back_lock 下调用
	mod_timer(&conn->transport_timer, jiffies + conn->recv_timeout) -> 动态更改定时器(连接上的传输定时器)的到期时间，从而可更改定时器的执行顺序 -> iscsi_check_transport_timeouts
		void iscsi_conn_error_event

超时堆栈, 传输超时检测
bpftrace -e 'kprobe:iscsi_check_transport_timeouts { printf("bt:%s\n", kstack); }'
iscsi_check_transport_timeouts+1
call_timer_fn+41
run_timer_softirq+474
__softirqentry_text_start+268
asm_call_sysvec_on_stack+18
do_softirq_own_stack+55
irq_exit_rcu+243
sysvec_apic_timer_interrupt+52
asm_sysvec_apic_timer_interrupt+18
native_safe_halt+14
acpi_idle_do_entry+75
acpi_idle_enter+90
cpuidle_enter_state+145
cpuidle_enter+41
do_idle+636
cpu_startup_entry+25
start_secondary+280
secondary_startup_64_no_verify+176


如果收到消息, 公共接口
iscsi_if_recv_msg(struct sk_buff *skb, struct nlmsghdr *nlh, uint32_t *group)
	case ISCSI_UEVENT_SEND_PDU:
	iscsi_if_transport_conn(transport, nlh) -> static int iscsi_if_transport_conn
		case ISCSI_UEVENT_START_CONN
		ev->r.retcode = transport->start_conn(conn)
			启动连接 -> iscsi_conn_start
			bpftrace -e 'kprobe:iscsi_conn_start { printf("bt:%s\n", kstack); }'
			iscsi_conn_start+1
			iscsi_if_recv_msg+3789
			iscsi_if_rx+202
			netlink_unicast+421
			netlink_sendmsg+539
			sock_sendmsg+91
			____sys_sendmsg+451
			___sys_sendmsg+124
			__sys_sendmsg+87
			do_syscall_64+51
			entry_SYSCALL_64_after_hwframe+68






ev->r.retcode =	transport->send_pdu( -> iscsi_conn_send_pdu -> (struct iscsi_hdr *)((char *)ev + sizeof(*ev))


拷贝源码
cd /lib/modules/5.10.38-21.hl10.el7.x86_64/build/include
rsync -ravpl root@node2:/lib/modules/5.10.38-21.hl10.el7.x86_64/build/include/* .




设置探测点: bpftrace -e 'kprobe:scsi_host_alloc { printf("bt:%s\n", kstack); }'
断开会话: iscsiadm -m node --logoutall=all
发现: iscsiadm -m discovery -t sendtargets -p 172.17.136.132
连接: iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:f2a23531433249f7bf2a6d01760fe755 -p 172.17.136.132:3260 -l
触发堆栈:
scsi_host_alloc+1
iscsi_host_alloc+17
iscsi_sw_tcp_session_create+50
iscsi_if_create_session+48
iscsi_if_recv_msg+1109
iscsi_if_rx+202
netlink_unicast+421
netlink_sendmsg+539
sock_sendmsg+91
____sys_sendmsg+451
___sys_sendmsg+124
__sys_sendmsg+87
do_syscall_64+51
entry_SYSCALL_64_after_hwframe+68

blk_request_module
	(*n)->probe(devt)

spi, scsi并行接口, 

执行SCSI命令，如 INQUIRY 和 SPINUP ，它们的执行都调用了 scsi_execute_req -> scsi_execute_cmd ?  函数

static int scsi_probe_lun
	scsi_cmd[0] = INQUIRY


static int scsi_vpd_inquiry
	cmd[0] = INQUIRY

Vital Product Data (VPD) , 重要产品数据, 

int scsi_execute_cmd
	scsi_alloc_request
	blk_rq_map_kern
	blk_mq_rq_to_pdu
	blk_execute_rq



块：将超时延迟到工作队列, 定时器上下文对于驱动程序执行任何有意义的中止操作不是很有用。 因此，不要从这个无用的上下文中调用驱动程序，而是尽快将其延迟到工作队列, 请注意，虽然 delayed_work 项目在这里似乎是正确的，但由于 blk_add_timer 中的魔法深入计时器内部，我不敢使用它。 但也许这会鼓励 Tejun 为工作队列 API 添加一个合理的 API，最终我们都会好起来的, 包含来自 Keith Bush 的重大更新：“此补丁删除同步超时工作，以便计时器可以在其自己的队列上开始冻结。计时器进入队列，因此计时器上下文只能开始冻结，但不能等待冻结
static void blk_mq_timeout_work
blk_mq_queue_tag_busy_iter(q, blk_mq_handle_expired, &expired)
static bool blk_mq_handle_expired
static void blk_mq_rq_timed_out
ret = req->q->mq_ops->timeout(req)
static const struct blk_mq_ops scsi_mq_ops
.timeout	= scsi_timeout
enum blk_eh_timer_return scsi_timeout
static const struct scsi_host_template iscsi_sw_tcp_sht
eh_timed_out
.eh_timed_out		= iscsi_eh_cmd_timed_out,




eh_strategy_handler

scsi_eh_scmd_add
scsi_unjam_host 取消干扰



[root@node1 ~]# bpftrace -e 'kprobe:scsi_error_handler { printf("bt:%s\n", kstack); }'
iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:f2a23531433249f7bf2a6d01760fe755 -p 172.17.136.132:3260 -l -d 8
scsi_error_handler+1
kthread+278
ret_from_fork+34



blk_mq_request_issue_directly
	static blk_status_t __blk_mq_issue_directly


IO路径, 块io, 
bool blk_mq_dispatch_rq_list
		ret = q->mq_ops->queue_rq(hctx, &bd)
		.queue_rq	= scsi_queue_rq
		static blk_status_t scsi_queue_rq(
			scsi_dispatch_cmd
				trace_scsi_dispatch_cmd_start(cmd)
				rtn = host->hostt->queuecommand(host, cmd) -> .queuecommand           = iscsi_queuecommand,
					iscsi_session_chkready -> 检查会话通过iscsi_session_chkready进行。当会话状态不是ISCSI_SESSION_LOGGED_IN时，不适合处理scsi指令。链接检查通过链接是否存在、链接状态、链接可接收的命令窗口是否达到最大值。这几个方面判断
					task = iscsi_alloc_task(conn, sc)
					iscsi_prep_scsi_cmd_pdu(task)
					list_add_tail(&task->running, &conn->cmdqueue) -> 将任务插入命令队列 cmdqueue -> 由 iscsi_xmitworker 线程发送命令
					iscsi_conn_queue_xmit(conn)
				...
				sd_init_command -> static blk_status_t sd_init_command(struct scsi_cmnd *cmd)
					case REQ_OP_WRITE
					sd_setup_read_write_cmnd(cmd)
						sd_setup_rw10_cmnd(cmd, write, lba, nr_blocks


static void iscsi_xmitworker(struct work_struct *work)
	do iscsi_data_xmit(conn)
		iscsi_xmit_task
			rc = conn->session->tt->xmit_task(task) -> .xmit_task		= iscsi_tcp_task_xmit 发送常规PDU任务
				rc = session->tt->xmit_pdu(task) -> static int iscsi_sw_tcp_pdu_xmit
					iscsi_sw_tcp_xmit
						while (1) iscsi_sw_tcp_xmit_segment(tcp_conn, segment) 传输分段
							tcp_sw_conn->sendpage(sk, sg_page(sg), offset
						segment->done(tcp_conn, segment) 首选按页发送
						kernel_sendmsg(sk, &msg, &iov, 1, copy) 其次降级为内核发送消息
							iov_iter_kvec
							sock_sendmsg(sock, msg)
					memalloc_noreclaim_restore
			iscsi_put_task(task)




static int iscsi_if_transport_conn
	.bind_conn		= iscsi_sw_tcp_conn_bind
	iscsi_sw_tcp_conn_bind
		static void iscsi_sw_tcp_conn_set_callbacks
			sk->sk_data_ready = iscsi_sw_tcp_data_ready
				iscsi_sw_tcp_recv_data(conn)
					tcp_read_sock(sk, &rd_desc, iscsi_sw_tcp_recv) -> 该例程为希望以“sendfile”方式直接处理从 skbuffs 复制的例程提供了 tcp_recvmsg() 的替代方案
						skb = tcp_recv_skb(sk, seq, &offset)
							skb_peek
							tcp_eat_recv_skb
								skb_attempt_defer_free
									defer_max = READ_ONCE(sysctl_skb_defer_max)
									...
					iscsi_tcp_segment_unmap(&tcp_conn->in.segment)



bpftrace -e 'kprobe:sd_init_command{ printf("bt:%s\n", kstack); }'
sd_init_command+1
scsi_queue_rq+1478
blk_mq_dispatch_rq_list+291
__blk_mq_sched_dispatch_requests+201
blk_mq_sched_dispatch_requests+48
__blk_mq_run_hw_queue+81
__blk_mq_delay_run_hw_queue+321
blk_mq_run_hw_queues+52
blk_mq_requeue_work+350
process_one_work+431
worker_thread+48
kthread+278
ret_from_fork+34




bpftrace -e 'kprobe:scsi_queue_rq { printf("bt:%s\n", kstack); }'
scsi_queue_rq+1
blk_mq_dispatch_rq_list+291
__blk_mq_sched_dispatch_requests+201
blk_mq_sched_dispatch_requests+48
__blk_mq_run_hw_queue+81
__blk_mq_delay_run_hw_queue+321
blk_mq_run_hw_queues+52
blk_mq_requeue_work+350
process_one_work+431
worker_thread+48
kthread+278
ret_from_fork+34




配置探测, 崩溃点
#ifdef CONFIG_KPROBES
	CRASHPOINT("INT_HARDWARE_ENTRY", "do_IRQ"),



static void scsi_starved_list_run -> 只要 shost 正在接受命令并且队列不足，就调用 blk_run_queue。 scsi_request_fn 删除 queue_lock 并可以将我们添加回 starved_list。 host_lock 保护 starved_list 和 starved_entry。 scsi_request_fn 在检查或修改 starved_list 或 starved_entry 之前必须获得 host_lock


include/scsi/scsi_cmnd.h

