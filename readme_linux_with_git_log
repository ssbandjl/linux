https://docs.kernel.org/driver-api/scsi.html
https://github.com/ssbandjl/linux.git
bio下发流程: https://blog.csdn.net/flyingnosky/article/details/121362813
io路径: https://zhuanlan.zhihu.com/p/545906763
用户态与内核态通信netlink: https://gist.github.com/lflish/15e85da8bb9200794255439d0563b195
实现rfc3720: https://github.com/ssbandjl/linux/commit/39e84790d3b65a4af1ea1fb0d8f06c3ad75304b3
管理内核模块,红帽: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/managing-kernel-modules_managing-monitoring-and-updating-the-kernel
存储技术原理: file:///D:/xb/project/c/%E5%AD%98%E5%82%A8%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90_%E5%9F%BA%E4%BA%8ELinux%202.6%E5%86%85%E6%A0%B8%E6%BA%90%E4%BB%A3%E7%A0%81.pdf
内核定时器: 
bpftrace: 
spec: 官方指南: https://rpm-packaging-guide.github.io/, 
io路径分析: https://codeantenna.com/a/ADadbOp8tQ
编译内核参考: https://wiki.centos.org/HowTos/Custom_Kernel
spec文件: 构建rpm包和spec基础: https://www.cnblogs.com/michael-xiang/p/10480809.html, 官方指南: https://rpm-packaging-guide.github.io/, 
kdir: https://stackoverflow.com/questions/59366772/what-does-the-kernel-compile-rule-exact-mean
kernel makefile: https://www.kernel.org/doc/html/latest/kbuild/makefiles.html
Linux模块文件如何编译到内核和独立编译成模块: https://z.itpub.net/article/detail/090A31801416081BC9D0781C05AC91AA
安装源码: https://wiki.centos.org/HowTos/I_need_the_Kernel_Source
编译内核模块: https://wiki.centos.org/HowTos/BuildingKernelModules

dm-verity简介 ——（1）: https://www.cnblogs.com/hellokitty2/p/12364836.html
管理工具源码_lvm_dmsetup: https://sourceware.org/dm/


监听ring_buffer日志: dmesg -w

git tag -d v5.10

nvme
static struct blk_mq_ops nvme_mq_ops -> struct blk_mq_ops 
static const struct blk_mq_ops bsg_mq_ops = {
	.queue_rq		= bsg_queue_rq,
	.init_request		= bsg_init_rq,
	.exit_request		= bsg_exit_rq,
	.complete		= bsg_complete,
	.timeout		= bsg_timeout,
};

驱动目录: drivers/nvme/Makefile
参考: https://mp.weixin.qq.com/s/GKVIY_NOXDfUCOho4zBMzw
Kconfig 文件的作用是：
1.控制make menuconfig时， 出现的配置选项；
2.根据用户配置界面的选择，将配置结果保存在.config 配置文件（该文件将提供给Makefile使用，用以决定要编译的内核组件以及如何编译）

drivers/nvme/host/core.c
module_init(nvme_core_init) -> static int __init nvme_core_init(void)
alloc_chrdev_region nvme
class_create("nvme")

drivers/nvme/target/rdma.c
static int __init nvmet_rdma_init(void)
ib_register_client -> drivers/infiniband/core/device.c -> int ib_register_client -> IB 驱动程序的上层用户可以使用 ib_register_client() 来注册 IB 设备添加和删除的回调。 添加 IB 设备时，将调用每个已注册客户端的 add 方法（按照客户端注册的顺序），而当删除设备时，将调用每个客户端的 remove 方法（按照客户端注册的相反顺序）。 另外，调用ib_register_client()时，客户端会收到所有已经注册的设备的add回调
nvmet_register_transport -> int nvmet_register_transport
	down_write
	up_write

EXPORT_SYMBOL(ib_register_client) 导出符号表

multi_path:
drivers/md/md.c
drivers/md/md-multipath.c


dm_early_create - 在早期启动时创建映射设备。
@dmi：包含要创建的设备映射的主要信息。
@spec_array：指向 struct dm_target_spec 的指针数组。 描述设备的映射表。
@target_params_array：包含特定目标参数的字符串数组。不要将 struct dm_target_spec 和每个目标的参数嵌入在 struct dm_ioctl 的末尾（如在普通 ioctl 中执行的那样），而是将它们作为参数传递，因此调用者不需要序列化它们。 Spec_array 和 target_params_array 的大小由下式给出
@dmi->目标计数。
该函数应该在早期启动时调用，因此不需要用于防止并发加载的锁定机制


long vfs_ioctl
error = filp->f_op->unlocked_ioctl(filp, cmd, arg)
static const struct file_operations _ctl_fops
.unlocked_ioctl	 = dm_ctl_ioctl
static long dm_ctl_ioctl
static int ctl_ioctl
static ioctl_fn lookup_ioctl(unsigned int cmd, int *ioctl_flags)
	{DM_DEV_CREATE_CMD, IOCTL_FLAGS_NO_PARAMS | IOCTL_FLAGS_ISSUE_GLOBAL_EVENT, dev_create},
static int dev_create
int dm_create
dm.c -> static struct mapped_device *alloc_dev(int minor) -> 分配并初始化具有给定次要的空白设备
	...
	md->disk->fops = &dm_blk_dops
	...


.map = multipath_map_bio -> static int multipath_map_bio

static void process_queued_bios


static const struct blk_mq_ops dm_mq_ops = {
	.queue_rq = dm_mq_queue_rq,
	.complete = dm_softirq_done,
	.init_request = dm_mq_init_request,
};
static blk_status_t dm_mq_queue_rq -> blk-mq 新的多队列块IO排队机制, struct request -> 尝试将引用的字段放在同一个缓存行中
	dm_start_request(md, rq)
		blk_mq_start_request -> 设备驱动程序使用的函数来通知块层现在将处理请求，因此 blk 层可以进行适当的初始化，例如启动超时计时器
			trace_block_rq_issue(rq) -> 下发io到驱动, Linux下block层的监控工具blktrace, https://blog.csdn.net/hs794502825/article/details/8541235, linux跟踪系统, https://elinux.org/Kernel_Trace_Systems
			test_bit(QUEUE_FLAG_STATS, &q->queue_flags) -> int test_bit(nr, void *addr) 原子的返回addr位所指对象nr位
			blk_add_timer(rq) -> 启动单个请求超时计时器
				mod_timer(&q->timeout, expiry)
			WRITE_ONCE(rq->bio->bi_cookie, blk_rq_to_qc(rq)) -> Linux内核中的READ_ONCE和WRITE_ONCE宏, https://zhuanlan.zhihu.com/p/344256943, 缓存一致性: https://blog.csdn.net/zxp_cpinfo/article/details/53523697, 所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取
		dm_get(md)
	init_tio(tio, rq, md) -> target_io
	map_request(tio)
		ti->type->clone_and_map_rq -> static int multipath_clone_and_map
			pgpath = choose_pgpath(m, nr_bytes)
				choose_path_in_pg -> dm mpath：消除 IO 快速路径中自旋锁的使用,此提交的主要动机是提高大型 NUMA 系统上 DM 多路径的可扩展性，其中 m->lock 自旋锁争用已被证明是真正快速存储的严重瓶颈在此提交中利用了使用 lockless_dereference() 以原子方式读取指针的能力。 但是所有指针写入仍然受到 m->lock 自旋锁的保护（这很好，因为这些现在都发生在慢速路径中）以下函数在其快速路径中不再需要 m->lock 自旋锁：multipath_busy()、__multipath_map() 和 do_end_io()并且 Choose_pgpath() 被修改为_不_更新 m->current_pgpath 除非它也切换路径组。 这样做是为了避免每次 __multipath_map() 调用 Choose_pgpath() 时都需要获取 m->lock。 但是如果通过fail_path()失败，m->current_pgpath将被重置
					path = pg->ps.type->select_path(&pg->ps, nr_bytes) -> static struct dm_path *st_select_path -> st_compare_load
			clone = blk_mq_alloc_request
				blk_mq_alloc_cached_request
			pgpath->pg->ps.type->start_io -> static int st_start_io -> 调用路径线算法的 start_io 函数, 如果成功则返回 DM_MAPIO_REMAPPED, 表明映射成功，通知DM框架重新投递请求, dm mpath：添加服务时间负载均衡器，此补丁添加了一个面向服务时间的动态负载均衡器 dm-service-time，它为传入 I/O 选择估计服务时间最短的路径。 通过将进行中的 I/O 大小除以每条路径的性能值来估计服务时间。性能值可以在表加载时作为表参数给出。 如果未给出性能值，则所有路径均被视为相同, 参考流程: https://my.oschina.net/LastRitter/blog/1541330
				atomic_add(nr_bytes, &pi->in_flight_size) -> 在 IO 开始与结束时，分别增减该路径正在处理的 IO 字节数 -> sz1 = atomic_read(&pi1->in_flight_size) <- static int st_compare_load
		case DM_MAPIO_REMAPPED -> 映射成功
		setup_clone -> dm：始终将请求分配推迟给 request_queue 的所有者, 如果底层设备是 blk-mq 设备，则 DM 已在底层设备的 request_queue 上调用 blk_mq_alloc_request,  但现在我们允许驱动程序分配额外的数据并提前初始化它，我们需要对所有驱动程序执行相同的操作。 这样做并在块层中使用新的 cmd_size 基础设施极大地简化了 dm-rq 和 mpath 代码，并且还应该使 SQ 和 MQ 设备与 SQ 或 MQ 设备映射器表的任意组合成为可能，作为进一步的步骤
			blk_rq_prep_clone dm_rq_bio_constructor
			clone->end_io = end_clone_request
		trace_block_rq_remap 
		blk_insert_cloned_request(clone) -> #ifdef CONFIG_BLK_MQ_STACKING -> blk-mq：使 blk-mq 堆栈代码可选，堆栈 blk-mq 驱动程序的代码仅由 dm-multipath 使用，并且最好保持这种方式。 使其可选并且仅由设备映射器选择，以便构建机器人更容易捕获滥用行为，例如在最后一个合并窗口中的 ufs 驱动程序中滑入的滥用行为。 另一个积极的副作用是，内核构建时没有设备映射器也会缩小一点
			if (blk_rq_sectors(rq) > max_sectors) -> 如果实际支持 Write Same/Zero，SCSI 设备没有一个好的返回方法。 如果设备拒绝非读/写命令（丢弃、写入相同内容等），则低级设备驱动程序会将相关队列限制设置为 0，以防止 blk-lib 发出更多违规操作。 在重置队列限制之前排队的命令需要使用 BLK_STS_NOTSUPP 完成，以避免 I/O 错误传播到上层
			blk_account_io_start(rq)
				blk_do_io_stat
				update_io_ticks
			blk_mq_run_dispatch_ops -> blk_mq_request_issue_directly -> rcu -> 读文件过程, 禁用调度器: https://blog.csdn.net/jasonactions/article/details/109614350
				if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(rq->q)) -> 队列禁止
				__blk_mq_issue_directly
					ret = q->mq_ops->queue_rq(hctx, &bd) -> 内核block层Multi queue多队列的一次优化实践: https://blog.csdn.net/hu1610552336/article/details/121072592







choose_path_in_pg
	path = pg->ps.type->select_path(&pg->ps, nr_bytes)
	static struct dm_path *st_select_path
		static int st_compare_load



dm-mpath.c -> static int fail_path #路径故障
	pgpath->pg->ps.type->fail_path(&pgpath->pg->ps, &pgpath->path)
	dm_path_uevent(DM_UEVENT_PATH_FAILED -> 此补丁添加了对失败路径和恢复路径的 dm_path_event 的调用 -> void dm_path_uevent
		dm_build_path_uevent
		dm_uevent_add -> void dm_uevent_add -> list_add(elist, &md->uevent_list) -> 挂链表
		dm_send_uevents -> void dm_send_uevents
			dm_copy_name_and_uuid
			kobject_uevent_env 发送uevent, kobject_uevent这个函数原型如下，就是向用户空间发送uevent，可以理解为驱动（内核态）向用户（用户态）发送了一个KOBJ_ADD
				kobject_uevent_net_broadcast -> 参考热拔插原理
	queue_work
	schedule_work

static int __multipath_map_bio
	__map_bio(m, bio)
	bio_set_dev(bio, pgpath->path.dev->bdev)
	pgpath->pg->ps.type->start_io(&pgpath->pg->ps -> static int st_start_io


bpftrace -e 'kprobe:st_start_io{ printf("bt:%s\n", kstack); }'
st_start_io+1
multipath_clone_and_map+373
dm_mq_queue_rq+264
blk_mq_dispatch_rq_list+291
__blk_mq_do_dispatch_sched+356
__blk_mq_sched_dispatch_requests+309
blk_mq_sched_dispatch_requests+48
__blk_mq_run_hw_queue+81
__blk_mq_delay_run_hw_queue+321
blk_mq_sched_insert_requests+106
blk_mq_flush_plug_list+281
blk_flush_plug_list+215
blk_finish_plug+33
ext4_writepages+769
do_writepages+65
__filemap_fdatawrite_range+199
ext4_release_file+108
__fput+155
task_work_run+123
exit_to_user_mode_prepare+335
syscall_exit_to_user_mode+60
entry_SYSCALL_64_after_hwframe+68



static struct pgpath *__map_bio
static struct pgpath *choose_pgpath
choose_path_in_pg
	pg->ps.type->select_path(&pg->ps, nr_bytes)



#########################################
tee<<EOF>write.c
#include <stdio.h>
#include <stdlib.h>

int main() {
    char sentence[1000] = "hello,world";
    // creating file pointer to work with files
    FILE *fptr;
    // opening file in writing mode
    fptr = fopen("write.txt", "w");
    // exiting program 
    if (fptr == NULL) {
        printf("Error!");
        exit(1);
    }
    fprintf(fptr, "%s", sentence);
    fclose(fptr);
    return 0;
}
EOF
gcc -o write write.c && ./write

#########################################

热拔插: 热插拔：在不重启系统的情况下，增减硬件设备。本文主要介绍linux下的热插拔 https://www.cnblogs.com/tzj-kernel/p/15307231.html
热插拔：实现了驱动向用户态通知设备插拔
（1）外设插入，硬件中断响应
（2）总线发现新的设备，驱动probe 再调用device_add(设备驱动？？)
（3）device_add调用kobject_uevent(, KOBJ_ADD)，向用户空间广播新设备加入事件通知；这里发出通知的方式，就是netlink；
（4）用户空间运行的daemon(udev)收到event事件广播；
（5）udev根据消息和环境变量，查询sysfs中的/sys的变化，按照规则(/etc/udev/rules.d/*)，在/dev目录下自动创建设备节点；
（6）运行 /sbin/hotplug 脚本
kobject_uevent_env--》kobject_uevent_net_broadcast--》uevent_net_broadcast_untagged或者uevent_net_broadcast_tagged--》netlink_broadcast，实际上就是将buf中的内容发送到用户空间，用户空间udev监听到此消息，则解析
/sys/kernel/uevent_helper

内核 uevents 和 udev 
必需的设备信息由 sysfs 文件系统导出。对于内核检测到并已初始化的设备，将创建一个带有该设备名称的目录。它包含带有特定于设备属性的属性文件。
每次添加或删除设备时，内核都会发送 uevent 来向 udev 通知更改。一旦启动，udev 守护程序便会读取并分析 /usr/lib/udev/rules.d/*.rules 和 /etc/udev/rules.d/*.rules 文件中的所有规则，并将它们保留在内存中。如果更改、添加或去除了规则文件，守护程序可以使用 udevadm control --reload 命令重新装载这些规则在内存中的表示。有关 udev 规则及其语法的更多细节，请参见第 22.6 节 “使用 udev 规则影响内核设备事件处理”。每个接收到的事件都根据所提供的规则集进行匹配。这些规则可以增加或更改事件环境键、为要创建的设备节点请求特定名称、添加指向该节点的符号链接或者添加设备节点创建后运行的程序。从内核 netlink 套接字接收驱动程序核心 uevent
事件处理: https://documentation.suse.com/zh-cn/sles/15-SP1/html/SLES-all/cha-udev.html
udev rule: ls -alh /usr/lib/udev/rules.d/
cat /usr/lib/udev/rules.d/62-multipath.rules




uevent设计: https://github.com/ssbandjl/linux/commit/7a8c3d3b92883798e4ead21dd48c16db0ec0ff6f
device-mapper uevent代码为device-mapper增加了创建和发送kobject uevents（uevents）的能力。 以前的设备映射器事件只能通过 ioctl 接口获得。 uevents 接口的优点是事件包含环境属性，为事件提供增加的上下文，避免在收到事件后查询设备映射器设备的状态
由 udevmonitor 捕获的生成的 uevent 示例


insert_work(pwq, work, worklist, work_flags)

dm_init_init
	dm_early_create
__bind
dm_table_event_callback(t, event_callback, md)
event_fn
dm_table_event
trigger_event


io_path, io路径
static const struct block_device_operations dm_blk_dops = {
	.submit_bio = dm_submit_bio,  -> static void dm_submit_bio

映射设备区分两种类型：基于通用块层请求的映射设备和基于块设备驱动层请求的映射设备, dm_request_based 函数判断映射设备是否是基于块设备驱动层请求的映射设备，如果是，则返回1；否则返回0。这种判断是根据映射设备的请求队列中是否设置了QUEUE_FLAG_STACKABLE标志作出的，而这个标志在创建映射设备时根据映射表的类型设定
static void dm_submit_bio
	dm_get_live_table_bio
	DMF_BLOCK_IO_FOR_SUSPEND 设备挂起时,io入队
	queue_io(md, bio)
		queue_work(md->wq, &md->work)
	dm_split_and_process_bio(md, map, bio) -> 分割io提交给目标设备

dm_split_and_process_bio
	__split_and_process_bio 选择正确的策略来处理非flush bio
		dm_table_find_target
		__map_bio -> static void __map_bio -> 映射io
			ti->type->map(ti, clone) -> .map = multipath_map_bio -> static int __multipath_map_bio -> bio_set_dev(bio, pgpath->path.dev->bdev) -> pgpath->pg->ps.type->start_io -> 查看DeviceMapper映射表 dmsetup table -> mpatha: 0 209715200 multipath 0 0 1 1 service-time 0 1 2 8:16 1 1 -> 根据路径的吞吐量以及未完成的字节数选择负荷较轻的路径 -> static struct path_selector_type st_ps
	bio_trim
	trace_block_split
	bio_inc_remaining
	submit_bio_noacct -> void submit_bio_noacct(struct bio *bio) ->  为 I/O 重新提交 bio 到块设备层 bio 描述内存和设备上的位置。 这是 submit_bio() 的一个版本，只能用于通过堆叠块驱动程序重新提交给较低级别驱动程序的 I/O。 块层的所有文件系统和其他上层用户应该使用 submit_bio() 代替, bio 在节流之前已经被检查过，所以在从节流队列中调度它之前不需要再次检查它。 为此目的添加 submit_bio_noacct_nocheck() 的助手
		__submit_bio_noacct_mq
		submit_bio_noacct_nocheck -> __submit_bio_noacct(bio) -> 确实可以通过递归调用 submit_bio_noacct 添加更多的 bios
			__submit_bio(bio)
				blk_mq_submit_bio -> void blk_mq_submit_bio -> 向块设备提交bio, 会执行调度,合并等操作
					blk_mq_bio_to_request
					blk_queue_bounce -> 弹跳
					__bio_split_to_limits
					blk_mq_bio_to_request -> request
					blk_mq_insert_request
						blk_mq_request_bypass_insert
					blk_mq_run_hw_queue
					blk_mq_try_issue_directly
						blk_mq_run_hw_queue
							blk_mq_delay_run_hw_queue
							kblockd_mod_delayed_work_on
						blk_mq_get_budget_and_tag
						__blk_mq_issue_directly
							ret = q->mq_ops->queue_rq(hctx, &bd)
				disk->fops->submit_bio(bio) -> static void dm_submit_bio -> device_mapper io






blk_mq_plug_issue_direct
blk_add_rq_to_plug

blk_mq_requeue_request
	blk_mq_sched_requeue_request
		e->type->ops.requeue_request(rq) -> requeue_request = 

blk_mq_submit_bio
blk_mq_run_hw_queue
__blk_mq_sched_dispatch_requests

static const struct blk_mq_ops nvme_rdma_mq_ops = {
	.queue_rq	= nvme_rdma_queue_rq,
	.complete	= nvme_rdma_complete_rq,
	.init_request	= nvme_rdma_init_request,
	.exit_request	= nvme_rdma_exit_request,
	.init_hctx	= nvme_rdma_init_hctx,
	.timeout	= nvme_rdma_timeout,
	.map_queues	= nvme_rdma_map_queues,
	.poll		= nvme_rdma_poll,
};
nvme_rdma_queue_rq
drivers/nvme/host/rdma.c -> static int nvme_rdma_post_send
	ib_post_send


三条链：current->bio_list存储在当前线程的所有bio; plug->mq_list使能plug/unplug机制时存放在缓存池的bio；若定义IO调度层，IO请求会发送到scheduler list中；若没有定义IO调度层，IO请求会发送到ctx->rq_lists
每个线程若已经在执行blk_mq_submit_bio()，将新下发BIO链入到线程current->bio_list;
依次处理current->list中的每个bio；
若bio中存在数据在高端内存区，在低端内存区分配内存，将高端内存区数据拷贝到新分配的内存区，称为bounce过程，后面单独一节介绍；
检查请求队列中的bio，若过大进行切分，称BIO的切分；
尝试将bio合并到plug->mq_list中，然后尝试合并到IO调度层链表或ctx->rq_lists中；
若没有合并，分配新的request；
若定义plug，且没达到冲刷数目，加入到plug->mq_list；若达到冲刷数目，将冲刷下发（plug/unplug机制）；
若定义IO调度器，往IO调度器中插入新的request（对于机械硬盘，通过IO调度层座合并和排序，有利于提高性能）;
若 没有定义IO调度器，可以直接下发（对于较快的硬盘如nvme盘，进入调度层可能会浪费时间，跳过IO调度层有利于性能提升）
（1）bounce过程
（2）bio的切分和合并
（3）IO请求和tag的分配
（4）plug/unplug机制
（5）IO调度器
（4）其他


app读写io:
int main()
{
       char buff[128] = {0};
       int fd = open("/var/pilgrimtao.txt", O_CREAT|O_RDWR);
​
       write(fd, "pilgrimtao is cool", 18);
       pread(fd, buff, 128, 0);
       printf("%s\n", buff);
​
       close(fd);
       return 0;
}


文件系统:
SYSCALL_DEFINE3(read, ...) -> ksys_read -> vfs_read -> read_iter -> xfs_file_read_iter
SYSCALL_DEFINE3(write, ...) -> ksys_write -> vfs_write -> new_sync_write -> call_write_iter ->write_iter -> xfs_file_write_iter

磁盘(disk)的访问模式有三种 BUFFERED、DIRECT、DAX。前面提到的由于page cache存在可以避免耗时的磁盘通信就是BUFFERED访问模式的集中体现；但是如果我要求用户的write请求要实时存储到磁盘里，不能只在内存中更新，那么此时我便需要DIRECT模式；大家可能听说过flash分为两种nand flash和nor flash，nor flash可以像ram一样直接通过地址线和数据线访问，不需要整块整块的刷，对于这种场景我们采用DAX模式。所以file_operations的read_iter和write_iter回调函数首先就需要根据不同的标志判断采用哪种访问模式
kernel在2020年12月的patch中提出了folio的概念，我们可以把folio简单理解为一段连续内存，一个或多个page的集合，他和page的关系如图
代码参考：xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __filemap_get_folio -> filemap_alloc_folio

读取磁盘inode代码参考：iomap_file_buffered_write -> iomap_iter -> .iomap_begin -> xfs_buffered_write_iomap_begin -> xfs_iread_extents -> xfs_btree_visit_blocks -> xfs_btree_readahead_ptr -> xfs_buf_readahead -> xfs_buf_readahead_map -> xfs_buf_read_map -> xfs_buf_read -> xfs_buf_submit -> __xfs_buf_submit -> xfs_buf_ioapply_map -> submit_bio

代码参考： xfs_file_read_iter -> xfs_file_buffered_read -> generic_file_read_iter -> filemap_read -> filemap_get_pages -> filemap_create_folio -> filemap_alloc_folio -> folio_alloc
filemap_get_pages -> filemap_readahead -> page_cache_async_ra -> ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> filemap_alloc_folio/filemap_add_folio

xfs_file_read_iter -> xfs_file_buffered_read -> generic_file_read_iter -> filemap_read -> copy_folio_to_iter(offset)

filemap_get_pages -> filemap_readahead -> page_cache_async_ra -> ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> read_pages -> aops.readahead -> xfs_vm_readahead -> iomap_readahead -> iomap_iter -> ops.iomap_begin（xfs文件系统维护的回调函数）

iomap_readahead -> iomap_readahead_iter -> iomap_readpage_iter -> bio_alloc/bio_add_folio

代码参考：iomap_readahead -> iomap_iter -> ops.iomap_begin（xfs文件系统维护的回调函数）

代码参考：
sys_read：ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> xa_load（在sys_read流程中，因为一开始就会把所有的folio都拿到，不是一个一个拿的）
iomap_readahead_iter -> readahead_folio
sys_write：
xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __filemap_get_folio -> mapping_get_entry/filemap_add_folio （在sys_write流程，是用完一个folio，再申请新的folio）


代码参考：
sys_read：iomap_readpage_iter -> bio_add_folio -> __bio_try_merge_page
sys_write：xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __iomap_write_begin -> iomap_read_folio_sync -> bio_init/bio_add_folio （一个bio只有一个bio_vec）


bpf: bpf.h

syscall 系统调用: linux/syscalls.h


iscsi:
ko:
[root@n73 linux-5.10.182]# find . -name "*.ko"|grep scsi
./drivers/firmware/iscsi_ibft.ko
./drivers/message/fusion/mptscsih.ko
./drivers/scsi/aacraid/aacraid.ko
./drivers/scsi/aic7xxx/aic79xx.ko
./drivers/scsi/arcmsr/arcmsr.ko
./drivers/scsi/be2iscsi/be2iscsi.ko
./drivers/scsi/bfa/bfa.ko
./drivers/scsi/bnx2fc/bnx2fc.ko
./drivers/scsi/bnx2i/bnx2i.ko
./drivers/scsi/csiostor/csiostor.ko
./drivers/scsi/cxgbi/cxgb3i/cxgb3i.ko
./drivers/scsi/cxgbi/cxgb4i/cxgb4i.ko
./drivers/scsi/cxgbi/libcxgbi.ko
./drivers/scsi/fcoe/fcoe.ko
./drivers/scsi/fcoe/libfcoe.ko
./drivers/scsi/fnic/fnic.ko
./drivers/scsi/isci/isci.ko
./drivers/scsi/libfc/libfc.ko
./drivers/scsi/libsas/libsas.ko
./drivers/scsi/lpfc/lpfc.ko
./drivers/scsi/megaraid/megaraid_sas.ko
./drivers/scsi/mpt3sas/mpt3sas.ko
./drivers/scsi/mvsas/mvsas.ko
./drivers/scsi/pm8001/pm80xx.ko
./drivers/scsi/qedf/qedf.ko
./drivers/scsi/qedi/qedi.ko
./drivers/scsi/qla2xxx/qla2xxx.ko
./drivers/scsi/qla2xxx/tcm_qla2xxx.ko
./drivers/scsi/smartpqi/smartpqi.ko
./drivers/scsi/ufs/ufshcd-core.ko
./drivers/scsi/ufs/ufshcd-pci.ko
./drivers/scsi/3w-9xxx.ko
./drivers/scsi/3w-sas.ko
./drivers/scsi/ch.ko
./drivers/scsi/hpsa.ko
./drivers/scsi/hv_storvsc.ko
./drivers/scsi/mvumi.ko
./drivers/scsi/pmcraid.ko
./drivers/scsi/libiscsi_tcp.ko
./drivers/scsi/scsi_debug.ko
./drivers/scsi/raid_class.ko
./drivers/scsi/scsi_transport_fc.ko
./drivers/scsi/scsi_transport_spi.ko
./drivers/scsi/scsi_transport_sas.ko
./drivers/scsi/hptiop.ko
./drivers/scsi/initio.ko
./drivers/scsi/iscsi_tcp.ko
./drivers/scsi/scsi_transport_srp.ko
./drivers/scsi/sd_mod.ko
./drivers/scsi/ses.ko
./drivers/scsi/sg.ko
./drivers/scsi/sr_mod.ko
./drivers/scsi/st.ko
./drivers/scsi/stex.ko
./drivers/scsi/virtio_scsi.ko
./drivers/scsi/vmw_pvscsi.ko
./drivers/target/iscsi/cxgbit/cxgbit.ko
./drivers/target/iscsi/iscsi_target_mod.ko
./drivers/target/target_core_pscsi.ko

磁盘驱动sd: ./drivers/scsi/sd_mod.ko
通用驱动sg: ./drivers/scsi/sg.ko

drivers/scsi/sd.h
include/scsi/scsi_host.h
struct scsi_host_template, Let it rip 分叉, 
drivers/scsi/scsi.c -> subsys_initcall(init_scsi) -> init -> static int __init init_scsi -> 驱动加载
	scsi_init_procfs -> int __init scsi_init_procfs -> proc_mkdir("scsi", NULL) -> struct proc_dir_entry *proc_mkdir -> /proc/scsi -> device_info  scsi  sg -> 创建proc目录或文件: https://www.cnblogs.com/lialong1st/p/8317143.html
		proc_create("scsi/scsi", 0, NULL, &scsi_scsi_proc_ops) -> static const struct proc_ops scsi_scsi_proc_ops -> struct proc_ops {
	scsi_init_devinfo 动态设备信息列表 -> int __init scsi_init_devinfo -> SCSI_DEVINFO_GLOBAL = 0 -> 增强多个表的设备信息匹配 当前的 scsi_devinfo.c 匹配例程对全局黑名单使用单个表。 但是，我们也需要将特定传输列入黑名单（特别是一些使用 SPI 的磁带驱动器，它们对高速协议的响应不佳）。 不是为每个需要它的传输类开发单独的黑名单匹配，而是增强当前的列表匹配以允许多个列表
		scsi_dev_info_add_list -> int scsi_dev_info_add_list -> scsi_devinfo_lookup_by_key(key) -> static LIST_HEAD(scsi_dev_info_list);
			devinfo_table = kmalloc(sizeof(*devinfo_table), GFP_KERNEL) -> GFP_KERNEL - 允许后台和直接回收，并使用默认的页面分配器行为。这意味着廉价的分配请求基本上是不会失败的，但不能保证这种行为, get_free_page -> 
			list_add_tail(&devinfo_table->node, &scsi_dev_info_list) -> 初始化,并加链表
		scsi_dev_info_list_add_str
			scsi_dev_info_list_add -> 
		scsi_dev_info_list_add -> scsi_static_device_list[] __initdata -> 废弃表
		proc_create("scsi/device_info", 0, NULL, &scsi_devinfo_proc_ops)
	scsi_init_hosts 注册shost_class
		int class_register(const struct class *cls) -> static struct class shost_class -> /sys/class/iscsi_host -> ls -alh /sys/class/scsi_* -> scsi_device/  scsi_disk/    scsi_generic/ scsi_host/
			klist_init
			kset_init
			lockdep_register_key(key) -> class_create() 用作围绕核心函数调用的宏包装器的一部分的静态分配的堆栈变量
			kobject_set_name
			cp->class = cls
			kset_register
			sysfs_create_groups
				internal_create_group
	scsi_sysfs_register -> static struct class sdev_class
		bus_register(&scsi_bus_type)
		class_register(&sdev_class)
	scsi_netlink_init -> scsi_netlink_init(void)
		struct netlink_kernel_cfg cfg
		.input	= scsi_nl_rcv_msg
		netlink_kernel_create(&init_net, NETLINK_SCSITRANSPORT




static struct pci_driver isci_pci_driver = {
	.name		= DRV_NAME,
	.id_table	= isci_id_table,
	.probe		= isci_pci_probe,
	.remove		= isci_pci_remove,
	.driver.pm      = &isci_pm_ops,
};
isci_pci_probe
isci_host_alloc
scsi_host_alloc -> bpftrace -> struct Scsi_Host *scsi_host_alloc
	shost->ehandler = kthread_run(scsi_error_handler -> scsi_error_handler -> int scsi_error_handler -> 错误恢复, 内核线程,所有主机(host) -> drivers/scsi/scsi_error.c
		scsi_restart_operations(shost) -> static void scsi_restart_operations
		shost->transportt->eh_strategy_handler(shost)
			

scsi_remove_host -> 模块卸载

Linux打印内核函数调用栈 dump_stack, krpobe, systemtap, ftrace, qemu等等, bpftrace, perf调性能


void blk_finish_plug
bpftrace -e 'tracepoint:block:block_rq_insert { printf("Block I/O by %s\n", kstack); }'
Block I/O by 
        __elv_add_request+259
        blk_flush_plug_list+320
        blk_finish_plug+20
        _xfs_buf_ioapply+820
        __xfs_buf_submit+114
        xlog_bdstrat+55
        xlog_sync+742
        xlog_state_release_iclog+123
        xfs_log_force_lsn+497
        xfs_file_fsync+253
        do_fsync+85
        sys_fsync+16
        system_call_fastpath+37

lsmod|grep sd_mod
modinfo sd_mod
make clean; make; rmmod sd_mod.ko; insmod sd_mod.ko

install mod
make -C /lib/modules/`uname -r`/build M=$PWD
make -C /lib/modules/`uname -r`/build M=$PWD modules_install



bpftrace -l | more

#!/usr/local/bin/bpftrace
tracepoint:syscalls:sys_enter_nanosleep
{
  printf("%s is sleeping.\n", comm);
}

bpftrace -e 'kprobe:do_sys_open { printf("opening: %s\n", str(arg1)); }'
bpftrace -e 'kprobe:ip_output { @[kstack] = count(); }'

bpftrace -e 'kprobe:scsi_error_handler { printf("bt:%s\n", kstack); }'

动态跟踪事件: bpftrace -e 'kprobe:scsi_host_set_state { printf("bt:%s\n", kstack); }'
bt:
        scsi_host_set_state+1
        scsi_remove_host+155
        iscsi_host_remove+120
        iscsi_sw_tcp_session_destroy+85 -> .destroy_session	= iscsi_sw_tcp_session_destroy
        iscsi_if_recv_msg+3382 -> ISCSI_UEVENT_DESTROY_SESSION
        iscsi_if_rx+202  调用 netlink_unicast 是将消息发送到内核 netlink 套接字的唯一路径。 但是，不幸的是，它也用于向用户发送数据
        netlink_unicast+421
        netlink_sendmsg+539
        sock_sendmsg+91
        ____sys_sendmsg+451
        ___sys_sendmsg+124
        __sys_sendmsg+87
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68
				
断开会话:
iscsiadm -m node --logoutall=all
ISCSI_UEVENT_DESTROY_SESSION
transport->destroy_session(session) -> iscsi_sw_tcp_session_destroy
	if (scsi_host_set_state(shost, SHOST_CANCEL))

rfc:
7. iSCSI Error Handling and Recovery ..............................92
bpftrace -e 't:syscalls:sys_enter_execve { printf("%s called %s\n", comm, str(args->filename)); }'

查看所有事件: bpftrace -l|grep scsi

static int isci_pci_probe
isci_pci_init
isci_host_alloc
scsi_scan_host -> void scsi_scan_host

sd_spinup_disk
	scsi_execute_cmd TEST_UNIT_READY START_UNIT

scsi_execute_req 
blk_execute_rq -> blk_status_t blk_execute_rq -> 底层调用高层(scci -> blk)


错误恢复:
● int (* eh_abort_handler) (struct scsi_cmnd *);
● int (* eh_device_reset_handler) (struct scsi_cmnd *);
● int (* eh_target_reset_handler) (struct scsi_cmnd *);
● int (* eh_bus_reset_handler) (struct scsi_cmnd *);
● int (* eh_host_reset_handler) (struct scsi_cmnd *);

scsi_eh_scmd_add
scsi_eh_abort_cmds
scsi_eh_flush_done_q

__blk_run_queue
scsi_send_eh_cmnd

查看会话: iscsiadm --mode session
连接: iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:622a4b9193bb4ee68cdb6cae6b76cc74 -p 172.16.156.3:3260 -l
bt:
        scsi_host_set_state+1
        scsi_add_host_with_dma.cold.9+294
        iscsi_sw_tcp_session_create+130
        iscsi_if_create_session+48 -> iscsi_if_create_session(struct iscsi_internal *priv  -> ISCSI_UEVENT_CREATE_SESSION
        iscsi_if_recv_msg+1109 -> iscsi_if_recv_msg(struct sk_buff *skb
        iscsi_if_rx+202 -> .input	= iscsi_if_rx
        netlink_unicast+421 -> int netlink_unicast
        netlink_sendmsg+539
        sock_sendmsg+91
        ____sys_sendmsg+451
        ___sys_sendmsg+124
        __sys_sendmsg+87
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68


netlink_kernel_create(&init_net, NETLINK_ISCSI, &cfg) -> netlink_kernel_create(struct net *net, int unit, struct netlink_kernel_cfg *cfg) ->input：为内核模块定义的netlink消息处理函数，当有消 息到达这个netlink socket时，该input函数指针就会被引用，且只有此函数返回时，调用者的sendmsg才能返回
	nlk_sk(sk)->netlink_rcv = cfg->input -> nlk->netlink_rcv(skb) <- static int netlink_unicast_kernel


netlink_unicast
	netlink_unicast_kernel
        fail_path+1
        multipath_message+342 -> .message = multipath_message
        target_message+627
        ctl_ioctl+431
        dm_ctl_ioctl+10
        __x64_sys_ioctl+132
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68

int dm_register_target(struct target_type *tt)

static ioctl_fn lookup_ioctl
{DM_TARGET_MSG_CMD, 0, target_message} -> static int target_message
	ti->type->message(ti, argc, argv, result, maxlen) -> static int multipath_message
		action = fail_path;
		action_dev(m, dev, action) -> static int action_dev
			action(pgpath) -> static int fail_path


Documentation/kbuild/modules.rst

cp /lib/modules/`uname -r`/build/Makefile ./Makefile_current

CONFIG_PREEMPT_BUILD=y
此选项通过使所有内核代码（不在关键部分中执行）可抢占来减少内核的延迟。 这允许通过允许低优先级进程被非自愿地抢占来对交互事件做出反应，即使它在内核模式下执行系统调用，否则不会到达自然抢占点。 这允许应用程序即使在系统处于负载下时也能更“流畅”地运行，但代价是吞吐量略低，内核代码的运行时开销也很小。
如果您正在为延迟要求在毫秒范围内的桌面或嵌入式系统构建内核，请选择此选项


动态跟踪事件: bpftrace -e 'kprobe:scsi_host_set_state { printf("bt:%s\n", kstack); }'
bt:
        scsi_host_set_state+1
        scsi_remove_host+155
        iscsi_host_remove+120
        iscsi_sw_tcp_session_destroy+85 -> .destroy_session	= iscsi_sw_tcp_session_destroy
        iscsi_if_recv_msg+3382 -> ISCSI_UEVENT_DESTROY_SESSION
        iscsi_if_rx+202
        netlink_unicast+421
        netlink_sendmsg+539
        sock_sendmsg+91
        ____sys_sendmsg+451
        ___sys_sendmsg+124
        __sys_sendmsg+87
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68
				
断开会话:
iscsiadm -m node --logoutall=all
ISCSI_UEVENT_DESTROY_SESSION
transport->destroy_session(session) -> iscsi_sw_tcp_session_destroy

rfc:
7. iSCSI Error Handling and Recovery ..............................92
bpftrace -e 't:syscalls:sys_enter_execve { printf("%s called %s\n", comm, str(args->filename)); }'

查看所有事件: bpftrace -l|grep scsi





故障处理
ip set link xxx down (close 1 path) -> iscsi_check_transport_timeouts <- iscsi_conn_setup
[12291.650194]  connection3:0: ping timeout of 5 secs expired, recv timeout 5, last rx 4306948309, last ping 4306953344, now 4306958848
[12291.650313]  connection3:0: detected conn error (1022) <- void iscsi_conn_error_event
[12296.770199]  session3: session recovery timed out after 5 secs
[12297.554572] device-mapper: multipath: 253:3: Failing path 8:16. <- static int fail_path
static int fail_path



static void iscsi_check_transport_timeouts
	iscsi_has_ping_timed_out
	iscsi_conn_failure(conn, ISCSI_ERR_NOP_TIMEDOUT)
		iscsi_conn_error_event(conn->cls_conn, err)
		iscsi_if_transport_lookup(conn->transport)
		alloc_skb
		__nlmsg_put
		ev->type = ISCSI_KEVENT_CONN_ERROR -> 给用户态发事件
		iscsi_multicast_skb
	iscsi_send_nopout



iscsi_if_recv_msg -> struct iscsi_uevent *ev = nlmsg_data(nlh) -> (unsigned char *) nlh + NLMSG_HDRLEN
	case ISCSI_UEVENT_SEND_PDU

iscsi_if_recv_msg
iscsi_if_transport_conn
	switch (nlh->nlmsg_type)
	case ISCSI_UEVENT_CREATE_CONN
		iscsi_if_create_conn, 创建连接的时候设置的故障处理函数
			iscsi_session_lookup
			conn = transport->create_conn(session, ev->u.c_conn.cid)
static struct iscsi_transport iscsi_sw_tcp_transport
.create_conn		= iscsi_sw_tcp_conn_create, <- conn = transport->create_conn(session, ev->u.c_conn.cid)
iscsi_sw_tcp_conn_create
	iscsi_tcp_conn_setup
		iscsi_conn_setup(struct iscsi_cls_session *cls_session
			iscsi_alloc_conn
			timer_setup(&conn->transport_timer, iscsi_check_transport_timeouts, 0) -> 作为 timer_setup 的 callback, 超时的时候执行回调
				void iscsi_conn_error_event
				...
			INIT_LIST_HEAD(&conn->mgmtqueue)
			...
			INIT_WORK(&conn->xmitwork, iscsi_xmitworker) -> 处理iscsi发送任务
			__get_free_pages
			err = iscsi_add_conn(cls_conn)
				iscsi_dev_to_session
				device_add
				transport_register_device
				list_add(&conn->conn_list, &connlist)
	INIT_WORK(&conn->recvwork, iscsi_sw_tcp_recv_data_work)
	tcp_sw_conn->queue_recv = iscsi_recv_from_iscsi_q
	crypto_alloc_ahash
	tcp_sw_conn->tx_hash = ahash_request_alloc
	ahash_request_set_callback
	tcp_sw_conn->rx_hash = ahash_request_alloc
	return cls_conn


超时处理:
static int iscsi_nop_out_rsp -> 处理来自使用或用户空间的 nop 响应。 在 back_lock 下调用
	mod_timer(&conn->transport_timer, jiffies + conn->recv_timeout) -> 动态更改定时器(连接上的传输定时器)的到期时间，从而可更改定时器的执行顺序 -> iscsi_check_transport_timeouts
		void iscsi_conn_error_event

超时堆栈, 传输超时检测
bpftrace -e 'kprobe:iscsi_check_transport_timeouts { printf("bt:%s\n", kstack); }'
iscsi_check_transport_timeouts+1
call_timer_fn+41
run_timer_softirq+474
__softirqentry_text_start+268
asm_call_sysvec_on_stack+18
do_softirq_own_stack+55
irq_exit_rcu+243
sysvec_apic_timer_interrupt+52
asm_sysvec_apic_timer_interrupt+18
native_safe_halt+14
acpi_idle_do_entry+75
acpi_idle_enter+90
cpuidle_enter_state+145
cpuidle_enter+41
do_idle+636
cpu_startup_entry+25
start_secondary+280
secondary_startup_64_no_verify+176


如果收到消息, 公共接口
iscsi_if_recv_msg(struct sk_buff *skb, struct nlmsghdr *nlh, uint32_t *group)
	case ISCSI_UEVENT_SEND_PDU:
	iscsi_if_transport_conn(transport, nlh) -> static int iscsi_if_transport_conn
		case ISCSI_UEVENT_START_CONN
		ev->r.retcode = transport->start_conn(conn)
			启动连接 -> iscsi_conn_start
			bpftrace -e 'kprobe:iscsi_conn_start { printf("bt:%s\n", kstack); }'
			iscsi_conn_start+1
			iscsi_if_recv_msg+3789
			iscsi_if_rx+202
			netlink_unicast+421
			netlink_sendmsg+539
			sock_sendmsg+91
			____sys_sendmsg+451
			___sys_sendmsg+124
			__sys_sendmsg+87
			do_syscall_64+51
			entry_SYSCALL_64_after_hwframe+68






ev->r.retcode =	transport->send_pdu( -> iscsi_conn_send_pdu -> (struct iscsi_hdr *)((char *)ev + sizeof(*ev))


拷贝源码
cd /lib/modules/5.10.38-21.hl10.el7.x86_64/build/include
rsync -ravpl root@node2:/lib/modules/5.10.38-21.hl10.el7.x86_64/build/include/* .




设置探测点: bpftrace -e 'kprobe:scsi_host_alloc { printf("bt:%s\n", kstack); }'
断开会话: iscsiadm -m node --logoutall=all
发现: iscsiadm -m discovery -t sendtargets -p 172.17.136.132
连接: iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:f2a23531433249f7bf2a6d01760fe755 -p 172.17.136.132:3260 -l
触发堆栈:
scsi_host_alloc+1
iscsi_host_alloc+17
iscsi_sw_tcp_session_create+50
iscsi_if_create_session+48
iscsi_if_recv_msg+1109
iscsi_if_rx+202
netlink_unicast+421
netlink_sendmsg+539
sock_sendmsg+91
____sys_sendmsg+451
___sys_sendmsg+124
__sys_sendmsg+87
do_syscall_64+51
entry_SYSCALL_64_after_hwframe+68

blk_request_module
	(*n)->probe(devt) -> static int nvme_probe
		nvme_pci_alloc_dev
		nvme_dev_map
		nvme_setup_prp_pools
		nvme_pci_alloc_iod_mempool
		nvme_pci_enable
		nvme_alloc_admin_tag_set
			blk_mq_alloc_tag_set -> 设置多队列, 将cpu与硬件队列做映射: https://blog.csdn.net/hu1610552336/article/details/111464548
				set->srcu = kmalloc(sizeof(*set->srcu), GFP_KERNEL)
				ret = init_srcu_struct(set->srcu)
				set->tags = kcalloc_node(set->nr_hw_queues
				set->map[i].mq_map = kcalloc_node(nr_cpu_ids
				blk_mq_update_queue_map(set)
				ret = blk_mq_alloc_set_map_and_rqs(set)
			ctrl->admin_q = blk_mq_init_queue(set) -> 添加通用助手来分配和拆除管理和 I/O 标记集，包括分配给它们的特殊队列
				blk_mq_init_queue_data(set, NULL)
					q = blk_alloc_queue(set->numa_node) -> 分配struct request_queue并初始化
					ret = blk_mq_init_allocated_queue(set, q) -> 分配每个CPU专属的软件队列，分配硬件队列，对二者做初始化，并建立软件队列和硬件队列联系


		

spi, scsi并行接口, 

执行SCSI命令，如 INQUIRY 和 SPINUP ，它们的执行都调用了 scsi_execute_req -> scsi_execute_cmd ?  函数

static int scsi_probe_lun
	scsi_cmd[0] = INQUIRY


static int scsi_vpd_inquiry
	cmd[0] = INQUIRY

Vital Product Data (VPD) , 重要产品数据, 

int scsi_execute_cmd
	scsi_alloc_request
	blk_rq_map_kern
	blk_mq_rq_to_pdu
	blk_execute_rq



块：将超时延迟到工作队列, 定时器上下文对于驱动程序执行任何有意义的中止操作不是很有用。 因此，不要从这个无用的上下文中调用驱动程序，而是尽快将其延迟到工作队列, 请注意，虽然 delayed_work 项目在这里似乎是正确的，但由于 blk_add_timer 中的魔法深入计时器内部，我不敢使用它。 但也许这会鼓励 Tejun 为工作队列 API 添加一个合理的 API，最终我们都会好起来的, 包含来自 Keith Bush 的重大更新：“此补丁删除同步超时工作，以便计时器可以在其自己的队列上开始冻结。计时器进入队列，因此计时器上下文只能开始冻结，但不能等待冻结
static void blk_mq_timeout_work
blk_mq_queue_tag_busy_iter(q, blk_mq_handle_expired, &expired)
static bool blk_mq_handle_expired
static void blk_mq_rq_timed_out
ret = req->q->mq_ops->timeout(req)
static const struct blk_mq_ops scsi_mq_ops
.timeout	= scsi_timeout
enum blk_eh_timer_return scsi_timeout
static const struct scsi_host_template iscsi_sw_tcp_sht
eh_timed_out
.eh_timed_out		= iscsi_eh_cmd_timed_out,




eh_strategy_handler

scsi_eh_scmd_add
scsi_unjam_host 取消干扰



[root@node1 ~]# bpftrace -e 'kprobe:scsi_error_handler { printf("bt:%s\n", kstack); }'
iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:f2a23531433249f7bf2a6d01760fe755 -p 172.17.136.132:3260 -l -d 8
scsi_error_handler+1
kthread+278
ret_from_fork+34


IO路径-文件系统-系统调用, iopath
int main()
{
       char buff[128] = {0};
       int fd = open("/var/pilgrimtao.txt", O_CREAT|O_RDWR);

       write(fd, "pilgrimtao is cool", 18);
       pread(fd, buff, 128, 0);
       printf("%s\n", buff);

       close(fd);
       return 0;
}

IO路径-块层
SYSCALL_DEFINE3(write, ...) -> ksys_write -> vfs_write -> new_sync_write -> call_write_iter ->write_iter -> xfs_file_write_iter
xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_iter -> .iomap_begin -> xfs_buffered_write_iomap_begin -> xfs_iread_extents -> xfs_btree_visit_blocks -> xfs_btree_readahead_ptr -> xfs_buf_readahead -> xfs_buf_readahead_map -> xfs_buf_read_map -> xfs_buf_read -> xfs_buf_submit -> __xfs_buf_submit -> xfs_buf_ioapply_map -> submit_bio -> submit_bio_noacct -> submit_bio_noacct_nocheck -> __submit_bio_noacct_mq/ __submit_bio_noacct -> __submit_bio -> blk_mq_submit_bio -> blk_add_rq_to_plug -> bio合并： blk_mq_submit_bio -> blk_mq_get_new_requests -> blk_mq_sched_bio_merge -> blk_bio_list_merge -> blk_attempt_bio_merge
request插入ctx：blk_mq_submit_bio -> blk_mq_sched_insert_request -> __ blk_mq_insert_request -> __ blk_mq_insert_req_list -> list_add(&rq->queuelist, &ctx->rq_lists[type])
取出request：blk_mq_run_hw_queue -> __ blk_mq_delay_run_hw_queue -> __ blk_mq_run_hw_queue -> blk_mq_sched_dispatch_requests -> __blk_mq_sched_dispatch_requests -> blk_mq_do_dispatch_ctx -> blk_mq_dequeue_from_ctx -> dispatch_rq_from_ctx -> __blk_mq_sched_dispatch_requests -> blk_mq_flush_busy_ctxs （取出）/ blk_mq_dispatch_rq_list （发送给磁盘）

IO路径, 块io, iscsi层, iopath, 
bool blk_mq_dispatch_rq_list
		ret = q->mq_ops->queue_rq(hctx, &bd) # 关键函数 queue_rq, IO请求入队列
		.queue_rq	= scsi_queue_rq
		static blk_status_t scsi_queue_rq( -> scsi处理流程: https://blog.csdn.net/marlos/article/details/131171560, 这个函数之后大致要完成的工作是，把队列中的request再转化为对硬件的command，接着下发command到硬件，完成io。也就是说，对于request的解析，一定是在command生成之前的。在上面代码的35行之前，是在做一些必要的检查，确保队列、硬件处于正常工作的状态，接着37行，出现一个关键的函数 scsi_prepare_cmd, 顾名思义，command可能会在这个函数中进行初始化
			struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req) -> cmd已经填充了?
			WARN_ON_ONCE(cmd->budget_token < 0) -> 预算令牌, scsi：blk-mq：从 .get_budget 回调中返回预算令牌 SCSI 使用全局原子变量来跟踪每个 LUN/请求队列的队列深度,当有很多 CPU 核心并且磁盘非常快时，这不能很好地扩展。 通过在 I/O 路径中的 sdev->device_busy 跟踪队列深度，观察到 IOPS 受到很大影响,从 .get_budget 回调中返回预算令牌。 预算令牌可以传递给驱动程序，这样我们就可以用 sbitmap_queue 替换原子变量，并以这种方式缓解缩放问题, 链接：https://lore.kernel.org/r/20210122023317.687987-9-ming.lei@redhat.com
			ret = BLK_STS_RESOURCE -> 块：引入新的块状态代码类型目前我们在块层中使用标准的 Linux errno 值，虽然我们接受任何错误，但一些错误具有超载的魔法含义。 这个补丁引入了一个新的 blk_status_t 值，它包含块层特定的状态代码并明确解释它们的含义。 现在提供了与以前的特殊含义相互转换的助手，但我怀疑我们希望从长远来看摆脱它们——那些有错误输入（例如网络）的驱动程序通常会得到不知道特殊块层的错误 重载，并类似地将它们返回到用户空间通常会返回一些严格来说对于文件系统操作不正确的东西，但这留作以后的练习。目前错误集是一个非常有限的集合，与之前重载的 errno 值密切相关 , 但有一些低挂果来改进它。blk_status_t (ab) 使用稀疏的 __bitwise 注释来允许稀疏类型检查，这样我们就可以很容易地捕捉到传递错误值的地方
			scsi_prepare_cmd -> static blk_status_t scsi_prepare_cmd(struct request *req)
				struct scsi_cmnd
				cmd->prot_op = SCSI_PROT_NORMAL 命令保护操作
				return scsi_cmd_to_driver(cmd)->init_command(cmd) -> .init_command		= sd_init_command -> scsi_init_command -> static blk_status_t sd_init_command -> scsi层里面，高级驱动可不止sd一个，因此，我们可以猜测这个函数只是在做一些通用性的命令初始化，对于特异性的初始化，一定会转交sd驱动处理，所以直接看代码的66行，调用了对应cmd绑定驱动的init_command函数
					case REQ_OP_WRITE
					return sd_setup_read_write_cmnd(cmd)
						bool write = rq_data_dir(rq) == WRITE
						scsi_alloc_sgtables
						dix = scsi_prot_sg_count(cmd) -> 数据保护
						if (protect && sdkp->protection_type == T10_PI_TYPE2_PROTECTION) -> T10保护信息(T10 Protection Information (PI))
						sd_setup_rw10_cmnd(cmd, write, lba, nr_blocks -> static blk_status_t sd_setup_rw10_cmnd -> 打印日志: SCSI_LOG_  -> SCSI_LOG_HLQUEUE -> [66521.609478] sd 6:0:0:0: [sda] tag#23 sd_setup_read_write_cmnd: block=893164736, count=8
							cmd->cmd_len = 10

			static int scsi_dispatch_cmd(struct scsi_cmnd *cmd)
				trace_scsi_dispatch_cmd_start(cmd)
				rtn = host->hostt->queuecommand(host, cmd) -> .queuecommand           = iscsi_queuecommand, -> int iscsi_queuecommand
					iscsi_session_chkready -> 检查会话通过iscsi_session_chkready进行。当会话状态不是ISCSI_SESSION_LOGGED_IN时，不适合处理scsi指令。链接检查通过链接是否存在、链接状态、链接可接收的命令窗口是否达到最大值。这几个方面判断
					task = iscsi_alloc_task(conn, sc)
					iscsi_prep_scsi_cmd_pdu(task)
						ISCSI_DBG_SESSION
					list_add_tail(&task->running, &conn->cmdqueue) -> 将任务插入命令队列 cmdqueue -> 由 iscsi_xmitworker 线程发送命令
					iscsi_conn_queue_xmit(conn)


static void iscsi_xmitworker(struct work_struct *work)
	do iscsi_data_xmit(conn)
		iscsi_xmit_task
			rc = conn->session->tt->xmit_task(task) -> .xmit_task		= iscsi_tcp_task_xmit 发送常规PDU任务
				rc = session->tt->xmit_pdu(task) -> static int iscsi_sw_tcp_pdu_xmit
					iscsi_sw_tcp_xmit
						while (1) iscsi_sw_tcp_xmit_segment(tcp_conn, segment) 传输分段
							tcp_sw_conn->sendpage(sk, sg_page(sg), offset
						segment->done(tcp_conn, segment) 首选按页发送
						kernel_sendmsg(sk, &msg, &iov, 1, copy) 其次降级为内核发送消息
							iov_iter_kvec
							sock_sendmsg(sock, msg)
					memalloc_noreclaim_restore
				iscsi_tcp_get_curr_r2t
				conn->session->tt->alloc_pdu
				iscsi_prep_data_out_pdu -> 初始化 Data-Out
					hdr->ttt = r2t->ttt
					hdr->opcode = ISCSI_OP_SCSI_DATA_OUT
				rc = conn->session->tt->init_pdu
			iscsi_put_task(task)

iscsit_send_r2t


/* SCSI Data Hdr */ -> 11.2.1.  Basic Header Segment (BHS)
struct iscsi_data


static int iscsi_if_transport_conn
	.bind_conn		= iscsi_sw_tcp_conn_bind
	iscsi_sw_tcp_conn_bind
		static void iscsi_sw_tcp_conn_set_callbacks
			sk->sk_data_ready = iscsi_sw_tcp_data_ready
				iscsi_sw_tcp_recv_data(conn)
					tcp_read_sock(sk, &rd_desc, iscsi_sw_tcp_recv) -> 该例程为希望以“sendfile”方式直接处理从 skbuffs 复制的例程提供了 tcp_recvmsg() 的替代方案
						skb = tcp_recv_skb(sk, seq, &offset)
							skb_peek
							tcp_eat_recv_skb
								skb_attempt_defer_free
									defer_max = READ_ONCE(sysctl_skb_defer_max)
									...
					iscsi_tcp_segment_unmap(&tcp_conn->in.segment)



bpftrace -e 'kprobe:sd_init_command{ printf("bt:%s\n", kstack); }'
sd_init_command+1
scsi_queue_rq+1478
blk_mq_dispatch_rq_list+291
__blk_mq_sched_dispatch_requests+201
blk_mq_sched_dispatch_requests+48
__blk_mq_run_hw_queue+81
__blk_mq_delay_run_hw_queue+321
blk_mq_run_hw_queues+52
blk_mq_requeue_work+350
process_one_work+431
worker_thread+48
kthread+278
ret_from_fork+34




bpftrace -e 'kprobe:scsi_queue_rq { printf("bt:%s\n", kstack); }'
scsi_queue_rq+1
blk_mq_dispatch_rq_list+291
__blk_mq_sched_dispatch_requests+201
blk_mq_sched_dispatch_requests+48
__blk_mq_run_hw_queue+81
__blk_mq_delay_run_hw_queue+321
blk_mq_run_hw_queues+52
blk_mq_requeue_work+350
process_one_work+431
worker_thread+48
kthread+278
ret_from_fork+34




配置探测, 崩溃点
#ifdef CONFIG_KPROBES
	CRASHPOINT("INT_HARDWARE_ENTRY", "do_IRQ"),



static void scsi_starved_list_run -> 只要 shost 正在接受命令并且队列不足，就调用 blk_run_queue。 scsi_request_fn 删除 queue_lock 并可以将我们添加回 starved_list。 host_lock 保护 starved_list 和 starved_entry。 scsi_request_fn 在检查或修改 starved_list 或 starved_entry 之前必须获得 host_lock


include/scsi/scsi_cmnd.h

last_sector_bug

iscsi协议 -> include/scsi/iscsi_proto.h, scsi_proto.h -> /* defined in T10 SCSI Primary Commands-2 (SPC2) */ 变长命令 -> struct scsi_varlen_cdb_hdr
/* iSCSI PDU Header */
struct iscsi_scsi_req

/* Extended CDB AHS */

11.3.5. CDB - SCSI Command Descriptor Block ..............160


queue_cmd_ring
target_cmd_init_cdb
scsi_command_size



ceph内核接管块层IO:
.queue_rq	= rbd_queue_rq -> static blk_status_t rbd_queue_rq
	rbd_img_is_write
	queue_work(rbd_wq, &img_req->work)


printk(KERN_ALERT, "init_scsi %s:%d\n", __FILE__, __LINE__);


SCSI Command, 命令



unsigned char cmnd[32]; /* SCSI CDB */
8 7 6 5 4 3 2 1 0
#define WRITE_10              0x2a
8 7 6 5 4 3 2 1 0
8 7 6 5 4 3 2 1 0
8 7 6 5 4 3 2 1 0
8 7 6 5 4 3 2 1 0
...


开启日志:
drivers/scsi/Kconfig
config SCSI_LOGGING
	bool "SCSI logging facility"

开启日志: scsi_logging_level -s -a 3
echo -1 > /proc/sys/dev/scsi/logging_level



multipath, 多路径, https://blog.csdn.net/superyongzhe/article/details/126439055
dm-mpath.c
libdevicemapper
加载模块
module_init(dm_multipath_init) -> static int __init dm_multipath_init(void)
	创建一个slab缓冲区，用于后续多路径中需要快速申请和释放内存的地方使用；
	向DeviceMapper框架中注册多路径的Target Type；
	申请两个工作队列，用于异步执行耗时的IO操作


构造实例
static int multipath_ctr


更新模块, 比较简单的方法是把原来的 *.ko 文件直接替换，但这样有不好的地方，如果想换回来，比较麻烦
cp libdrm/linux-core/drm.ko /lib/modules/`uname -r`/updates/
depmod
/sbin/modinfo drm


查看路径(multipath -l/-ll)

清空全部路径(multipath -F) -> dm_flush_maps




md(multi_disk多磁盘)模块初始化
subsys_initcall(md_init) -> static int __init md_init(void)



dm, 栈式块设备层，其本质原理, 就在于重定向请求方式, dmsetup, drivers/md, 线性映射规则, 条带映射规则, 
映射设备: struct mapped_device
目标设备: struct dm_target
映射表: struct dm_table
映射目标: struct dm_target

dm(device_mapper 设备映射虚拟层)模块初始化 -> drivers/md/dm.c
module_init(dm_init) -> static int __init dm_init
	r = _inits[i]() -> static int (*_inits[])
	local_init,
	dm_target_init,
	dm_linear_init,
	dm_stripe_init,
	dm_io_init,
	dm_kcopyd_init,
	dm_interface_init -> 映射设备的创建, Device Mapper控制设备的名字为device-mapper，文件操作表为_ctrl_fops，它的主设备号为MISC字符设备的主设备号（10），次设备号由系统自动分配，用户空间库函数将相应为Device Mapper控制设备创建一个设备节点，其路径名为/dev/mapper/control, 用户空间管理工具通过Device Mapper控制设备实现对映射设备的管理操作。具体来讲，用户空间以/dev/mapper/control为参数调用系统调用open，获得控制设备的文件句柄，之后针以该文件句柄为参数调用ioctl，调用时还传入代表操作类型的I/O控制码, man dmsetup, 
		misc_register(&_dm_misc)
			dev = MKDEV(MISC_MAJOR, misc->minor)
	dm_statistics_init,


#define DM_DEV_CREATE    _IOWR(DM_IOCTL, DM_DEV_CREATE_CMD, struct dm_ioctl) -> 创建dm设备

The userspace code (dmsetup and libdevmapper) is now maintained alongside the LVM2 source available from http://sourceware.org/lvm2/. To build / install it without LVM2 use 'make device-mapper' / 'make install_device-mapper'.

iopath_dm, 7.5 映射设备的请求执行, 
dm_request -> drivers/md/dm.c
dm_make_request


static const struct block_device_operations dm_blk_dops = {
	.submit_bio = dm_submit_bio,
	.poll_bio = dm_poll_bio,
	.open = dm_blk_open,
	.release = dm_blk_close,
	.ioctl = dm_blk_ioctl,
	.getgeo = dm_blk_getgeo,
	.report_zones = dm_blk_report_zones,
	.pr_ops = &dm_pr_ops,
	.owner = THIS_MODULE
};


dm_submit_bio+1
submit_bio_noacct+251
submit_bio+67
submit_bio_wait+84
blkdev_issue_flush+89
xfs_file_fsync+461
do_fsync+56
__x64_sys_fdatasync+19
do_syscall_64+51
entry_SYSCALL_64_after_hwframe+68



____cacheline_aligned_in_smp, 作用是：在对称多处理器系统中，结构体的起始地址和长度都是一级缓存行长度的整数倍, 独占cache line, 对于数据结构中频繁访问的成员我们可以设置它独占cache line。为啥要让它独占呢，还是cache伪缓存问题，这个成员可能导致互相干架，频繁导入导出cache line。例如zone->lock和zone->lru_lock这两个频繁的锁，有助于提高获取锁的效率。在SMP系统中，自旋锁的争用会导致严重的cache line颠簸现象, 正如这些宏的字面意思，可以简单理解为按高速缓存行对齐, 
https://github.com/ssbandjl/linux/commit/320ae51feed5c2f13664aa05a76bec198967e04d, blk-mq：新的多队列块 IO 排队机制 Linux 目前有两种块设备模型： - 经典的基于 request_fn 的方法，其中驱动程序使用 struct request 单元进行 IO。 块层提供各种帮助器功能，让驱动程序共享代码，例如标签管理、超时处理、排队等。“堆叠”方法，驱动程序挤在块层和 IO 提交者之间。 由于这会绕过 IO 堆栈，因此驱动程序通常必须自己管理所有内容。随着为新的高 IOPS 设备编写驱动程序，基于经典 request_fn 的驱动程序工作得不够好。 该设计可以追溯到 SMP 和高 IOPS 都还很少见的时候。 它在扩展到更大的机器时存在问题，并且当每个设备具有数十万 IOPS 时，即使在较小的机器上也会遇到扩展问题。因此，通常选择堆栈方法作为驱动程序的模型。 但这意味着每个人都必须重新发明一切，随之而来的是我们再次遇到共享方法解决的所有问题。此提交引入了 blk-mq，块多队列支持。 该设计以用于排队 IO 的每 CPU 队列为中心，然后向下汇集到 x 个硬件提交队列。两者之间可能有 1:1 映射，也可能是 N:M 映射。 这一切都取决于硬件支持的内容。blk-mq 提供各种辅助功能，其中包括： - 对请求标记的可扩展支持。 大多数设备需要能够唯一地标识驱动程序和硬件中的请求。 标记使用每个 cpu 缓存来释放标记，以实现缓存热重用。 - 超时处理，无需跟踪每个设备的请求。 基本上，如果请求失败，驱动程序应该能够收到通知。- 可选支持问题和提交队列之间的非 1:1 映射。 blk-mq 可以将 IO 完成重定向到所需位置。- 支持每个请求的有效负载。 驱动程序几乎总是需要将请求结构与某些驱动程序私有命令结构相关联。 驱动程序可以在初始化时告诉 blk-mq，然后任何传递给驱动程序的请求都将具有与之关联的所需内存大小。- 支持合并 IO 和插入。 堆叠模型没有得到这些。 即使对于高 IOPS 设备，合并顺序 IO 也会减少每个命令的开销，从而增加带宽。目前，这是作为潜在的第三种排队模型提供的，希望随着它的成熟，它可以取代经典模型和堆栈模型。 这将使我们回到只有 1 个用于块设备的真实模型，而将堆叠方法留给 dm/mddevices（正如最初预期的那样）。以下人员在此补丁中做出的贡献：
Linux 以与 TLB 非常相似的方式管理 CPU 缓存。CPU 高速缓存与 TLB 高速缓存一样，利用了程序往往表现出引用局部性的事实。为了避免每次引用时都必须从主内存中获取数据，CPU 会在 CPU 高速缓存中缓存非常少量的数据。通常，有两个级别，称为 1 级和 2 级 CPU 缓存。2 级 CPU 缓存比 L1 缓存更大但速度更慢，但 Linux 只关心 1 级或 L1 缓存。CPU 缓存被组织成行。每行通常很小，通常为 32 字节，并且每行与其边界大小对齐。换句话说，32 字节的高速缓存行将在 32 字节的地址上对齐。对于 Linux，行的大小L1_CACHE_BYTES由每个体系结构定义。地址映射到缓存行的方式因体系结构而异，但映射分为三个标题，直接映射、关联映射和集合关联映射。直接映射是最简单的方法，其中每个内存块仅映射到一个可能的缓存行。通过关联映射，任何内存块都可以映射到任何缓存行。集合关联映射是一种混合方法，其中任何内存块都可以映射到任何行，但仅限于可用行的子集内。不管映射方案如何，它们都有一个共同点，靠近并与缓存大小对齐的地址很可能使用不同的行。因此，Linux 使用简单的技巧来尝试最大化缓存的使用经常访问的结构字段位于结构的开头，以增加只需要一行来寻址公共字段的机会；结构中不相关的项应尽量分开至少缓存大小字节，以避免 CPU 之间的错误共享；通用缓存中的对象（例如 mm_struct 缓存）与 L1 CPU 缓存对齐，以避免错误共享。如果 CPU 引用不在高速缓存中的地址，则会发生高速缓存未命中并从主内存中获取数据。缓存未命中的成本非常高，因为对缓存的引用通常可以在不到 10ns 的时间内执行，而对主内存的引用通常将花费 100ns 到 200ns。基本目标是尽可能多的缓存命中和尽可能少的缓存未命中。正如某些架构不会自动管理其 TLB 一样，某些架构也不会自动管理其 CPU 缓存。挂钩被放置在虚拟到物理映射发生变化的位置，例如在页表更新期间。CPU 高速缓存刷新应始终首先进行，因为当从高速缓存刷新虚拟地址时，某些 CPU 需要存在虚拟到物理映射

addr2line -f -e /boot/vmlinuz-5.10.38-21.hl10.el7.x86_64.debug choose_pgpath+164
line_num, 显示行号
找符号表: [root@node1 kernel-alt-5.10.38-21.01.el7]# grep -rn 'multipath_clone_and_map'
gdb "$(modinfo -n dm_mod)"
[root@node1 rpmbuild]# gdb ~/rpmbuild/BUILD/kernel-alt-5.10.38-21.01.el7/linux-5.10.38-21.hl10.el7.x86_64/drivers/md/dm-mod.ko
(gdb) list *(choose_pgpath+164)

bpftrace -e 'kprobe:st_select_path{ printf("bt:%s\n", kstack); }'
st_select_path+1
choose_path_in_pg+44
choose_pgpath+164
multipath_clone_and_map+219
dm_mq_queue_rq+264
blk_mq_dispatch_rq_list+291
__blk_mq_do_dispatch_sched+356
__blk_mq_sched_dispatch_requests+309
blk_mq_sched_dispatch_requests+48
__blk_mq_run_hw_queue+81
process_one_work+431
worker_thread+48
kthread+278
ret_from_fork+34

st_select_path+1
choose_path_in_pg+44
choose_pgpath+164
multipath_clone_and_map+219
dm_mq_queue_rq+264
blk_mq_dispatch_rq_list+291
__blk_mq_do_dispatch_sched+356
__blk_mq_sched_dispatch_requests+309
blk_mq_sched_dispatch_requests+48
__blk_mq_run_hw_queue+81
__blk_mq_delay_run_hw_queue+321
blk_mq_sched_insert_requests+106
blk_mq_flush_plug_list+281
blk_flush_plug_list+215
blk_finish_plug+33
__ext4_get_inode_loc+592
__ext4_get_inode_loc_noinmem+52
__ext4_iget+276
ext4_lookup+274
__lookup_slow+139
walk_component+312
path_lookupat.isra.43+103
filename_lookup.part.57+160
vfs_statx+114
__do_sys_newstat+57
do_syscall_64+51
entry_SYSCALL_64_after_hwframe+68



static void dm_softirq_done
dm_done
	rq_end_io(tio->ti, clone, error, &tio->info)


static int __init blk_mq_init
	open_softirq(BLOCK_SOFTIRQ, blk_done_softirq) -> static __latent_entropy void blk_done_softirq -> blk_complete_reqs
		rq->q->mq_ops->complete(rq) -> 

dm_softirq_done+1
blk_done_softirq+185
__softirqentry_text_start+268
asm_call_sysvec_on_stack+18
do_softirq_own_stack+55
do_softirq.part.19+70
__local_bh_enable_ip+120
ip_finish_output2+429
ip_output+109
__ip_queue_xmit+335
__tcp_transmit_skb+2565
tcp_write_xmit+896
__tcp_push_pending_frames+50
tcp_sendmsg_locked+3053
tcp_sendmsg+39
sock_sendmsg+84
sock_write_iter+151
new_sync_write+412
vfs_write+388
ksys_write+181
do_syscall_64+51
entry_SYSCALL_64_after_hwframe+68



TRACE_EVENT, 跟踪事件, TRACE_EVENT(block_rq_remap, 


nvme -> module_init(nvme_init) -> static int __init nvme_init(void)
	pci_register_driver


blk_mq_get_driver_tag+1
blk_mq_dispatch_rq_list+178
__blk_mq_do_dispatch_sched+356
__blk_mq_sched_dispatch_requests+309
blk_mq_sched_dispatch_requests+48
__blk_mq_run_hw_queue+81
__blk_mq_delay_run_hw_queue+321
blk_mq_sched_insert_requests+106
blk_mq_flush_plug_list+281
blk_flush_plug_list+215
blk_finish_plug+33
xfs_buf_delwri_submit_buffers+524
xfsaild+923
kthread+278
ret_from_fork+34
