https://github.com/ssbandjl/linux.git
upstream: git remote add upstream https://github.com/torvalds/linux.git
git remote add upstream
git fetch upstream master
git merge upstream/master
git pull --rebase upstream master

代码: https://elixir.bootlin.com/linux/v6.0-rc1/source/drivers/nvme/host/pci.c#L504

nvme协议参考: https://github.com/andyBrake/andyBrake.github.io/blob/master/doc/nvme_express.md


----- nvme start -----
nvme
static struct blk_mq_ops nvme_mq_ops -> struct blk_mq_ops 
static const struct blk_mq_ops bsg_mq_ops = {
    .queue_rq		= bsg_queue_rq,
    .init_request		= bsg_init_rq,
    .exit_request		= bsg_exit_rq,
    .complete		= bsg_complete,
    .timeout		= bsg_timeout,
};

驱动目录: drivers/nvme/Makefile
参考: https://mp.weixin.qq.com/s/GKVIY_NOXDfUCOho4zBMzw
Kconfig 文件的作用是：
1.控制make menuconfig时， 出现的配置选项；
2.根据用户配置界面的选择，将配置结果保存在.config 配置文件（该文件将提供给Makefile使用，用以决定要编译的内核组件以及如何编译）

drivers/nvme/host/core.c
module_init(nvme_core_init) -> static int __init nvme_core_init(void)
    _nvme_check_size
        BUILD_BUG_ON(sizeof(struct nvme_common_command) != 64)
        ...
    nvme_wq = alloc_workqueue("nvme-wq",
    nvme_reset_wq
    nvme_delete_wq
    alloc_chrdev_region(&nvme_ctrl_base_chr_devt, 0, NVME_MINORS, "nvme")
    class_create("nvme")
    nvme_class->dev_uevent = nvme_class_uevent
        add_uevent_var(env, "NVME_TRTYPE=%s", ctrl->ops->name)
        NVME_TRADDR
        NVME_TRSVCID
        NVME_HOST_TRADDR
        NVME_HOST_IFACE
    nvme_subsys_class = class_create("nvme-subsystem")
    alloc_chrdev_region(&nvme_ns_chr_devt, 0, NVME_MINORS, "nvme-generic")
    nvme_ns_chr_class = class_create("nvme-generic")
    nvme_init_auth -> 注册“.nvme”密钥环以保存 TLS 和 DH-HMAC-CHAP 的密钥，并添加新的配置选项 NVME_KEYRING。 我们需要一个单独的 NVMe 密钥环，因为配置是通过单独的命令（例如 configfs）完成的，并且不能使用通常的每会话或每进程密钥环
        nvme_auth_wq
        nvme_chap_buf_cache = kmem_cache_create("nvme-chap-buf-cache"
        nvme_chap_buf_pool = mempool_create(16, mempool_alloc_slab, mempool_free_slab, nvme_chap_buf_cache) -> usage -> chap->buf = mempool_alloc(nvme_chap_buf_pool, GFP_KERNEL)




target驱动实现: https://github.com/torvalds/linux/commit/8f000cac6e7a6edca7ab93bafc7ed28b27c1545b
enum {
    NVMET_RDMA_REQ_INLINE_DATA	= (1 << 0),
    NVMET_RDMA_REQ_INVALIDATE_RKEY	= (1 << 1),
};
所有 NVMe 逻辑都位于通用目标中，该模块只是在它与 RDMA 子系统中的通用代码之间提供了一个小粘合剂。



drivers/nvme/target/rdma.c
static int __init nvmet_rdma_init(void)
ib_register_client -> drivers/infiniband/core/device.c -> int ib_register_client -> IB 驱动程序的上层用户可以使用 ib_register_client() 来注册 IB 设备添加和删除的回调。 添加 IB 设备时，将调用每个已注册客户端的 add 方法（按照客户端注册的顺序），而当删除设备时，将调用每个客户端的 remove 方法（按照客户端注册的相反顺序）。 另外，调用ib_register_client()时，客户端会收到所有已经注册的设备的add回调
nvmet_register_transport -> int nvmet_register_transport
    down_write
    up_write

EXPORT_SYMBOL(ib_register_client) 导出符号表


内存区域: zone_type
内核将zone按照不同的使用用途划分为不同的type，位于include/linux/
mmzone.h文件中：
ZONE_DMA:主要为了兼容ISA设备，在该设备中DMA只能访问低于16M内存地址，只能将其单独划出来进行管理。
ZONE_DMA32:针对32位系统进行兼容，一般在使用ZONE_DMA时由于16M内存过小，而有些设备DMA寻址能达到32位，在64位系统中为了能够兼容32位系统，划分了ZONE_DMA32，该物理内存为低于32位，以满足32位寻址的DMA。
ZONE_NORMAL：使用正常的物理内存区域，大部分申请的内存都使用的该区域。
ZONE_HIGHMEM:只会出现在32位系统中，这是由于在32位系统中，物理内存最多能够直接映射到内核中896M内存，但是为了兼容大于896M内存系统，将大于896M的高端内存以弥补地址空间不足的问题。高端内存映射不会采用一一映射，而是在使用的时候才映射，在64位系统中由于使用空间足够，不需要ZONE_HIGHMEM。
ZONE_MOVEABLE:可移动或收收区域，该zone一般称为伪ZONE，所管理的物理内存来自于ZONE_NORMAL或者ZONE_HIGHMEM中根据配置划分出的一片物理内存为ZONE_MOVABLE


内存块标签:
enum memblock_flags {
    MEMBLOCK_NONE		= 0x0,	/* No special request */
    MEMBLOCK_HOTPLUG	= 0x1,	/* hotpluggable region */
    MEMBLOCK_MIRROR		= 0x2,	/* mirrored region */
    MEMBLOCK_NOMAP		= 0x4,	/* don't add to kernel direct mapping */
    MEMBLOCK_DRIVER_MANAGED = 0x8,	/* always detected via a driver */
    MEMBLOCK_RSRV_NOINIT	= 0x10,	/* don't initialize struct pages */
};


iopath,
write(fd, "pilgrimtao is cool", 18)
    ssize_t ksys_write(unsigned int fd, const char __user *buf, size_t count)
        vfs_write(f.file, buf, count, ppos)
            if (file->f_op->write) -> 首选普通写
                ret = file->f_op->write(file, buf, count, pos)
            else if (file->f_op->write_iter) -> 如果没有实现普通写,但是实现了迭代写, 则调用同步写
                ret = new_sync_write(file, buf, count, pos)
                    call_write_iter(filp, &kiocb, &iter)
                        return file->f_op->write_iter(kio, iter) -> fs.h, -> xfs_file_write_iter
                            if (IS_DAX(inode)) -> 如果是直接访问（Direct Access，DAX）
                                return xfs_file_dax_write(iocb, from) -> ...
                            if (iocb->ki_flags & IOCB_DIRECT) -> 如果在写的逻辑__generic_file_write_iter里面，发现设置了IOCB_DIRECT，则调用generic_file_direct_write，里面同样会调用address_space的direct_IO的函数，将数据直接写入硬盘: https://www.cnblogs.com/luozhiyun/p/13061199.html
                                ret = xfs_file_dio_write(iocb, from)
                            return xfs_file_buffered_write(iocb, from) -> 默认带缓存的写
                                iomap_file_buffered_write -> iomap_iter -> .iomap_begin -> xfs_buffered_write_iomap_begin



blk_mq_rdma_map_queues

nvme块层队列函数操作表
static const struct blk_mq_ops nvme_mq_ops = {
    .queue_rq	= nvme_queue_rq,
    .queue_rqs	= nvme_queue_rqs,
    .complete	= nvme_pci_complete_rq,
    .commit_rqs	= nvme_commit_rqs,
    .init_hctx	= nvme_init_hctx,
    .init_request	= nvme_pci_init_request,
    .map_queues	= nvme_pci_map_queues,
    .timeout	= nvme_timeout,
    .poll		= nvme_poll,
};






nvme请求入队:
static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx, const struct blk_mq_queue_data *bd)
    ret = nvme_prep_rq(dev, req) -> 准备nvme请求
        nvme_setup_cmd(req->q->queuedata, req) -> 设置nvme命令(重要函数)
        nvme_start_request(req)



static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(struct blk_mq_tag_set *set, struct request_queue *q, int hctx_idx, int node)
    static int blk_mq_init_hctx(struct request_queue *q, struct blk_mq_tag_set *set, struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)
		hctx->queue_num = hctx_idx
        初始化块层的IO入队请求
        static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq, unsigned int hctx_idx, int node)
            set->ops->init_request(set, rq, hctx_idx, node) -> static int nvme_pci_init_request(struct blk_mq_tag_set *set, struct request *req, unsigned int hctx_idx, unsigned int numa_node)




static int blk_mq_alloc_rqs(struct blk_mq_tag_set *set,struct blk_mq_tags *tags, unsigned int hctx_idx, unsigned int depth)


struct blk_mq_tag_set {
	const struct blk_mq_ops	*ops;
	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
	unsigned int		nr_maps;
	unsigned int		nr_hw_queues;
	unsigned int		queue_depth;
	unsigned int		reserved_tags;
	unsigned int		cmd_size;
	int			numa_node;
	unsigned int		timeout;
	unsigned int		flags;
	void			*driver_data;

	struct blk_mq_tags	**tags;

	struct blk_mq_tags	*shared_tags;

	struct mutex		tag_list_lock;
	struct list_head	tag_list;
	struct srcu_struct	*srcu;
};


host头文件: 
drivers/nvme/host/nvme.h



blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req)
{
    struct nvme_command *cmd = nvme_req(req)->cmd;
    blk_status_t ret = BLK_STS_OK;

    if (!(req->rq_flags & RQF_DONTPREP))
        nvme_clear_nvme_request(req);

    switch (req_op(req)) {
    case REQ_OP_DRV_IN:
    case REQ_OP_DRV_OUT:
        /* these are setup prior to execution in nvme_init_request() */
        break;
    case REQ_OP_FLUSH:
        nvme_setup_flush(ns, cmd);
        break;
    case REQ_OP_ZONE_RESET_ALL:
    case REQ_OP_ZONE_RESET:
        ret = nvme_setup_zone_mgmt_send(ns, req, cmd, NVME_ZONE_RESET);
        break;
    case REQ_OP_ZONE_OPEN:
        ret = nvme_setup_zone_mgmt_send(ns, req, cmd, NVME_ZONE_OPEN);
        break;
    case REQ_OP_ZONE_CLOSE:
        ret = nvme_setup_zone_mgmt_send(ns, req, cmd, NVME_ZONE_CLOSE);
        break;
    case REQ_OP_ZONE_FINISH:
        ret = nvme_setup_zone_mgmt_send(ns, req, cmd, NVME_ZONE_FINISH);
        break;
    case REQ_OP_WRITE_ZEROES:
        ret = nvme_setup_write_zeroes(ns, req, cmd);
        break;
    case REQ_OP_DISCARD:
        ret = nvme_setup_discard(ns, req, cmd);
        break;
    case REQ_OP_READ:
        ret = nvme_setup_rw(ns, req, cmd, nvme_cmd_read);
        break;
    case REQ_OP_WRITE:
        ret = nvme_setup_rw(ns, req, cmd, nvme_cmd_write);
        break;
    case REQ_OP_ZONE_APPEND:
        ret = nvme_setup_rw(ns, req, cmd, nvme_cmd_zone_append);
        break;
    default:
        WARN_ON_ONCE(1);
        return BLK_STS_IOERR;
    }

    cmd->common.command_id = nvme_cid(req);
    trace_nvme_setup_cmd(req, cmd);
    return ret;
}


nvme io命令:
参考: include/linux/nvme.h
enum nvme_opcode {
    nvme_cmd_flush		= 0x00,
    nvme_cmd_write		= 0x01,
    nvme_cmd_read		= 0x02,
    nvme_cmd_write_uncor	= 0x04,
    nvme_cmd_compare	= 0x05,
    nvme_cmd_write_zeroes	= 0x08,
    nvme_cmd_dsm		= 0x09,
    nvme_cmd_verify		= 0x0c,
    nvme_cmd_resv_register	= 0x0d,
    nvme_cmd_resv_report	= 0x0e,
    nvme_cmd_resv_acquire	= 0x11,
    nvme_cmd_resv_release	= 0x15,
    nvme_cmd_zone_mgmt_send	= 0x79,
    nvme_cmd_zone_mgmt_recv	= 0x7a,
    nvme_cmd_zone_append	= 0x7d,
    nvme_cmd_vendor_start	= 0x80,
};

封装读写命令:
static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns, struct request *req, struct nvme_command *cmnd, enum nvme_opcode op)
    ...
    cmnd->rw.opcode = op;
    cmnd->rw.slba = cpu_to_le64(nvme_sect_to_lba(ns, blk_rq_pos(req)))
    cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1)
    ...

----- nvme end -----


blk:
Software Staging Queue 會以 struct blk_mq_ctx 這個結構來表示
Hardware Dispatch Queue 會以 struct blk_mq_hw_ctx 這個結構來表示

struct blk_mq_hw_ctx {
	dispatch -> @dispatch：用于已准备好发送到硬件但由于某种原因（例如缺乏资源）无法发送到硬件的请求。一旦驱动程序可以发送新请求，此列表中的请求将首先发送，以实现更公平的调度
	...
	queue_num: Index of this hardware queue

}



blk_rq_timed_out_timer
綜合以上，當 blk_mq_start_request 被呼叫而開始一個 request 的處理流程。blk_mq_start_request 會執行 blk_add_timer -> mod_timer。藉此讓之前提到的 timer task blk_rq_timed_out_timer 在一段時間後被執行。而這個 timer task 最後就會啟動 &q->timeout_work 下註冊的 blk_mq_timeout_work，後者最後可以透過 blk_mq_check_expired -> blk_mq_rq_timed_out 去觸發 timeout 方法。
而 NVMe driver 中定義的 timeout 方法為 nvme_timeout。不過由於其本身內容與 blk-mq 自身機制關係較少，比較多是與 NVMe 相關，細節我們就不在此章節釐清


nvme完成io:
queue_request_irq
static irqreturn_t nvme_irq(int irq, void *data)
    nvme_poll_cq nvme_poll
        while (nvme_cqe_pending(nvmeq))
            nvme_handle_cqe(nvmeq, iob, nvmeq->cq_head)
                nvme_pci_complete_rq(req)
            nvme_update_cq_head(nvmeq)
        nvme_ring_cq_doorbell(nvmeq) -> 敲环形完成队列的门铃
            nvme_dbbuf_update_and_check_event


nvme块设备:
nvme块设备驱动的注册位置在哪呢？居然在nvme_scan_work里面，它我们前面分析过，启动nvme设备的一堆work queue里面就有它。它内部的操作是遍历nvme设备的所有namespace，发现某个ns不存在就会用nvme_alloc_ns去注册一个块设备并加入到ns链表中，所以nvme设备的namespace才是一个块设备，而nvme设备是被视为了一个字符设备。这个块设备的内存结构为nvme_ns，操作集则是nvme_fops，一堆函数里面重点看看nvme_ioctl，这个函数和最开始注册的字符设备驱动的nvme_dev_ioctl的名称和内部实现都很像，看到这里我已经有点懵了，怎么整了两套ioctrl出来，实现也像。为什么要整两套出来，暂时没想通。但是两者还是有一点区别的，字符设备处理了NVME_IOCTL_ADMIN_CMD、NVME_IOCTL_IO_CMD、NVME_IOCTL_RESET、NVME_IOCTL_SUBSYS_RESET、NVME_IOCTL_RESCAN；而块设备处理了NVME_IOCTL_ADMIN_CMD、NVME_IOCTL_IO_CMD、NVME_IOCTL_ID、NVME_IOCTL_SUBMIT_IO。

3-1、块设备结构nvme_ns内部包含了request_queue,gendisk这两个块设备驱动必要成员。request_queue就用nvme_ctrl的tagset（是个指针，指向的是nvme_dev结构中的那个tagset）创建出来的，所以这里才去创建了nvme设备io请求的request queue哦。disk里面就会注册这个块设备的操作集struct block_device_operations nvme_fops，按照块设备的实现机制，这个操作集里面是不包含读写操作的，读写通过前面的queue来下发。而下发的实现就是通过nvme_submit_io这个函数来实现的，前面也已经提到过了。
3-2、NVME_IOCTL_SUBMIT_IO，NVME_IOCTL_ADMIN_CMD 这些命令宏是定义在公共头文件内的，所以用户态程序可见。猜测用户态下发命令请求，io请求的时候，应该都是通过用户态的ioctrl来实现的，连读写请求也是哦，因为仔细看nvme_submit_io这个函数内的实现，是将用户态的io请求参数拷贝到了内核空间，并且使用了里面的addr字段，表示数据所在的地址


nvme_sq_copy_cmd
static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
    nvmeq->last_sq_tail = nvmeq->sq_tail -> 然后向SQ的doorbell里写入tail信息

nvme_handle_cqe
    nvme_pci_complete_rq


intel e810,
commit: https://lore.kernel.org/all/20210520143809.819-7-shiraz.saleem@intel.com/T/
以下补丁系列介绍了适用于 X722 iWARP 设备的统一英特尔 RDMA (irdma) 以太网协议驱动程序以及支持 iWARP 和 RoCEv2 的新 E810 设备。 irdma 模块取代了 X722 的旧版 i40iw 模块，并扩展了已为 i40iw 定义的 ABI。 它向后兼容旧版 X722 rdma 核心提供程序 (libi40iw)。 X722 和 E810 是支持 RDMA 的 PCI 网络设备。 该父设备的 RDMA 块通过使用最近为 5.11 内核添加的核心辅助总线基础结构导出到“irdma”的辅助设备来表示。 父 PCI netdev 驱动程序“i40e”和“ice”使用封装的私有数据/操作注册辅助 RDMA 设备，这些数据/操作绑定到在 irdma 模块中注册的辅助驱动程序。 该补丁集最初作为 RFC 提交，我们在其中得到了反馈，提出了 RDMA 驱动程序附加到 netdev PCI 驱动程序 [1] 拥有的 PCI 设备的通用方案。 探索了使用平台总线和 MFD 的解决方案，但被社区拒绝，共识是添加新的总线基础设施来支持这种使用模型。 该系列的进一步修订以及辅助总线已提交[2]。 此时，Greg KH 要求我们将辅助总线审查和修订流程纳入内部邮件列表，并获得受人尊敬的内核贡献者的认可，以及包括 Nvidia 在内的所有主要利益相关者的共识（用于 mlx5 子功能使用） -案例）和英特尔声音驱动程序。 这个过程花了一段时间，并阻碍了该 netdev/irdma 系列的进一步开发/审查。 辅助总线最终在5.11中被合并。 在本次提交的 v1-->v2 和 v4-->v5 之间，IIDC 根据反馈进行了重大重写，我们希望它现在更符合社区的需求。 目前，E810 默认为 RoCEv2。 在未来的补丁中，将通过 devlink 提供对协议切换到 iWARP 的运行时支持。 该系列是针对 5.13-rc1 构建的，目前包含 netdev 补丁以方便查看。 这包括更新“ice”驱动程序以提供 RDMA 支持，并将“i40e”驱动程序转换为使用辅助总线基础设施。 一旦社区确认此提交，就可以提交共享拉取请求。 v5-->v6：*将 aux 设备名称从 <模块>.intel_rdma_<rdma 协议>.<num> 压缩为 <模块>.<rdma 协议>.<num> *修复 alloc_hw_stats 的驱动程序 API，仅导出端口统计信息 sysfs v4-->v5：*导出所有 IIDC 核心操作回调并从 irdma 直接调用。 irdma 依赖于 i40e 和ice。 *删除协议切换的 devlink 运行时选项。 E810 上默认为 RoCEv2。 将通过 [3] 中讨论的社区工作添加切换到 IWARP 的运行时选项 *导出 iidc_auxiliary_dev 中的ice_pf 指针，该指针在 irdma drv.probe() 中可用，并使用它派生 PCI 函数相关子字段。 *使用定义来设置辅助开发名称，而不是 kasprintf。 *删除 IIDC 中的所有未来配置。 删除 IIDC 中的多个辅助驱动程序支持。 *添加核心驱动程序的辅助操作回调以直接在 iidc_auxiliary_drv 对象中使用。 *修复ice中的auxdevice ida资源泄漏，并更新到i40e中最新的ida API。 *删除 IIDC 和 irdma 驱动程序中任何残留的 VF 残渣。 *简化 IIDC API 以添加和删除 RDMA qset。 删除 iidc_res_base 联合使用。 *使用直接调用转换 irdma 中的所有单一实现间接 .ops 回调 *添加 rsvd 仅用于 irdma ABI 中的对齐 *清理 iw_memreg_type enum v3-->v4： * 修复冰补丁中的 W=1 警告 * 修复由 用于创建用户 AH 和多播的 pyverbs * 修复在 v2 提交 v2-->v3 中移植到 FIELD_PREP 期间引入的快速寄存器的描述符集问题： * rebase rdma for-next。 适应核心更改'1fb7f8973f51（“RDMA：支持超过255个rdma端口”）' * irdma Kconfig更新以符合linux编码风格。 * 修复 0-day 构建问题 * 删除 rdma_resource_limits 选择器 devlink 参数。 按照 Parav 的建议提交补丁以使用 devlink 资源。 * Ice idc 中的缩写大写。 例如 'aux' 到 'AUX' v1-->v2： * 删除 IIDC 通道操作 - 打开、关闭、peer_register 和peer_unregister。 及其所有相关的FSM都在ice PCI核心驱动程序中。 * 在发出 IIDC ops 回调时，在ice PCI 核心驱动程序中使用 device_lock。 * 从共享 IIDC 标头中删除peer_* 措辞，并使用 iidc_core*/iidc_auxiliary* 重命名结构和通道操作。 * 在irdma gen2辅助驱动程序中开始时分配ib_device并在drv.probe()结束时注册它。 * 在大多数驱动程序中广泛使用 ibdev_* 打印删除 idev_to_dev、ihw_to_dev 宏，因为新打印方案中不再需要这些宏。 * 不要修改 ABI 版本。 irdma 为 6。 维护 irdma ABI 版本。 5 表示旧版 i40iw 用户-提供商兼容性。 * 在 irdma_alloc_ucontext 中添加边界检查，以防止与 < 4 用户空间提供程序版本的绑定失败。 * 从 irdma 中删除 devlink。 添加 2 个新的 rdma 相关内容devlink 参数添加到ice PCI 核心驱动程序中。 * 在描述符字段的获取/设置上使用FIELD_PREP/FIELD_GET/GENMASK，而不是自行开发的LS_*/RS_*。 * 在 irdma 中绑定 2 个独立的辅助驱动程序 - 一个用于第 1 代，一个用于第 2 代和未来的设备。 * 其他。 irdma 中的驱动程序修复 [1] https://patchwork.kernel.org/project/linux-rdma/patch/20190215171107.6464-2-shiraz.saleem@intel.com/ [2] https://lore.kernel.org/ linux-rdma/20200520070415.3392210-1-jeffrey.t.kirsher@intel.com/ [3] https://lore.kernel.org/linux-rdma/20210407224631.GI282464@nvidia.com/ Dave Ertman (4)：iidc ：引入iidc.hice：初始化RDMA支持ice：实现iidc操作ice：注册辅助设备以提供RDMA Michael J. Ruhl（1）：RDMA/irdma：为CM Mustafa Ismail添加动态跟踪（13）：RDMA/irdma： 注册辅助驱动程序并实现专用通道 OP RDMA/irdma：实现设备初始化定义 RDMA/irdma：实现硬件管理队列 OP RDMA/irdma：添加 HMC 后备存储设置函数 RDMA/irdma：添加特权 UDA 队列实现 RDMA/irdma：添加 QoS 定义 RDMA/irdma：添加连接管理器 RDMA/irdma：添加 PBLE 资源管理器 RDMA/irdma：实现设备支持的动词 API RDMA/irdma：添加 RoCEv2 UD OP 支持 RDMA/irdma：添加用户/内核共享库 RDMA/irdma：添加杂项 实用程序定义 RDMA/irdma：添加 ABI 定义 Shiraz Saleem (4)：i40e：为 aux 总线转换准备 i40e 标头 i40e：注册辅助设备以提供 RDMA RDMA/irdma：添加 irdma Kconfig/Makefile 并删除 i40iw RDMA/irdma：更新维护者 文件



intel e810, drivers/net/ethernet/intel/ice/ice_main.c
module_init(ice_module_init) -> entry, 
    pr_info("%s\n", ice_driver_string); -> Intel(R) Ethernet Connection E800 Series Linux Driver -> Copyright (c) 2018, Intel Corporation
    ice_adv_lnk_speed_maps_init -> ice：重构查找建议的链路速度，重构ice_get_link_ksettings以使用强制速度到链接模式映射 -> static struct ethtool_forced_speed_map qede_forced_speed_maps[] -> 标记内核的一部分为只读内容
        ethtool_forced_speed_maps_init(ice_adv_lnk_speed_maps,
            linkmode_set_bit_array -> static inline void linkmode_set_bit_array
    ice_wq = alloc_workqueue("%s", 0, 0, KBUILD_MODNAME) -> ice：初始化PF并设置杂项中断，该补丁继续初始化流程如下：1）在ice_pf实例中分配并初始化必要的字段（如vsi，num_alloc_vsi，irq_tracker等）。 2) 设置杂项中断处理程序。 这也称为“其他中断原因”(OIC) 处理程序，用于处理非热路径中断（如控制队列事件、链接事件、异常等）。 3) 实现后台任务来处理管理队列接收 (ARQ) 事件
    ice_lag_wq = alloc_ordered_workqueue("ice_lag_wq", 0) -> ice：添加对 LAG 固件更改的驱动程序支持，添加 SRIOV 的 LAG 固件支持的定义、字段和检测代码。 还公开了一些以前的静态函数以允许在滞后代码中进行访问。 清理未使用或 LAG 支持不需要的代码。 还添加一个有序工作队列来处理 LAG 事件
    ice_debugfs_init()
		ice_debugfs_root = debugfs_create_dir(KBUILD_MODNAME, NULL) -> echo <log level> > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/<module>
    status = pci_register_driver(&ice_driver) > ice_probe


static struct pci_driver ice_driver = {
	.name = KBUILD_MODNAME,
	.id_table = ice_pci_tbl,
	.probe = ice_probe,
	.remove = ice_remove,
#ifdef CONFIG_PM
	.driver.pm = &ice_pm_ops,
#endif /* CONFIG_PM */
	.shutdown = ice_shutdown,
	.sriov_configure = ice_sriov_configure,
        ice_check_sriov_allowed
            test_bit(ICE_FLAG_SRIOV_CAPABLE, pf->flags)
            ice_is_safe_mode(pf)
            ice_pf_state_is_nominal
                bitmap_set(check_bits, 0, ICE_STATE_NOMINAL_CHECK_BITS) -> 当检查 PF 是否处于正常操作状态时，需要检查列表开头分组的位。将检查ICE_STATE_NOMINAL_CHECK_BITS 之前的位。如果您需要添加一个位来考虑正常操作状态，则必须将其添加到 ICE_STATE_NOMINAL_CHECK_BITS 之前。未经适当考虑，请勿移动此条目的位置
                bitmap_intersects(pf->state, check_bits, ICE_STATE_NBITS) -> 求交集
        pci_vfs_assigned -> return guest vf num
        ice_pci_sriov_ena
            ice_ena_vfs(pf, num_vfs)
                pf->sriov_irq_bm = bitmap_zalloc(total_vectors, GFP_KERNEL)
                pci_enable_sriov(pf->pdev, num_vfs)
                ice_set_per_vf_res(pf, num_vfs)
                    ice_sriov_set_msix_res(pf, num_msix_per_vf * num_vfs)
                ice_create_vf_entries(pf, num_vfs)
                    ice_initialize_vf_entry(vf)
                        ice_mbx_init_vf_info(&pf->hw, &vf->mbx_info)
                            list_add(&vf_info->list_entry, &snap->mbx_vf)
                    ice_vc_set_default_allowlist
                ice_eswitch_reserve_cp_queues(pf, num_vfs)
                    pf->eswitch.qs.to_reach = pf->eswitch.qs.value + change
                ice_start_vfs(pf)
                    ice_for_each_vf(pf, bkt, vf)
                        retval = ice_init_vf_vsi_res(vf)
                            vf->first_vector_idx = ice_sriov_get_irqs(pf, vf->num_msix)
                            vsi = ice_vf_vsi_setup(vf)
                                vsi = ice_vsi_setup(pf, &params)
                                    vsi = ice_vsi_alloc(pf)
                                        vsi = devm_kzalloc(dev
                                        pf->next_vsi = ice_get_free_slot
                                    ice_vsi_cfg
                                        ice_vsi_cfg_def
                                            ice_vsi_alloc_def
                                        ice_vsi_cfg_tc_lan
                                            ice_cfg_vsi_lan
                                    if (!ice_is_safe_mode(pf) && vsi->type == ICE_VSI_PF) -> 添加交换机规则以丢弃所有 Tx 流控制帧，查找，来自 VSI 的类型为 ETHERTYPE，并限制恶意 VF 发送 PAUSE 或 PFC 帧。如果启用，FW 仍然可以发送 FC 帧。该规则为 PF VSI 添加一次以创建适当的配方，因为 VSI/VSI 列表被丢弃操作忽略...还添加规则以处理 LLDP Tx 数据包。需要丢弃 Tx LLDP 数据包，以便 VF 无法发送 LLDP 数据包来重新配置 HW 中的 DCB 设置
                                        ice_fltr_add_eth(vsi, ETH_P_PAUSE, ICE_FLTR_TX,ICE_DROP_PACKET)
                                        ice_cfg_sw_lldp(vsi, true, true)
                                    if (!vsi->agg_node)
                                        ice_set_agg_vsi(vsi)
                            ice_vf_init_host_cfg(vf, vsi)
                                eth_broadcast_addr
                                ice_fltr_add_mac
                                ice_vsi_apply_spoofchk
                        retval = ice_eswitch_attach(pf, vf)
                        ice_ena_vf_mappings(vf)
                        wr32(hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE)
                ice_irq_dynamic_ena(hw, NULL, NULL)
            set_bit(ICE_FLAG_SRIOV_ENA, pf->flags)
	.sriov_get_vf_total_msix = ice_sriov_get_vf_total_msix,
	.sriov_set_msix_vec_count = ice_sriov_set_msix_vec_count,
	.err_handler = &ice_pci_err_handler
};


intel, ice, eth, drivers/net/ethernet/intel/ice/ice_main.c
ice_probe(struct pci_dev *pdev, const struct pci_device_id __always_unused *ent)
    struct ice_pf *pf
    struct ice_hw *hw
    is_kdump_kernel -> 当在 kdump 内核下时，在启用设备之前启动重置，以清除任何挂起的 DMA 事务。 这些事务可能会导致某些系统在执行下面的 pcim_enable_device() 时进行机器检查
        pci_save_state
        pci_clear_master
        pcie_flr -> ice：在故障转储内核中首先重置 当系统在发生紧急情况后启动到故障转储内核时，ice 网络设备可能仍然有挂起的事务，这些事务可能会在设备重新启用时导致错误或机器检查。 这可以防止故障转储内核加载驱动程序或收集故障数据。 为了避免此问题，请在崩溃内核上启用之前，通过 PCIe 配置空间在 Ice 设备上执行功能级别重置 (FLR)。 这将清除所有未完成的事务并停止所有队列和中断。 恢复FLR后的配置空间，否则测试时发现驱动加载不成功。 以下序列导致原始问题： - 使用 modprobe ice 加载ice驱动程序 - 使用 2 个 VF 启用 SR-IOV：echo 2 > /sys/class/net/eth0/device/sriov_num_vfs - 使用 echo c > /proc 触发崩溃 /sysrq-trigger - 使用 modprobeice 再次加载 Ice 驱动程序（或让它自动加载） - 系统在 pcim_enable_device() 期间再次崩溃
        pci_restore_state
    pcim_enable_device -> devres 是一种资源管理机制, 类似于一种垃圾收集处理器. 而资源的处理时机在 driver 的 install / remove 时候. 这样我们在为 device 分配相关资源之后, 就不必要关心如何释放它们了. 与 device 相关的资源有 memory / dma / iomap / regmap / interrupt / gpio 等, 这些资源都可以用 devres 机制管理起来, 使用相关资源封闭的 devres 接口, 就可以让这些资源自动销毁, 设备资源管理: https://blog.csdn.net/tiantianhaoxinqing__/article/details/125959030
    err = pcim_iomap_regions(pdev, BIT(ICE_BAR0), dev_driver_string(dev))
    ice_allocate_pf -> 分配物理方法, 为该设备分配一个 devlink 实例，并将私有区域作为 PF 结构返回。 通过添加一个操作在展开时将其删除，可以通过 devres 来跟踪 devlink 内存
        devlink_alloc(&ice_devlink_ops
        devm_add_action_or_reset(dev, ice_devlink_free, devlink)
    dma_set_mask_and_coherent
    pci_set_master
    ...
    pcim_iomap_table
    pci_read_config_byte PCI_REVISION_ID
    ice_set_ctrlq_len -> 设置硬件规格/参数
		hw->adminq.num_rq_entries = ICE_AQ_LEN;
		hw->adminq.num_sq_entries = ICE_AQ_LEN;
		hw->adminq.rq_buf_size = ICE_AQ_MAX_BUF_LEN;
		hw->adminq.sq_buf_size = ICE_AQ_MAX_BUF_LEN;
		hw->mailboxq.num_rq_entries = PF_MBX_ARQLEN_ARQLEN_M;
		hw->mailboxq.num_sq_entries = ICE_MBXSQ_LEN;
		hw->mailboxq.rq_buf_size = ICE_MBXQ_MAX_BUF_LEN;
		hw->mailboxq.sq_buf_size = ICE_MBXQ_MAX_BUF_LEN;
		hw->sbq.num_rq_entries = ICE_SBQ_LEN;
		hw->sbq.num_sq_entries = ICE_SBQ_LEN;
		hw->sbq.rq_buf_size = ICE_SBQ_MAX_BUF_LEN;
		hw->sbq.sq_buf_size = ICE_SBQ_MAX_BUF_LEN;
    netif_msg_init -> static inline u32 netif_msg_init
    err = ice_init(pf)
        ice_init_dev
            ice_init_hw
                ice_set_mac_type
                FIELD_GET(PF_FUNC_RID_FUNC_NUM_M, rd32(hw, PF_FUNC_RID)) -> 读寄存器
                ice_reset
                    wr32(hw, GLGEN_RTRIG, val)
                    ice_check_reset
                ice_get_itr_intrl_gran -> 带宽
                ice_create_all_ctrlq
                    ice_init_all_ctrlq
                        ice_init_ctrlq(hw, ICE_CTL_Q_ADMIN)
                            ice_adminq_init_regs
                                ICE_CQ_INIT_REGS
                        ice_init_check_adminq
                            ice_aq_get_fw_ver
                                ice_fill_dflt_direct_cmd_desc ice_aqc_opc_get_ver
                                ice_aq_send_cmd
                                    ice_sq_send_cmd_retry
                                        ice_sq_send_cmd -> ATQ
                        ice_shutdown_ctrlq
                            ice_shutdown_sq
                                停止管理队列上的任务
                                ICE_FREE_CQ_BUFS
                                ice_free_cq_ring
                                    dmam_free_coherent
                            ice_shutdown_rq
                        ...
                        ice_init_ctrlq(hw, ICE_CTL_Q_SB)
                        ice_init_ctrlq(hw, ICE_CTL_Q_MAILBOX)
                ice_fwlog_init -> Ice：配置固件日志记录，用户希望能够通过从 E8xx 设备检索固件日志来调试固件问题。 使用 debugfs 允许用户配置固件日志记录的日志级别和消息数量。 如果 E8xx 支持固件日志记录，则将在ice 驱动程序的 PCI 设备 ID 下创建文件“fwlog”。 如果该文件不存在，则 E8xx 不支持固件日志记录或系统上未启用 debugfs。 用户想要做的一件事是控制报告哪些事件。 用户可以读写“fwlog/modules/<模块名称>”来获取/设置日志级别。 固件中的每个模块都支持将 ht 记录为“fwlog/modules”下的文件，支持读取（查看当前日志级别是什么）和写入（更改日志级别）
                    ice_fwlog_set_supported
                        ice_aqc_opc_fw_logs_query
                    ice_fwlog_supported
                    ice_fwlog_get
                    ice_fwlog_alloc_ring_buffs
                    ice_debugfs_fwlog_init -> 创建debugfs所需的目录和文件, 关联相应的函数操作
                        debugfs_create_dir(name, ice_debugfs_root)
                        ...
                ice_clear_pf_cfg -> 清除任何现有 PF 配置（VSI、VSI 列表、交换机规则、端口配置、流量导向器过滤器等）
                    cmd ice_aqc_opc_clear_pf_cfg
                wr32(hw, PFQF_FD_ENA, PFQF_FD_ENA_FD_ENA_M) -> 启用流过滤, Ice：初始化FlowDirector资源，FlowDirector允许基于ntuple规则进行重定向。 规则使用 ethtool set-ntuple 接口进行编程。 支持的操作包括重定向到队列和丢弃。 设置初始框架来处理 Flow Director 过滤器。 创建和分配资源来管理过滤器并将过滤器编程到硬件。 过滤器通过边带接口进行处理； 创建控制VSI来管理通过边带的通信和处理请求。 分配资源后，更新硬件表以接受完美的过滤器
                ice_clear_pxe_mode
                ice_init_nvm
                    无论 NVM 编程模式如何，都会存储 SR 大小，因为工厂生产线中可能会使用空白模式
                    ice_discover_flash_size -> ice：创建flash_info结构和单独的NVM版本，ice_nvm_info结构已经成为与flash版本相关的所有字段的垃圾场。 它包含 NVM 版本和 EETRACK id、OptionROM 信息结构、闪存大小、ShadowRAM 大小等。 未来的更改将添加从非活动 NVM 组读取 NVM 版本和 EETRACK ID 的功能。 为了使这更简单，将这些 NVM 版本信息字段提取到它们自己的结构中非常有用。 将ice_nvm_info重命名为ice_flash_info，并创建一个单独的ice_nvm_info结构，其中将包含eetrack和NVM映射版本。 将netlist_ver结构移动到ice_flash_info中，并重命名为ice_netlist_info以保持一致性。 修改静态ice_get_orom_ver_info以将option rom结构作为指针。 这使得硬件结构的哪一部分被修改更加明显。 对ice_get_netlist_ver_info 执行相同操作。 引入一个新的ice_get_nvm_ver_info函数，该函数将类似于ice_get_orom_ver_info和ice_get_netlist_ver_info，用于保持NVM版本提取代码共置
                        ice_acquire_nvm
                        ice_read_flat_nvm
                    ice_determine_active_flash_banks -> ice：缓存 NVM 模块库信息，ice flash 包含 NVM、Option ROM 和 Netlist 模块各两个副本。 每个存储体都有一个指针字和一个大小字。 为了正确地从活动闪存组中读取数据，驱动程序必须手动计算偏移量。 在 NVM 初始化期间，读取 Shadow RAM 控制字并确定每个 NVM 模块的哪个存储体处于活动状态。 此外，缓存大小和指针值以用于计算正确的偏移量
                    读取 Shadow RAM 控制字并确定 NVM、OROM 和网表模块的哪些存储体处于活动状态。 还读取并计算关联的指针和大小。 然后将这些值缓存到ice_flash_info结构中以供以后使用，以便计算从活动模块读取的正确偏移量
                        ice_read_sr_word
                    ice_get_nvm_ver_info -> ice：引入从闪存模块读取的功能，当从设备的闪存读取时，ice驱动程序有两个可用的接口。 首先，它可以通过允许指定模块 ID 的固件使用中介接口。 这允许从活动闪存组的特定模块读取。 第二个可用的接口是执行平面读取。 这允许对整个闪存的完全访问。 然而，使用它需要软件来处理计算模块位置并解释指针地址。 虽然大多数所需数据都可以通过方便的第一个接口访问，但某些闪存内容却不能。 这包括与选项 ROM 和 NVM 存储体相关的 CSS 标头信息，以及对用作执行闪存更新的暂存空间的“非活动”存储体的任何访问。 为了访问所有相关的闪存内容，软件必须使用平面读取。 不是强制所有流执行平面读取计算，而是引入一个用于从闪存读取的新抽象：ice_read_flash_module。 该函数提供了从请求模块的活动或非活动闪存库中读取数据的抽象。 该接口与通过固件提供的抽象非常相似，但允许访问附加模块，并提供请求访问两个闪存组的机制。 乍一看，这种抽象允许精确指定调用者希望读取的银行（第一或第二）可能是有意义的。 这实现起来更简单，但使用起来更困难。 实际上，大多数呼叫者只知道他们想要活跃的银行还是不活跃的银行。 不是强迫调用者自己确定从哪个存储体读取，而是根据“活动”与“非活动”来实现ice_read_flash_module。 这显着简化了调用者级别的实现，并且是对闪存内容的更有用的抽象。 利用这个新接口重构主要 NVM 版本信息的读取。 不使用固件中介的 ShadowRAM 函数，而是使用ice_read_flash_module 抽象。 为此，请注意 NVM 的大多数读取都将以 2 字节字块的形式进行。 为了简化这种情况下ice_read_flash_module的使用，引入了ice_read_nvm_module。 这是ice_read_flash_module的一个简单包装，它获取NVM存储体的正确指针地址，并将2字节字格式强制传递给调用者。 读取 NVM 版本时，某些字段是从 Shadow RAM 中读取的。 Shadow RAM 是闪存的第一个 64KB，在设备加载期间填充。 大多数字段是从活动 NVM 组内的部分复制的。 为了从活动和非活动 NVM 存储体中读取这些数据，我们需要的不是从闪存的前 64KB 读取，而是从正确的偏移量读取到 NVM 存储体中。 为此引入ice_read_nvm_sr_copy。 该函数包装了ice_read_nvm_module，并具有与ice_read_sr_word相同的接口，不同之处在于允许调用者指定是否读取活动或非活动闪存组。 通过此更改，现在可以轻松重构ice_get_nvm_ver_info以使用软件介导的ice_read_flash_module接口进行读取，而不是依赖于固件介导的接口。 这将在以下更改中使用，以实现对 devlink 信息报告中存储版本的支持。 此外，将使用和扩展整个ice_read_flash_module接口以支持所有三个主要闪存组，并另外支持读取闪存映像安全修订信息
                    ice_get_orom_ver_info -> ice：通过devlink信息显示存储的UNDI固件版本，就像我们最近添加了对其他存储的固件闪存版本的支持一样，支持通过devlink信息显示存储的UNDI选项ROM版本。 为此，我们需要引入一个新的ice_get_inactive_orom_ver函数。 这比其他 Flash 版本有点棘手。 选项 ROM 版本数据是从 NVM 保留字段区域的特殊“引导配置”块中读取的。 该块仅包含*活动*选项 ROM 版本数据。 当设备固件完成更新选项 ROM 时，它会被填充。 此方法在读取存储的选项 ROM 版本数据时无效。 不用从闪存的这一部分读取，而是用从选项 ROM 二进制文件中查找组合版本信息的版本提取替换此版本提取。 该数据以简单的结构化格式存储在选项 ROM 中，偏移量为 512 字节。 该结构使用简单的模 256 校验和进行完整性验证。 扫描选项 ROM 以找到 CIVD 数据部分，并提取 Combo 版本。 重构ice_get_orom_ver_info，使其采用bank select枚举参数。 使用它来实现ice_get_inactive_orom_ver。 尽管所有ice器件在NVM PFA中都有引导配置块，但并非所有器件都有有效的选项ROM。 在这种情况下，旧的ice_get_orom_ver_info将“成功”，但报告全零的版本。 新的实现将无法在选项 ROM 中找到 $CIV 部分并报告错误。 因此，我们必须确保如果ice_get_orom_ver_info失败，ice_init_nvm不会失败。 使用新的ice_get_inactive_orom_ver允许通过devlink信息报告待更新的选项ROM版本
                    ice_get_netlist_info
                        ice_read_netlist_module
                            ice_read_flash_module -> Ice：通过devlink info显示存储的网表版本，添加读取非活动网表库以获取版本信息的功能。 为了支持这一点，重构我们读取网表版本数据的方式。 不使用带有模块 ID 的固件 AQ 接口，而是使用ice_read_flash_module 从闪存中读取作为平面 NVM。 此更改需要对所使用的偏移值进行轻微调整，因为从平面 NVM 读取包括类型字段（之前已被固件剥离）。 清理宏名称并将它们移动到ice_type.h。 为了清楚地说明我们如何计算偏移量，并使程序员可以轻松地将偏移值映射到数据表，请使用包装宏来考虑偏移量调整。 使用新添加的ice_get_inactive_netlist_ver函数从待处理的网表模块更新中提取版本数据。 将存储的“fw.netlist”和“fw.netlist.build”变体添加到信息版本映射数组中。 通过此更改，我们现在将“fw.netlist”和“fw.netlist.build”版本报告到 devlink 信息报告的存储部分。 与主要 NVM 模块版本一样，如果没有挂起的更新，我们会报告存储的当前活动值
                                ice_get_flash_bank_offset
                                ice_acquire_nvm
                                ice_read_flat_nvm
                ice_get_caps -> ice：获取交换机配置、调度程序配置和设备功能，此补丁通过获取交换机配置、调度程序配置和设备功能来添加初始化流程。 交换机配置：启动时，会在每个物理功能的固件中创建一个 L2 交换机元素。 每个物理功能还映射到其交换元件所连接的端口。 换句话说，该交换机可以被视为嵌入式 vSwitch，可以将物理功能的虚拟站接口 (VSI) 连接到出口/入口端口。 最终将创建出口/入口过滤器并将其应用到该开关元件上。 作为初始化流程的一部分，驱动程序从该开关元件获取配置数据并存储它。 调度程序配置：Tx 调度程序是负责设置和实施 QoS 的子系统。 作为初始化流程的一部分，驱动程序查询并存储给定物理功能的默认调度程序配置。 设备功能：作为初始化的一部分，驱动程序必须确定设备的功能（例如最大队列、VSI 等）。 该信息从固件获取并由驱动程序存储
                    ice_discover_dev_caps
                        ice_aq_list_caps
                        ice_parse_dev_caps
                            ice_parse_common_caps
                            ice_parse_valid_functions_cap
                            ice_parse_vsi_dev_caps
                            ice_parse_1588_dev_caps
                            ice_parse_fdir_dev_caps
                            ice_parse_sensor_reading_cap
                            ...
                    ice_discover_func_caps
                        ice_aq_list_caps
                        ice_parse_func_caps
                ice_get_initial_sw_cfg
                    ice_init_port_info
						pi->lport = (u8)(vsi_port_num & ICE_LPORT_MASK)
						pi->sw_id = swid;
						pi->pf_vf_num = pf_vf_num;
						pi->is_vf = is_vf;
                xa_init_flags(&hw->port_info->sched_node_ids, XA_FLAGS_ALLOC) -> ice：在ice_sched_node中引入新的参数，为了支持新的devlink-rate API，ice_sched_node结构需要存储一些额外的参数。 这包括 tx_max、tx_share、tx_weight 和 tx_priority。 将新字段添加到ice_sched_node结构中。 添加新功能以使用新参数配置硬件。 引入新的xarray来唯一标识节点
                ice_sched_query_res_alloc
                    hw->layer_info = devm_kmemdup -> 复制内存
                ice_sched_get_psm_clk_freq -> ice：使用 PSM 时钟频率来计算 RL 配置文件，核心时钟频率目前硬编码为 446 MHz，用于 RL 配置文件计算。 这会导致问题，因为并非所有设备都使用该时钟频率。 读取 GLGEN_CLKSTAT_SRC 寄存器以确定选择哪个 PSM 时钟频率。 这可确保速率限制器配置文件计算正确
                ice_sched_init_port
                    ice_sched_add_root_node
                    ice_sched_add_node
                ice_aq_get_phy_caps
                ice_aq_get_link_info
                ice_cfg_rl_burst_size
                ice_aq_manage_mac_read
                ice_aq_set_mac_cfg
                ice_alloc_fd_res_cntr
                ice_init_hw_tbls -> ice：初始化DDP包结构，添加函数来初始化、解析和清理表示DDP包的结构。 包下载完成后，读取DDP包内容并将其存储到这些结构中。 此配置用于识别默认行为，稍后用于更新 HW 表条目
					for (i = 0; i < ICE_BLK_COUNT; i++)
						ice_init_flow_profs(hw, i)
						...
            if ice_is_pf_c827
                ice_wait_for_fw
            ice_init_feature_support -> Ice：添加功能位图、帮助程序和 DSCP 检查，DSCP 又名 L3 QoS 仅在某些设备上受支持。 为了强制执行此操作，此补丁引入了功能位图和辅助函数。 功能位图是根据驱动程序初始化时的设备 ID 设置的。 目前，DSCP 是该位图中的唯一功能，但将来会有更多功能。 在 DCB netlink 流程中，在执行 DSCP 之前检查功能位是否已设置
                ice_set_feature_support
                ice_is_phy_rclk_in_netlist
                ice_is_cgu_in_netlist
                ice_gnss_is_gps_present -> Ice：添加管理命令以访问cgu配置，添加固件管理命令以访问时钟生成单元配置，需要在驱动程序中启用扩展PTP和SyncE功能。 添加与时钟生成单元和访问数据的功能相关的输入和输出引脚的可能硬件变化的定义
                ice：为E810T设备的GNSS模块添加TTY，添加新的ice_gnss.c文件用于保存基本的GNSS模块功能。 如果设备支持 GNSS 模块，请在适当的情况下调用新的ice_gnss_init 和ice_gnss_release 函数。 实现使用 TTY 设备从 GNSS 模块读取数据的基本功能。 添加I2C读取AQ命令。 现在需要通过 E810-T 适配器上的外部 I2C 端口扩展器来控制外部物理连接器。 未来的变化将引入写入功能
            ice_request_fw
            ice_init_pf
                timer_setup(&pf->serv_tmr, ice_service_timer, 0)
                INIT_WORK(&pf->serv_task, ice_service_task)
                ice_mbx_init_snapshot
            err = ice_init_interrupt_scheme(pf) -> 确定适当的中断方案, @pf：要初始化的板级私有结构
                vectors = ice_ena_msix_range(pf)
                    pf->num_rdma_msix = num_cpus + ICE_RDMA_NUM_AEQ_MSIX -> 4
                    v_actual = pci_alloc_irq_vectors(pf->pdev, ICE_MIN_MSIX, v_wanted, PCI_IRQ_MSIX) -> 分配中断向量
				ice_init_irq_tracker(pf, max_vectors, vectors)
            ice_req_irq_msix_misc -> ice_req_irq_msix_misc - 设置 misc 向量以处理非队列事件 @pf：板级私有结构 这将设置 MSIX 0 的处理程序，用于管理非队列中断，例如 AdminQ 和错误。在 MSI 或传统中断模式下不使用
			irq = ice_alloc_irq(pf, false) -> 为中断跟踪器保留一个中断, 用于混合中断, -> ice_alloc_irq - 分配新的中断向量 @pf：主板私有结构 @dyn_only：强制动态分配中断 为给定所有者 ID 分配新的中断向量。返回带有中断详细信息的 struct msi_map 并适当跟踪分配的中断。此函数从 irq_tracker 保留新的 irq 条目。如果根据跟踪器信息，所有使用 ice_pci_alloc_irq_vectors 分配的中断都已被使用，并且支持动态分配的中断，则将使用 pci_msix_alloc_irq_at 分配新中断。一些调用者可能只支持动态分配的中断。这用 dyn_only 标志表示。如果失败，则返回带有负 .index 的映射。调用者应该检查返回的映射索引
        err = ice_alloc_vsis(pf)
            pf->num_alloc_vsi = pf->hw.func_caps.guar_num_vsi
            pf->vsi = devm_kcalloc(dev, pf->num_alloc_vsi, sizeof(*pf->vsi)
            pf->vsi_stats = devm_kcalloc(dev, pf->num_alloc_vsi, sizeof(*pf->vsi_stats), GFP_KERNEL)
        ice_init_pf_sw
        ice_init_wakeup
        ice_init_link
        ice_send_version
        ice_verify_cacheline_size
        ice_set_safe_mode_vlan_cfg or
        pcie_print_link_status
        mod_timer(&pf->serv_tmr, round_jiffies(jiffies + pf->serv_tmr_period))
    ice_init_eth(struct ice_pf *pf)
        ice_get_main_vsi -> ice：添加 ice_get_main_vsi 来获取PF/main VSI，目前我们在多个地方使用 ice_find_vsi_by_type 来获取PF（又名主）VSI。 根据定义，PF VSI 始终是 pf->vsi 数组中的第一个元素（即 pf->vsi[0]）。 因此，添加并使用新的辅助函数ice_get_main_vsi，它只返回 pf->vsi[0]
        INIT_LIST_HEAD(&vsi->ch_list)
        ice_cfg_netdev(vsi)
            netdev = alloc_etherdev_mqs(sizeof(*np), vsi->alloc_txq, vsi->alloc_rxq)
            set_bit(ICE_VSI_NETDEV_ALLOCD, vsi->state); <- DECLARE_BITMAP(state, ICE_VSI_STATE_NBITS)
            ice_set_netdev_features(netdev) -> Ice：启用DDP包下载，尝试请求可选的特定于设备的DDP包文件（名称中带有PCIe设备序列号的文件，以便可以在不同的设备上使用不同的DDP包文件）。 如果可选包文件存在，请将其下载到设备中。 如果没有，请下载默认包文件。 根据 DDP 包文件是否存在以及尝试将其下载到设备的返回代码记录相应的消息。 如果下载失败并且设备上还没有软件包文件，请进入“安全模式”，其中某些功能不受支持
                bool is_dvm_ena = ice_is_dvm_ena(&pf->hw) -> ice：为 PF netdev 通告 802.1ad VLAN 过滤和卸载，为了使驱动程序支持 802.1ad VLAN 过滤和卸载，它需要通告这些 VLAN 功能并支持修改这些 VLAN 功能，因此对ice_set_netdev_features 进行必要的更改（ ）。 默认情况下，为单 VLAN 模式和双 VLAN 模式 (SVM/DVM) 启用 CTAG 插入/剥离和 CTAG 过滤。 另外，在 DVM 中，默认启用 STAG 过滤。 这是通过设置 netdev->features 中的功能位来完成的。 此外，在 DVM 中，支持切换 STAG 插入/剥离，但默认情况下不启用它们。 这是通过设置 netdev->hw_features 中的功能位来完成的。 由于 802.1ad VLAN 过滤和卸载仅在 DVM 中受支持，因此请确保默认情况下不启用它们，并且当设备处于 SVM 中时，它们无法在运行时启用。 添加 ndo_fix_features() 回调的实现。 这是必需的，因为硬件无法支持同时进行 VLAN 插入/剥离的多个 VLAN 以太网类型，并且必须同时启用或禁用所有支持的 VLAN 过滤。 启用 DVM 时，默认禁用内部 VLAN 剥离。 如果 VSI 支持在 DVM 中剥离内部 VLAN，则必须在运行时进行配置。 例如，如果在启用 DVM 的情况下在端口 VLAN 中配置 VF，则将允许卸载内部 VLAN -> ice_is_dvm_ena - 检查是否启用了双 VLAN 模式，@hw：指向 HW 结构的指针 设备在初始化时配置为单或双 VLAN 模式，并且在运行时不能动态更改。 基于此，驱动程序无需每次需要了解 VLAN 模式时都进行 AQ 调用。 相反，请使用缓存 VLAN 模式。 ice：支持将设备配置为双 VLAN 模式 为了支持将设备配置为双 VLAN 模式 (DVM)，DDP 和 FW 必须支持 DVM。 如果两者都支持DVM，则下载包的PF需要更新默认配方、设置VLAN模式并更新boost TCAM条目。 要支持更新 DVM 中的默认配方，请添加对更新现有交换机配方的 lkup_idx 和掩码的支持。 这是通过首先使用所需的配方 ID 调用获取配方 AQ (0x0292) 来完成的。 然后，如果成功，则更新查找索引之一 (lkup_idx) 及其关联的掩码（如果掩码有效），否则将使用已经存在的掩码。 在下载 DDP 时（特别是在下载 DDP 之后）保持全局配置锁定时，必须配置设备的 VLAN 模式。 如果支持，设备将默认为 DVM
                dflt_features
                csumo_features
                vlano_features
                tso_features
                netif_set_tso_max_size(netdev, ICE_MAX_TSO_SIZE) -> ice：在 IPv6 上添加对 BIG TCP 的支持，启用在ice驱动程序中使用通用 ipv6_hopopt_jumbo_remove 帮助程序在 IPv6 上发送 BIG TCP 数据包来剥离 HBH 标头。 测试：netperf -t TCP_RR -H 2001:db8:0:f101::1 -- -r80000,80000 -O MIN_LATENCY,P90_LATENCY,P99_LATENCY,TRANSACTION_RATE 在两种不同的设置上进行测试。 在这两种情况下，加载更改的驱动程序后都会应用以下设置： ip link set dev enp175s0f1np1 gso_max_size 130000 ip link set dev enp175s0f1np1 gro_max_size 130000 ip link set dev enp175s0f1np1 mtu 9000 首次设置：之前：最小第 90 个第 99 个事务延迟百分比百分率微秒 延迟 延迟 Tran/s 微秒 微秒 134 279 410 3961.584 之后：最小第 90 个第 99 个事务延迟百分位数 百分率微秒 延迟 延迟 Tran/s 微秒 微秒 135 178 216 6093.404 其他设置： 之前：最小第 90 个第 99 个事务延迟百分位数 百分率微秒延迟时间 Tran/s 微秒 微秒 218 414 478 2944.765 之后：最小第 90 个第 99 个事务延迟百分位数 百分率微秒延迟 延迟 Tran/s 微秒 微秒 146 238 266 4700.596 -> net：不允许用户空间解除设备限制，直到提交 46e6b992c250（“rtnetlink：允许在设备创建时设置 GSO 最大值”）设备的 gso_max_segs 和 gso_max_size 不受用户空间控制。 由于以下设置，引用的提交添加了控制它们的能力：netns A | netns B veth<->veth eth0 如果 eth0 有 TSO 限制，并且用户希望在 eth0 和 veth 之间有效转发流量，他们应该将 eth0 的 TSO 限制复制到 veth 上。 对于 macvlans 或 ipvlan 来说，这会自动发生，但 veth 用户就没那么幸运了（考虑到松散耦合）。 不幸的是，有问题的提交还允许用户覆盖真实硬件设备的限制。 控制最大 GSO 大小可能很有用，并且有人可能正在使用该功能（据我所知，没有任何用户），因此创建一组单独的旋钮来可靠地记录 TSO 限制。 验证用户请求
            ice_set_ops -> ice：使用xdp multi-buff更新xdp_features，现在ice驱动程序支持xdp multi-buffer，因此将其添加到xdp_features。 设置 xdp_features 标志之前检查 vsi 类型
                if (ice_is_safe_mode(pf))
                    netdev->netdev_ops = &ice_netdev_safe_mode_ops
                    netdev->ethtool_ops = &ice_ethtool_safe_mode_ops
                netdev->netdev_ops = &ice_netdev_ops
                netdev->udp_tunnel_nic_info = &pf->hw.udp_tunnel_nic -> ice：转换为新的udp_tunnel基础设施，将ice转换为新的基础设施，使用共享端口表。 比平时多做一点错误检查，因为这个驱动程序确实有很多魔力。 我们需要计算固件保留的 VxLAN 和 GENEVE 条目的数量。 由于转换，驱动程序将不再休眠在原子部分
                netdev->xdp_metadata_ops = &ice_xdp_md_ops -> ice：支持HW时间戳提示，使用之前重构的代码并创建一个允许XDP代码读取HW时间戳的函数。 另外，引入数据包上下文，其中将存储与提示相关的数据。 ice_xdp_buff 仅包含指向该结构的指针，以避免在本系列后面的 ZC 模式中复制它。 HW时间戳是驱动程序中第一个支持的提示，因此还要添加xdp_metadata_ops
                ice_set_ethtool_ops(netdev)
                    netdev->ethtool_ops = &ice_ethtool_ops -> Ice：添加统计信息和ethtool支持，该补丁实现了看门狗任务以从设备获取数据包统计信息。 此补丁还添加了对以下 ethtool 操作的支持： ethtool devname ethtool -s devname [msglvl N] [msglevel type on|off] ethtool -g|--show-ring devname ethtool -G|--set-ring devname [rx N] [tx N] ethtool -i|--driver devname ethtool -d|--register-dump devname [raw on|off] [hex on|off] [文件名] ethtool -k|--show-features| --show-offload devname ethtool -K|--features|--offload devname 功能 on|off ethtool -P|--show-permaddr devname ethtool -S|--statistics devname ethtool -a|--show-pause devname ethtool -A|--pause devname [autoneg on|off] [rx on|off] [tx on|off] ethtool -r|--negotiate devname
            ether_addr_copy(mac_addr, vsi->port_info->mac.perm_addr)
            eth_hw_addr_set(netdev, mac_addr)
            ice_vsi_cfg_netdev_tc(vsi, vsi->tc_cfg.ena_tc) -> Setup netdev TC information
            netdev->max_mtu = ICE_MAX_MTU -> (9728 - (14 + 4 + (VLAN_HLEN * 2)))
        ice_dcbnl_setup -> Ice：实现DCBNL支持，实现DCBNL子系统的接口层。 这些是支持 dcbnl_rtnl_ops 结构中定义的回调的函数。 这些回调将用于与设备的 DCB 设置进行交互。 dcb_nl集合函数的实现和支持SW DCB函数 -> Intel® Ethernet Data Center Bridging (DCB) Service (iSCSI) FAQ -> 适用于 iSCSI 的 ntel® 以太网数据中心桥接 (DCB) 服务与 Microsoft Server 2012* 本机 NIC 组合不兼容，适用于 iSCSI 的英特尔以太网 DCB 服务与 Microsoft Server 2012 NIC 组合（也称为负载平衡/故障转移 (LBFO)）不兼容。 安装适用于 iSCSI 的英特尔以太网 DCB 服务时，请勿使用英特尔® 以太网 10 GB 端口创建 LBFO 组。 如果英特尔以太网 10 GB 端口是 LBFO 团队的一部分，请勿为 iSCSI 安装英特尔以太网 DCB 服务。 如果在同一端口上使用 iSCSI 和 LBFO 的英特尔以太网 DCB 服务，可能会出现安装失败和持续链路丢失的情况
            netdev->dcbnl_ops = &dcbnl_ops
            ice_dcbnl_set_all(vsi)
        ice_init_mac_fltr
        ice_devlink_create_pf_port
            ice_devlink_set_port_split_options
            ice_devlink_set_switch_id -> Ice：获取 switchdev 设备上的交换机 id，驱动程序上每个网络设备的交换机 id 应该相同。 同一系统上的设备之间的 id 必须唯一，但不同系统上的设备之间不需要唯一。 交换机 ID 用于定位交换机上的端口并了解聚合端口是否属于同一交换机。 为了满足此要求，请使用 pci_get_dsn 作为交换机 ID 值，因为这是同一系统上每个设备的唯一值。 kubernetes 的自动工具需要实现 switch id。 通过设置 devlink 端口属性并在创建 pf（用于上行链路）和 vf（用于表示器）devlink 端口时调用 devlink_port_attrs_set 来设置交换机 id。 获取交换机 ID（在 switchdev 模式下）： cat /sys/class/net/$PF0/phys_switch_id
            devlink_port_register_with_ops
            devlink_port_register_with_ops(devlink, devlink_port, vsi->idx, &ice_devlink_port_ops)
        ice_register_netdev
            netif_carrier_off(vsi->netdev)
            netif_tx_stop_all_queues(vsi->netdev) -> Ice：延迟netdev注册，一旦注册了netdev，相应的网络接口就可以立即被用户空间实用程序（例如NetworkManager）使用。 如果驱动程序在技术上尚未完全启动，这可能会出现问题。 将 netdev 注册移动到探测器的末尾，因为此时驱动程序数据结构和设备将按预期初始化。 但是，延迟 netdev 注册会导致 aRFS 流程失败，其中检查 netdev->reg_state == NETREG_REGISTERED 条件。 目前尚不清楚为什么要添加此检查，因此请将其删除。 本地测试并未表明此更改有任何问题。 ice_open 中的状态位检查是作为权宜之计，以防止过早的接口启动操作。 不再需要它，因此将其删除
                for (i = 0; i < dev->num_tx_queues; i++)
                    netif_tx_stop_queue(txq)
        ice_tc_indir_block_register -> ice：支持间接通知，实现间接通知机制以支持在隧道设备上卸载TC规则。 将间接阻止列表保留在 netdev priv 中。 通知会调用设置tc cls的花函数。 目前我们只能卸载入口类型。 其他流块活页夹不支持返回
            flow_indr_dev_register(ice_indr_setup_tc_cb, np) -> net: flow_offload：整合间接 flow_block 基础设施，隧道设备不提供 dev->netdev_ops->ndo_setup_tc(...) 接口。 隧道设备和路由控制平面没有提供将隧道和物理设备关联起来的明显方式。 该补丁允许驱动程序通过 flow_indr_dev_register() 和 flow_indr_dev_unregister() 为 tc 和 netfilter 前端注册隧道设备卸载处理程序。 前端调用 flow_indr_dev_setup_offload() 来迭代提供隧道设备硬件卸载支持的驱动程序列表，并为此隧道设备设置流块。 如果删除驱动程序模块，则间接 flow_block 最终会出现陈旧的回调引用。 模块删除路径触发 dev_shutdown() 路径来删除物理设备的 qdisc 和 flow_blocks。 然而，这对于隧道设备来说没有用，因为物理设备和隧道设备之间的关系并不明确。 此补丁引入了一个清理回调，当删除驱动程序模块以清理隧道设备 flow_block 时会调用该回调。 该补丁定义了 struct flow_block_indr 并使用 flow_block_cb 中的它来存储前端在模块删除时执行 flow_block_cb 清理所需的信息
        ice_napi_add
            ice_for_each_q_vector(vsi, v_idx)
                netif_napi_add(vsi->netdev, &vsi->q_vectors[v_idx]->napi, ice_napi_poll) -> NAPI polling Rx/Tx cleanup routine
                    ice_for_each_tx_ring(tx_ring, q_vector->tx)
                        wd = ice_xmit_zc(tx_ring)
                        or wd = ice_clean_tx_irq(tx_ring, budget)
                        ...
                ice_q_vector_set_napi_queues(vsi->q_vectors[v_idx], false)
    err = ice_init_rdma(pf)
        ice_is_rdma_ena(pf)
            return test_bit(ICE_FLAG_RDMA_ENA, pf->flags) -> Check RDMA ENABLE/DISABLE
        ret = ice_alloc_rdma_qvectors(pf) -> ice：添加单独的中断分配，目前中断分配，根据某个特性是批量分配的。 此外，分配后还有一系列操作，通过该批中断分配每个 irq 设置。 尽管驱动程序尚不支持动态中断分配，但将分配的中断保留在池中并添加分配抽象逻辑以使代码更加灵活。 将每个中断信息保留在 ice_q_vector 结构中，这会产生ice_vsi::base_vector冗余。 此外，因此有一些功能可以删除
            pf->msix_entries = kcalloc(pf->num_rdma_msix, sizeof(*pf->msix_entries), GFP_KERNEL)
			for (i = 0; i < pf->num_rdma_msix; i++)
				struct msix_entry *entry = &pf->msix_entries[i]
				struct msi_map map
				map = ice_alloc_irq(pf, false) -> ice：添加动态中断分配，目前驱动程序只能在init阶段通过调用pci_alloc_irq_vectors分配中断向量。 对此进行更改并使用新的 pci_msix_alloc_irq_at/pci_msix_free_irq API，并在启用 MSIX 后启用分配和释放更多中断。 由于并非所有平台都支持动态分配，请使用 pci_msix_can_alloc_dyn 检查。 扩展跟踪器以跟踪最初分配的中断数量，因此当所有此类向量都已使用时，会自动动态分配其他中断。 记住每个中断分配方法，然后适当地释放。 由于某些功能可能需要动态分配的中断，因此添加适当的 VSI 标志并在分配新中断时将其考虑在内
				为给定所有者 ID 分配新的中断向量。 返回包含中断详细信息的 struct msi_map 并适当跟踪分配的中断。 该函数从 irq_tracker 保留新的 irq 条目。 如果根据跟踪器信息，使用ice_pci_alloc_irq_vectors分配的所有中断都已使用并且支持动态分配的中断，则将使用pci_msix_alloc_irq_at分配新中断。 一些调用者可能只支持动态分配的中断。 这由 dyn_only 标志指示。 失败时，返回 .index 为负的映射。 调用者应该检查返回的map索引
					struct ice_irq_entry *entry
					entry = ice_get_irq_res(pf, dyn_only)
						entry = kzalloc(sizeof(*entry), GFP_KERNEL)
						ret = xa_alloc(&pf->irq_tracker.entries, &index, entry, limit, GFP_KERNEL)
						entry->index = index
						entry->dynamic = index >= num_static
					pci_msix_can_alloc_dyn
					pci_msix_alloc_irq_at or
					map.index = entry->index
					map.virq = pci_irq_vector(pf->pdev, map.index)
				entry->entry = map.index;
				entry->vector = map.virq
        ice_plug_aux_dev -> 在每个 PCIe 设备功能的辅助总线上注册ice客户端辅助 RDMA 设备，以便辅助驱动程序 (irdma) 附加到。 它允许实现单个 RDMA 驱动程序 (irdma)，该驱动程序能够通过支持 RDMA 的多代 Intel 硬件与多个 netdev 驱动程序配合使用。 ice 和 irdma 之间不存在加载顺序依赖性
            auxiliary_device_init
            auxiliary_device_add
    ice_init_devlink
    ice_init_features
        ice_hwmon_init
            hdev = hwmon_device_register_with_info(dev, "ice", pf, &ice_chip_info,



intel, eth, ice, send msg:
ice_start_xmit
    tx_ring = vsi->tx_rings[skb->queue_mapping]
    skb_put_padto(skb, ICE_MIN_TX_LEN) -> 填充缓冲区以确保尾随字节存在并被清空。 如果缓冲区已包含足够的数据，则不会更改。 否则延长。 成功时返回零。 skb 因错误而被释放。
    ice_xmit_frame_ring(skb, tx_ring) -> Sends buffer on Tx ring
        ice_trace(xmit_frame_ring, tx_ring, skb) -> Ice：添加跟踪点，此补丁仿照 Scott Peterson 为 i40e 开发的补丁。 通过新文件ice_trace.h 将跟踪点添加到驱动程序，并在驱动程序中有趣的位置添加一些新的跟踪调用。 添加一些 DIMLIB 跟踪以帮助调试中断调节问题。 性能不应受到影响，这对于将来调试和向路径添加新的跟踪事件非常有用。 注意 eBPF 程序可以附加到这些事件，并且 perf 可以对它们进行计数，因为我们附加到内核中的事件子系统
        count = ice_xmit_desc_count(skb) -> calculate number of Tx descriptors needed
            ice_txd_use_count
        if (ice_chk_linearize(skb, count)) -> Check if there are more than 8 fragments per packet
            __skb_linearize(skb)
        ice_maybe_stop_tx -> ice：修复 PF 驱动程序中的 tx_timeout，在此提交之前，当队列压力足够大时，驱动程序会遇到 tx_timeouts。 发生这种情况是因为硬件尾部和软件尾部 (NTU) 错误地不同步。 因此，这导致硬件头部与硬件尾部发生冲突，这对于硬件来说意味着为 Tx 发布的所有描述符都已被处理。 由于驱动器中使用的 Tx 逻辑，SW 尾部和 HW 尾部允许不同步。 这样做是为了优化，因为它允许驱动程序尽可能不频繁地写入 HW 尾部，同时仍然更新 SW 尾部索引以进行跟踪。 然而，在某些情况下，这会导致尾部永远不会更新，从而导致 Tx 超时。 Tx HW tail 写入条件： if (netif_xmit_stopped(txring_txq(tx_ring) || !skb->xmit_more) writel(sw_tail, tx_ring->tail); 在 Tx 逻辑中发现问题，导致上述更新硬件的条件 tail 永远不会发生，导致 tx_timeouts。在ice_xmit_frame_ring中，我们根据内核交给我们的skb计算Tx事务需要多少个描述符，然后将其与一些额外的填充一起传递到ice_maybe_stop_tx，以确定我们是否有足够的可用描述符。 如果我们不这样做，那么我们将-EBUSY返回到堆栈，否则我们继续并最终在ice_tx_map中相应地准备Tx描述符并设置next_to_watch。在ice_tx_map中，我们再次调用ice_maybe_stop_tx，其值为MAX_SKB_FRAGS + 4。 这里的关键是这个值可能小于我们在ice_xmit_frame_ring中第一次调用ice_maybe_stop_tx时发送的值现在，如果未使用的描述符的数量在MAX_SKB_FRAGS + 4和ice_xmit_frame_ring中第一次调用ice_maybe_stop_tx时使用的值之间。 由于上面的“Tx HW tail 写入条件”，我们不更新 HW tail。 这是因为在ice_maybe_stop_tx中，我们从ice_maybe_stop_tx返回成功，而不是调用__ice_maybe_stop_tx并随后调用netif_stop_subqueue，这会设置__QUEUE_STATE_DEV_XOFF位。 然后，通过调用 netif_xmit_stopped 在“Tx HW tail 写入条件”中检查该位，如果设置了上述位，则随后更新 HW tail。 在ice_clean_tx_irq中，如果next_to_watch不为NULL，我们最终会清理HW设置DD位的描述符，并且我们有预算。 根据上段的描述，HW 头最终将遇到 HW 尾部。 下次通过ice_xmit_frame_ring，我们使用堆栈中的另一个skb对ice_maybe_stop_tx进行初始调用。 这次我们没有足够的可用描述符，我们将 NETDEV_TX_BUSY 返回到堆栈并最终将 next_to_watch 设置为 NULL。 这就是我们被困住的地方。 在ice_clean_tx_irq中，我们从不清理任何东西，因为next_to_watch始终为NULL，而在ice_xmit_frame_ring中，我们从不更新HW尾部，因为我们已经将NETDEV_TX_BUSY返回到堆栈，最终我们遇到了tx_timeout。 通过确保ice_tx_map 中对ice_maybe_stop_tx 的第二次调用传递的值大于等于ice_xmit_frame_ring 中对ice_maybe_stop_tx 的初始调用时使用的值，已修复此问题。 这是通过添加以下定义来使逻辑更加清晰并减少再次混乱的机会： ICE_CACHE_LINE_BYTES 64 ICE_DESCS_PER_CACHE_LINE (ICE_CACHE_LINE_BYTES / \ sizeof(structice_tx_desc)) ICE_DESCS_FOR_CTX_DESC 1 ICE_DESCS_FOR_SKB_DATA_PTR 1 ICE_CACHE_LINE _BYTES 为 64 是一个假设 这样我们就不必在每次通过 Tx 路径时都弄清楚这一点。 相反，我在ice_probe 中添加了健全性检查，以验证缓存行大小并在不是 64 字节时打印一条消息。 如果从 GLPCI_CNF2 寄存器读取时缓存行大小不是 64 字节，则可以更轻松地提交问题
        netdev_txq_bql_enqueue_prefetchw -> prefetch bql data for write ->  -ice：使用预取方法，内核提供了一些预取机制来加速接收处理期间的普通冷缓存行访问。 由于这些是软件结构，因此策略性地放置预取会有所帮助。 请注意，仅对非 XDP 队列调用 BQL 预取完成
            prefetchw(&dev_queue->dql.num_queued)
        first = &tx_ring->tx_buf[tx_ring->next_to_use]
        ice_tx_prepare_vlan_flags(tx_ring, first)
        tso = ice_tso(first, &offload)
        csum = ice_tx_csum(first, &offload)
        ice_tstamp(tx_ring, skb, first, &offload)
        ice_tx_map(tx_ring, first, &offload) -> Build the Tx descriptor
            dma_map_single -> 线性区的 skb->data 做 dma 映射，得到 硬件可以读取操作 dma 地址
            tx_desc->cmd_type_offset_bsz = ice_build_ctob
            size = skb_frag_size(frag)
            skb_frag_dma_map
            skb_tx_timestamp(first->skb)
            wmb()
            first->next_to_watch = tx_desc
            ice_maybe_stop_tx(tx_ring, DESC_NEEDED)
            notify HW of packet
            kick = __netdev_tx_sent_queue(txring_txq(tx_ring), first->bytecount, netdev_xmit_more())
            writel(i, tx_ring->tail)


dpu driver:
drivers/net/ethernet/mellanox/mlx5/core/sf/devlink.c
mlx5_sf_add

$ devlink dev eswitch set pci/0000:06:00.0 mode switchdev

$ devlink port show
pci/0000:06:00.0/65535: type eth netdev ens2f0np0 flavour physical port 0 splittable false

$ devlink port add pci/0000:06:00.0 flavour pcisf pfnum 0 sfnum 88
pci/0000:06:00.0/32768: type eth netdev eth6 flavour pcisf controller 0 pfnum 0 sfnum 88 external false splittable false
function:
hw_addr 00:00:00:00:00:00 state inactive opstate detached

$ devlink port show ens2f0npf0sf88
pci/0000:06:00.0/32768: type eth netdev ens2f0npf0sf88 flavour pcisf controller 0 pfnum 0 sfnum 88 external false splittable false
function:
hw_addr 00:00:00:00:00:00 state inactive opstate detached



static int probe_one(struct pci_dev *pdev, const struct pci_device_id *id)
    devlink = mlx5_devlink_alloc(&pdev->dev)
        ida_alloc -> 其中ida_destory是释放所有和IDA关联的资源。IDA结构体是一个树状结构体，是内核工作的一个机制。　  这里先介绍一下IDR，IDR机制是内核中将一个整数ID号和指针关联在一起的机制。 如果使用数组进行索引，当ID号很大时，数组索引会占据大量的存储空间，如果使用链表，在总线上设备特别多的情况下，链表的查询效率不高。而IDR机制内部采用红黑树，可以很方便的将整数和指针关联起来，并且有很高的搜索效率。  IDA只是用来分配id，并不将某数据结构和id关联起来。 例如sd设备的设备名，如sda，驱动在生成设备文件的时候会向系统申请一个ida，也就是唯一id，然后把id映射成设备文件名。在nvme-core中有使用到ida，所以在最后中需要释放。static DEFINE_IDA(nvme_subsystems_ida);
    dev->priv.adev_idx = mlx5_adev_idx_alloc()
        ida_alloc(&mlx5_adev_ida, GFP_KERNEL)
    mlx5_mdev_init
        lockdep_register_key(&dev->lock_key) -> 当前lockdep实现的一个缺点是它需要静态分配锁密钥。 这会强制给定数据结构中出现的锁对象的所有实例共享一个锁密钥。 由于锁依赖分析将每个密钥的锁对象分组，共享锁密钥可能会导致误报 lockdep 报告。 通过允许动态分配锁密钥，可以避免此类误报。 要求在使用之前通过调用 lockdep_register_key() 注册动态分配的锁密钥。 抱怨尝试两次注册相同的锁密钥指针，而没有在连续的注册调用之间调用 lockdep_unregister_key() 。 跟踪所有动态密钥的新 lock_keys_hash[] 数据结构的目的有两个： - 验证 lockdep_register_key() 和 lockdep_unregister_key() 函数是否正确使用。 - 避免 lockdep_init_map() 在遇到动态分配的密钥时抱怨
        mutex_init(&priv->bfregs.reg_head.lock)
        INIT_LIST_HEAD(&priv->bfregs.reg_head.list)
        mlx5_cmd_init
            cmd->wq = create_singlethread_workqueue(cmd->wq_name)
                alloc_workqueue
            mlx5_cmdif_debugfs_init
                debugfs_create_dir("commands", dev->priv.dbg.dbg_root)
                ...
        mlx5_tout_init -> set default timeout
            tout_def_sw_val
        mlx5_health_init
        mlx5_pagealloc_init
            create_singlethread_workqueue("mlx5_page_allocator")
            xa_init(&dev->priv.page_root_xa)
            mlx5_pages_debugfs_init -> net/mlx5：添加页面 debugfs 添加页面 debugfs 以公开以下计数器以实现可调试性： fw_pages_total - 已向固件提供但尚未返回的页面数。 vfs_pages - 对于 SRIOV，为 FW 提供了多少页供虚拟功能使用。 host_pf_pages - 对于 ECPF，为外部主机物理功能使用提供给 FW 的页数。
        mlx5_adev_init -> 在新的虚拟总线下创建辅助设备。 这将取代定制的 mlx5 ->add()/->remove() 接口，下一个补丁将填充缺失的回调并删除旧的接口逻辑。 辅助驱动程序与设备的连接只能以一对一的方式进行，并且需要我们为每个协议创建设备，以便设备（模块）能够连接到它。
            kcalloc(ARRAY_SIZE(mlx5_adev_devices) -> 
        mlx5_hca_caps_alloc -> net/mlx5：分配单个功能当前 mlx5_core_dev 包含功能数组。 它包含设备的 19 个有效功能、2 个保留条目和 12 个孔。 因此，对于 14 个未使用的条目，mlx5_core_dev 分配了 14 * 8K = 112K 字节的从未使用过的内存。 由于这个 mlx5_core_dev 结构大小为 270Kbytes 奇数。 此分配进一步与 2 的下一个幂到 512Kbytes 对齐
            for (i = 0; i < ARRAY_SIZE(types); i++)
                kzalloc(sizeof(*cap)
        ida_alloc_range -> struct ida sw_vhca_ida
    mlx5_pci_init
        pci_set_drvdata
        dev->bar_addr = pci_resource_start(pdev, 0)
        mlx5_pci_enable_device
        request_bar
            pci_resource_flags(pdev, 0) & IORESOURCE_MEM
            pci_request_regions
        pci_set_master -> pci_set_master() 将通过设置 PCI_COMMAND 寄存器中的总线主控位来启用 DMA。 如果 BIOS 设置为虚假值，它还会修复延迟计时器值。 pci_clear_master() 将通过清除总线主控位来禁用 DMA
        set_dma_caps
        pci_enable_atomic_ops_to_root
        ...
        mlx5_pci_vsc_init
    mlx5_init_one
        mlx5_init_one_devl_locked -> net/mlx5：轻探测本地 SF 如果用户想要配置 SF，例如：仅使用 vdpa 功能，则他需要完全探测 SF，配置他想要的内容，然后重新加载 SF。 为了节省重新加载的时间，本地SF将在没有任何辅助子设备的情况下进行探测，从而可以在其完全探测之前对SF进行配置。 这些 SF 的 enable_* devlink 参数的默认值设置为 false
        vdpa: https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework, cx6 vdpa 虚拟io硬件卸载: https://wangzheng422.github.io/docker_env/notes/2021/2021.10.cx6dx.vdpa.offload.html
            mlx5_dev_is_lightweight
            mlx5_function_setup
                mlx5_function_enable -> net/mlx5：拆分 function_setup() 以启用和打开函数 mlx5_cmd_init_hca() 大约需要 0.2 秒。 如果用户希望禁用某些 SF aux 设备，例如对于大规模 1K SF，该用户将在 mlx5_cmd_init_hca() 上浪费超过 3 分钟的时间，而该阶段并不需要该功能。 下游补丁将更改通过 E-switch 进行探测的 SF、本地 SF，以便在没有任何辅助开发的情况下进行探测。 为了支持这一点，请拆分 function_setup() 以避免执行 mlx5_cmd_init_hca()
                    wait_fw_init
                    mlx5_cmd_enable
                        cmdif_rev
                        sema_init
                        dma_pool_create("mlx5_cmd", mlx5_core_dma_dev(dev), size, align, 0)
                        alloc_cmd_page
                            dma_alloc_coherent
                        iowrite32be
                        wmb()
                        create_msg_cache
                            for * 2
                                mlx5_alloc_cmd_msg
                                    mlx5_calc_cmd_blocks
                                    alloc_cmd_box
                                        mailbox->buf = dma_pool_zalloc
                                list_add_tail(&msg->list, &ch->head)
                        create_debugfs_files
                             debugfs_create_dir("cmd"
                    mlx5_tout_query_iseg -> net/mlx5：从 init 段读取超时值 用存储在固件 init 段中的值替换硬编码超时。 在驱动程序加载期间从 init 段读取超时。 如果不支持 init 段超时，则回退到硬编码默认值。 还将无法从固件读取的预初始化超时移至新机制
                        ...
                        tout_convert_reg_field_to_ms -> 时间寄存器由两个字段to_multiplier（超时乘数）和to_value（超时值）组成。 to_value 是时间单位的数量，to_multiplier 是类型，应该是这四个值之一
                            int_pow - 计算给定底数和指数的幂
                    wait_fw_init
                    mlx5_read_embedded_cpu
                    mlx5_cmd_set_state
                    mlx5_start_health_poll
                        timer_setup(&health->timer, poll_health, 0)
                            mlx5_health_check_fatal_sensors
                    mlx5_core_enable_hca
                        mlx5_cmd_exec_in(dev, enable_hca, in)
                            ...
                            cmd_exec
                                u16 opcode = in_to_opcode(in) -> const char *mlx5_command_str(int command) -> 命令转可读字符串
                                mlx5_cmd_invoke
                                    cmd_alloc_ent
                                    init_completion
                                    cb_timeout_handler
                                    cmd_work_handler -> Mellanox ConnectX-6-dx智能网卡 openvswitch 流表卸载源码分析: https://blog.csdn.net/qq_20679687/article/details/131632198
                                        ...
                                    wait_func
                                    xa_load
                                    cmd_ent_put
                    mlx5_core_set_issi -> net/mlx5：扩展 mlx5_core 以支持 ConnectX-4 以太网功能 这是 Mellanox ConnectX(R)-4 单/双端口适配器驱动程序的以太网部分，通过 VPI 支持 100Gb/s。 该驱动程序通过以太网功能扩展了现有的 mlx5 驱动程序。 该补丁包含驱动程序入口点，但不包括发送和接收（请参阅本系列中的上一个补丁）例程。 它还向 Kconfig 添加了选项 MLX5_CORE_EN 以启用/禁用以太网功能。 目前，Kconfig 被编程为使以太网和 Infiniband 功能相互排斥。 还将 MLX5_INFINIBAND 更改为依赖于 MLX5_CORE，而不是选择它，因为可以在不选择 MLX5_INFINIBAND 的情况下选择 MLX5_CORE
                        MLX5_CMD_OP_QUERY_ISSI
                    mlx5_satisfy_startup_pages -> mlx5_core：实现新的初始化序列引入enbale_hca和disable_hca命令来表示驱动程序何时启动或停止在设备上运行。 此外，驱动程序将使用引导和初始化页面计数； 需要引导页来允许固件完成引导命令，而另一个则完成 init hca。 命令接口版本已增加到 4，以强制使用支持的固件。 该补丁破坏了与旧版本固件（< 4）的兼容性； 但是，我们将发布的第一个 GA 固件将支持版本 4，因此这应该不是问题
                        mlx5_cmd_query_pages -> net/mlx5_core：支持 MANAGE_PAGES 和 QUERY_PAGES 固件命令更改 在之前的 QUERY_PAGES 命令版本中，我们使用一个命令来获取所需数量的引导、初始化和启动后页面。 新版本使用 op_mod 字段来指定查询是否针对所需数量的引导、初始化或后初始化页面。 此外，所需页面数量的输出字段大小从 16 位增加到 32 位。 在 MANAGE_PAGES 命令中，input_num_entries 和 output_num_entries 字段大小从 16 位更改为 32 位，并且 PAS 表偏移量更改为 0x10。 在页面请求事件中，num_pages 字段也更改为 32 位。 在 HCA-capability-layout 中，max_qp_mcg 字段的大小和位置已更改为支持 24 位。 该补丁与固件版本<5不兼容； 然而，事实证明我们将发布的第一个 GA 固件将不支持以前的版本，所以这应该没问题
                        give_pages -> net/mlx5：引入 Mellanox SmartNIC 并修改页面管理逻辑 Mellanox 的 SmartNIC 将嵌入式 CPU（例如 ARM）处理能力与高级网络卸载相结合，以加速多种安全、网络和存储应用程序。 随着SmartNIC的推出，出现了一种新的PCI功能，称为嵌入式CPU物理功能(ECPF)。 PF 可以从 ECPF PCI 功能获取其 ICM 页面。 驱动程序应通过读取初始化段中的位来识别它是否正在运行此类函数。 当固件请求页面时，它将发出页面请求事件，指定其请求多少页面以及针对哪个功能。 该驱动程序使用manage_pages命令进行响应，提供所请求的页面以及它正在提供这些页面的功能的指示。 此补丁之前的编码如下： function_id == 0：为接收 EQE 的函数请求页面。 function_id != 0：为由 function_id 值标识的 VF 请求页面 EQE 中新的一位字段标识为 ECPF 请求页面。 这里可以引入 page_supplier 的概念，为了支持这一点，修改了管理页面和查询页面，以便固件可以区分以下情况： 1. 函数为其自身提供页面 2. PF 为其 VF 提供页面 3. ECPF 为其自身提供页面 4. ECPF 为另一个函数提供页面 这种区别可以通过在 query_pages、manage_pages 和页面请求 EQE 中引入“embedded_cpu_function”位来实现
                            mlx5：为 Mellanox Connect-IB 适配器添加驱动程序 该驱动程序由两个内核模块组成：mlx5_ib 和 mlx5_core。 此分区类似于 mlx4 的分区，不同之处在于 mlx5_ib 是 pci 设备驱动程序，而不是 mlx5_core。 mlx5_core 本质上是一个提供通用功能的库，旨在供将来推出的其他 Mellanox 设备使用。 mlx5_ib 与 drivers/infiniband/hw 下的任何硬件设备具有类似的作用
                            kvzalloc
                            alloc_4k
                            alloc_system_page
                            mlx5_cmd_do
                                cmd_exec
                                op_mod = MLX5_GET(mbox_in, in, op_mod)
                                opcode = in_to_opcode(in)
                            mlx5_cmd_check
                    mlx5_tout_query_dtor -> default timeouts register (DTOR)
                        mlx5_core_access_reg
                            mlx5_access_reg
                mlx5_function_open
                    set_hca_ctrl
                        ...
                        MLX5_REG_HOST_ENDIANNESS
                    set_hca_cap -> 将所有 HCA 功能设置器合并到一个函数下，并编译出 ODP 相关函数，以防编译内核时不支持 ODP
                        handle_hca_cap
                        ...
                        handle_hca_cap_roce -> net/mlx5：启用软件定义的 RoCEv2 UDP 源端口 启用此选项后，RoCEv2 数据包的 UDP 源端口由软件而不是固件定义
                        handle_hca_cap_port_selection -> net/mlx5：检测并启用旁路端口选择流表 使用端口选择功能 port_select_flow_table_bypass 位来检测并启用显式端口关联，即使在链路聚合哈希模式下也是如此
                            mlx5_core_get_caps MLX5_CAP_PORT_SELECTION
                            set_caps MLX5_SET_HCA_CAP_OP_MOD_PORT_SELECTION
                    mlx5_satisfy_startup_pages
                    mlx5_cmd_init_hca -> net/mlx5：在 init HCA 期间设置软件所有者 ID 为每个主机生成唯一的 128 位标识符，并在 INIT_HCA 命令中将该值传递给固件（如果报告了 sw_owner_id 功能）。 绑定到 mlx5_core 驱动程序的每个设备都将具有相同的软件所有者 ID。 在后续补丁中，mlx5_core 设备将通过新的 VPort 命令进行绑定，以便它们可以在单个 InfiniBand 设备下一起运行。 只能绑定具有相同软件所有者 ID 的设备，以防止发往一台主机的流量到达另一台主机。 INIT_HCA 命令长度扩展了 128 位。 命令长度作为输入 FW 命令提供。 较旧的固件以新的较长形式接收此命令没有问题
                        MLX5_CMD_OP_INIT_HCA
                    mlx5_set_driver_version
                        MLX5_CMD_OP_SET_DRIVER_VERSION
                    mlx5_query_hca_caps
                        mlx5_core_get_caps_mode
                    mlx5_start_health_fw_log_up
                        queue_delayed_work(health->wq -> mlx5_health_log_ts_update
            mlx5_init_once
                mlx5_devcom_register_device -> net/mlx5：Devcom，基础设施更改将 devcom 基础设施更新为更通用，不依赖于最大支持端口定义或设备 GUID，并且封装程度更高，因此调用者无需在每次事件调用时传递注册 devcom 组件 ID
                mlx5_register_hca_devcom_comp
                    mlx5_devcom_register_component MLX5_DEVCOM_HCA_PORTS
                mlx5_query_board_id
                    MLX5_CMD_OP_QUERY_ADAPTER
                mlx5_irq_table_init -> net/mlx5：将 IRQ 数据与 EQ 表数据分开 IRQ 表应该仅存在于 mlx5_core_dev（仅适用于 PF 和 VF）。 中断设备的 EQ 表应保存指向父 PCI 设备的 IRQ 表的指针
                net/mlx5：为 SF 分配 MSI-X 矢量池 SF（子功能）当前使用其父物理功能所具有的全局 IRQ 表中的 IRQ。 为了更好地扩展，我们需要分配更多的IRQ并在不同的SF之间共享它们。 驱动程序将维护 3 个独立的 irq 池： 1. 为 PF 使用者提供服务的池（PF 的 netdev、rdma 堆栈），类似于此补丁之前的驱动程序。 即，该池将在 rdma 和 netev 之间共享 irq，并将保留 irq 索引和分配顺序。 最后一个对于 PF netdev rmap (aRFS) 很重要。 2. SF 的控制 IRQ 池。 该池的大小是可以创建的 SF 数量除以 SFS_PER_IRQ。 该池将服务于 SF 的控制路径 EQ。 3. SF 传输队列的完成数据路径 IRQ 池。 该池的大小为：num_irqs_alulated - pf_pool_size - sf_ctrl_pool_size。 该池将为 netdev 和 rdma 堆栈提供服务。 此外，SF 不支持 rmap。 SF 池的共享方法将在下一个补丁中解释。 重要提示：SF 不支持 rmap，因为 rmap 映射无法为不同 core/netdev RX 环共享的 IRQ 正常工作
                mlx5_eq_table_init
                    mlx5_eq_debugfs_init
                mlx5_events_init
                    INIT_WORK(&events->pcie_core_work, mlx5_pcie_event)
                    BLOCKING_INIT_NOTIFIER_HEAD(&events->sw_nh)
                mlx5_fw_reset_init
                    INIT_WORK mlx5_fw_live_patch_event
                    ...
                    init_completion -> 完成：使用简单的等待队列完成使用 wait_queue_head_t 将等待者排队。 wait_queue_head_t 包含一个 spinlock_t 来保护等待者列表，从而排除它在启用 PREEMPT_RT 的内核上的真正原子上下文中使用。 等待队列头中的自旋锁不能被 raw_spinlock 替换，因为： - 等待队列可以有自定义唤醒回调，它获取其他 spinlock_t 锁并且执行时间可能很长 -wake_up() 在唤醒期间遍历无限数量的列表条目 并可能唤醒无数的服务员。 出于简单性和性能原因，complete() 应该可在启用 PREEMPT_RT 的内核上使用。 完成不使用自定义唤醒回调，并且通常是单个等待者，除了一些极端情况。 将完成中的等待队列替换为简单的等待队列（swait），该队列使用 raw_spinlock_t 来保护等待列表，因此可以安全地在 PREEMPT_RT 上的真正原子区域内使用。 没有语义或功能上的变化： - 完成使用 swait 提供的独占等待模式 -complete() 唤醒一个独占等待者 -complete_all() 唤醒所有等待者，同时持有锁，以保护等待队列免受新传入等待者的影响。 转换为 swait 保留了此行为。 complete_all() 可能会导致大量等待者同时被唤醒，从而导致未绑定的延迟，但大多数complete_all() 使用站点要么在测试或初始化代码中，要么只有很少数量的并发等待者，目前不会导致延迟 问题。 现在保持简单。 USB 小工具驱动程序中警告检查的修复只是无锁等待检查从一种等待队列类型到另一种等待队列类型的直接转换
                mlx5_cq_debugfs_init
                mlx5_init_reserved_gids
					dev->roce.reserved_gids.start = tblsz
                mlx5_vxlan_create
                    mlx5_vxlan_add_port -> net：将 IANA_VXLAN_UDP_PORT 定义添加到 vxlan 头文件 将 IANA_VXLAN_UDP_PORT (4789) 定义添加到 vxlan 头文件，以便驱动程序可以使用它而不是本地定义。 更新了本地定义为 4789 的驱动程序以使用它
                mlx5_geneve_create
                mlx5_init_rl_table -> net/mlx5：速率限制表支持配置和管理硬件速率限制表。 硬件保存一个速率限制表，每个速率都与该表中的一个索引相关联。 稍后发送队列使用该索引来设置速率限制。 多个发送队列可以具有相同的速率限制，这由该表中的单个条目表示。 尽管可以共享速率，但每个队列都受到独立于其他队列的速率限制。 该表的 SW 影子保存速率本身、HW 表中的索引以及使用该速率的引用计数（队列数）。 导出的函数为 mlx5_rl_add_rate 和 mlx5_rl_remove_rate。 不同速率的数量及其值源自硬件功能
                    MLX5_CAP_QOS
                mlx5_mpfs_init -> net/mlx5：当启用多 PF 配置以允许将用户配置的单播 MAC 地址传递到请求的 PF 时，需要将 E-Switch 和 MPFS 多物理功能交换机 (MPF) 分开。 在此补丁 eswitch.c 用于管理 HW MPFS L2 表之前，E-Switch 始终（无论 sriov）启用 vport(0) (NIC PF) vport 的上下文在单播 mac 地址列表更改时更新，以填充 PF 的 MPFS L2 表 因此。 在下游补丁中，我们希望允许编译没有 E-Switch 功能的驱动程序，为此，我们将 MPFS l2 表逻辑从 eswitch.c 移至其自己的文件中，并提供 Kconfig 标志 (MLX5_MPFS) 以允许为那些需要编译 MPFS 的人 不想要多 PF 支持。 NIC PF netdevice 现在将通过新的 MPFS API 直接更新 MPFS l2 表。 VF netdevice 无权访问 MPFS L2 表，因此 E-Switch 将继续负责代表其 VF 更新其 MPFS L2 表。 由于此更改，当未启用 SRIOV 时，我们也不再需要启用 vport(0)（PF vport）单播 mac 更改事件。 这意味着 E-Switch 现在仅在 SRIOV 激活时激活，否则不需要
                    mpfs->bitmap = bitmap_zalloc(l2table_size, GFP_KERNEL)
                mlx5_sriov_init -> net/mlx5_core：添加基本 sriov 支持 此补丁为 mlx5 支持的设备添加了 SRIOV 基本支持。 PF 和 VF 使用相同的驱动程序； VF 由驱动程序通过添加到 pci 表条目的标志 MLX5_PCI_DEV_IS_VF 来识别。 像往常一样，通过将值写入 PF 设备的 sriov_numvs sysfs 文件来创建虚拟函数。 实例化 VF 后，虚拟机管理程序上的驱动程序将探测它们。 人们可以通过 /sys/bus/pci/drivers/mlx5_core/unbind 优雅地解除它们的绑定。 添加 mlx5_wait_for_vf_pages() 是为了确保当 VF 在没有执行正确拆卸的情况下死亡时，虚拟机管理程序驱动程序会等待，直到返回在虚拟机管理程序中分配的用于维持其操作的所有页面。 为了使 VF 能够运行，PF 需要为其调用enable_hca。 这可以在通过调用 pci_enable_sriov 创建 VF 之前完成。 如果卸载 PF 的驱动程序时有分配给 VM 的 VF，则所有 VF 都会出现系统错误，并且 PF 驱动程序卸载干净； 在这种情况下，不会调用 pci_disable_sriov，并且运行 lspci 时将显示设备。 重新加载 PF 驱动程序后，它将同步其数据结构，以维护其 VF 上的状态
                    pci_sriov_get_totalvfs
                        dev->sriov->driver_max_VFs
                    mlx5_get_max_vfs
                    pci_num_vf
                    mlx5_core_ec_sriov_enabled
                mlx5_eswitch_init -> net/mlx5：介绍 E-Switch 和 l2 表 E-Switch 是代表和管理 ConnectX4 HCA 间以太网 l2 交换的软件实体。 E-Switch有自己的虚拟端口，每个Vport/vNIC/VF都可以通过e-switch的vport连接到设备。 每个 e-switch 由 HCA_CAP.vport_group_manager 标识的一个 vNIC（通常是 PF/vport[0]）管理，其主要职责是将每个数据包转发到正确的 vport。 e-Switch需要管理自己的l2表和FDB表。 L2 表是由 FW 管理的流表，多主机（多 PF）配置需要它以在 PF 之间进行 HCA 间切换。 FDB表是完全由e-Switch驱动程序管理的流表，其主要职责是在属于同一e-Swtich内部vport和上行链路vport之间交换数据包。 此补丁仅引入 e-Swtich l2 表管理，FDB 管理将在稍后启用以太网 SRIOV/VF 时提供。 以太网 sriov 和 l2 表管理的准备
                    devl_params_register
                    mlx5_esw_vports_init
                    esw_offloads_init
                        esw_offloads_init_reps
                        devl_params_register
                mlx5_fpga_init -> net/mlx5：FPGA，添加对 Innova Mellanox 的基本支持 Innova 是在同一板上具有 ConnectX 和 FPGA 的 NIC。 FPGA 是线上凸块，因此会影响 ConnectX ASIC 上 mlx5_core 驱动程序的运行。 在 mlx5_core 中添加对 Innova 的基本支持。 这允许通过检测 FPGA 功能位并在初始化 ConnectX 接口之前验证其负载状态，将 Innova 卡用作常规 NIC。 还可以检测 FPGA 致命运行时故障并在发生时进入错误状态。 所有新的 FPGA 相关逻辑都放置在其自己的子目录“fpga”中，可以通过选择 CONFIG_MLX5_FPGA 来构建该子目录。 这为以后的补丁集中进一步支持各种 Innova 功能做好了准备。 随着更多功能的提交，将提供有关硬件架构的更多详细信息
                    mlx5_fpga_device_alloc
                mlx5_vhca_event_init
                    MLX5_NB_INIT(&notifier->nb, mlx5_vhca_state_change_notifier, VHCA_STATE_CHANGE)
                mlx5_sf_hw_table_init -> net/mlx5：SF，添加端口添加删除功能为了将 eswitch 之外的 SF 端口管理作为独立的软件层进行处理，请引入 eswitch 通知程序 API，以便希望在 switchdev 模式下支持 sf 端口管理的 mlx5 上层可以在 eswitch 时执行其任务 mode 设置为 switchdev 或在 eswitch 禁用之前。 在此类 eswitch 事件上初始化 sf 端口表。 在switchdev模式下添加SF端口添加和删除功能。 禁用 eswitch 时销毁所有 SF 端口。 通过 devlink 命令向用户公开 SF 端口添加和删除
                    mlx5_sf_hw_table_hwc_init
                mlx5_sf_table_init
                    ...
                    blocking_notifier_chain_register
                mlx5_fs_core_alloc
                    mlx5_init_fc_stats -> net/mlx5_core：流计数器基础结构 如果计数器在创建时设置了老化标志，则会将其添加到将从工作队列定期查询的计数器列表中。 查询结果和上次使用时间戳被缓存。 添加/删除计数器必须非常高效，因为一秒钟可能会发出数千个此类操作。 只有一个对计数器的引用，没有老化，因此不需要锁。 但是，启用老化的计数器存储在列表中。 为了使代码尽可能无锁，所有列表操作和对硬件的访问都是从单个上下文（周期性计数器查询线程）完成的。 硬件支持每个 FTE 使用多个计数器，但目前我们为每个 FTE 使用一个计数器
                        mlx5_fc_stats_work
                        mlx5_fc_pool_init -> net/mlx5：添加流量计数器池 根据流量计数器批量添加流量计数器池，从而无需在流量创建过程中通过昂贵的 FW 命令分配新计数器。 获取/释放流量计数器所需的时间从约 50 [us] 缩短至约 50 [ns]。 该池是 mlx5 驱动程序实例的一部分，并为老化流提供流量计数器。 mlx5_fc_create() 已修改为默认为池中的老化流提供计数器，并且 mlx5_destroy_fc() 已修改为将计数器释放回池以供以后重用。 如果批量分配不受支持或失败，并且对于非老化流，后备行为是分配并释放各个计数器。 该池由流量计数器批量的三个列表组成：完全使用的批量之一、部分使用的批量之一和未使用的批量之一。 首先从部分使用的块中提供计数器，以帮助限制块碎片。 该池维护一个阈值，并努力将可用计数器的数量维持在该阈值以下。 当发出计数器获取请求且没有可用计数器时，池的大小会增加；当批量释放最后一个计数器且可用计数器多于阈值时，池的大小会减小。 所有池大小的更改都是在获取/释放过程的上下文中完成的。 阈值与池提供的已用计数器数量直接相关，同时受到硬性最大值的限制，并且每次分配/释放批量时都会重新计算。 这可确保池仅在大量使用时才为可用计数器消耗大量内存。 当完全填充且处于硬最大值时，可用计数器的缓冲区消耗约 40 [MB]
                            ft_pool = kzalloc(sizeof(*ft_pool),
                            ft_pool->ft_left[i] = FT_SIZE / FT_POOLS[i]
                    mlx5_ft_pool_init -> 固件目前支持 4 个 4 种大小的池 (FT_POOLS)，以及 16M 的虚拟内存区域 (MLX5_FT_SIZE)，该区域为每个流表池复制。 我们可以为每个池分配最多 	16M 的空间，并通过 mlx5_ft_pool_get_avail_sz 跟踪我们使用了多少空间。 固件目前不会报告任何此类情况。 ESW_POOL预计从大到小排序并匹配固件池
                    kmem_cache_create -> kernel里分配一些小内存用到的是slab分配器, kmem_cache_create()只是分配size大小的缓存，并不会调用对象的构造函数，只有当再调用kmem_cache_alloc()时才会构造对象，另外调用kmem_cache_create()并没有分配slab，是在创建对象的时候发现没有空闲对象，调用cache_grow()分配一个slab，然后再分配对象
                mlx5_dm_create -> net/mlx5：将设备内存管理移至 mlx5_core 将设备内存分配和释放命令 SW ICM 内存移至 mlx5_core，以向所有 mlx5_core 用户公开此 API。 这是为支持内核中的软件控制做准备，其中需要分配和注册设备内存以进行直接规则插入。 此外，还引入了一个 API，用于使用 create_mkey 命令注册此设备内存以供将来的远程访问操作
                    bitmap_zalloc
                mlx5_fw_tracer_create -> 实现固件跟踪器逻辑和寄存器访问、初始化和清理流程。 初始化跟踪器将是加载一个流程的一部分，因为多个 PF 将尝试获取所有权，但只有一个会成功并成为跟踪器所有者
                    INIT_WORK mlx5_fw_tracer_ownership_change
                    ...
                    mlx5_query_mtrc_caps
                    mlx5_fw_tracer_create_log_buf -> net/mlx5：固件跟踪器，创建跟踪缓冲区并复制字符串数据库 对于每个 PF，执行以下操作： 1- 为跟踪器字符串数据库分配内存，并将字符串从固件读取到软件。 这些字符串稍后将用于解析跟踪。 2- 分配和 DMA 映射跟踪器缓冲区。 将写入缓冲区的跟踪将被解析为一组一个或多个跟踪，称为跟踪消息。 跟踪消息表示类似 C 语言的 printf 字符串。 消息的第一个跟踪保存了指向字符串数据库中正确字符串的指针。 以下跟踪保存消息的变量
                        mlx5_core_dma_dev
                        dma_map_single
                    mlx5_fw_tracer_allocate_strings_db
                    mlx5_fw_tracer_init_saved_traces_array
                mlx5_hv_vhca_create -> net/mlx5：添加 HV VHCA 基础设施 HV VHCA 是一个基于 HyperV PCI 配置通道提供 PF 到 VF 通信通道的层。 它实现了 Mellanox 的 Inter VHCA 控制通信协议。 该协议包含用于在 PF 和 VF 驱动程序之间传递消息的控制块，以及用于传递实际数据的数据块。 基础设施是基于代理的。 每个代理将负责 VHCA 配置空间中的连续缓冲区块。 该基础设施将代理绑定到它们的块，并且这些代理只能访问读/写分配给它们的缓冲区块。 每个代理将提供三个回调（控制、无效、清理）。 当使用与此代理相关的命令使 block-0 无效时，将调用控制。 如果分配给该代理的块之一无效，则将调用无效回调。 在释放代理之前将调用清理，以清理其所有开放资源或延迟的工作。 Block-0 用作控制块。 来自 PF 的所有执行命令都将由 PF 写入该块。 VF 也会通过在 block-0 上写入来解决这些问题。 其格式由 struct mlx5_hv_vhca_control_block 布局描述
                    create_singlethread_workqueue("mlx5_hv_vhca")
                mlx5_rsc_dump_create -> net/mlx5：添加对资源转储的支持在驱动程序加载时： - 初始化资源转储数据结构和内存访问工具（mkey 和 pd）。 - 读取包含固件段标识符的资源转储菜单。 每条记录均由段名称 (ASCII) 标识。 在驾驶员的一生中，用户（例如记者）可能会请求每个路段的转储。 用户应创建一个提供段标识符（SW 枚举）和命令键的命令。 作为回报，用户收到命令上下文。 为了接收转储，用户应提供命令上下文和将在其上写入转储内容的内存（与页对齐）。 由于转储可能大于给定内存，因此用户可以重新提交命令，直到收到转储结束的指示。 用户有责任销毁该命令
            mlx5_load -> net/mlx5：将 load_one 分为三个阶段 使用先前补丁的基础将 mlx5_load_one 流程分解为三个阶段： 1. mlx5_function_setup() 从先前补丁到设置函数 2. mlx5_init_once() 从先前补丁到根据硬件上限 3 初始化软件对象 新的 mlx5_load() 用于加载 mlx5 组件 这为 mlx5 核心设备初始化流程提供了更好的逻辑分离，并将有助于无缝支持创建不同的 mlx5 设备类型，例如 PF、VF 和 SF mlx5 子功能虚拟设备。 此补丁不会更改任何功能
                mlx5_get_uars_page -> net/mlx5：添加接口以获取对 UAR 的引用 需要对 UAR 的引用才能生成 CQ 或 EQ 门铃。 由于 CQ 或 EQ 门铃都可以使用相同的 UAR 区域生成，而不会对性能产生任何影响，因此我们只是获取对任何可用 UAR 的引用，如果一个不可用，我们会分配它，但我们不会浪费蓝色火焰寄存器 可以提供，我们将使用它们进行后续分配。 我们获取对此类 UAR 的引用并将其放入 mlx5_priv 中，以便任何内核使用者都可以使用它
                    alloc_uars_page
                mlx5_events_start
                    for (i = 0; i < ARRAY_SIZE(events_nbs_ref); i++)
                        events->notifiers[i].nb  = events_nbs_ref[i]
                        mlx5_eq_notifier_register(dev, &events->notifiers[i].nb)
                            atomic_notifier_chain_register(&eqt->nh[nb->event_type], &nb->nb)
                mlx5_pagealloc_start
                mlx5_eq_table_create
                    create_async_eqs
                        MLX5_NB_INIT(&table->cq_err_nb, cq_err_event_notifier, CQ_ERROR) -> new
                        mlx5_eq_notifier_register(dev, &table->cq_err_nb)
			table->ctrl_irq = mlx5_ctrl_irq_request(dev)
			MLX5_NB_INIT(&table->cq_err_nb, cq_err_event_notifier, CQ_ERROR)
			mlx5_eq_notifier_register(dev, &table->cq_err_nb)
                        mlx5_cmd_allowed_opcode(dev, MLX5_CMD_OP_CREATE_EQ)
                        err = setup_async_eq(dev, &table->cmd_eq, &param, "cmd")
                    alloc_rmap -> rmap 是 irq 编号和队列编号之间的映射。每个 irq 只能分配给一个 rmap。由于 SF 共享 IRQ，因此 rmap 映射无法正确用于不同核心/netdev RX 环之间共享的 irq。因此我们不允许 SF 使用 netdev rmap -> 为 IRQ 分配 CPU 亲和性反向映射
                mlx5_fw_tracer_init
                mlx5_fw_reset_events_start
                mlx5_hv_vhca_init
                mlx5_rsc_dump_init
                mlx5_fpga_device_start -> net/mlx5：FPGA 和 IPSec 初始化在流量控制之前进行，某些流量控制命名空间初始化（即出口命名空间）可能取决于 FPGA 功能。 更改初始化顺序，以便 FPGA 在流程引导之前进行初始化。 流控制 fs 命令初始化可能取决于 IPSec 功能。 更改初始化顺序，以便 IPSec 也将在流量引导之前初始化 -> net/mlx5：FPGA，添加对Innova的基本支持，Mellanox Innova是一块带有ConnectX的网卡和一块FPGA在同一块板上。 FPGA 是线上凸块，因此会影响 ConnectX ASIC 上 mlx5_core 驱动程序的运行。 在 mlx5_core 中添加对 Innova 的基本支持。 这允许通过检测 FPGA 功能位并在初始化 ConnectX 接口之前验证其负载状态，将 Innova 卡用作常规 NIC。 还可以检测 FPGA 致命运行时故障并在发生时进入错误状态。 所有新的 FPGA 相关逻辑都放置在其自己的子目录“fpga”中，可以通过选择 CONFIG_MLX5_FPGA 来构建该子目录。 这为以后的补丁集中进一步支持各种 Innova 功能做好了准备。 随着更多功能的提交，将提供有关硬件架构的更多详细信息, commit: https://github.com/ssbandjl/linux/commit/e29341fb3a5b885a4bb5b9a38f2814ca07d3382c
                    mlx5_fpga_caps
                    mlx5_fpga_device_load_check
                    mlx5_is_fpga_lookaside
                    mlx5_core_reserve_gids
                    fpga_err_event
                    fpga_qp_err_event
                    mlx5_fpga_conn_device_init
                        mlx5_nic_vport_enable_roce
                            mlx5_nic_vport_update_roce_state(mdev, MLX5_VPORT_ROCE_ENABLED)
                            MLX5_CMD_OP_MODIFY_NIC_VPORT_CONTEXT
                        mlx5_get_uars_page
                        mlx5_core_alloc_pd
                        mlx5_fpga_conn_create_mkey
                    mlx5_fpga_device_brb
                mlx5_fs_core_init
                mlx5_core_set_hca_defaults
                mlx5_vhca_event_start
                mlx5_sf_hw_table_create
                mlx5_ec_init
                    mlx5_host_pf_init
                        mlx5_cmd_host_pf_enable_hca
                            MLX5_CMD_OP_ENABLE_HCA
                mlx5_lag_add_mdev -> net/mlx5：更改链路聚合的所有权模型 链路聚合用于将同一 HCA 的两个 PCI 功能组合到单个逻辑单元中。 这是核心功能，因此应由核心驱动程序管理。 目前情况并非如此。 当我们将链路聚合软件结构存储在较低设备内时，其生命周期（创建/销毁）由 mlx5e 部分决定。 更改所有权模型，使延迟与较低级别驱动程序的生命周期相关，而不是与 mlx5e 部分相关
                    __mlx5_lag_dev_add_mdev -> net/mlx5：链路聚合，如果需要，请正确锁定 eswitch 目前，在进行硬件链路聚合时，我们会检查 eswitch 模式，但由于这不是在锁定下完成的，因此检查无效。 由于代码需要在两个不同设备之间同步，因此需要格外小心。 - 当要更改 eswitch 模式时，如果硬件链路聚合处于活动状态，则销毁它。 - 更改 eswitch 模式时会阻止任何硬件绑定创建。 - 延迟处理绑定事件，直到没有正在进行的模式更改。 - 当附加一个新的 mdev 到 lag 时，阻塞直到没有正在进行的模式改变。 为了完成模式更改，必须锁定界面。 释放锁定并休眠 100 毫秒以允许前进。 由于这是一种非常罕见的情况（如果用户取消绑定和绑定 PCI 功能，同时更改其他 PCI 功能的 eswitch 模式，则可能会发生），因此它对现实世界没有影响。 由于现在需要采用多个 eswitch 模式锁定，lockdep 会抱怨可能的死锁。 为每个 eswitch 注册一把钥匙，让 Lockdep 满意
                        mlx5_devcom_get_next_peer_data
                        mlx5_ldev_get
                        mlx5_ldev_add_mdev
                    mlx5_ldev_add_debugfs
                mlx5_sriov_attach -> net/mlx5：实现轻量级和模块化内部/pci 错误处理所需的 SRIOV 连接/分离流程。 实现 sriov Attach 功能，该功能可以在设备端预先保存 vfs 的数量。 实现 sriov detach 功能，禁用设备端当前的 vfs。 初始化/清理函数仅处理 sriov 软件上下文分配和销毁
                    mlx5_device_enable_sriov
                        mlx5_eswitch_enable -> net/mlx5：E-switch，保护 eswitch 模式更改 目前，eswitch 模式更改是从以下 2 个不同的执行上下文中发生的。 1. sriov sysfs 启用/禁用 2. devlink eswitch set 命令 两者都需要以同步方式访问 eswitch 相关数据结构。 在没有任何同步的情况下，存在以下竞争条件。 通过 devlink eswitch 模式更改启用/禁用 SR-IOV：
                        mlx5_get_default_msix_vec_count
                        mlx5_set_msix_vec_count
                            ec_vf_function = mlx5_core_ec_sriov_enabled(dev)
                            vport = mlx5_core_func_to_vport(dev, function_id, ec_vf_function)
                            MLX5_SET(set_hca_cap_in, hca_cap, opcode, MLX5_CMD_OP_SET_HCA_CAP)
                        sriov_restore_guids
                mlx5_sf_dev_table_create
                mlx5_devlink_traps_register
            set_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state)
            mlx5_register_device(dev)
                mlx5_rescan_drivers_locked
    mlx5_crdump_enable -> net/mlx5：添加 Crdump 支持，Crdump 允许驱动程序检索 FW PCI crspace 的转储。 这在发生可能需要重置固件的灾难性问题时非常有用。 crspace 转储可用于以后的调试
        ...
    mlx5_hwmon_dev_register -> net/mlx5：通过硬件监控内核 API 暴露 NIC 温度 通过实现 hwmon 内核 API 暴露 NIC 温度，这会将当前热区内核 API 变为冗余。 对于每个受支持和公开的热二极管传感器，公开以下属性： 1) 输入温度。 2) 最高温度。 3) 温度标签：取决于固件功能，如果固件不支持传感器命名，则后备命名约定为：“sensorX”，其中 X 是硬件规格（MTMP 寄存器）传感器索引。 4) 温度临界最大值：指警告事件的高阈值。 将公开为 `tempY_crit` hwmon 属性（RO 属性）。 例如，对于 ConnectX5 HCA，该温度值为 105 摄氏度，比硬件关闭温度低 10 度。 5) 温度重置历史记录：重置最高温度。 例如，对于具有单个 IC 热二极管传感器的双端口 ConnectX5 NIC，将在“/sys/class/hwmon/hwmon[X,Y]”下有 2 个 hwmon 目录（每个 PCI 功能一个）。 列出上面的目录之一 (hwmonX/Y) 会生成以下相应的输出：grep -H -d skip . /sys/class/hwmon/hwmon0/*, sensors
    pci_save_state
    devlink_register
    mlx5_devlink_alloc
    mlx5_adev_idx_alloc
    mlx5_mdev_init



init debugfs, wq, Workqueue
/sys/kernel/debug/mlx5/
tree -L 2 /sys/kernel/debug/mlx5/

probe flow ref:
.probe()
  |-init_one(struct pci_dev *pdev, pci_device_id id)
      |-mlx5_devlink_alloc()
      |   |-devlink_alloc(ops)        // net/core/devlink.c
      |       |-devlink = kzalloc()
      |       |-devlink->ops = ops
      |       |-return devlink
      |
      |-mlx5_mdev_init(dev, prof_sel)
      |   |-debugfs_create_dir
      |   |-mlx5_pagealloc_init(dev)
      |       |-create_singlethread_workqueue("mlx5_page_allocator")
      |           |-alloc_ordered_workqueue
      |               |-alloc_workqueue     // kernel/workqueue.c
      |
      |-mlx5_pci_init(dev, pdev, id);
      |   |-mlx5_pci_enable_device(dev);
      |   |-request_bar(pdev);             // Reserve PCI I/O and memory resources
      |   |-pci_set_master(pdev);          // Enables bus-mastering on the device
      |   |-set_dma_caps(pdev);            // setting DMA capabilities mask, set max_seg <= 1GB
      |   |-dev->iseg = ioremap()
      |
      |-mlx5_load_one(dev, true);
      |   |-if interface already STATE_UP
      |   |   return
      |   |
      |   |-dev->state = STATE_UP
      |   |-mlx5_function_setup       // Init firmware functions
      |   |-if boot:
      |   |   mlx5_init_once
      |   |     |-mlx5_irq_table_init // Allocate IRQ table memory
      |   |     |-mlx5_eq_table_init  // events queue
      |   |     |-dev->vxlan  = mlx5_vxlan_create
      |   |     |-dev->geneve = mlx5_geneve_create
      |   |     |-mlx5_sriov_init
      |   | else:
      |   |   mlx5_attach_device
      |   |-mlx5_load
      |   |   |-mlx5_irq_table_create     // 初始化硬中断
      |   |   |  |-pci_alloc_irq_vectors(MLX5_IRQ_VEC_COMP_BASE + 1, PCI_IRQ_MSIX);
      |   |   |  |-request_irqs
      |   |   |      for i in vectors:
      |   |   |        request_irq(irqn, mlx5_irq_int_handler) // 注册中断处理函数
      |   |   |-mlx5_eq_table_create      // 初始化事件队列（EventQueue）
      |   |      |-create_comp_eqs(dev)   // Completion EQ
      |   |          for ncomp_eqs:
      |   |            eq->irq_nb.notifier_call = mlx5_eq_comp_int;
      |   |            create_map_eq()
      |   |            mlx5_eq_enable()
      |   |-set_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state);
      |
      |-pci_save_state(pdev);
      |
      |-if (!mlx5_core_is_mp_slave(dev))
          devlink_reload_enable(devlink);


init pci:
mlx5_pci_init -> mlx5：为 Mellanox Connect-IB 适配器添加驱动程序 该驱动程序由两个内核模块组成：mlx5_ib 和 mlx5_core。 此分区类似于 mlx4 的分区，不同之处在于 mlx5_ib 是 pci 设备驱动程序，而不是 mlx5_core。 mlx5_core 本质上是一个提供通用功能的库，旨在供将来推出的其他 Mellanox 设备使用。 mlx5_ib 与 drivers/infiniband/hw 下的任何硬件设备具有类似的作用
    ioremap(dev->iseg_base, sizeof(*dev->iseg)) -> mapping initialization segment


MSI-X 中断是比较推荐的方式，尤其是对于支持多队列的网卡。 因为每个 RX 队列有独立的 MSI-X 中断，因此可以被不同的 CPU 处理（通过 irqbalance 方式，或者修改/proc/irq/IRQ_NUMBER/smp_affinity）。后面会看到 ，处理中断的 CPU 也是随后处理这个包的 CPU。这样的话，从网卡硬件中断的层面就可 以设置让收到的包被不同的 CPU 处理


mlx5e_nic
  |-mlx5e_ipsec_build_inverse_table();
  |-mlx5e_build_ptys2ethtool_map();
  |-mlx5_register_interface(&mlx5e_interface)
     |-list_add_tail(&intf->list, &intf_list);
     |
     |-for priv in mlx5_dev_list
         mlx5_add_device(intf, priv)
          /
         /
mlx5_add_device(intf, priv)
 |-if !mlx5_lag_intf_add
 |   return // if running in InfiniBand mode, directly return
 |
 |-dev_ctx = kzalloc()
 |-dev_ctx->context = intf->add(dev)
     |-mlx5e_add
        |-netdev = mlx5e_create_netdev(mdev, &mlx5e_nic_profile, nch);
        |  |-mlx5e_nic_init
        |     |-mlx5e_build_nic_netdev(netdev);
        |        |-netdev->netdev_ops = &mlx5e_netdev_ops; // 注册 ethtool_ops, poll
        |-mlx5e_attach
        |  |-mlx5e_attach_netdev
        |      |-profile->init_tx()
        |      |-profile->init_rx()
        |      |-profile->enable()
        |          |-mlx5e_nic_enable
        |              |-mlx5e_init_l2_addr
        |              |-queue_work(priv->wq, &priv->set_rx_mode_work)
        |              |-mlx5e_open(netdev);
        |              |  |-mlx5e_open_locked
        |              |     |-mlx5e_open_channels
        |              |     |  |-mlx5e_open_channel
        |              |     |     |-netif_napi_add(netdev, &c->napi, mlx5e_napi_poll, 64);
        |              |     |     |-mlx5e_open_queues
        |              |     |         |-mlx5e_open_cq
        |              |     |         |   |-mlx5e_alloc_cq
        |              |     |         |       |-mlx5e_alloc_cq_common
        |              |     |         |           |-mcq->comp = mlx5e_completion_event;
        |              |     |         |-napi_enable(&c->napi)
        |              |     |-mlx5e_activate_priv_channels
        |              |        |-mlx5e_activate_channels
        |              |            |-for ch in channels:
        |              |                mlx5e_activate_channel
        |              |                  |-mlx5e_activate_rq
        |              |                     |-mlx5e_trigger_irq
        |              |-netif_device_attach(netdev);
                            test_and_set_bit(__LINK_STATE_PRESENT, &dev->state)
                            netif_running(dev)
                            netif_tx_wake_all_queues
                            __netdev_watchdog_up


thtool 是一个命令行工具，可以查看和修改网卡配置，常用于收集网卡统计数据。 内核实现了一个通用 ethtool 接口，网卡驱动只要实现这些接口，就可以使用 ethtool 来查看或修改网络配置； 在底层，它是通过 ioctl 和设备驱动通信的


mlx5e_init_nic_rx
    mlx5e_create_q_counters -> net/mlx5e：移动 Q 计数器分配并将 RQ 删除到 init_rx 并非所有配置文件都会在 update_stats() 回调中查询 HW Q 计数器。 每个设备的 HW Q 计数器都是有限的，对于表示器，它们的所有 Q 计数器都分配在父 PF 设备上。 通过将分配移动到 init_rx 配置文件回调来避免硬件 Q 计数器的重复分配
        MLX5_CMD_OP_ALLOC_Q_COUNTER
    mlx5e_open_drop_rq
        struct mlx5e_cq -> net/mlx5：扩展 mlx5_core 以支持 ConnectX-4 以太网功能，这是 Mellanox ConnectX(R)-4 单/双端口适配器驱动程序的以太网部分，支持带 VPI 的 100Gb/s。 该驱动程序通过以太网功能扩展了现有的 mlx5 驱动程序。 该补丁包含驱动程序入口点，但不包括发送和接收（请参阅本系列中的上一个补丁）例程。 它还向 Kconfig 添加了选项 MLX5_CORE_EN 以启用/禁用以太网功能。 目前，Kconfig 被编程为使以太网和 Infiniband 功能相互排斥。 还将 MLX5_INFINIBAND 更改为依赖于 MLX5_CORE，而不是选择它，因为可以在不选择 MLX5_INFINIBAND 的情况下选择 MLX5_CORE
        mlx5e_build_drop_rq_param -> net/mlx5e：允许在通道上下文之外进行 CQ，为了能够在通道上下文之外创建 CQ，请删除 cq->channel 直接指针。 这需要添加指向通道统计信息、netdevice、priv 和 mlx5_core 的直接指针，以支持属于 mlx5e_channel 一部分的 CQ。 此外，以前从通道派生的参数（如 napi、NUMA 节点、通道统计信息和索引）现在组装在 struct mlx5e_create_cq_param 中，该结构被赋予 mlx5e_open_cq() 而不是通道指针。 通用化 mlx5e_open_cq() 允许在通道上下文之外打开 CQ，该通道上下文将在补丁集中的后续补丁中使用
            mlx5e_get_rqwq_log_stride
                order_base_2 -> 计算参数以 2 为底的对数（向上舍入）
        mlx5e_alloc_drop_cq -> mlx5e_alloc_cq_common
            mlx5_cqwq_create
            mcq->cqe_sz     = 64
            mcq->comp       = mlx5e_completion_event -> napi_schedule(cq->napi)
            mcq->event      = mlx5e_cq_error_event -> net/mlx5：以太网数据路径文件，en_[rt]x.c 包含特定于 tx 或 rx 的数据路径相关代码。 en_txrx.c 包含 rx 和 tx 通用的数据路径代码，这主要是 napi 相关代码。 以下是数据路径中硬件和驱动程序使用的对象： 通道 - 每个 IRQ 一个通道。 每个通道对象包含： RQ - 描述接收队列 TIR - 每种流类型一个 TIR（传输接口接收）对象。 TIR 包含接收流类型的属性（例如 IPv4、IPv6 等）。 流在流表中定义。 目前，TIR 描述了 RSS 哈希参数（如果存在）和 LRO 属性。 SQ - 描述 tx 队列。 每个TC（流量类别）有一个SQ（发送队列）。 TIS - 每个 TC 有一个 TIS（传输接口发送）。 它描述了 TC，稍后可能会扩展以描述更多传输属性。 RQ和SQ都继承自对象WQ（工作队列）。 描述 CQE 的 WQE 在内存中的布局的通用代码位于文件 wq 中。[cj] 对于每个通道，都有一个用于 RX 和 TX 的 NAPI 上下文。 驱动程序正在使用 netdev_alloc_skb() 来分配 skb
            cqe->validity_iteration_count = 0xff
        mlx5e_create_cq
            mlx5_comp_eqn_get -> net/mlx5：动态分配完成 EQ，此提交支持在运行时动态分配 EQ，从而可以更灵活地管理完成 EQ 并减少驱动程序负载的内存开销。 每当为给定向量索引创建 CQ 时，驱动程序都会查找该向量是否已映射完成 EQ，如果有，则使用它。 否则，根据需要分配新的 EQ，然后将其用于 CQ 完成事件。 向 EQ 表添加保护锁，以防止并发 EQ 创建尝试。 在此期间，将 mlx5_vector2irqn()/mlx5_vector2eqn() 替换为 mlx5_comp_eqn_get() 和 mlx5_comp_irqn_get()，如果给定向量没有找到 EQ，则将按需分配 EQ
                create_comp_eq
                    lockdep_assert_held
                    comp_irq_request
                        comp_irq_request_sf -> net/mlx5：重构 EQ 层中的完成 IRQ 请求/释放处理程序，将完成 IRQ 请求/释放函数分解为 EQ 层中 PCI 设备和 SF 的每个向量处理程序。 在创建 EQ 表时，循环遍历所有向量并使用新的每向量函数为每个向量请求 IRQ。 在 EQ 表清理中释放 IRQ 时执行对称更改
                            struct mlx5_irq_pool *pool
                            cpumask_copy
                            cpumask_andnot
                            mlx5_irq_affinity_request -> 根据给定的掩码请求 IRQ
                                irq_pool_find_least_loaded
                                irq_pool_request_irq
                                    mlx5_irq_alloc
                                        pci_msix_can_alloc_dyn
                                        irq->map.virq = pci_irq_vector(dev->pdev, i)
                                        irq->map = pci_msix_alloc_irq_at
                                        err = request_irq(irq->map.virq, irq_int_handler, 0, irq->name, -> atomic_notifier_call_chain(nh, 0, NULL) -> cq_err_event_notifier
                                        irq_set_affinity_and_hint(irq->map.virq, irq->mask)
                            cpumask_or mlx5_irq_get_affinity_mask
                        or comp_irq_request_pci
                            mlx5_eq_table_get_pci_rmap -> net/mlx5：在没有 SF IRQ 池的情况下处理 SF IRQ 请求，如果 SF IRQ 池由于设置限制而不可用，则 SF 目前依赖已分配的 PF IRQ 来满足其 IRQ 矢量请求。 然而，随着下一个补丁中引入的动态EQ分配，驱动程序加载后可能不会分配PF的所有IRQ。 在这种情况下，如果SF在没有自己的独立IRQ池的情况下请求完成IRQ，则SF将缺乏PF IRQ可利用。 为了解决这种情况，请根据需要从 PF 的 IRQ 池中为 SF 分配 IRQ。 新的 IRQ 将在 SF 和它的 PF 之间共享
                            mlx5_cpumask_default_spread -> net/mlx5：引入mlx5_cpumask_default_spread，为了在完成IRQ请求代码中获得更好的代码可读性，在单独的函数中定义每个完成向量逻辑的CPU查找。 给定向量索引“n”的新方法 mlx5_cpumask_default_spread() 将返回“第 n”个 cpu。 这个新方法也将在下一个补丁中使用
                                for_each_numa_hop_mask
                                    for_each_cpu_andnot
                            mlx5_irq_request_vector -> 为mlx5设备绑中断, 每个中断至少绑到1个CPU核上
                                cpumask_clear(&af_desc.mask)
                                cpumask_set_cpu(cpu, &af_desc.mask)
                                mlx5_irq_request -> 为 mlx5 PF/VF 设备请求 IRQ -> net/mlx5：基于 IRQ 的循环 EQ，每当用户为 EQ 创建请求提供亲和力时，将 EQ 映射到匹配的 IRQ。 将 IRQ=IRQ 与所创建的 EQ 具有相同亲和力和类型（完成/控制）进行匹配。 这种映射是在积极的专用 IRQ 分配方案中完成的，如下所述。 首先，我们检查是否存在匹配的IRQ，其最小阈值未耗尽。 - min_eqs_threshold = 3 用于控制 EQ。 - min_eqs_threshold = 1 用于完成 EQ。 如果未找到匹配的 IRQ，请尝试请求新的 IRQ。 如果我们无法请求新的 IRQ，请重用最少使用的匹配 IRQ
                                    irq = irq_pool_request_vector(pool, vecidx, af_desc, rmap) -> net/mlx5：使用动态 msix 向量分配，当前实现计算可用中断向量的数量和分区，然后分配所有中断向量。 在这里，每当支持动态 msix 分配时，我们都会将其更改为动态使用 msix 向量，以便实际上仅在需要时分配向量。 当前的池逻辑保持不变，以负责在消费者之间划分向量并负责引用计数。 但是，仅在需要时才分配向量。 后续补丁将利用它为 VDPA 分配向量
                                        mlx5_irq_alloc
                    comp_eq_depth_devlink_param_get -> net/mlx5：让用户配置io_eq_size参数，目前每个I/O EQ占用128KB内存。 并非所有用例都需要此大小，并且对于大规模来说至关重要。 因此，允许用户配置 I/O EQ 的大小。 例如，要将 I/O EQ 大小减少到 64，请执行： $ devlink dev param set pci/0000:00:0b.0 name io_eq_size value 64 \ cmode driverinit $ devlink dev reload pci/0000:00:0b
                        devl_param_driverinit_value_get DEVLINK_PARAM_GENERIC_ID_IO_EQ_SIZE -> 获取驱动程序初始化的配置参数值 @devlink: devlink @param_id: 参数 ID @val: 存储 driverinit 配置模式下参数值的指针 该函数应该由驱动程序用来获取 driverinit 配置，以便在 reload 命令后进行初始化。 请注意，此函数的无锁调用依赖于驱动程序来维护以下基本理智行为： 1) 驱动程序确保对此函数的调用不会与使用相同参数 ID 注册/注销参数竞争。 2) 驱动程序确保对此函数的调用不能与具有相同参数 ID 的 devl_param_driverinit_value_set() 调用竞争。 3) 驱动程序确保对此函数的调用不会与重新加载操作竞争。 如果驱动程序无法遵守，则在调用此函数时必须获取 devlink->lock
                            devlink_param_find_by_id
                    tasklet_setup(&eq->tasklet_ctx.task, mlx5_cq_tasklet_cb) -> net/mlx5：实现单个完成 EQ 创建/销毁方法，目前，create_comp_eqs() 函数处理驱动程序加载上所有向量的所有完成 EQ 的创建。 在驱动程序卸载时，destroy_comp_eqs() 执行等效的工作。 在准备动态 EQ 创建时，请将 create_comp_eqs() / destroy_comp_eqs() 替换为 create_comp_eq() / destroy_comp_eq() 函数，该函数将接收向量索引并为该特定向量分配/销毁 EQ。 因此，完成情商的管理具有更大的灵活性 -> net: mlx: 将tasklet转换为使用新的tasklet_setup() API，为了准备无条件地将struct tasklet_struct指针传递给所有tasklet回调，请切换到使用新的tasklet_setup()和from_tasklet()来显式传递tasklet指针
                        list_splice_tail_init(&ctx->list, &ctx->process_list)
                        mcq->tasklet_ctx.comp(mcq, NULL) -> mlx5_add_cq_to_tasklet
                        tasklet_schedule(&ctx->task)
                    eq->irq_nb.notifier_call = mlx5_eq_comp_int
                    create_map_eq
                        u8 log_eq_size = order_base_2(param->nent + MLX5_NUM_SPARE_EQE)
                        INIT_RADIX_TREE(&cq_table->tree, GFP_ATOMIC) -> 初始化基数树1
                        mlx5_frag_buf_alloc_node -> net/mlx5：对 EQ 使用 order-0 分配，目前我们正在为 EQ 分配高阶页面。 在碎片系统的情况下，例如虚拟机中的 VF 热删除/添加，没有足够的连续内存用于 EQ 分配，这会导致虚拟机崩溃。 因此，请改用 0 阶片段进行 EQ 分配。 性能测试：ConnectX-5 100Gbps，CPU：Intel(R) Xeon(R) CPU E5-2697 v3 @ 2.60GHz 性能测试显示没有明显下降 -> net/mlx5e：实现分段工作队列 (WQ)，添加新类型的 struct mlx5_frag_buf，用于分配分段缓冲区而不是连续缓冲区，并使完成队列 (CQ) 使用它，因为它们很大（默认情况下每个 CQ 为 2MB） 跨越RQ）。 这修复了当用户尝试设置更多或更大的环时由于 dma_zalloc_coherent 没有足够的连续一致内存来满足驱动程序的请求而导致的类型失败：“mlx5e_open_locked：mlx5e_open_channels 失败，-12” -> commit: https://github.com/ssbandjl/linux/commit/1c1b522808a18402f043c1418b4e48c7355480cc
                            buf->npages = DIV_ROUND_UP(size, PAGE_SIZE)
                            mlx5_dma_zalloc_coherent_node -> net/mlx5e：在读卡器NUMA节点上分配DMA相干内存，通过亲和性提示和XPS，为每个mlx5e通道分配一个CPU核心。 由 NIC 写入并由 SW 读取的通道 DMA 相干内存（例如 CQ 缓冲区）分配在分配给该通道的 CPU 核心的 NUMA 节点上。 由SW写入并由NIC读取的通道DMA相干存储器（例如SQ/RQ缓冲区）分配在NIC的NUMA节点上。 门铃记录（由SW写入并由NIC读取）是一个例外，因为它被SW更频繁地访问 -> 队列缓冲区的处理——我们分配一堆内存并将其注册到 HCA 虚拟地址 0 的内存区域中
                                cpu_handle = dma_alloc_coherent(device, size, dma_handle, -> net/mlx5：将 dma 设备与 pci 设备分离并通用，mlx5 子功能 (SF) 子设备将在后续补丁中引入。 它将被创建为中介设备并属于 mdev 总线。 有必要以统一的方式处理 PF、VF 和 SF 上的 dma 操作，从而减少对 pdev pci dev struct 的依赖，并直接使用之前补丁中新引入的“struct device”进行工作
                        mlx5_init_fbc -> net/mlx5：重构分段缓冲区结构字段和初始化流程，从 mlx5_frag_buf_ctrl 中取出 struct mlx5_frag_buf，因为不需要管理和控制分段缓冲区 API 的数据路径。 struct mlx5_frag_buf 包含用于管理分段缓冲区的分配和取消分配的控制信息。 它的字段与数据路径无关，因此这里我将它们从 struct mlx5_frag_buf_ctrl 中取出，除了片段数组本身。 此外，还修改了 mlx5_fill_fbc 以初始化 frags 指针。 这意味着必须在调用函数之前分配缓冲区。 一组特定于类型的 *_get_byte_size() 函数被替换为通用函数
                            mlx5_init_fbc_offset
                                fbc->log_frag_strides = PAGE_SHIFT - fbc->log_stride
                        init_eq_buf
                            eqe = get_eqe(eq, i)
                        mlx5_irq_get_index
                        in = kvzalloc(inlen, GFP_KERNEL)
                        mlx5_fill_page_frag_array -> net/mlx5：支持设置dma地址的访问权限，mlx5_fill_page_frag_array()用于将dma地址填充到需要它的资源中，例如QP、RQ等。使用资源时，PA列表权限将被忽略。 对于使用MTT列表的资源，用户需要提供访问权限。 后续补丁使用需要 MTT 列表的资源，因此修改 API 和实现以支持该资源
                            mlx5_fill_page_frag_array_perm
                                pas[i] = cpu_to_be64(buf->frags[i].map | perm)
                        mlx5_cmd_exec MLX5_CMD_OP_CREATE_EQ
                        eq->irqn = pci_irq_vector(dev->pdev, vecidx)
                        mlx5_debug_eq_add(dev, eq) -> net/mlx5：将中断处理程序更改为调用链通知程序，多个 EQ 可能在后续补丁中共享相同的 IRQ。 EQ 将注册到原子链通知器，而不是直接调用 IRQ 处理程序。 不使用 Linux 内置共享 IRQ，因为它强制调用者在调用 free_irq() 之前禁用 IRQ 并清除关联性。 该补丁是 IRQ 和 EQ 逻辑分离的第一步
                            add_res_tree(dev, MLX5_DBG_RSC_EQ, dev->priv.dbg.eq_debugfs,
                                d->root = debugfs_create_dir(resn,  root)
                                debugfs_create_file(field[i], 0400, d->root, &d->fields[i],
                    mlx5_eq_enable
                        mlx5_irq_attach_nb -> net/mlx5：为 SF 分配 MSI-X 矢量池，SF（子功能）当前使用其父物理功能所具有的全局 IRQ 表中的 IRQ。 为了更好地扩展，我们需要分配更多的IRQ并在不同的SF之间共享它们。 驱动程序将维护 3 个独立的 irq 池： 1. 为 PF 使用者提供服务的池（PF 的 netdev、rdma 堆栈），类似于此补丁之前的驱动程序。 即，该池将在 rdma 和 netev 之间共享 irq，并将保留 irq 索引和分配顺序。 最后一个对于 PF netdev rmap (aRFS) 很重要。 2. SF 的控制 IRQ 池。 该池的大小是可以创建的 SF 数量除以 SFS_PER_IRQ。 该池将服务于 SF 的控制路径 EQ。 3. SF 传输队列的完成数据路径 IRQ 池。 该池的大小为：num_irqs_alulated - pf_pool_size - sf_ctrl_pool_size。 该池将为 netdev 和 rdma 堆栈提供服务。 此外，SF 不支持 rmap。 SF 池的共享方法将在下一个补丁中解释。 重要提示：SF 不支持 rmap，因为 rmap 映射无法为不同 core/netdev RX 环共享的 IRQ 正常工作
                            ret = irq_get(irq)
                            atomic_notifier_chain_register
                            mlx5_irq_put
                        eq_update_ci
                            __be32 __iomem *addr = eq->doorbell + (arm ? 0 : 2)
                            u32 val = (eq->cons_index & 0xffffff) | (eq->eqn << 24)
                            __raw_writel((__force u32)cpu_to_be32(val), addr) -> write register
                            mb()
            mlx5_fill_page_frag_array
            mlx5_core_create_cq -> net/mlx5：在核心 create_{cq,dct} 中使用 mlx5_cmd_do()，mlx5_core_create_{cq/dct} 函数是重要的 mlx5 命令函数。 他们自己检查命令执行状态并隐藏有价值的固件故障信息。 对于 mlx5_core/eth 内核用户，这是我们真正想要的，但对于 devx/rdma 用户，隐藏信息至关重要，应该传播给调用者，因此我们将这些命令转换为使用 mlx5_cmd_do 返回 FW/驱动程序和命令 发件箱状态保持不变，并让呼叫者决定如何处理它。 对于 mlx5_core_create_{cq/dct} 的内核调用者或那些只关心二进制状态 (FAIL/SUCCESS) 的调用者，他们必须通过 mlx5_cmd_check() 自行检查状态以恢复当前行为。 err = mlx5_create_cq(in, out) err = mlx5_cmd_check(err, in, out) if (err) // 处理错误 对于 DEVX 用户和那些关心完全可见性的人来说，他们只会将错误传播到用户空间，应用程序可以 检查 err == -EREMOTEIO 是否有效，则 outbox.{status,syndrome} 是否有效。 API 注意：mlx5_cmd_check() 必须由内核用户使用，因为它允许驱动程序拦截命令执行状态并在驱动程序引发错误处理或重置/恢复流程时返回驱动程序模拟状态
                mlx5_create_cq
                    eq = mlx5_eqn2comp_eq(dev, eqn)
                    mlx5_cmd_do MLX5_CMD_OP_CREATE_CQ
                    init_completion(&cq->free)
                    cq->comp = mlx5_add_cq_to_tasklet
						list_add_tail(&cq->tasklet_ctx.list, &tasklet_ctx->list)
                    mlx5_eq_add_cq -> net/mlx5：EQ，不同的 EQ 类型，在 mlx5 中，EQ 的用法有三种， 1. 异步 EQ，由 mlx5 核心内部使用。 FW 命令完成 b． FW 页面请求 c. 一个 EQ 用于所有其他异步事件 2. 完成 EQ，用于 CQ 完成（我们为每个核心创建一个） 3. *用于 RDMA 按需寻呼 (ODP) 的特殊类型 EQ（页面错误）。 *第三种类型至少在mlx5核心中不应该是特殊的，它是另一个具有特定用例的异步事件EQ，它将在接下来的两个补丁中删除，并将其逻辑完全移动到mlx5_ib，因为它是rdma 具体的。 在此补丁中，我们将 struct mlx5_eq 中的用例（eq 类型）特定字段删除为新的 eq 类型特定结构。 结构mlx5_eq_async； 结构mlx5_eq_comp； 结构mlx5_eq_pagefault； 区分特定类型的流程。 将来我们将允许用户创建自己的通用均衡器。 目前，我们将在下一个补丁中只允许 ODP 使用一个。 我们将为那些想要接收 mlx5 异步事件的人引入事件侦听器注册 API。 之后，mlx5 eq 处理将从功能/用户特定处理中清除
                        radix_tree_insert(&table->tree, cq->cqn, cq)
                    mlx5_get_async_eq -> net/mlx5：EQ，私有化 eq_table 和朋友，将不必要的 EQ 表结构和声明从公共 include/linux/mlx5/driver.h 移动到 mlx5_core 的私有区域和 eq.c/eq.h 中。 引入新的 mlx5 EQ API：mlx5_comp_vectors_count(dev)； mlx5_comp_irq_get_affinity_mask(dev, 矢量); 并从 mlx5_ib 或 mlx5e netdevice 使用它们，而不是直接访问 mlx5_core 内部结构
                    mlx5_debug_cq_add -> add_res_tree(dev, MLX5_DBG_RSC_CQ
                mlx5_cmd_check -> net/mlx5：cmdif，添加用于命令执行的新 api，添加 mlx5_cmd_do。 与 mlx5_cmd_exec 不同，此函数不会修改或转换 outbox.status。 该函数将返回： return = 0：命令已执行，outbox.status == MLX5_CMD_STAT_OK。 返回 = -EREMOTEIO：已执行，outbox.status！= MLX5_CMD_STAT_OK。 return < 0：固件或驱动程序无法执行命令。 并记录其他 mlx5_cmd_exec 函数
                    u16 opcode = in_to_opcode(in) -> net/mlx5：cmdif、cmd_check 重构，不要破坏内部低级 cmd_exec 和 cmd_invoke 函数中的命令发件箱。 相反，返回正确的唯一错误代码并将驱动程序错误检查移至 mlx5_cmd_exec() 中的更高级别
                    mlx5_internal_err_ret_value
                    cmd_status_to_err
                    cmd_status_print -> mlx5_core_err_rl cmd_status_str(status), status, syndrome, cmd_status_to_err(status)
            mlx5e_cq_arm -> mlx5_cq_arm
                sn = cq->arm_sn & 3
                ci = cons_index & 0xffffff
                *cq->arm_db = cpu_to_be32(sn << 28 | cmd | ci)
                doorbell[0] = cpu_to_be32(sn << 28 | cmd | ci)
                doorbell[1] = cpu_to_be32(cq->cqn)
                wmb()
                mlx5_write64(doorbell, uar_page + MLX5_CQ_DOORBELL) -> write register
        mlx5e_alloc_drop_rq
            mlx5_wq_cyc_create -> net/mlx5e: RX，在旧版 RQ 中使用循环 WQ，由于旧版 RQ 不支持 LRO，因此 WQ 中没有乱序完成的来源，我们可以使用循环完成。 这具有多个优点： - 减少 WQE 大小（较小的 PCI 事务）。 - 数据路径的开销较低（不处理“下一个”指针）。 - 没有为 WQ 头保留 WQE（链表中需要）。 - 允许在下游补丁中使用 frag 和 dma_info 结构之间的常量映射。 性能测试：ConnectX-4、单核、单 RX 环。 单环 XDP 丢弃的数据包速率大幅提高。 瓶颈从 HW（16Mpps）转移到 SW（20Mpps
                mlx5_db_alloc_node
                mlx5_frag_buf_alloc_node
                mlx5_init_fbc
            xdp_rxq_info_unused -> xdp/mlx5：设置 xdp_rxq_info，mlx5 驱动程序有一个特殊的 drop-RQ 队列（每个接口一个），可以简单地丢弃所有传入流量。 它可以帮助驱动程序在向下/向上操作时保持其他硬件对象（流转向）处于活动状态。 在接口设置期间以及接口关闭时，流量控制对象临时指向它。 它缺少常规 RQ 中设置的许多字段（例如，其状态永远不会切换到 MLX5_RQC_STATE_RDY）。 （感谢 Tariq Toukan 的解释）。 此 drop-RQ 的 XDP RX 队列信息标记为未使用，这允许我们使用与其他 RX 队列相同的删除/释放代码路径。 xdp_rxq_info 的驱动程序挂钩点： * reg ：mlx5e_alloc_rq() * 未使用：mlx5e_alloc_drop_rq() * unreg ：mlx5e_free_rq() 使用示例/bpf 程序在实际硬件上进行测试 -> xdp：新的 XDP rx-queue info 概念的基础 API，此补丁仅介绍核心数据结构和 API 函数。 所有启用 XDP 的驱动程序都必须先使用 API，然后才能使用此信息。 XDP 需要了解有关给定 XDP 帧到达的 RX 队列的更多信息。 对于 XDP bpf-prog 和内核端。 该补丁不是在每次需要新信息时扩展 xdp_buff，而是创建一个单独的主要读取结构 xdp_rxq_info，其中包含此信息。 我们强调此数据/缓存行用于只读信息。 这不适用于动态的每个数据包信息，请使用 data_meta 来处理此类用例。 性能优势是该信息可以在 RX 环初始化时设置，而不是更新 xdp_buff 中的 N 成员。 一个可能的（驱动程序级别）微优化是，每个 XDP/NAPI 循环可以执行一次 xdp_buff->rxq 分配。 额外的指针 deref 仅发生在需要访问此信息的程序中（因此，不会减慢现有用例）
                xdp_rxq->reg_state = REG_STATE_UNUSED
        mlx5e_create_rq
            MLX5_SET(rqc,  rqc, cqn,		rq->cq.mcq.cqn)
            MLX5_RQC_STATE_RST
            mlx5_fill_page_frag_array
            mlx5_core_create_rq
                MLX5_CMD_OP_CREATE_RQ
        mlx5e_modify_rq_state -> net/mlx5e：添加接口关闭丢弃的数据包统计信息，添加了以下数据包丢弃计数器： Rx 接口关闭丢弃的数据包 - 对 ETH 接口关闭时收到的数据包进行计数。 该计数器将在 ethtool 上显示为名为 rx_if_down_packets 的新计数器。 该实现为 drop rq 分配一个 q_counter，它在接口关闭时获取所有接收到的流量 -> net/mlx5e：引入mlx5e_flush_rq函数，添加刷新RQ的函数：清理描述符，释放页面并重置RQ。 此过程由恢复流程使用，并且还将在以下提交中使用，以在将通道切换到 XSK 模式时释放一些内存
            mlx5e_rqwq_reset -> net/mlx5e：在将 RQ 状态从 RST 移动到 RDY 之前重置 RQ 门铃计数器，在将 RQ 从 RST 移动到 RDY 状态之前将 RQ 门铃计数器初始化为零。 根据硬件规范，当 RQ 返回 RDY 状态时，完成时的描述符 ID 会被重置。 门铃记录必须符合
                mlx5_wq_ll_reset
                    mlx5_wq_ll_init_list
                    mlx5_wq_ll_update_db_record
                mlx5_wq_cyc_reset
            mlx5_core_modify_rq
    mlx5_tunnel_inner_ft_supported -> net/mlx5e：不缓存隧道卸载功能，当 mlx5e 在设备运行状况恢复后再次连接时，设备功能可能已被 eswitch 管理器更改。 例如，在一个流中，当 ECPF 在传统模式和 switchdev 之间更改 eswitch 模式时，它会更新流表隧道功能。 缓存的值仅在一处使用，因此只需检查该处的功能即可
        mlx5_tunnel_any_rx_proto_supported
            mlx5_tunnel_proto_supported_rx
    mlx5e_rx_res_create
        mlx5e_rx_res_rss_init_def -> net/mlx5e: 支持多个 RSS 上下文，添加对多个 RSS 上下文的支持。 非默认RSS上下文的资源是按需分配和创建的。 每个 RSS 上下文都可以通过实现的 ethtool 操作单独控制和配置。 这里我们将上下文总数限制为 16。我们不对间接表内容强制执行任何类型的新限制。 更具体地，两个单独的上下文可以被配置为完全或部分指向同一组接收环。 默认 RSS 上下文（索引 0）是使用其完整的 TIR 集创建的。 所有其他上下文均使用空集创建，然后在添加转向规则时在首次使用时添加 TIR。 我们使用引用计数机制来确保 RSS 上下文在规则指向它之前不会被删除。 当存在多个 RSS 上下文时阻止 ethtool set_channels 操作，因为当前内核无法防止不一致的通道配置破坏非默认 RSS 上下文
            mlx5e_rss_init -> net/mlx5e：重构 mlx5e_rss_init() 和 mlx5e_rss_free() API，引入以下代码重构： 1) 引入用于创建和销毁 rss 对象的单个 API，分别为 mlx5e_rss_create() 和 mlx5e_rss_destroy()。 2) mlx5e_rss_create() 构造并初始化 RSS 对象取决于函数 new param enum mlx5e_rss_create_type。 调用者（如 rx_res.c）将不再需要通过 mlx5e_rss_alloc() 分配 RSS 对象并通过 mlx5e_rss_init_no_tirs() 或 mlx5e_rss_init() 立即初始化它，这将通过对 mlx5e_rss_create() 的单次调用来完成。 因此，mlx5e_rss_alloc() 和 mlx5e_rss_init_no_tirs() 已从 rss.h 文件中删除并成为静态函数
                mlx5e_rss_params_indir_init
                    indir->table = kvmalloc_array -> net/mlx5e：为支持更多通道数做准备，数据中心服务器CPU数量随着时间的推移不断变大。 目前，我们的驱动程序将通道数限制为 128。最大通道数是由硬编码定义 (en.h/MLX5E_MAX_NUM_CHANNELS) 强制执行和限制的，即使设备和机器 (CPU 数) 可以允许更多通道数。 重构当前的实现以处理更多通道。 后续补丁中将增加最大支持通道数。 下面介绍 RQT 大小计算/分配方案： 1) 保留当前 RQT 大小 256，通道数最多为 128（旧限制）。 2) 对于更大的通道数，RQT 大小的计算方法是将通道数乘以 2，并将结果四舍五入到最接近的 2 次方。如果计算出的 RQT 大小超过 NIC 支持的最大大小，则回退到此最大 RQT 大小 (1 << log_max_rqt_size)。 由于 RQT 大小不再是静态的，因此可以动态分配和释放间接表 SW 影子
                mlx5e_rss_init_no_tirs
                    mlx5e_rss_params_init
                        rss->hash.hfunc = ETH_RSS_HASH_TOP
                        netdev_rss_key_fill -> net：提供每主机 RSS 密钥通用基础设施，RSS（接收方缩放）通常使用 Toeplitz 哈希和 40 或 52 字节的 RSS 密钥。 有些驱动程序使用常量（且众所周知的密钥），有些驱动程序每个端口使用随机密钥，使得绑定设置难以调整。 考虑到队列数量通常是 2 的幂，众所周知的密钥会增加攻击面。 该补丁提供了基础设施来帮助驾驶员做正确的事情。 驱动程序应使用 netdev_rss_key_fill() 来初始化其 RSS 密钥，即使它们提供 ethtool -X 支持以让用户稍后重新定义密钥。 新的 /proc/sys/net/core/netdev_rss_key 文件可用于获取主机 RSS 密钥，即使驱动程序不提供 ethtool -x 支持，以防某些应用程序想要精确设置流以匹配某些 RX 队列。 测试： myhost:# cat /proc/sys/net/core/netdev_rss_key 11:63:99:bb:79:fb:a5:a7:07:45:b2:20:bf:02:42:2d:08: 1a:dd:19:2b:6b:23:ac:56:28:9d:70:c3:ac:e8:16:4b:b7:c1:10:53:a4:78:41:36:40: 74:b6:15:ca:27:44:aa:b3:4d:72 myhost:# ethtool -x eth0 具有 8 个 RX 环的 eth0 的 RX 流哈希间接表：0: 0 1 2 3 4 5 6 7 RSS 哈希密钥：11:63:99:bb:79:fb:a5:a7:07:45:b2:20:bf:02:42:2d:08:1a:dd:19:2b:6b:23 :ac:56:28:9d:70:c3:ac:e8:16:4b:b7:c1:10:53:a4:78:41
                            net_get_random_once -> get_random_bytes
                        mlx5e_rss_get_default_tt_config -> static const struct mlx5e_rss_params_traffic_type rss_default_config[MLX5E_NUM_INDIR_TIRS]
                            u8 l3_prot_type
                            u8 l4_prot_type
                            u32 rx_hash_fields
                    mlx5e_rqt_init_direct -> mlx5e_rqt_init
                        mlx5_core_create_rqt -> MLX5_CMD_OP_CREATE_RQT
                mlx5e_rss_create_tirs
                    mlx5e_rss_create_tir
                        rss_get_tirp
                        mlx5e_tir_builder_alloc
                        mlx5e_tir_builder_build_rqt
                            mlx5e_tir_builder_get_tirc
                        mlx5e_tir_builder_build_packet_merge
                            const unsigned int rough_max_l2_l3_hdr_sz = 256
                            MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ - rough_max_l2_l3_hdr_sz) >> 8 -> net/mlx5e：为 SHAMPO 功能添加控制路径，此提交引入了 SHAMPO 功能的控制路径基础设施。 SHAMPO 功能通过将数据包拆分为标头和有效负载来实现数据包拼接，标头放置在专用缓冲区上，有效负载放置在 RX 环上，这允许在接收缓冲区中将流的数据部分连续拼接在一起。 SHAMPO 功能被实现为链表跨步 RQ 功能。 为了支持数据包分割和有效负载拼接： - 放大ICOSQ 和相应的CQ 以支持报头缓冲存储器区域。 - 添加支持在 open_rq 函数中使用 SHAMPO 功能集创建链表跨步 RQ。 - 添加释放函数和 SHAMPO 标头缓冲区的相应调用。 - 添加 mlx5e_create_umr_klm_mkey 以支持标头缓冲区的 KLM mkey。 - 将 mlx5e_create_umr_mkey 重命名为 mlx5e_create_umr_mtt_mkey
                        mlx5e_rss_get_tt_config
                        mlx5e_tir_builder_build_rss
                        mlx5e_tir_init
                            mlx5_core_create_tir -> MLX5_CMD_OP_CREATE_TIR
                            list_add(&tir->list, &res->td.tirs_list)
                        mlx5e_tir_builder_free
                            kvfree
            mlx5e_rss_set_indir_uniform
                indir->table[i] = i % num_channels
        mlx5e_rx_res_channels_init
            mlx5e_tir_builder_alloc -> net/mlx5e：隐藏 mlx5e_rx_res 的所有实现细节，此提交将 struct mlx5e_rx_res 的所有实现细节移至 en/rx_res.c 下。 现在，所有对 RX 资源的访问都是使用方法完成的。 将 RX 资源封装到一个对象中可以实现更好的可管理性，因为所有实现细节现在都位于一个位置，外部代码只能使用一组有限的 API 方法来初始化/拆卸整个事物、重新配置 RSS 和 LRO 参数、连接 TIR 用于流量控制和激活/停用 TIR。 mlx5e_rx_res 是独立的，不依赖于 struct mlx5e_priv 或 include en.h
            mlx5e_rqt_init_direct
            mlx5e_tir_builder_build_rqt
            mlx5e_tir_builder_build_packet_merge
            mlx5e_tir_builder_build_direct
            mlx5e_tir_init
            mlx5e_tir_builder_clear
        mlx5e_rx_res_ptp_init
            mlx5e_tir_builder_alloc
            mlx5e_rqt_init_direct
            mlx5e_tir_builder_build_rqt
            mlx5e_tir_builder_build_direct
            mlx5e_tir_init
    mlx5e_create_flow_steering
        mlx5_get_flow_namespace -> net/mlx5：使用mlx5_en的流量控制基础设施，公开新的流量控制API并删除旧的。 需要进行的更改很少： 1. 以太网流量控制遵循现有实现，但使用新的控制 API。 旧的流量控制实现已被删除。 2. 将 E-switch FDB 管理移至使用新 API。 3. 加载驱动程序时，调用 mlx5_init_fs 初始化流控制树结构，为 NIC 接收和 E-switch FDB 打开命名空间。 4.驱动卸载时调用mlx5_cleanup_fs
            enum mlx5_flow_namespace_type
        mlx5e_fs_set_ns
        mlx5e_arfs_create_tables
            arfs->wq = create_singlethread_workqueue("mlx5e_arfs")
            arfs_create_table
                mlx5_create_flow_table -> net/mlx5：流量控制，添加 vport ACL 支持，更新相关流量控制设备结构和命令以支持 vport。 更新流量控制核心 API 以接收 vport 号。 添加入口和出口 ACL 流表名称空间。 添加ACL流表支持： * ACL（访问控制列表）流表是仅包含允许/丢弃转向规则的表。 * 我们有两种类型的 ACL 流表 - 入口和出口。 * ACL 处理从 E-Switch FDB 表发送/到 E-Switch FDB 表的流量，入口是指从 Vport 发送到 E-Switch 的流量，Egress 是指从 E-Switch 发送到 vport 的流量。 * 根据从 VF 发送的流量检查入口 ACL 流表允许/丢弃规则。 * 根据发送到 VF 的流量检查出口 ACL 流表允许/丢弃规则
                    __mlx5_create_flow_table -> net/mlx5：重构创建流表方法以接受底层QP，IB流表需要底层QP来执行流控制。 这里我们更改流表创建的 API 以接受底层 QP 编号作为参数，以支持 IB (IPoIB) 流控制
                        fs_prio = find_prio(ns, ft_attr->prio)
                        alloc_flow_table
                        tree_init_node(&ft->node, del_hw_flow_table, del_sw_flow_table) -> net/mlx5：支持并行更新导向规则，添加新的流量导向规则大部分时间花在执行固件命令上。 最常见的操作是添加新的流量引导条目。 为了提高更新率，我们通过执行以下操作并行化命令：1）用读写器信号量替换互斥锁，并仅在必要时才获取写锁（例如，分配新的流表条目索引或向流表条目添加节点） 父母的子女名单）。 当我们尝试在父级的子级列表中找到合适的子级时（例如，搜索具有相同规则 match_criteria 的流组），那么我们只获取读锁。 2）添加版本控制机制——每个转向实体（FT、FG、FTE、DST）都会有一个增量版本。 当实体更改时，版本会增加（例如，当新的 FTE 添加到 FG 时 - FG 的版本会增加）。 使用版本控制来确定实体的子实体的最后一次遍历是否有效或需要在写锁定下重新扫描。 此支持将转向规则的插入速率从 ~5k/秒提高到 ~40k/秒 -> net/mlx5_core：添加流转向基础数据结构，介绍将代表 ConnectX-4 Flow Steering 的基础数据结构及其操作，该数据结构基本上是一棵树和所有流转向对象，例如（流表/流组） /FTE/etc ..) 表示为 fs_node(s)。 fs_node 是描述基本树节点的基础对象，具有以下额外信息： type：描述节点（Object）的运行时类型。 lock：锁定该节点子树。 ref_count：子级数+当前引用数。 remove_func：通用析构函数。 一旦在以下补丁中添加用法，fs_node 类型将被使用和解释
                            INIT_LIST_HEAD(&node->list)
                arfs_create_groups -> net/mlx5e：创建 aRFS 流表，创建以下四个流表以供 aRFS 使用： 1. IPv4 TCP - 过滤 IPv4 TCP 数据包的 4 元组。 2. IPv6 TCP - 过滤 IPv6 TCP 数据包的 4 元组。 3. IPv4 UDP - 过滤 IPv4 UDP 数据包的 4 元组。 4. IPv6 UDP - 过滤 IPv6 UDP 数据包的 4 元组。 每个流表有两个流组：一个用于四元组过滤（完全匹配），另一个包含 * 规则作为未命中规则。 完全匹配规则意味着 aRFS 命中，数据包将被转发到专用 RQ/Core，未命中规则数据包将被转发到默认 RSS 哈希
                    ft->g[ft->num_groups] = mlx5_create_flow_group(ft->t, in) -> net/mlx5_core: 引入流控制自动分组流表，当用户向自动分组流表添加规则时，我们会搜索具有相同匹配条件的流组，如果没有找到这样的组，则我们会创建具有所需匹配条件的新流组 并将规则插入到该组中。 我们将流表分为 required_groups + 1，以便为与任何现有组都不匹配的规则保留一部分流表
                        down_write_ref_node -> net/mlx5：向节点删除函数添加锁定标志，向节点删除函数添加锁定标志，以指示父级是否已从调用者函数锁定，作为外部锁定的预处理。 当前始终使用 false 且没有功能更改
                        alloc_insert_flow_group -> net/mlx5：重构FTE和FG创建代码，将创建代码拆分为两部分：1）对象分配-分配转向节点并初始化其资源。 2）固件命令执行。 向每个节点添加活动标志 - 该标志指示该对象是否存在于硬件中，如果不存在，我们不会在错误流中释放硬件资源。 此更改将使我们能够仅在第一部分上对父节点（例如用于 FTE 创建的 FG）进行写锁定
                            struct mlx5_flow_steering *steering = get_steering(&ft->node) -> net/mlx5：添加FG和FTE内存池，添加流组和流表条目的内存池分配。 它很有用，因为这些对象并不小，并且可以多次分配/释放
                            alloc_flow_group
                                fg = kmem_cache_zalloc
                                rhashtable_init
                                ida_init(&fg->fte_allocator)
                                fg->node.type =  FS_TYPE_FLOW_GROUP
                            rhltable_insert
                            tree_init_node(&fg->node, del_hw_flow_group, del_sw_flow_group)
                            tree_add_node(&fg->node, &ft->node)
                        trace_mlx5_fs_add_fg
                arfs_add_default_rule
                    dest.type = MLX5_FLOW_DESTINATION_TYPE_TIR
                    tt = arfs_get_tt(type) -> net/mlx5e：使用函数将aRFS映射到流量类型，为了更好的代码重用和可读性，使用现有函数arfs_get_tt()将arfs_type映射到mlx5e_traffic_types，而不是重复switch-case逻辑
                    mlx5e_rx_res_get_tirn_rss -> mlx5e_rss_get_tirn -> tir = rss_get_tir(rss, tt, inner)
                        rss_get_tirp -> inner ? &rss->inner_tir[tt] : &rss->tir[tt]
                    mlx5_add_flow_rules
            mlx5e_fs_set_arfs
        mlx5e_create_inner_ttc_table
            mlx5_tunnel_inner_ft_supported
            mlx5e_set_inner_ttc_params
                mlx5_get_flow_namespace
                mlx5e_rx_res_get_tirn_direct
                mlx5e_rx_res_get_tirn_rss_inner
            mlx5_create_inner_ttc_table
            return PTR_ERR_OR_ZERO(fs->inner_ttc)
        mlx5e_create_ttc_table
            mlx5e_set_ttc_params
            mlx5_create_ttc_table -> net/mlx5：嵌入 mlx5_ttc_table，mlx5_ttc_table 结构不应暴露给用户，因此此补丁使其成为 ttc 的内部结构。 另外添加一个getter函数来获取TTC流表，方便需要添加指向其的规则的用户
                mlx5_create_flow_table
                mlx5_create_ttc_table_groups
                    ttc->g[ttc->num_groups] = mlx5_create_flow_group(ttc->t, in) -> net/mlx5_core: 引入流控制自动分组流表，当用户向自动分组流表添加规则时，我们会搜索具有相同匹配条件的流组，如果没有找到这样的组，则我们会创建具有所需匹配条件的新流组 并将规则插入到该组中。 我们将流表分为 required_groups + 1，以便为与任何现有组都不匹配的规则保留一部分流表
                        struct mlx5_flow_root_namespace *root = find_root(&ft->node) -> net/mlx5：在 fs 和 cmd 之间添加填充层，填充层允许每个命名空间为添加/删除/更新命令定义可能不同的功能。 这里介绍的垫片层将用于支持 FPGA 的流量控制
                        alloc_insert_flow_group
                        err = root->cmds->create_flow_group(root, ft, fg_in, fg) -> mlx5_cmd_create_flow_group -> net/mlx5：将流控制对象传递给 fs_cmd，将流控制对象而不是其属性传递给 fs_cmd 以减少参数数量，此外它将用于更新对象字段。 传递流控制根命名空间而不是设备，因此将具有 fs_cmd 层中的命名空间的上下文
                            MLX5_CMD_OP_CREATE_FLOW_GROUP
                mlx5_generate_ttc_table_rules
                    mlx5_generate_ttc_rule
                        MLX5_CAP_FLOWTABLE_NIC_RX
                        mlx5_etype_to_ipv
                        mlx5_add_flow_rules
                    mlx5_tunnel_inner_ft_supported
                    mlx5_tunnel_proto_supported_rx
        mlx5e_create_l2_table
            mlx5_create_flow_table
            mlx5e_create_l2_table_groups
                eth_zero_addr(mc_dmac) -> Assign zero address
        mlx5e_fs_create_vlan_table
            mlx5e_create_vlan_table_groups -> __mlx5e_create_vlan_table_groups
            mlx5e_fs_add_vlan_rules
                mlx5e_add_vlan_rule
                    mlx5e_vport_context_update_vlans
                        mlx5_modify_nic_vport_vlans -> net/mlx5：引入访问函数来修改/查询 vport vlan，这些函数需要通知即将到来的 L2 表和 SR-IOV E-Switch（FDB）管理器（PF）、NIC vport（vf）vlan 表更改。 以太网 sriov 和 l2 表管理的准备
                            MLX5_CMD_OP_MODIFY_NIC_VPORT_CONTEXT
                    __mlx5e_add_vlan_rule
                        dest.ft = fs->l2.ft.t
                        *rule_p = mlx5_add_flow_rules(ft, spec, &flow_act, &dest, 1) -> net/mlx5：设置新的转向条目时支持 encap id，为了支持添加封装标头的转向规则，需要 encap_id 参数。 添加新的 mlx5_flow_act 结构，其中包含与操作相关的参数：action、flow_tag 和 encap_id。 添加新的转向规则时使用 mlx5_flow_act 结构。 此补丁不会改变任何功能
                for_each_set_bit(i, fs->vlan->active_cvlans
                for_each_set_bit(i, fs->vlan->active_svlans
                mlx5e_fs_add_any_vid_rules
                    mlx5e_add_vlan_rule
        mlx5e_ptp_alloc_rx_fs
            mlx5e_profile_feature_cap
            mlx5e_fs_set_ptp
        mlx5e_ethtool_init_steering
    mlx5e_tc_nic_init -> net/mlx5e：对卸载的 TC eswitch 流使用共享表，目前，每个表示器 netdev 使用自己的哈希表来保留从 TC 流 (f->cookie) 到驱动程序卸载实例的映射。 该表最初是为了卸载 TC NIC（而非 eswitch）规则而添加的。 当核心 TC 代码要求我们添加相同的流两次时（例如在 egdev 用例下），此方案就会中断，因为我们没有发现这一点，并且使用错误的源 vport 将第二个流卸载到硬件中。 作为解决此问题的前期步骤，我们转而使用单个表来保存所有卸载的 TC eswitch 流。 该表位于 eswitch 上行链路表示器对象
        mlx5e_mod_hdr_tbl_init
            hash_init(tbl->hlist)
        hash_init(tc->hairpin_tbl)
        lockdep_set_class -> 自行创建一种新的class。很多复杂的子系统都自己设置自己的class，比如inode，各种文件系统等
        lockdep_init_map
        mlx5_query_nic_system_image_guid -> net/mlx5：缓存系统映像 GUID，系统映像 GUID 是一个只读字段，TC 卸载代码使用它来确定两个 mlx5 设备在添加流时是否属于同一 ASIC。 读取一次并将其保存在核心设备上，而不是每次添加卸载流时都进行查询
            mlx5_query_nic_vport_system_image_guid
                mlx5_query_nic_vport_context -> net/mlx5：更新查询/修改 vport MAC 地址的访问功能，为了准备 SR-IOV，我们在此处添加一个 API，使每个 e-switch 客户端 (PF/VF) 能够配置其 L2 MAC 地址并为 e-switch 管理器（通常是 PF）访问它们，以便能够将它们配置到 e-switch 中。 因此，我们现在将 vport num 参数传递给 mlx5_query_nic_vport_context，以便 PF 可以访问其他 vport 上下文。 以太网 sriov 和 l2 表管理的准备
                    MLX5_CMD_OP_QUERY_NIC_VPORT_CONTEXT
            mlx5_query_hca_vport_system_image_guid -> net/mlx5_core：添加新的查询 HCA vport 命令，添加了以下命令的实现： 1. QUERY_HCA_VPORT_GID 2. QUERY_HCA_VPORT_PKEY 3. QUERY_HCA_VPORT_CONTEXT 当我们在 IB 驱动程序中使用 ISSI > 0 时也将需要它们
                MLX5_CMD_OP_QUERY_HCA_VPORT_CONTEXT
                rep->port_guid ...
        mapping_create_for_id
            mapping_create -> net/mlx5：引入映射基础设施，用于将唯一 id 映射到数据，添加一个新接口，用于将数据映射到给定 id 范围 (max_id)，然后再映射回来。 它使用 xarray 作为 id 分配器并查找给定的 id。 对于锁定，它使用 xa_lock (spin_lock) 进行 add()/del()，使用 rcu_read_lock 进行 find()。 该映射接口还支持通过工作队列延迟映射删除。 这是针对我们需要映射有一些宽限期以便再次找到它的情况，例如，来自硬件的数据包被规则标记为不再存在的旧映射
                INIT_DELAYED_WORK(&ctx->dwork, mapping_work_handler)
                xa_init_flags
        mlx5e_tc_nic_create_miss_table -> net/mlx5e：TC网卡模式，修复tc链miss表，引用的commit更改了按需创建的promisc表，网卡表中的最高优先级替换了vlan表，这导致tc NIC表miss流跳过prmoisc表 因为它使用 vlan 表作为 miss 表。 NIC 模式下的 OVS 卸载默认使用 promisc，因此由 tc NIC 表未命中流处理的任何单播数据包都将跳过 promisc 规则并被丢弃。 通过在新的 tc 级别中添加低优先级的新空表并将 nic tc 链未命中指向它来修复此问题，新表受到管理，因此如果禁用 promisc，它将指向 vlan 表；如果启用，它将指向 promisc 表
            mlx5_get_flow_namespace
            mlx5_create_auto_grouped_flow_table
                mlx5_create_flow_table
        mlx5_chains_create -> net/mlx5：重构多链和 prio 支持，将链基础设施与 eswitch 分离，并使其通用以支持其他转向命名空间。 该更改定义了一个不可知的数据结构，以保留用于在任何转向命名空间中维护流表链接的所有相关信息。 每个需要表链接的命名空间都需要分配这样的数据结构。 链创建代码将从调用者处接收转向命名空间和流表参数，因此在创建维护表链接功能所需的资源时，它将以不可知的方式进行操作，同时与 eswitch 特定功能相关的部分代码将移动到 eswitch 文件中
            mlx5_chains_init
                rhashtable_init(&chains_ht(chains), &chain_params)
                rhashtable_init(&prios_ht(chains), &prio_params)
        mlx5_chains_print_info
        mlx5e_tc_post_act_init -> net/mlx5e：引入后操作基础设施，一些 tc 操作在硬件中使用多个表进行建模，导致 tc 操作列表拆分。 例如，CT 动作是通过跳转到由 nf flowtable 控制的 ct 表来建模的。 sFlow 在硬件中跳转到示例表，该示例表继续到“默认表”，并应在其中继续处理操作列表。 多表操作使用唯一的 fte_id 在硬件中建模。 fte_id 在跳转到表之前设置。 拆分操作继续到操作后表，其中匹配的 fte_id 值继续执行 tc 操作列表。 目前动作后设计仅通过ct动作实现。 引入后操作基础设施作为通过 sFlow 卸载功能重用它的前置步骤。 初始化和销毁公共后操作表。 重构 ct 卸载以在下一个补丁中使用通用的帖子表基础设施
            mlx5_chains_create_global_table
                mlx5_chains_create_global_table
                chain = mlx5_chains_get_chain_range(chains)
                prio = mlx5_chains_get_prio_range(chains)
                level = mlx5_chains_get_level_range(chains)
                mlx5_chains_create_table
                    mlx5_chains_get_nf_ft_chain
                    mlx5_create_auto_grouped_flow_table
            xa_init_flags(&post_act->ids, XA_FLAGS_ALLOC1)
        register_netdevice_notifier_dev_net -> __register_netdevice_notifier_net
            raw_notifier_chain_register
        mlx5e_tc_debugfs_init
            tc->dfs_root = debugfs_create_dir("tc"
            debugfs_create_file("hairpin_num_active"
            debugfs_create_file("hairpin_table_dump"
        mlx5e_tc_act_stats_create -> net/mlx5e：TC，将 tc 操作 cookie 映射到硬件计数器，当前硬件计数器与流 cookie 关联。 这不适用于使用分支操作的流，这些操作需要返回每个操作的统计信息。 单个计数器可以应用于多个操作。 反向扫描流程操作（从最后一个操作到第一个操作），同时缓存最后一个计数器。 将所有流属性 tc action cookie 与当前缓存的计数器相关联
            rhashtable_init
    mlx5e_accel_init_rx -> net/mlx5e：kTLS，添加 kTLS RX 硬件卸载支持，实现对 kTLS RX 硬件卸载功能的驱动程序支持。 下游补丁中添加了重新同步支持。 新的卸载上下文通过每通道异步 ICOSQ 发布其静态/进度参数 WQE，并受到自旋锁的保护。 通道/RQ 根据套接字的 rxq 索引进行选择。 该功能默认处于关闭状态。 可以通过以下方式打开： $ ethtool -K <if> tls-hw-rx-offload on 新的 TLS-RX 工作队列用于允许在 NAPI 上下文之外异步添加转向规则。 它还将在重新同步过程中的下游补丁中使用
        mlx5e_ktls_init_rx
            mlx5e_is_ktls_rx
            priv->tls->rx_wq = create_singlethread_workqueue("mlx5e_tls_rx")
            mlx5e_accel_fs_tcp_create
                mlx5e_fs_set_accel_tcp
                accel_fs_tcp_create_table
                    accel_fs_tcp_create_groups
                    accel_fs_tcp_add_default_rule
                accel_fs_tcp_enable
                    mlx5_ttc_fwd_dest -> mlx5_modify_rule_destination
                        _mlx5_modify_rule_destination -> net/mlx5：添加多目标支持，当前在调用 mlx5_add_flow_rule 时我们仅接受一个流目标，此提交允许传递多个目标。 这一变化迫使我们将回报结构改为更灵活的结构。 我们引入一个流句柄（struct mlx5_flow_handle），它在内部保存创建的规则的编号，并保存一个数组，其中每个单元格都指向一个流规则。 从消费者（mlx5_add_flow_rule）的角度来看，此更改只是装饰性的，只需要更改它们存储的返回值的类型。 从核心角度来看，我们现在需要在分配和删除规则时使用循环（例如给我们一个流处理程序）
                            err = root->cmds->update_fte(root, ft, fg,



queue and queue depth(descriptor)


hash:
ethtool -x eth0

内核有一种称为 NAPI（New API）的机制，允许网卡注册自己的 poll() 方法，执行这个方法就会从相应的网卡收包。 关于 NAPI 后面会有更详细介绍，这里只看一下注册时的调用栈


mlx5e_open(netdev);
 |-mlx5e_open_locked
    |-mlx5e_open_channels
    |  |-mlx5e_open_channel
    |     |-netif_napi_add(netdev, &c->napi, mlx5e_napi_poll, 64); // 注册 NAPI
    |     |-mlx5e_open_queues
    |         |-mlx5e_open_cq
    |         |   |-mlx5e_alloc_cq
    |         |       |-mlx5e_alloc_cq_common
    |         |           |-mcq->comp = mlx5e_completion_event;
    |         |-napi_enable(&c->napi)                              // 启用 NAPI
    |-mlx5e_activate_priv_channels
       |-mlx5e_activate_channels
           |-for ch in channels:
               mlx5e_activate_channel                              // 启用硬中断（IRQ）
                 |-mlx5e_activate_rq
                    |-mlx5e_trigger_irq
                        mlx5_wq_cyc_ctr2ix
                        mlx5e_post_nop
                        mlx5e_notify_hw

中断方式针对高吞吐场景的改进是 NAPI 方式，简单来说它结合了轮询和中断两种方式。 绝大部分网卡都是这种模式，本文所用的 mlx5_core 就属于这一类



4 触发硬件中断（IRQ）
DMA 将包复制到 ring buffer（内核内存）之后，网卡发起对应的中断（在 MSI-X 场景，中断和 RX 队列绑定）。 来个具体例子，下面是台 40 核的机器，
cat /proc/interrupts



4.1 中断处理函数（ISR）注册
这个过程其实是在网卡驱动初始化（第一章）时完成的，但是第一章的内容太多了，所以我们放到这里看一下：

mlx5_load
 |-mlx5_irq_table_create
 |  |-table->irq = kcalloc()
 |  |-pci_alloc_irq_vectors(dev->pdev, MLX5_IRQ_VEC_COMP_BASE + 1, nvec, PCI_IRQ_MSIX);
 |  |-request_irqs
 |     |-for i in vectors:
 |         irq  = mlx5_irq_get(dev, i);
 |         irqn = pci_irq_vector(dev->pdev, i);
 |         irq_set_name(sprintf("mlx5_comp%d", vecidx-MLX5_IRQ_VEC_COMP_BASE), i);
 |         snprintf(irq->name, "%s@pci:%s", name, pci_name(dev->pdev));
 |         request_irq(irqn, mlx5_irq_int_handler, 0, irq->name, &irq->nh); // 注册中断处理函数
 |
 |-mlx5_eq_table_create      // 初始化事件队列（EventQueue）
    |-create_comp_eqs(dev)   // Completion EQ
        for ncomp_eqs:
		  tasklet_setup(&eq->tasklet_ctx.task, mlx5_cq_tasklet_cb)
          eq->irq_nb.notifier_call = mlx5_eq_comp_int; // 每个 EQ 事件完成时的回调函数
		  		eqe = next_eqe_sw(eq)
				cqn = be32_to_cpu(eqe->data.comp.cqn) & 0xffffff
				cq = mlx5_eq_cq_get(eq, cqn)
				++eq->cons_index
				eq_update_ci(eq, 1)
				tasklet_schedule(&eq_comp->tasklet_ctx.task)
          create_map_eq()
          mlx5_eq_enable()


驱动的硬中断处理函数做的事情很少，但软中断将会在和硬中断相同的 CPU 上执行。这就 是为什么给每个 CPU 一个特定的硬中断非常重要：这个 CPU 不仅处理这个硬中断，而且通 过 NAPI 处理接下来的软中断来收包



5.1 内核网络设备子系统初始化
网络设备（netdev）的初始化在 net_dev_init()，在系统启动期间执行：

// net/core/dev.c
static int __init net_dev_init(void)
    dev_proc_init();        // 注册 /proc/net/{dev,softnet_stat,ptytpe}


内核的软中断系统是一种在硬中断处理上下文（驱动中）之外执行代码的机制。  可以把软中断系统想象成一系列内核线程（每个 CPU 一个）， 这些线程执行针对不同事件注册的处理函数（handler）。  如果用过 top 命令，可能会注意到 ksoftirqd/0 这个内核线程，其表示这个软中断线程跑在 CPU 0 上。  内核子系统（比如网络）能通过 open_softirq() 注册软中断处理函数。


5.2 内核调度器与调用栈概览
5.2.1 调用栈
调度执行到某个特定线程的调用栈：

smpboot_thread_fn
  |-while (1) {
      set_current_state(TASK_INTERRUPTIBLE); // 设置当前 CPU 为可中断状态

      if !thread_should_run {                // 无 pending 的软中断
          preempt_enable_no_resched();
          schedule();
      } else {                               // 有 pending 的软中断
          __set_current_state(TASK_RUNNING);
          preempt_enable();
          thread_fn(td->cpu);                // 如果此时执行的是 ksoftirqd 线程，
            |-run_ksoftirqd                  // 那会执行 run_ksoftirqd() 回调函数
                |-local_irq_disable();       // 关闭所在 CPU 的所有硬中断
                |
                |-if local_softirq_pending() {
                |    __do_softirq();
                |    local_irq_enable();      // 重新打开所在 CPU 的所有硬中断
                |    cond_resched();          // 将 CPU 交还给调度器
                |    return;
                |-}
                |
                |-local_irq_enable();         // 重新打开所在 CPU 的所有硬中断
      }
    }
如果此时调度到的是 ksoftirqd 线程，那 thread_fn() 执行的就是 run_ksoftirqd()。



cat /proc/net/ptype

如今大部分网卡都在硬件层支持多队列。这意味着收进来的包会被通过 DMA 放到 位于不同内存的队列上，而不同的队列有相应的 NAPI 变量管理软中断 poll()过程。因此， 多个 CPU 同时处理从网卡来的中断，处理收包过程。 这个特性被称作 RSS（Receive Side Scaling，接收端水平扩展


I/O 加速技术( I/OAT ) 是Intel与高端服务器主板捆绑在一起的DMA 引擎（嵌入式DMA 控制器） ，它通过执行直接内存访问(DMA) 从主处理器卸载内存副本。它通常用于加速网络流量，但支持任何类型的复制。


pci_request_regions

req irq:
pci_alloc_irq_vectors



mlx5_send:
xmit
mlx5e_xmit



code struct:
net/mlx5：以太网数据路径文件
en_[rt]x.c 包含特定于 tx 或 rx 的数据路径相关代码。
en_txrx.c 包含 rx 和 tx 通用的数据路径代码，这主要是 napi 相关代码。

以下是数据路径中硬件和驱动程序正在使用的对象：
通道 - 每个 IRQ 一个通道。 每个通道对象包含：
RQ - 描述接收队列
TIR - 每种流类型一个 TIR（传输接口接收）对象。 TIR 包含接收流类型的属性（例如 IPv4、IPv6 等）。
流在流表中定义。 目前，TIR 描述了 RSS 哈希参数（如果存在）和 LRO 属性。
SQ - 描述 tx 队列。 每个TC（流量类别）有一个SQ（发送队列）。
TIS - 每个 TC 有一个 TIS（传输接口发送）。 它描述了 TC，稍后可能会扩展以描述更多传输属性。
RQ和SQ都继承自对象WQ（工作队列）。 描述 CQE 的 WQE 在内存中的布局的通用代码位于文件 wq.[cj]
对于每个通道，都有一个用于 RX 和 TX 的 NAPI 上下文。
驱动程序正在使用 netdev_alloc_skb() 来分配 skb。


通过netdev_alloc_skb()函数分配一个足够大的缓冲区来包含一个数据包和以太网头: · 第二步通过减少尾部空间为头部保留对齐的内存


操作码:
enum {
    MLX5_OPCODE_NOP			= 0x00,
    MLX5_OPCODE_SEND_INVAL		= 0x01,
    MLX5_OPCODE_RDMA_WRITE		= 0x08,
    MLX5_OPCODE_RDMA_WRITE_IMM	= 0x09,
    MLX5_OPCODE_SEND		= 0x0a,
    MLX5_OPCODE_SEND_IMM		= 0x0b,
    MLX5_OPCODE_LSO			= 0x0e,
    MLX5_OPCODE_RDMA_READ		= 0x10,
    MLX5_OPCODE_ATOMIC_CS		= 0x11,
    MLX5_OPCODE_ATOMIC_FA		= 0x12,
    MLX5_OPCODE_ATOMIC_MASKED_CS	= 0x14,
    MLX5_OPCODE_ATOMIC_MASKED_FA	= 0x15,
    MLX5_OPCODE_BIND_MW		= 0x18,
    MLX5_OPCODE_CONFIG_CMD		= 0x1f,
    MLX5_OPCODE_ENHANCED_MPSW	= 0x29,

    MLX5_RECV_OPCODE_RDMA_WRITE_IMM	= 0x00,
    MLX5_RECV_OPCODE_SEND		= 0x01,
    MLX5_RECV_OPCODE_SEND_IMM	= 0x02,
    MLX5_RECV_OPCODE_SEND_INVAL	= 0x03,

    MLX5_CQE_OPCODE_ERROR		= 0x1e,
    MLX5_CQE_OPCODE_RESIZE		= 0x16,

    MLX5_OPCODE_SET_PSV		= 0x20,
    MLX5_OPCODE_GET_PSV		= 0x21,
    MLX5_OPCODE_CHECK_PSV		= 0x22,
    MLX5_OPCODE_DUMP		= 0x23,
    MLX5_OPCODE_RGET_PSV		= 0x26,
    MLX5_OPCODE_RCHECK_PSV		= 0x27,

    MLX5_OPCODE_UMR			= 0x25,

    MLX5_OPCODE_FLOW_TBL_ACCESS	= 0x2c,

    MLX5_OPCODE_ACCESS_ASO		= 0x2d,
};




opa_netdev_start_xmit
ndo_start_xmit
.ndo_start_xmit          = mlx5e_xmit
当要更改的队列被禁用时，对 txq2sq 的所有更改都与 mlx5e_xmit 同步执行，并且 smp_wmb 保证在 mlx5e_xmit 尝试从 txq2sq 读取之前更改可见。 它保证当 mlx5e_xmit 在队列号 qid 上运行时 txq2sq[qid] 的值不会更改。 smb_wmb 与 ndo_start_xmit 周围的 HARD_TX_LOCK 配对，用作 ACQUIRE
skb_get_queue_mapping
mlx5e_accel_tx_begin -> net/mlx5e：将 TX 加速卸载分为两个阶段经过之前的修改，卸载不再一一调用，在 TLS 和 IPSEC 卸载之间计算 pi 并清除 wqe，这不太符合 mlx5e_accel_handle_tx 的目的 。 此补丁将 mlx5e_accel_handle_tx 拆分为两个函数，对应于运行卸载的两个逻辑阶段： 1. 在获取 WQE 之前。 这里运行的代码可以在获取主 WQE 之前自行发布 WQE。 它是 TLS 卸载的主要部分。 2. 获取 WQE 后。 这里运行更新 WQE 字段的代码，但无法再发布其他 WQE。 这是 TLS 卸载的一小部分，它在 cseg 中设置 tisn 字段，以及基于 eseg 的卸载（当前是 IPSEC，后续补丁也将 GENEVE 和校验和卸载移到那里）。 它允许 mlx5e_xmit 处理按正确顺序传输数据包所需的所有操作，改进代码结构并减少不必要的操作。 该结构将在后续补丁中得到进一步改进（所有基于 eseg 的卸载将移动到一个位置，并且为主 WQE 保留空间将在卸载的第 1 阶段和第 2 阶段之间进行，以消除不必要的数据移动）。
mlx5e_sq_xmit_prepare
mlx5e_tx_skb_supports_mpwqe -> net/mlx5e：使用 MACsec skb_metadata_dst 实现 MACsec Tx 数据路径 MACsec 驱动程序使用保存 64 位 SCI 编号的专用 skb_metadata_dst 标记用于设备卸载的 Tx 数据包。 先前设置的规则将匹配该号码，因此正确的 SA 用于 MACsec 操作。 由于设备驱动程序只能向流表提供 32 位元数据，因此需要使用从 64 位到 32 位标记或 id 的映射，这可以通过在控制路径中提供 32 位唯一流 id 来实现，并使用 哈希表将 64 位映射到数据路径中的唯一 ID
    mlx5e_txwqe_build_eseg
    mlx5e_sq_xmit_mpwqe -> net/mlx5e：SKB 的增强型 TX MPWQE 此提交添加了对常规 (SKB) 数据路径中的增强型 TX MPWQE 功能的支持。 MPWQE（多数据包工作队列元素）可以服务多个数据包，从而减少控制流量上的 PCI 带宽。 添加了两个新的统计数据（tx*_mpwqe_blks 和 tx*_mpwqe_pkts）。 该功能默认开启，并由 skb_tx_mpwqe 私有标志控制。 在 MPWQE 中，eseg 在所有数据包之间共享，因此基于 eseg 的卸载（IPSEC、GENEVE、校验和）在单独的 eseg 上运行，该 eseg 与当前 MPWQE 会话的 eseg 进行比较，以确定是否可以将新数据包添加到 同一次会议。 MPWQE 与某些卸载和功能不兼容，例如 TLS 卸载、TSO、非线性 SKB。 如果使用此类不兼容的功能，驱动程序会正常回退到非 MPWQE。 此更改对 TCP 单流测试和 XDP_TX 单流测试没有性能影响。
    mlx5e_tx_mpwqe_ensure_complete
mlx5e_sq_calc_wqe_attr
mlx5e_txqsq_get_next_pi -> net/mlx5e：统一为 WQE 保留空间 在我们的快速路径设计中，WQE（工作队列元素）不得跨越页面边界。 为了强制执行这一点，对于由多个 BB（基本块）组成的 WQE，驱动程序会提前检查 WQ 中的可用连续空间，如果不够，则用 NOP 填充。 此补丁修改了计算下一个 WQE 位置的代码，考虑填充，并准备 WQE。 此代码对于所有 SQ 类型都是通用的。 在此补丁中，它进行了重新组织，使所有 SQ 类型的使用模式统一，并使实现独立且看起来几乎相同，准备重复代码以进一步尝试对其进行重复数据删除。 保留一个地方：里面有mlx5e_sq_xmit和mlx5e_fill_sq_frag_edge调用，因为它的特殊之处在于它在保留空间时也可能复制WQE的cseg和eseg。 这将在以下补丁之一中消除，并且该位置也将转换为新方法。
mlx5e_accel_tx_finish
mlx5e_txwqe_build_eseg
mlx5e_sq_xmit_wqe -> send wqe
    ihs -> mlx5：支持 BIG TCP 数据包 mlx5 支持 LSOv2。 IPv6 gro/tcp 堆栈为大数据包插入带有 JUMBO TLV 的临时逐跳标头。 当填充 TX 描述符时，我们需要忽略/跳过这个 HBH 标头。 请注意，ipv6_has_hopopt_jumbo() 仅识别非常具体的数据包布局，因此 mlx5e_sq_xmit_wqe() 仅处理此布局。 v7：采用 unsafe_memcpy() 和 MLX5_UNSAFE_MEMCPY_DISCLAIMER v2：清除 mlx5e_tx_get_gso_ihs() 中的 hopbyhop v4：修复 CONFIG_MLX5_CORE_IPOIB=y 的编译错误
    hopbyhop -> IPv6 BIG TCP 允许网络协议栈准备更大的 GSO（发送）和 GRO（接收）数据包，以减少协议栈的遍历次数，从而提高性能和延迟。它可减少 CPU 负载，有助于实现更高的速度（即 100Gbit/s 及以上）。为了让这些数据包通过协议栈，BIG TCP 在 IPv6 头之后添加了一个临时的 "逐跳"（Hop-By-Hop）头，并在通过线路传输数据包之前将其剥离。BIG TCP 可在双协议栈设置中运行，IPv4 数据包将使用旧的下限（64k），IPv6 数据包将使用新的较大下限（192k）。请注意，Cilium 假定 GSO 和 GRO 的默认内核值为 64k，只有在必要时才会进行调整，也就是说，如果启用了 BIG TCP，而当前的 GSO/GRO 最大值小于 192k，那么 Cilium 会尝试增加这些值；如果禁用了 BIG TCP，而当前的最大值大于 64k，那么 Cilium 会尝试减少这些值。BIG TCP 不需要更改网络接口 MTU
    mlx5e_insert_vlan
    mlx5e_txwqe_build_dsegs -> net/mlx5e：Xmit 流分解 将当前的 mlx5e xmit 流分解为更小的块（辅助函数），以便将它们重新用于 IPoIB SKB 传输
        dma_map_single
        mlx5e_dma_push
        for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
            mlx5e_dma_push
mlx5e_txwqe_complete -> net/mlx5e：使 tx_port_ts 逻辑能够适应无序 CQE 使用映射结构将包含端口时间戳信息的 CQE 与适当的 skb 相关联。 跟踪使用 FIFO 提交的 WQE 的顺序。 检查 FIFO 中查找值中的相应端口时间戳 CQE 是否被视为由于时间流逝而被丢弃。 使用 skb 后将查找值返回到空闲列表。 在未来的 WQE 提交迭代中重用释放的查找。 Map 结构使用整数标识符作为键，并返回与该标识符对应的 skb。 当 SQ 是 PTP（端口时间戳）SQ 时，将整数标识符嵌入提交给传输路径 WQ 的 WQE 中。 然后可以使用相应端口时间戳CQ的CQE中的字段来查询嵌入的标识符。 在端口时间戳napi_poll上下文中，从CQ轮询的CQE中查询标识符，并用于从WQE提交路径查找相应的skb。 skb 引用从映射中删除，然后嵌入来自 CQE 的端口硬件时间戳信息并最终被消耗。 元数据空闲列表 FIFO 是一个包含整数标识符的数组，可以在 FIFO 中压入和弹出这些整数标识符。 此结构的目的是记录哪些标识符值可以在后续 WQE 提交中安全使用，并且不应包含尚未通过在端口时间戳 CQ 上处理相应 CQE 完成而收获的标识符。 ts_cqe_pending_list 结构是数组和链表的组合。 该数组预先填充了将在链表头部添加和删除的节点。 每个节点都包含与在 WQE 中提交并在端口时间戳 CQE 中检索的值关联的唯一标识符值。 当提交WQE时，数组中与从元数据空闲列表中弹出的标识符相对应的节点被添加到CQE挂起列表的末尾，并被标记为“使用中”。 在两种情况下，节点会从链表中删除。 第一个条件是在 PTP napi_poll 上下文中轮询相应的端口时间戳 CQE。 第二个条件是自 WQE 提交对应的 DMA 时间戳值以来已经过去了一秒以上。 当第一个条件发生时，链表节点中的“使用中”位被清零，然后释放WQE提交对应的资源。 然而，第二个条件表明端口时间戳 CQE 可能永远不会被传递。 设备在无限长的时间后发布 CQE 并非不可能，尽管可能性极小。 为了应对这种不可能的情况，与相应的 WQE 提交相关的资源仍然保留，标识符值不会返回到空闲列表，并且节点上的“正在使用”位被清除以指示它不再 “可能交付”端口时间戳 CQE 标识符链接列表的一部分。 维护被认为极有可能永远不会被设备传送的端口时间戳 CQE 的数量的计数。 如果在 PTP napi_poll 上下文中轮询被认为不太可能交付的端口时间戳 CQE，则此计数会减少。
    mlx5e_tx_skb_update_hwts_flags
    mlx5e_tx_check_stop
    mlx5e_skb_cb_hwtstamp_init -> net/mlx5e：添加 TX 端口时间戳支持 使用来自端口的时间戳（而不是数据包 CQE 创建时间戳）时，可以提高传输数据包时间戳的准确性，因为它可以更好地反映数据包传输的实际时间。 从 ConnectX6-DX 硬件开始支持 TX 端口时间戳。 尽管在原始完成时，只能附加 CQE 时间戳，但我们可以通过与 SQ 关联的特殊 CQ（除了常规 CQ 之外）上的附加完成来获取 TX 端口时间戳。 驱动程序忽略原始数据包完成时间戳，并报告特殊 CQ 完成的时间戳。 如果两次完成之间的绝对时间戳差异大于 1 / 128 秒，请忽略 TX 端口时间戳，因为它的抖动太大。 额外的补全不会生成任何 skb。 为每个 ptpsq 分配额外的 CQ，以接收 TX 端口时间戳。 驱动程序保存 skb FIFO，以便将传输的 skb 映射到两个预期完成。 使用 ptpsq 时，在 skb 上保留双引用计数，以保证在两个完成到达之前它不会被释放。 公开 ptp 附加 CQ 的专用计数器并将其连接到 TX 运行状况报告器。 该补丁将 TX 硬件时间戳偏移改进为在 100Gbps 线路速率下小于 40ns，而之前为 600ns。 这样，我们的硬件就符合 G.8273.2 C 类标准，并允许 Linux 系统部署在 5G 电信边缘，而该标准是必须的。
    mlx5e_ptp_metadata_map_put
    mlx5e_ptpsq_track_metadata -> net/mlx5e：填充元数据映射后跟踪 xmit 提交到 PTP WQ 在跟踪元数据索引以检测未传递的 CQE 之前，确保 skb 在映射到 skb 的元数据中可用。 如果在将 skb 放入映射之前将元数据索引放入跟踪列表中，则元数据索引可能会用于在相关 skb 在映射中可用之前检测未传递的 CQE，这可能导致 null-ptr-deref。
    mlx5e_ptpsq_track_metadata -> net/mlx5e：扩展 SKB 空间检查以包括 PTP-SQ 设置 tx_port_ts 时，驱动程序将 PTP 端口上的所有 UPD 流量转移到专用 PTP-SQ。 SKB 会被缓存，直到有线 CQE 到达。 当数据包大小大于 MTU 时，固件可能会丢弃它，并且数据包不会传输到线路，因此线路 CQE 将无法到达驱动程序。 在这种情况下，SKB 累积在 SKB fifo 中。 添加房间检查以考虑 PTP-SQ SKB fifo，当 SKB fifo 已满时，驱动程序会停止队列，导致 TX 超时。 Devlink TX-reporter 可以从中恢复。
    send_doorbell = __netdev_tx_sent_queue(sq->txq, attr->num_bytes, xmit_more)
        netdev_tx_sent_queue
    mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg)
        dma_wmb()
        wmb()
        mlx5_write64((__be32 *)ctrl, uar_map)

then:
mlx5e_poll_tx_cq




send:
dev_queue_xmit

.send		= dev_queue_xmit
    ...
    ndo_start_xmit



struct mlx5e_priv {



net/mlx5e: refactor xmit send function:
一个庞大的函数mlx5e_sq_xmit被拆分成几个来实现多个
目标：
1. 重用IPoIB中的代码。
2. 更好地与 TLS、IPSEC、GENEVE 和校验和卸载集成。 现在可以在运行基于 eseg 的卸载之前在 WQ 中预留空间，因此：
2.1. mlx5e_fill_sq_frag_edge 之后不再需要复制 cseg 和 eseg。
2.2. 将使用 mlx5e_txqsq_get_next_pi 代替旧版 mlx5e_fill_sq_frag_edge，以实现更好的代码可维护性和重用性。
3. 为即将到来的 SKB TX MPWQE 做好准备。 之后就会介入
mlx5e_sq_calc_wqe_attr 检查是否可以使用 MPWQE，以及
代码流将分为两条路径：MPWQE 和非 MPWQE。
提供了两个高级函数来发送数据包：
* mlx5e_xmit 由网络堆栈调用，运行卸载并发送数据包。 在以下补丁之一中，MPWQE 支持将添加到此流程中。
* mlx5e_sq_xmit_simple 由 TLS 卸载调用，仅运行校验和卸载并发送数据包。
此更改对 TCP 单流测试和 XDP_TX 单流测试没有性能影响。
当使用最新的 GCC 编译时，此更改显示不可见
对 UDP pktgen（突发 32）单流测试的性能影响：
数据包速率：16.86 Mpps (±0.15 Mpps) -> 16.95 Mpps (±0.15 Mpps)
每包指令：434 -> 429
每包周期：158 -> 160
每个周期指令数：2.75 -> 2.69
CPU：Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz (x86_64)
网卡：Mellanox ConnectX-6 Dx
gcc 10.2.0

gso:
skb_is_gso
GSO用来扩展之前的TSO，目前已经并入upstream内核。TSO只能支持tcp协议，而GSO可以支持tcpv4, tcpv6, udp等协议


RX Multi-packet WQE, MPWQE:
net/mlx5e：SKB 的增强型 TX MPWQE
此提交添加了对常规 (SKB) 数据路径中的增强型 TX MPWQE 功能的支持。 MPWQE（多数据包工作队列元素）可以服务多个数据包，从而减少控制流量上的 PCI 带宽。



mlx5e_sq_calc_wqe_attr -> net/mlx5e：添加缺少的最大 TX WQE 大小的完整性检查下面引用的提交开始使用最大 TX WQE 大小的固件功能。 此提交添加了一项重要的检查，以验证驱动程序不会尝试超出此功能，并且还恢复在引用的提交中错误删除的另一项检查（WQE 不得超过页面大小）。


struct mlx5e_txqsq


net/mlx5e：避免在配置更改时重置 netdev 统计信息 将所有 RQ、SQ 和通道计数器从通道对象移至 priv 结构中。 通过此更改，计数器将不会在通道配置更改时重置。 与高于零的 TC 关联的 SQ 的通道统计信息将在 ethtool -S 中显示，仅适用于自模块加载以来至少打开一次的 SQ（无论其打开/关闭当前状态如何）。 这样做是为了减少为常见的开箱即用（无 QoS）而呈现和计算的统计总量。 mlx5e_channel_stats 是 CH、RQ、SQ 统计数据的组合，以便在处理同一通道的 TX 和 RX 时为 NAPI 创建局部性。 对齐每个环的新统计结构，以避免多个通道同时更新到同一缓存行。 测试了数据包速率，没有感觉到降级。

mlx5：支持 BIG TCP 数据包 mlx5 支持 LSOv2。 IPv6 gro/tcp 堆栈为大数据包插入带有 JUMBO TLV 的临时逐跳标头。 当填充 TX 描述符时，我们需要忽略/跳过这个 HBH 标头。 请注意，ipv6_has_hopopt_jumbo() 仅识别非常具体的数据包布局，因此 mlx5e_sq_xmit_wqe() 仅处理此布局。 v7：采用 unsafe_memcpy() 和 MLX5_UNSAFE_MEMCPY_DISCLAIMER v2：清除 mlx5e_tx_get_gso_ihs() 中的 hopbyhop v4：修复 CONFIG_MLX5_CORE_IPOIB=y 的编译错误

skb send/receive:
TX 方向
用户态应用程序基于socket系统调用接口，传送需要TX 的HTTP应用层数据（如 Ngnix产生）
内核态socket 层读取用户态数据，并按照应用层的协议类型，将数据发送到对应的传输层（如HTTP对应TCP）
传输层申请 skb 数据结构，并填充数据到skb，然后skb 会向下传送到网络层和链路层（链路层在内核对应网络设备层），继续添加 IP 和 MAC header到skb中
最后skb 到达网络设备驱动，其从skb 中获取到packet data 数据的虚拟地址（skb->data)，并映射出dma总线地址给 网卡进行DMA读取（如对虚拟地址和总线地址有疑问，请参考另一篇文章“一文读懂 内存DMA 及 设备内存控制”）
RX 方向
如高速网卡收到从光纤上传入的数据包后，基于网卡内DMA控制器 和 PCIe 总线，将数据包送入设备驱动层
设备驱动（Device Driver）收到数据包后，申请SKB，并将数据放入SKB 结构指向的数据空间（位于 skb结构 的head 和 end 指针之间）注：驱动放数据到skb 有两种方式，一种方式是dev_alloc_skb + memcpy；第二种是 page + build_skb 实现零拷贝，效率更高
驱动调用 NAPI Schedule 将RX数据送入TCP/IP Stack, 到达 IP 层则去除 SKB 指向数据的 IP Header，到达TCP 层去除 TCP Head
最后在内核基于IP等路由信息，查找到对应的socket，将数据基于socket 送入用户态应用程序



static inline struct sk_buff *dev_alloc_skb(unsigned int length)



send complete:
int mlx5e_napi_poll(struct napi_struct *napi, int budget)
    struct mlx5e_channel *c = container_of(napi, struct mlx5e_channel, napi)
    busy |= mlx5e_poll_xdpsq_cq(&c->xdpsq.cq)
        mlx5_cqwq_get_cqe
        ...
        mlx5e_txqsq_wake
            mlx5_cqwq_update_db_record
                *wq->db = cpu_to_be32(wq->cc & 0xffffff)
            netif_tx_wake_queue
    mlx5e_poll_ico_cq -> net/mlx5e：允许多个 RQ 使用 ICO SQ，准备创建 XSK RQ，这也需要发布 UMR。 相同的 ICO SQ 将用于两个 RQ，并通过发布 NOP 来触发中断。 UMR WQE 不能再重复使用。 提交 ab966d7e4ff98 中引入的优化（“net/mlx5e：RX，UMR WQE 的回收缓冲区”）已恢复
    if (busy)
        mlx5e_channel_no_affinity_change
    for (i = 0; i < c->num_tc; i++)
        mlx5e_handle_tx_dim
            dim_update_sample(sq->cq.event_ctr, stats->packets, stats->bytes, &dim_sample)
            net_dim(&sq->dim, dim_sample)
                switch (dim->state)
                case DIM_MEASURE_IN_PROGRESS
                    dim_calc_stats
                    net_dim_decision
                    schedule_work(&dim->work)
                case DIM_START_MEASURE
                    dim_update_sample
    mlx5e_handle_rx_dim
    mlx5e_cq_arm
    



IB/mlx5：实现分段完成队列（CQ）
目前create CQ的实现需要连续的内存，这样的要求是有问题的，一旦内存碎片或者系统内存不足，就会导致dma_zalloc_coherent()失败。
该补丁实现了分段 CQ 的新方案，通过引入新类型“struct mlx5_frag_buf_ctrl”来分配分段缓冲区（而不是连续缓冲区）来克服此问题。 完成队列（CQ）基于这个新的分段缓冲区。
它修复了以下崩溃：
kworker/29:0：页面分配失败：顺序：6，模式：0x80d0
CPU：29 PID：8374 通讯：kworker/29:0 受污染：G OE 3.10.0
工作队列：ib_cm cm_work_handler [ib_cm]
调用轨迹：
[<>] 转储堆栈+0x19/0x1b
[<>] warn_alloc_failed+0x110/0x180
[<>] __alloc_pages_slowpath+0x6b7/0x725
[<>] __alloc_pages_nodemask+0x405/0x420
[<>] dma_generic_alloc_coherent+0x8f/0x140
[<>] x86_swiotlb_alloc_coherent+0x21/0x50
[<>] mlx5_dma_zalloc_coherent_node+0xad/0x110 [mlx5_core]
[<>]？ mlx5_db_alloc_node+0x69/0x1b0 [mlx5_core]
[<>] mlx5_buf_alloc_node+0x3e/0xa0 [mlx5_core]
[<>] mlx5_buf_alloc+0x14/0x20 [mlx5_core]
[<>] create_cq_kernel+0x90/0x1f0 [mlx5_ib]
[<>] mlx5_ib_create_cq+0x3b0/0x4e0 [mlx5_ib]


我们说过，Linux在硬中断里只完成简单必要的工作，剩下的大部分的处理都是转交给软中断的。通过上面代码可以看到，硬中断处理过程真的是非常短。只是记录了一个寄存器，修改了一下下CPU的poll_list，然后发出个软中断。就这么简单，硬中断工作就算是完成了
trigger soft_irq:
__raise_softirq_irqoff
or_softirq_pending(1UL << nr)
...
deal_with irq:
static void run_ksoftirqd(unsigned int cpu)
{
    local_irq_disable();
    if (local_softirq_pending()) {
        __do_softirq();
        rcu_note_context_switch(cpu);
        local_irq_enable();
        cond_resched();
        return;
    }
    local_irq_enable();
}

net_rx_action



# LKMM 相关术语的简要定义

# 参考

tools/memory-model/Documentation/glossary.txt

tools/memory-model/Documentation/explanation.txt
ib_drain_qp 排空队列


rdma driver:
mlx5_ifc.h

e1000
e1000_hw.h
static int __init e1000_init_module(void)
drivers/net/ethernet/intel/e1000/e1000_main.c
e1000_probe
    need_ioport = e1000_is_need_ioport(pdev)
    bars = pci_select_bars(pdev, IORESOURCE_MEM | IORESOURCE_IO)
    or bars = pci_select_bars(pdev, IORESOURCE_MEM)
    
static const struct net_device_ops e1000_netdev_ops = {
	.ndo_open		= e1000_open,
	.ndo_stop		= e1000_close,
	.ndo_start_xmit		= e1000_xmit_frame,
	.ndo_set_rx_mode	= e1000_set_rx_mode,
	.ndo_set_mac_address	= e1000_set_mac,
	.ndo_tx_timeout		= e1000_tx_timeout,
	.ndo_change_mtu		= e1000_change_mtu,
	.ndo_eth_ioctl		= e1000_ioctl,
	.ndo_validate_addr	= eth_validate_addr,
	.ndo_vlan_rx_add_vid	= e1000_vlan_rx_add_vid,
	.ndo_vlan_rx_kill_vid	= e1000_vlan_rx_kill_vid,
#ifdef CONFIG_NET_POLL_CONTROLLER
	.ndo_poll_controller	= e1000_netpoll,
#endif
	.ndo_fix_features	= e1000_fix_features,
	.ndo_set_features	= e1000_set_features,
};

e1000_open
    e1000_configure
        e1000_configure_rx


e1000_clean_rx_ring
    e1000_clean_rx_irq
        e1000_receive_skb


struct net_device_ops e1000_netdev_ops 

ndo_start_xmit e1000_xmit_frame

现在的中断驱动程序都采用的是NAPI方式，需要提供poll函数，本驱动中是e1000_netpoll轮询函数
当有新数据包要发送时候，首先上层协议调用e1000_xmit_frame函数，然后在该函数中调用e1000_tx_queue来根据相应的参数找到缓冲块存放，缓冲块中有dma成员，表示该数据包所在的总线地址，控制总线会把内容映射到总线地址，然后由网卡传送出去。

当有新数据包达到时，首先触动中断处理函数e1000_intr，该中断函数会将数据包放在buffer_info的缓冲块中()，就是讲总线地址指向的内容复制到skb中，然后根据skb中的协议将其传给上层协议的接收函数

netif_napi_add(netdev, &adapter->napi, e1000e_poll);


drivers/net/ethernet/intel/


kernel source code:
https://elixir.bootlin.com/linux/v6.8-rc1/source/drivers/net/ethernet/intel/e1000/e1000_main.c


mlx5 driver:
drivers/net/ethernet/mellanox/mlx5/core/main.c
module_init(mlx5_init); -> net: mlx5: 消除匿名 module_init 和 module_exit ，消除匿名 module_init() 和 module_exit()，这可能会在读取 System.map、崩溃/oops/bug 或 initcall_debug 日志时导致混乱或歧义
get_random_bytes -> void get_random_bytes -> 这将返回任意数量的随机字节。 随机字节的质量与 /dev/urandom 一样好。 为了确保该函数提供的随机性良好，应调用函数 wait_for_random_bytes() 并在之前的任何时刻至少返回一次 0
static u32 sw_owner_id[4] -> net/mlx5：在 init HCA 期间设置软件所有者 ID，为每个主机生成唯一的 128 位标识符，并在 INIT_HCA 命令中将该值传递给固件（如果报告了 sw_owner_id 功能）。 绑定到 mlx5_core 驱动程序的每个设备都将具有相同的软件所有者 ID。 在后续补丁中，mlx5_core 设备将通过新的 VPort 命令进行绑定，以便它们可以在单个 InfiniBand 设备下一起运行。 只能绑定具有相同软件所有者 ID 的设备，以防止发往一台主机的流量到达另一台主机。 INIT_HCA 命令长度扩展了 128 位。 命令长度作为输入 FW 命令提供。 较旧的固件以新的较长形式接收此命令没有问题
mlx5_core_verify_params -> net/mlx5：验证模块参数 验证 mlx5_core 模块参数，确保它们在预期范围内，如果未将其恢复为默认值。
    profile -> static struct mlx5_profile profile[] -> 默认配置文件 -> mlx5：将 pci 设备处理从 mlx5_ib 移至 mlx5_core 在为 VPI 的新 mlx5 设备（即端口可以是 IB 或 ETH）做准备时，请将 pci 设备功能从 mlx5_ib 移至 mlx5_core。 这涉及以下更改： 1. 将 mlx5_core_dev 结构移出 mlx5_ib_dev。 mlx5_core_dev 现在是由 mlx5_core 维护的独立结构。 mlx5_ib_dev 现在有一个指向该结构的指针。 这需要更改通过 mlx5_ib_dev 访问 core_dev 结构的很多地方（现在，这需要是指针取消引用）。 2. 所有 PCI 初始化现在都在 mlx5_core 中完成。 因此，现在是 mlx5_core 执行 pci_register_device （而不是像以前那样的 mlx5_ib）。 3. mlx5_ib 现在将自身注册为 mlx5_core 作为“接口”驱动程序。 这与 mlx4 (ConnectX) 驱动程序采用的机制非常相似。 一旦 HCA 初始化（由 mlx5_core），它就会调用接口驱动程序来进行初始化。 4. 核心注册了一个新的事件处理程序：mlx5_core_event()。 该事件处理程序调用接口注册的事件处理程序
    {net，RDMA}/mlx5：修复其他设备对 log_max_qp 的覆盖，mlx5_core_dev 保存指向静态配置文件的指针，因此当配置文件的 log_max_qp 被某些设备覆盖时，它会影响共享相同配置文件的所有其他 mlx5 设备。 通过为每个 mlx5 设备提供一个配置文件实例来修复此问题
    index为3的配置文件是, net/mlx5：为 SF 创建新的配置文件，为 SF 创建新的配置文件以禁用命令缓存。 每个功能命令缓存消耗约 500KB 的内存，当使用大量 SF 时，这种节省在内存受限的系统上非常显着。 使用新的配置文件来提供 SF 和 PF 之间未来的差异。 mr_cache 不用于非 PF 函数，因此从新配置文件中排除
mlx5_register_debugfs
    mlx5_debugfs_root = debugfs_create_dir("mlx5", NULL) -> 第一个参数是目录的名称，第二个参数用来指定这个目录的上级目录，如果是NULL，则表示是放在debugfs的根目录里,  /sys/kernel/debug/
mlx5e_init -> mlx5_eth -> net/mlx5e：将以太网部分连接到辅助总线，重用辅助总线对mlx5驱动程序的以太网部分进行设备管理 -> 目前，删除和重新加载流程可以与模块清理并行运行。 这种设计很容易出错。 例如：从具有不同锁定的清理和删除流中调用 aux_drivers 回调，这可能会导致死锁[1]。 因此，通过重新加载和删除来序列化模块清理
    mlx5e_build_ptys2ethtool_map -> net/mlx5e：使用新的 ethtool 获取/设置链接 ksettings API，使用新的获取/设置链接 ksettings 并删除获取/设置设置旧回调。 这允许我们为支持和通告的链接模式使用超过 32 位的位掩码，并使用以前不支持的模式
        __ETHTOOL_DECLARE_LINK_MODE_MASK
            DECLARE_BITMAP(name, __ETHTOOL_LINK_MODE_MASK_NBITS) -> 定义一个位图变量，本质就是一个unsigned long类型的数组, 宏的参数：name是变量名，bits是位图有多少个位
        MLX5_BUILD_PTYS2ETHTOOL_CONFIG
        ...
    auxiliary_driver_register(&mlx5e_driver) -> __auxiliary_driver_register
    mlx5e_rep_init -> net/mlx5e：首先取消注册 eth-reps 设备 当我们清理所有接口（即重新扫描或重新加载模块）时，我们需要先清理 eth-reps 设备，然后再清理 eth 设备。 我们将重新使用本机 NIC 端口网络设备实例作为上行链路表示器。 更改 eswitch 模式将跳过销毁 eth 设备，因此网络设备不会被销毁，而只会更改配置文件。 创建上行eth-rep将初始化representor相关资源。 从这个意义上说，当我们销毁所有设备时，我们首先需要销毁 eth-rep 设备，因此上行链路 eth-rep 将清除所有与表示器相关的资源，然后才销毁 eth 设备，这将销毁其余资源和网络设备
        auxiliary_driver_register(&mlx5e_rep_driver)
mlx5_sf_driver_register -> static struct auxiliary_driver
    auxiliary_driver_register(&mlx5_sf_driver)
pci_register_driver(&mlx5_core_driver)


static struct auxiliary_driver mlx5e_rep_driver = {
    .name = "eth-rep",
    .probe = mlx5e_rep_probe,
    .remove = mlx5e_rep_remove,
    .id_table = mlx5e_rep_id_table,
};


mlx5e_rep_probe
    mlx5_eswitch_register_vport_reps(esw, &rep_ops, REP_ETH) -> net/mlx5：E-Switch，将代表 reg/unreg 集中到 eswitch 驱动程序，Eswitch 有两个用户：IB 和 ETH。 它们都在添加 mlx5 接口时注册代表器，并在删除 mlx5 接口时取消注册代表器。 理想情况下，每个驱动程序应该只处理其独有的实体。 但是，当前的 IB 和 ETH 驱动程序必须执行以下 eswitch 操作： 1. 注册时，指定要注册的 vport 数量。 对于两个驱动程序来说，这个数字是相同的，即可用 vport 总数。 2. 取消注册时，指定要取消注册的已注册 vport 的数量。 另外，卸载已经加载的代表器。 eswitch 驱动程序没有必要将上述操作的控制权交给各个驱动程序用户，因为它们对于每个驱动程序来说并不是唯一的。 相反，此类操作应集中到 eswitch 驱动程序。 这整合了 eswitch 控制流程，并简化了 IB 和 ETH 驱动程序
        mlx5_esw_for_each_rep
            atomic_set(&rep_data->state, REP_REGISTERED

enum {
	REP_UNREGISTERED,
	REP_REGISTERED,
	REP_LOADED,
};
net/mlx5：E-Switch，向 eswitch vport 代表添加状态，当前 eswitch vport 代表有一个有效的指示符，该指示符在注册时设置，在取消注册时取消设置。 但是，在取消注册时可以加载或不加载代表，当前驱动程序会检查该代表的 vport 是否已启用作为表示该代表已加载的标志。 但是，对于 ECPF，这无效，因为主机 PF 将为其 VF 启用 vport。 添加三种状态：{未注册、已注册、已加载}，不同操作之间的状态变化如下：
create: (none)       -> unregistered
reg:    unregistered -> registered
load:   registered   -> loaded
unload: loaded       -> registered
unreg:  registered   -> unregistered
请注意，状态只能在 eswitch 驱动程序内部更新，而不是单独的驱动程序（例如 ETH 或 IB）



辅助设备驱动
static struct auxiliary_driver mlx5e_driver = {
    .name = "eth",
    .probe = mlx5e_probe,
    .remove = mlx5e_remove,
    .suspend = mlx5e_suspend,
    .resume = mlx5e_resume,
    .id_table = mlx5e_id_table,
};

mlx5e_probe -> static int _mlx5e_probe
    const struct mlx5e_profile *profile = &mlx5e_nic_profile
    mlx5e_dev = mlx5e_create_devlink(&adev->dev, mdev)
        devlink_alloc_ns
        devl_nested_devlink_set -> net/mlx5e：将辅助 devlink 实例设置为嵌套，受益于之前引入 devlink 实例关系公开的提交，并为辅助设备设置嵌套实例 -> 嵌套的 devlink 信息通过 devlink netlink 的对象特定属性暴露给用户空间 -> https://docs.kernel.org/networking/devlink/
        devlink_register
    auxiliary_set_drvdata
    mlx5e_devlink_port_register -> net/mlx5e：使用交换机 ID 注册 nic devlink 端口，我们将重新使用本机 NIC 端口网络设备实例作为上行链路表示器。 由于当我们使用 switchdev 模式时，netdev 将保持注册状态，因此 devlink 也将保持注册状态。 使用交换机 ID 注册 nic devlink 端口，以便在更改配置文件时可用
        mlx5_esw_vport_to_devlink_port_index
        devlink_port_attrs_set -> 扩展 devlink_port_attrs_set() 以传递属于交换机一部分的端口的交换机 ID，并将其存储在端口 attrs 中。 对于其他端口，该值为 NULL。 请注意，这允许驱动程序根据实际拓扑将 devlink 端口分组到一个或多个交换机中
        devlink_port_register -> devlink-port是设备上存在的端口。它具有逻辑上独立的设备入口/出口点。 devlink 端口可以是多种风格中的任何一种。 devlink 端口风格和端口属性描述了端口代表的内容。  打算发布 devlink 端口的设备驱动程序设置 devlink 端口属性并注册 devlink 端口, https://docs.kernel.org/networking/devlink/devlink-port.html
    mlx5e_create_netdev -> net/mlx5e：添加 PTP 和 QOS HTB 功能的配置文件指示，让配置文件指示支持 PTP 和 HTB (QOS) 功能。 这统一了计算功能所需的 netdev 队列数量的逻辑，并允许简化 mlx5e_create_netdev()，不再需要 rx/tx 队列数量作为参数
        mlx5e_get_max_num_txqs -> net/mlx5e：允许对最大通道数进行特定于配置文件的限制，让 SF/VF 表示器的 netdev 对 max_nch 使用特定于配置文件的限制，以减少其内存和硬件资源消耗。 这对于内存有限和 SF 数量较多的环境尤其重要
            mlx5e_profile_max_num_channels
                max_nch_limit
        mlx5e_get_max_num_rxqs
        alloc_etherdev_mqs -> 分配并设置以太网设备 @sizeof_priv：要为此以太网设备分配的附加驱动程序私有结构的大小 @txqs：该设备具有的 TX 队列数。 @rxqs：该设备拥有的 RX 队列数。 使用以太网通用值填充设备结构的字段。 基本上可以完成除注册设备之外的所有操作。 构造一个新的网络设备，并包含大小为 (sizeof_priv) 的私有数据区域。 此私有数据区域强制执行 32 字节（而非位）对齐 -> net/mlx5e：netdev 对象和 mlx5e 配置文件初始化之间分开，1) 在 netdevice 分配上和 mlx5e 配置文件外部初始化 netdevice 功能和结构。 2) 由于现在只有在设置了 netdevice 功能之后才会在配置文件 init 上设置 mlx5e netdevice 私有参数，因此我们添加对 netde_update_features() 的调用来解决任何冲突。 这很好，因为如果配置文件需要不同的默认功能，我们会重用 fix_features ndo 代码，而不是在配置文件初始化时重复功能冲突解决代码。 3) 这样，我们就实现了 mlx5e 配置文件和网络设备之间的完全分离，并且允许动态替换 mlx5e 配置文件，以便为多个配置文件重复使用相同的网络设备。 例如 对于上行链路表示器配置文件，如以下补丁所示 4) 配置文件回调不再允许直接触及 netdev->features，因为在下游补丁中，我们将动态分离/附加 netdev 到配置文件，因此我们移动处理 netdev-> 的代码 从 profile->init() 到 fix_features ndo 的功能，我们将在 mlx5e_attach_netdev(profile, netdev) 上调用 netdev_update_features()；
        mlx5e_priv_init -> net/mlx5e：保持最大通道数的值同步，最大通道数的值首先根据 netdev 的配置文件和当前函数资源（具体来说，MSIX 向量的数量，这取决于其他因素）计算 系统中在线核心的数量）。 然后使用该值来计算 netdev 的 rxqs/txqs 数量。 一旦创建（通过 alloc_etherdev_mqs ），netdev 的 rxqs/txqs 数量是恒定的，我们不能超过它。 为了实现这一点，请在任何网络设备重新连接时保持最大数量的通道同步。 使用 mlx5e_get_max_num_channels() 计算 netdev 的 rxqs/txqs 数量。 创建 netdev 后，使用 mlx5e_calc_max_nch()（它创建核心设备资源、配置文件和 netdev）来初始化或更新 priv->max_nch。 在此补丁之前，priv->max_nch 的值可能会不同步，从而错误地允许访问越界对象，从而导致系统崩溃。 跟踪在单独字段中使用的通道统计结构的数量，因为它们持续挂起/恢复操作。 应保留曾经存在的每个通道索引的所有收集的统计数据。 仅当 struct mlx5e_priv 在 mlx5e_priv_cleanup()（配置文件更改流程的一部分）中时，它们才会重置。 由于 mlx5e_netdev_change_profile() 中的 max_nch 不匹配，阻止配置文件更改已没有任何意义。 解除限制
            mlx5e_calc_max_nch -> net/mlx5e：通过在 netdev priv 中使用动态分配来节省内存，priv 中的许多数组都是使用预定义的最大值（对于 num 个通道、num TC 等）进行静态分配，在某些情况下明显大于 实际最大值。 示例： - 支持的 VF 越多，每个 VF 可以拥有的 MSIX 向量就越少。 这限制了每个的 max_nch 。 - 内核或 MSIX 数量有限 (< 64) 的系统。 - 不支持的 Netdev 配置文件：QoS (DCB / HTB)、PTP TX 端口时间戳。 在这里，我们通过移动几个结构和数组来遵循实际的最大值来节省一些内存。 此补丁还准备了代码，以实现更多节省。 例如，在最大通道数为 8 的系统上，仅通道统计结构就会从每个接口的 3648*64 = 228 KB 下降到 3648*8 = 28.5 KB。 这对于具有大量 VF/SF 或内存有限的环境非常重要
            alloc_cpumask_var -> cpuset子系统为cgroup 中的任务分配独立 CPU（在多核系统）和内存节点。Cpuset子系统为定义了一个叫cpuset的数据结构来管理cgroup中的任务能够使用的cpu和内存节点 -> net/mlx5e：修复了极端情况下 XPS cpumask 和 netdev 队列的配置，目前，mlx5e 会通知内核有关队列数量的信息，并在激活通道时设置默认 XPS cpumask。 此实现有几个极端情况，其中内核可能无法按时更新，或者 XPS cpumasks 可能在用户未直接接触时重置。 此提交修复了这些极端情况，以匹配以下预期行为： 1. 队列数量始终与配置的通道数量相对应。 2. XPS cpumasks 在 netdev Attach 上设置为驱动程序的默认值。 3. 用户设置的 XPS cpumasks 不会重置，除非通道数发生变化。 如果通道数量发生变化，它们将重置为驱动程序的默认值。 （一般情况下，当通道数增加或减少时，不可能猜测如何转换当前的 XPS cpumasks 以适应新的通道数，因此如果用户更改通道数，我们让用户重新配置它。 ) XPS cpumask 不再按通道存储。 仅使用一个临时 cpumask。 旧的存储的 cpumasks 没有反映用户的更改，并且在应用它们后没有被使用。 结构体 mlx5e_priv 中添加了暂存器区域。 由于 cpumask_var_t 需要分配，并且 preactivate hook 不能失败，因此我们需要提前预分配临时 cpumask。 它存储在暂存器中
            mlx5e_selq_init -> net/mlx5e：引入select队列参数，ndo_select_queue可以随时调用，并且没有办法阻止内核调用它来同步配置更改（real_num_tx_queues、num_tc）。 此提交引入了 mlx5e 中的一种内部方法，用于将 mlx5e_select_queue() 与这些更改同步。 该函数所需的配置存储在 struct mlx5e_selq_params 中，可以使用 RCU 方法以原子方式修改和访问该结构。 整个 ndo_select_queue 在 RCU 锁下调用，提供必要的保证。 存储在新结构 mlx5e_selq_params 中的参数只能在 mlx5e_select_queue 内部使用。 这是 mlx5e_select_queue 有效完成其工作所需的最小参数集，源自存储在其他地方的参数。 这意味着当配置更改时，mlx5e_selq_params 可能需要更新。 在这种情况下，应使用 mlx5e_selq_prepare/mlx5e_selq_apply API。 struct mlx5e_selq 包含两个参数槽：活动和备用。 mlx5e_selq_prepare 更新备用插槽，mlx5e_selq_apply 使用 RCU API 以安全原子方式交换插槽。 它与配置更改流程的打开/激活阶段很好地集成
                rcu_assign_pointer
            INIT_WORK(&priv->update_carrier_work, mlx5e_update_carrier_work);
            INIT_WORK(&priv->set_rx_mode_work, mlx5e_set_rx_mode_work);
            INIT_WORK(&priv->tx_timeout_work, mlx5e_tx_timeout_work);
            INIT_WORK(&priv->update_stats_work, mlx5e_update_stats_work);
            priv->wq = create_singlethread_workqueue("mlx5e")
        netif_carrier_off -> 网络适配器硬件电路可以检测出链路上是否有载波，载波反映了网络的连接是否正常。网络设备驱动可以通过 netif_carrier_on() 和 netif_carrier_off() 函数改变设备的连接状态，如果驱动检测到连接状态发生变化，也应该以 netif_carrier_on() 和 netif_carrier_off() 函数显式地通知内核
        netif_tx_disable
        dev_net_set
    mlx5e_build_nic_netdev
        netdev->netdev_ops = &mlx5e_netdev_ops
        netdev->xdp_metadata_ops = &mlx5e_xdp_metadata_ops
        netdev->xsk_tx_metadata_ops = &mlx5e_xsk_tx_metadata_ops
        mlx5e_dcbnl_build_netdev(netdev)
            netdev->dcbnl_ops = &mlx5e_dcbnl_ops
        netdev->watchdog_timeo    = 15 * HZ
        netdev->ethtool_ops	  = &mlx5e_ethtool_ops
        mlx5_query_port_fcs(mdev, &fcs_supported, &fcs_enabled)
            mlx5_query_ports_check
                mlx5_core_access_reg MLX5_REG_PCMR
    profile->init(mdev, netdev) -> mlx5e_nic_init
    mlx5e_resume
        mlx5e_create_mdev_resources
            mlx5_core_alloc_pd(mdev, &res->pdn) -> net/mlx5e：仅创建 NIC 全局资源一次，为了允许在同一 PCI 功能上创建多个 netdev，我们更改了驱动程序，以便全局 NIC 资源创建一次，然后在该端口上运行的所有 mlx5e netdev 之间共享。 将 CQ UAR、PD (pdn)、传输域 (tdn)、MKey 资源从保留在 mlx5e priv 部分中移动到放置在 mlx5_core 设备下的新资源结构 (mlx5e_resources)。 该补丁没有添加任何新功能
                mlx5_cmd_exec_inout MLX5_CMD_OP_ALLOC_PD
            mlx5_core_alloc_transport_domain(mdev, &res->td.tdn) -> MLX5_CMD_OP_ALLOC_TRANSPORT_DOMAIN
            mlx5e_create_mkey -> net/mlx5e：暴露内存密钥创建（mkey）函数，暴露mlx5e_create_mkey函数，以供将来的macsec系列补丁使用。 上述函数创建一个内存密钥，它描述了内存中的一个区域，以后可以由硬件和软件使用。 对应的销毁功能已经公开
                mlx5_core_create_mkey -> MLX5_CMD_OP_CREATE_MKEY
            mlx5_alloc_bfreg -> net/mlx5e：适用于所有 mlx5e SQ 和 netdev 的单个 bfreg (UAR)，一个就足够了，因为不再支持 Blue Flame。 这对于 switchdev 模式来节省资源也很有用，因为 VF 表示器也将使用相同的单个 UAR 来用于它们自己的 SQ -> net/mlx5：引入蓝焰寄存器分配器，这里是分配蓝焰寄存器的分配器的实现。 蓝色火焰寄存器用于生成发送门铃。 蓝焰寄存器可用于生成常规门铃或蓝焰门铃，其中要发送的数据被写入设备的 I/O 存储器，从而节省了从存储器读取数据的需要。 为了让蓝焰门铃成功，蓝焰寄存器需要映射为写组合。 用户可以指定她希望使用哪种发送门铃。 如果她请求写入组合映射但失败，则分配器将回退到非写入组合映射并向用户指示这一点。 本系列的后续补丁将使用此分配器 -> commit: https://github.com/ssbandjl/linux/commit/a6d51b68611e98f05042ada662aed5dbe3279c1e
                alloc_bfreg(mdev, bfreg, map_wc, fast_path)
                    up = alloc_uars_page(mdev, map_wc)
                        bfregs = uars_per_sys_page(mdev) * MLX5_BFREGS_PER_UAR
                        up->reg_bitmap = bitmap_zalloc_node(bfregs, GFP_KERNEL, node)
                        mlx5_cmd_alloc_uar -> MLX5_CMD_OP_ALLOC_UAR
                        pfn = uar2pfn(mdev, up->index)
                        up->map = ioremap_wc(pfn << PAGE_SHIFT, PAGE_SIZE)
                        or up->map = ioremap(pfn << PAGE_SHIFT, PAGE_SIZE)
            mlx5e_create_tises -> net/mlx5：将 TIS 从 priv 移动到 mdev 硬件资源，传输接口发送 (TIS) 对象负责执行传输侧的所有传输相关操作。 来自发送队列的消息由 TIS 进行分段和传输，包括所有传输所需的含义，例如 在大量发送卸载的情况下，TIS 负责分段。 这些是无状态对象，可以由共享同一核心设备的多个网络开发人员（例如代表者）使用。 将 TIS 作为从核心层到 netdev 层的服务提供可减少重复的 TIS 对象数量（在多个 netdev 的情况下），并且将简化向具有多个 mdev 的 netdev 的过渡
                mlx5_lag_should_assign_affinity
                mlx5e_create_tis
                    mlx5_core_create_tis -> MLX5_CMD_OP_CREATE_TIS -> net/mlx5：以太网资源处理文件，此补丁包含资源处理文件： - flow_table.c：此文件包含处理低级 API 以配置硬件流表的代码。 它与 flow_table_en.c 分开，因为将来 mlx5_ib 中的原始以太网 QP 也会使用它。 - en_flow_table.[ch]：以太网流控制处理。 流表对象包含流规范和 TIR 之间的映射。 将来当添加 SR-IOV 支持时，该机制还将用于配置 e-switch。 - transobj.[ch] - 用于创建/修改/销毁传输对象的低级函数：RQ/SQ/TIR/TIS - vport.[ch] - 处理嵌入式交换机中虚拟端口 (vPort) 的属性。 目前此交换机是直通交换机，直到添加 SR-IOV 支持
            mlx5_crypto_dek_init
                mlx5_crypto_cmd_sync_crypto -> MLX5_CMD_OP_SYNC_CRYPTO
        mlx5e_attach_netdev
            mlx5e_calc_max_nch
            profile->init_tx(priv)
            profile->init_rx(priv)
            profile->enable(priv)
            mlx5e_update_features(priv->netdev)
                netdev_update_features(netdev) -> 重新计算设备功能
    register_netdev
    mlx5e_dcbnl_init_app -> net/mlx5e：将 dcbnl dscp 添加到优先级支持，此补丁实现 dcbnl 挂钩来设置和删除 DCB 子系统定义的优先级映射中的 DSCP。 设备维护内部信任状态，需要将其设置为 DSCP 状态以执行 DSCP 到优先级映射。   当用户添加第一个 dscp topriority APP 条目时，信任状态将更改为 dscp。   当用户删除最后一个 dscp 到优先级 APP 条目时，信任状态更改为 pcp。   如果用户在同一 dscp 上向优先级 APP 条目发送多个 dscp，则最后发送的 dscp 生效。 之前发送的所有内容都将被删除。   使用 dcb_ieee_setapp/getapp 在 net/dcb APP 数据库中添加和删除 dscp 优先级 APP 条目
        mlx5e_dcbnl_dscp_app
            dcb_ieee_setapp(priv->netdev, &temp) -> dcb_ieee_setapp - 将 IEEE dcb 应用程序数据添加到应用程序列表，@dev：网络接口 @new：要添加的应用程序数据 这会将应用程序数据添加到列表。 只要优先级不同，同一选择器和协议就可以存在多个应用程序条目。 优先级预计为 3 位无符号整数
                dcb_app_lookup
                dcb_app_add
    mlx5_core_uplink_netdev_set -> net/mlx5e：在上行链路 netdev 更改时传播内部事件，每当设置/清除上行链路 netdev 时，传播新引入的事件以通知通知程序块 netdev 已添加/删除。 将 set() 帮助器从标头移至 core.c，引入clear() 和 netdev_added_event_replay() 帮助器。 最后一个将从 rdma 驱动程序调用，因此将其导出
        mlx5_blocking_notifier_call_chain MLX5_DRIVER_EVENT_UPLINK_NETDEV -> net/mlx5：通过阻塞事件通知陷阱操作，为了允许 mlx5 核心驱动程序向其使用者触发同步操作，请添加阻塞事件处理程序。 将包装器添加到blocking_notifier_[call_chain/chain_register/chain_unregister]。 为操作集添加陷阱回调并通知此更改。 该集中的以下补丁添加了此事件的侦听器
            blocking_notifier_call_chain
    mlx5e_params_print_info -> net/mlx5e：将 params 内核日志打印移至探针函数，Params 信息打印本应在加载时打印。 随着时间的推移，添加了对 mlx5e_init_rq_type_params 和 mlx5e_build_rq_params 的新调用，再次错误地打印了参数。 将打印内容移至其所属位置，在 mlx5e_probe 中


static const struct mlx5e_profile mlx5e_nic_profile = {
    .init		   = mlx5e_nic_init,
    .cleanup	   = mlx5e_nic_cleanup,
    .init_rx	   = mlx5e_init_nic_rx,
    .cleanup_rx	   = mlx5e_cleanup_nic_rx,
    .init_tx	   = mlx5e_init_nic_tx,
    .cleanup_tx	   = mlx5e_cleanup_nic_tx,
    .enable		   = mlx5e_nic_enable,
    .disable	   = mlx5e_nic_disable,
    .update_rx	   = mlx5e_update_nic_rx,
    .update_stats	   = mlx5e_stats_update_ndo_stats,
    .update_carrier	   = mlx5e_update_carrier,
    .rx_handlers       = &mlx5e_rx_handlers_nic,
    .max_tc		   = MLX5_MAX_NUM_TC,
    .stats_grps	   = mlx5e_nic_stats_grps,
    .stats_grps_num	   = mlx5e_nic_stats_grps_num,
    .features          = BIT(MLX5E_PROFILE_FEATURE_PTP_RX) |
        BIT(MLX5E_PROFILE_FEATURE_PTP_TX) |
        BIT(MLX5E_PROFILE_FEATURE_QOS_HTB) |
        BIT(MLX5E_PROFILE_FEATURE_FS_VLAN) |
        BIT(MLX5E_PROFILE_FEATURE_FS_TC),
};




git remote add origin git@gitlab.nsv6.b122.top:bin/linux_kernel.git


register_chrdev

pci_enable_device

filter: ./drivers/net/ethernet/intel/



static struct ib_client uverbs_client = {
    .name   = "uverbs",
    .no_kverbs_req = true,
    .add    = ib_uverbs_add_one,
    .remove = ib_uverbs_remove_one,
    .get_nl_info = ib_uverbs_get_nl_info,
};




ibv_reg_mr, Register a user memory region, 
drivers/infiniband/hw/irdma/verbs.c
IB_USER_VERBS_CMD_REG_MR
ib_uverbs_reg_mr -> 
    uobj = uobj_get_write(UVERBS_OBJECT_MR, cmd.mr_handle, attrs)
    ib_check_mr_access -> enum ib_access_flags
    uobj_get_obj_read(pd, UVERBS_OBJECT_PD, cmd.pd_handle,
    mr = pd->device->ops.reg_user_mr(pd, cmd.start, cmd.length, cmd.hca_va, cmd.access_flags, &attrs->driver_udata) -> .reg_user_mr = irdma_reg_user_mr <- DECLARE_UVERBS_WRITE
    ...
    region = ib_umem_get(pd->device, start, len, access) -> pin住以及通过DMA映射的用户空间内存, IB/uverbs：将 ib_umem_get()/ib_umem_release() 导出到模块，导出 ib_umem_get()/ib_umem_release() 并让低级驱动程序控制何时调用 ib_umem_get() 来pin和 DMA 映射用户空间，而不是总是调用它 在调用低级驱动程序的 reg_user_mr 方法之前在 ib_uverbs_reg_mr() 中。 还将这些函数移至 ib_core 模块而不是 ib_uverbs 中，以便使用它们的驱动程序模块不依赖于 ib_uverbs。 这具有许多优点： - 从使通用代码成为可以根据特定设备的详细信息指示由设备特定代码使用或覆盖的库的角度来看，这是更好的设计。 - 不需要固定用户空间内存区域的驱动程序不需要承受调用 ib_mem_get() 的性能损失。 例如，虽然我没有尝试在此补丁中实现它，但 ipath 驱动程序应该能够避免固定内存并仅使用 copy_{to,from}_user() 来访问用户空间内存区域。 - 需要特殊映射处理的缓冲区可以由低级驱动程序识别。 例如，通过使用额外标志映射 CQ 缓冲区，可以解决用户空间中 mthca CQ 的一些特定于 Altix 的内存排序问题。 - 需要为内存区域以外的内容固定和 DMA 映射用户空间内存的驱动程序可以直接使用 ib_umem_get()，而不是使用其 reg_phys_mr 方法的额外参数进行黑客攻击。 例如，待合并的 mlx4 驱动程序需要引脚和 DMA 映射 QP 和 CQ 缓冲区，但不需要为这些缓冲区创建内存密钥。 因此，最干净的解决方案是 mlx4 在 create_qp 和 create_cq 方法中调用 ib_umem_get() -> 调用 ib_umem_get 来完成内存页面pin操作 + 将pin住的内存页面组织为DMA SG List
        can_do_mlock -> [PATCH] IB uverbs：内存固定实现(pin)，添加对固定用户空间内存区域并返回该区域中的页面列表的支持。 这包括根据 vm_locked 跟踪固定内存并防止非特权用户超过 RLIMIT_MEMLOCK
        mmgrab(mm) -> 抓住内存(引用+1)
        __get_free_page -> get continue pa(return va) -> https://blog.csdn.net/do2jiang/article/details/5450705
        npages = ib_umem_num_pages(umem) -> 先按4K页对齐, 然后计算4KB页数量
            ib_umem_num_dma_blocks(umem, PAGE_SIZE) -> RDMA/umem：将 ib_umem_num_pages() 拆分为 ib_umem_num_dma_blocks()，ib_umem_num_pages() 只能由直接在 CPU 页面中使用 SGL 的事物使用。 构建 DMA 列表的驱动程序应使用新的 ib_num_dma_blocks()，它返回 rdma_umem_for_each_block() 将返回的块数。 要使 DMA 驱动程序通用，需要不同的实现。 仅当请求的页面大小为 < PAGE_SIZE 和/或 IOVA == umem->address 时，基于 umem->address 计算 DMA 块计数才有效。 相反，DMA 页数应在 IOVA 地址空间中计算，而不是 umem->address。 因此，IOVA 必须存储在 umem 内，以便可以用于这些计算。 现在默认将其设置为 umem->address 并在调用 ib_umem_find_best_pgsz() 时修复它。 这允许驱动程序安全地转换为 ib_umem_num_dma_blocks()
                ALIGN ALIGN_DOWN / pgsz
        cond_resched -> RDMA/umem：在 ib_umem_get() 中添加一个调度点，映射小至 64GB 可能需要 10 秒以上，触发 CONFIG_PREEMPT_NONE=y 的内核问题。 ib_umem_get() 已经在 x86_64 上以 2MB 为单位分割工作，在持久循环中添加 cond_resched() 足以解决问题。 请注意，sg_alloc_table() 仍然可以使用超过 100 毫秒，这也是有问题的。 这可能稍后在 ib_umem_add_sg_table() 中解决，按需在 sql 中添加新块. 在一些比较耗时的处理中如文件系统和内存回收的一些路径会调用cond_resched, 用cond_resched来进行检查是否具备调度时机, 对于非抢占式内核来说，在内核的很多地方，特别是文件系统操作和内存管理相关的一些耗时路径中，都已经被内核开发者识别出来，并使用cond_resched来减小延迟, cond_resched() 函数，它的功能是主动放权，等待下一次的调度运行, 参考: https://www.zhihu.com/question/35004859
        pin_user_pages_fast -> 与 get_user_pages_fast() 几乎相同，只是设置了 FOLL_PIN。 有关函数参数的文档，请参阅 get_user_pages_fast()，因为此处的参数是相同的。 FOLL_PIN 意味着必须通过 unpin_user_page() 释放页面。 请参阅 Documentation/core-api/pin_user_pages.rst 了解更多详细信息。 请注意，如果返回的页面中有一个 zero_page，则其中不会有pin，并且 unpin_user_page() 不会从中删除pin -> mm/gup：从内部 GUP 函数中删除 vmas 数组，现在我们已经消除了所有使用 vmas 参数的 GUP API 的调用者，完全消除了它。 这消除了一类错误，其中 vmas 的保留时间可能比 mmap_lock 的时间长，因此我们不必担心在此操作期间锁定被删除，留下悬空指针。 这简化了 GUP API 并使其用途更加清晰 - 应用跟随标志，如果固定，则返回页面数组
            is_valid_gup_args
                WARN_ON_ONCE()会将调试信息输出到dmesg，类似于BUG_ON()打印的内容。与BUG_ON()的区别在于，WARN_ON_ONCE()仅会在第一次触发时打印调试信息
            internal_get_user_pages_fast -> 获取用户空间页面: https://sanli-b.com.cn/posts/35092.html -> mm/gup：跟踪 FOLL_PIN 页面 添加对通过 FOLL_PIN 固定的页面的跟踪。 此跟踪是通过 page->_refcount 的重载来实现的：通过将 GUP_PIN_COUNTING_BIAS (1024) 添加到 refcount 来添加引脚。 这提供了固定的模糊指示，并且可能会出现误报（但这没关系）。 请参阅现有的 Documentation/core-api/pin_user_pages.rst 了解详细信息。 正如 pin_user_pages.rst 中提到的，有效设置 FOLL_PIN（通常通过 pin_user_pages*()）的调用者需要最终通过 unpin_user_page() 释放此类页面。 另请注意 pin_user_pages.rst 中“TODO：适用于 1GB 及更大的大页面”部分中讨论的限制。 （该限制将在后续补丁中删除。）FOLL_PIN 标志的效果与 FOLL_GET 的效果类似，并且可以被视为“用于 DIO 和/或 RDMA 使用的 FOLL_GET”。 通过 FOLL_PIN 固定的页面可通过新函数调用进行识别： bool page_maybe_dma_pinned(struct page *page); 遇到这样的页面该怎么办，就留给以后的补丁了。 在[1]、[2]、[3]和[4]中对此进行了讨论。 这也会将 follow_page_mask() 中的 BUG_ON() 更改为 WARN_ON()
                mm_set_has_pinned_flag
                    set_bit(MMF_HAS_PINNED, mm_flags) -> flag -> MMF_HAS_PINNED：该mm是否已固定任何页面。 当它变得稳定时，它可以在将来被 mm.pinned_vm 取代，或者自己成长为一个计数器。 我们现在对此非常积极：即使稍后取消固定的页面，我们仍然会在该 mm 的生命周期中保留此位设置，只是为了简单起见
                untagged_addr -> 调用 untagged_addr() 宏直接去除指针标记
                check_add_overflow
                lockless_pages_from_mm -> mm/gup：重组 internal_get_user_pages_fast()，补丁系列“在gup_fast和copy_page_range()之间添加seqcount”，v4。 正如 Linus 所讨论和建议的，使用 seqcount 来结束 gup_fast 和 copy_page_range() 之间的小竞争。 Ahmed 确认 raw_write_seqcount_begin() 是在这种情况下使用的正确 API，并且它不会触发任何 lockdeps。 我能够使用两个线程对其进行测试，一个线程分叉，另一个使用 ibv_reg_mr() 快速触发 GUP。 将 copy_page_range() 修改为睡眠状态使窗口足够大，可以可靠地命中来测试逻辑。 此补丁（共 2 个）：本系列中的下一个补丁使无锁流程变得更加复杂，因此将整个块移至新函数中并删除一定程度的缩进。 整理一些废话： - addr 始终与 start 相同，因此使用 start - 使用现代的 check_add_overflow() 进行计算 end = start + len - nr_pinned/pages << PAGE_SHIFT 需要 LHS 为 unsigned long 以避免移位溢出 ，将变量设置为 unsigned long 以避免在两个地方进行编码转换。 nr_pinned 缺少其演员 - ret 和 nr_pinned 的处理可以稍微简化一点 没有功能变化 -> https://gwzlchn.github.io/202208/rdma-stack-02/
                    IS_ENABLED
                    gup_fast_permitted
                    raw_read_seqcount
                    gup_pgd_range
                    read_seqcount_retry
                    unpin_user_pages_lockless
                __gup_longterm_locked
        sg_alloc_append_table_from_pages
        ret = ib_dma_map_sgtable_attrs(device, &umem->sgt_append.sgt, DMA_BIDIRECTIONAL, dma_attr) -> Map a scatter/gather table to DMA addresses, 不清楚传输方向
            if (ib_uses_virt_dma(dev))
                nents = ib_dma_virt_map_sg(dev, sgt->sgl, sgt->orig_nents)
                    for_each_sg(sg, s, nents, i)
                        sg_dma_address(s) = (uintptr_t)sg_virt(s);
                        sg_dma_len(s) = s->length;
            else
                dma_map_sgtable(dev->dma_device, sgt, direction, dma_attrs)
                    nents = __dma_map_sg_attrs(dev, sgt->sgl, sgt->orig_nents, dir, attrs)
                        dma_direct_map_sg
                            is_pci_p2pdma_page -> MEMORY_DEVICE_PCI_P2PDMA
                            pci_p2pdma_map_segment
                            sg->dma_address = dma_direct_map_page(dev, sg_page(sg), sg->offset, sg->length, dir, attrs)
								swiotlb_map(dev, phys, size, dir, attrs)
    ib_copy_from_udata(&req, udata, min(sizeof(req), udata->inlen)
    iwmr = irdma_alloc_iwmr(region, pd, virt, req.reg_type)
        iwmr->page_size = ib_umem_find_best_pgsz(region, pgsz_bitmap, virt)
            umem->iova = va = virt
            mask = pgsz_bitmap & GENMASK(BITS_PER_LONG - 1, bits_per((umem->length - 1 + virt) ^ virt))
            for_each_sgtable_dma_sg(&umem->sgt_append.sgt, sg, i)
                mask |= (sg_dma_address(sg) + pgoff) ^ va
                va += sg_dma_len(sg) - pgoff
        ib_umem_num_dma_blocks
            return (size_t)((ALIGN(umem->iova + umem->length, pgsz) - ALIGN_DOWN(umem->iova, pgsz))) / pgsz
    case IRDMA_MEMREG_TYPE_QP
        irdma_reg_user_mr_type_qp(req, udata, iwmr)
            irdma_handle_q_mem(iwdev, &req, iwpbl, lvl)
                irdma_setup_pbles -> 将用户页拷贝到PBLE中
                case IRDMA_MEMREG_TYPE_QP
                    irdma_check_mem_contiguous
            rdma_udata_to_drv_context
            list_add_tail(&iwpbl->list, &ucontext->qp_reg_mem_list)
    case IRDMA_MEMREG_TYPE_MEM
        irdma_reg_user_mr_type_mem
            lvl = iwmr->page_cnt != 1 ? PBLE_LEVEL_1 | PBLE_LEVEL_2 : PBLE_LEVEL_0
            irdma_setup_pbles -> 使用位掩码重构 PBLE 函数来表示所需的 PBLE 级别，而使用 2 个参数 use_pble 和 lvl_one_only 使代码变得混乱
            HW 使用主机内存作为许多协议上下文对象和队列状态跟踪的后备存储。 主机内存缓存 (HMC) 是负责管理存储在主机内存中的这些对象的组件。 添加函数和数据结构来管理 HMC 为各种对象使用的支持页面的分配, 本文主要分析inux内核intel/hns3/mlx5等RDMA驱动上下文内存管理机制优缺点: https://zhuanlan.zhihu.com/p/610503666, 实现物理缓冲区列表条目 (PBLE) 资源管理器来管理 PBLE HMC 资源对象池, 
                irdma_get_pble -> if (lvl)
                    get_lvl1_lvl2_pble
                        status = get_lvl1_pble(pble_rsrc, palloc)
                            irdma_prm_get_pbles(&pble_rsrc->pinfo, &lvl1->chunkinfo, palloc->total_cnt << 3, &lvl1->addr, &fpm_addr)
                                bits_needed = DIV_ROUND_UP_ULL(mem_size, BIT_ULL(pprm->pble_shift))
                                pchunk = (struct irdma_chunk *)chunk_entry
                                bit_idx = bitmap_find_next_zero_area(pchunk->bitmapbuf, pchunk->sizeofbitmap, 0, bits_needed, 0)
                                offset = bit_idx << pprm->pble_shift
                                bitmap_set(pchunk->bitmapbuf, bit_idx, bits_needed)
                                pprm->free_pble_cnt -= chunkinfo->bits_used << (pprm->pble_shift - 3)
                            palloc->level = PBLE_LEVEL_1
                            lvl1->idx = fpm_to_idx(pble_rsrc, fpm_addr)
                        get_lvl2_pble
                            ...
                    add_pble_prm -> Host Memory Cache (HMC) 主机内存缓存, PBLE资源管理, 为PBLE资源添加一个SD条目
                        get_sd_pd_idx(pble_rsrc, idx)
							idx->sd_idx = (u32)pble_rsrc->next_fpm_addr / IRDMA_HMC_DIRECT_BP_SIZE;
							idx->pd_idx = (u32)(pble_rsrc->next_fpm_addr / IRDMA_HMC_PAGED_BP_SIZE);
							idx->rel_pd_idx = (idx->pd_idx % IRDMA_HMC_PD_CNT_IN_SD);
						sd_entry = &hmc_info->sd_table.sd_entry[idx->sd_idx]
                        irdma_get_type
                        add_sd_direct
                            irdma_add_sd_table_entry
                            dma_alloc_coherent
                            memcpy(&sd_entry->u.pd_table.pd_page_addr, &dma_mem,
                        add_bp_pages -> 为段描述添加后端内存页 -> add backing pages for sd (BP)
                            irdma_pble_get_paged_mem
                                irdma_map_vm_page_list
                                    vmalloc_to_page -> 找到由vmalloc( )所分配的内存的虚拟地址所映射的物理页，并返回该页的指针描述符
                                    dma_map_page -> 将一页物理内存进行映射
								chunk->type = PBLE_SD_PAGED
                            irdma_add_sd_table_entry
                            irdma_add_pd_table_entry
                                dma_alloc_coherent
                                memcpy(&pd_entry->bp.addr, page, sizeof(pd_entry->bp.addr))
                                memcpy(pd_addr, &page_desc, sizeof(*pd_addr))
                                irdma_invalidate_pf_hmc_pd -> 使 PF 硬件中的 pd 缓存无效
                                    writel(val, dev->hw_regs[IRDMA_PFHMC_PDINV])
                        irdma_prm_add_pble_mem
                            pchunk->bitmapbuf = bitmap_zalloc(sizeofbitmap, GFP_KERNEL)
                        irdma_hmc_sd_one
                            irdma_set_sd_entry
                            or irdma_clr_sd_entry
                            dev->cqp->process_cqp_sds(dev, &sdinfo) -> irdma_update_sds_noccq
                                cqp_sds_wqe_fill
                                    irdma_sc_cqp_get_next_send_wqe_idx
                                        IRDMA_ATOMIC_RING_MOVE_HEAD(cqp->sq_ring, *wqe_idx, ret_code)
                                        wqe = cqp->sq_base[*wqe_idx].elem -> 从DMA的VA基址依次往后填充
                                        IRDMA_CQP_INIT_WQE(wqe) -> #define IRDMA_CQP_INIT_WQE(wqe) memset(wqe, 0, 64) -> WQE大小为64字节
                                    switch (wqe_entries)
                                    case 3:
                                    ...
                                irdma_get_cqp_reg_info
                                irdma_sc_cqp_post_sq(cqp)
                                irdma_cqp_poll_registers
                        list_add(&chunk->list, &pble_rsrc->pinfo.clist) -> 将 chunk 添加到 PBLE 资源链表上
                    get_lvl1_lvl2_pble
                irdma_copy_user_pgaddrs -> 将用户页面地址复制到本地 pble 中
                    sg_page -> 获取scatterlist所对应的page指针
                    rdma_umem_for_each_dma_block -> 迭代 umem 的连续 DMA 块
                        *pbl = rdma_block_iter_dma_address(&biter) -> 获取块迭代器持有的当前块的对齐 dma 地址
                            biter->__dma_addr & ~(BIT_ULL(biter->__pg_bit) - 1) -> aligned dma address
                        pbl = irdma_next_pbl_addr(pbl, &pinfo, &idx)
            if (lvl)
                irdma_check_mr_contiguous
                    irdma_check_mem_contiguous
                        if ((*arr + (pg_size * pg_idx)) != arr[pg_idx])
            irdma_create_stag -> 随机stag
                get_random_bytes
                irdma_alloc_rsrc -> RDMA/irdma：注册辅助驱动程序并实现私有通道 OP，注册可以从 Intel PCI netdev 驱动程序 i40e 和ice 连接到辅助 RDMA 设备的辅助驱动程序。 实现私有通道操作，并注册网络通知程序
            irdma_hwreg_mr(iwdev, iwmr, access) -> 发送cqp命令进行内存注册
                irdma_alloc_and_get_cqp_request
                irdma_get_mr_access
                cqp_info->cqp_cmd = IRDMA_OP_MR_REG_NON_SHARED -> irdma_sc_mr_reg_non_shared
                    irdma_sc_cqp_get_next_send_wqe
                    ...
                    set_64bit_val(wqe, 32, info->reg_addr_pa)
                    irdma_sc_cqp_post_sq
                stag_info->reg_addr_pa = iwmr->pgaddrmem[0]
                irdma_handle_cqp_op
                irdma_put_cqp_request
    rdma_restrack_new(&new_mr->res, RDMA_RESTRACK_MR)
    rdma_restrack_set_name(&new_mr->res, NULL)
    rdma_restrack_add(&new_mr->res)



enum ib_access_flags {
	IB_ACCESS_LOCAL_WRITE = IB_UVERBS_ACCESS_LOCAL_WRITE,
	IB_ACCESS_REMOTE_WRITE = IB_UVERBS_ACCESS_REMOTE_WRITE,
	IB_ACCESS_REMOTE_READ = IB_UVERBS_ACCESS_REMOTE_READ,
	IB_ACCESS_REMOTE_ATOMIC = IB_UVERBS_ACCESS_REMOTE_ATOMIC,
	IB_ACCESS_MW_BIND = IB_UVERBS_ACCESS_MW_BIND,
	IB_ZERO_BASED = IB_UVERBS_ACCESS_ZERO_BASED,
	IB_ACCESS_ON_DEMAND = IB_UVERBS_ACCESS_ON_DEMAND,
	IB_ACCESS_HUGETLB = IB_UVERBS_ACCESS_HUGETLB,
	IB_ACCESS_RELAXED_ORDERING = IB_UVERBS_ACCESS_RELAXED_ORDERING,
	IB_ACCESS_FLUSH_GLOBAL = IB_UVERBS_ACCESS_FLUSH_GLOBAL, -> 启用 RDMA FLUSH 功能。它可以支持全局可见性和持久性放置类型(PM), rxe flush  commit: https://patchew.org/linux/20221116081951.32750-1-lizhijian@fujitsu.com/20221116081951.32750-3-lizhijian@fujitsu.com/
	IB_ACCESS_FLUSH_PERSISTENT = IB_UVERBS_ACCESS_FLUSH_PERSISTENT,

	IB_ACCESS_OPTIONAL = IB_UVERBS_ACCESS_OPTIONAL_RANGE,
	IB_ACCESS_SUPPORTED =
		((IB_ACCESS_FLUSH_PERSISTENT << 1) - 1) | IB_ACCESS_OPTIONAL,
};


intel irdma/ice/e810 driver:
drivers/infiniband/hw/irdma/main.c
module_init(irdma_init_module);
    auxiliary_driver_register(&i40iw_auxiliary_drv)
        .probe = i40iw_probe
    auxiliary_driver_register(&irdma_auxiliary_drv.adrv) -> rdma -> register rdma device
    irdma_register_notifiers
        register_inetaddr_notifier(&irdma_inetaddr_notifier);
        register_inet6addr_notifier(&irdma_inetaddr6_notifier);
        register_netevent_notifier(&irdma_net_notifier);
        register_netdevice_notifier(&irdma_netdevice_notifier);
...		

irdma_ib_register_device
    irdma_init_rdma_device
        irdma_init_roce_device(iwdev)
        or
        irdma_init_iw_device
        ib_set_device_ops(&iwdev->ibdev, &irdma_dev_ops)
            .owner = THIS_MODULE,
            ...
            INIT_RDMA_OBJ_SIZE(ib_pd, irdma_pd, ibpd), -> set size, 计算整个结构体大小, 强制判断第一个字段
                BUILD_BUG_ON_ZERO(offsetof(struct drv_struct, member))
    ib_device_set_netdev
    dma_set_max_seg_size(iwdev->rf->hw.device, UINT_MAX)
    ib_register_device(&iwdev->ibdev, "irdma%d", iwdev->rf->hw.device)
    irdma_port_ibevent(iwdev)
        event.event = iwdev->iw_status ? IB_EVENT_PORT_ACTIVE : IB_EVENT_PORT_ERR
        ib_dispatch_event(&event)






static struct notifier_block irdma_inetaddr_notifier = {
	.notifier_call = irdma_inetaddr_event
};
static struct notifier_block irdma_inetaddr6_notifier = {
	.notifier_call = irdma_inet6addr_event
};
static struct notifier_block irdma_net_notifier = {
	.notifier_call = irdma_net_event
};
static struct notifier_block irdma_netdevice_notifier = {
	.notifier_call = irdma_netdevice_event
};
irdma_netdevice_event
    ibdev = ib_device_get_by_netdev(netdev, RDMA_DRIVER_IRDMA)
    irdma_port_ibevent(iwdev)




ioctl -> ib_uverbs_modify_qp -> return modify_qp(attrs, &cmd)
    ...
    if (cmd->base.attr_mask & IB_QP_RQ_PSN)
    ...



irdma_modify_qp
irdma_modify_qp_roce
    irdma_query_pkey
    rdma_get_udp_sport -> 根据 grh.flow_label 或 lqpn/rqrpn 获取 QP 的源 udp 端口号。 这可以更好地跨 NIC RX 队列传播流量
    irdma_qp_rem_qos
    dev->ws_remove
    rdma_read_gid_l2_fields
    irdma_roce_get_vlan_prio
        vlan_dev_get_egress_qos_mask
    irdma_qp_add_qos
        list_add(&qp->list, &vsi->qos[qp->user_pri].qplist)
    ib_modify_qp_is_ok
        qp_state_table[cur_state][next_state].valid) -> IB：添加 ib_modify_qp_is_ok() 库函数，内核中的 mthca 驱动程序包含一个表，其中的属性对于每个队列对状态转换都有效。 事实证明，正在准备合并的其他两个 IB 驱动程序（ipath 和 ehca）都复制了该表、错误等。 为了防止代码重复，请将此表和用于检查参数的代码移动到中间层库函数 ib_modify_qp_is_ok() 中
        qp状态表及参数: qp_state_table[IB_QPS_ERR + 1][IB_QPS_ERR + 1]
    wait_event(iwqp->mod_qp_waitq, !atomic_read(&iwqp->hw_mod_qp_pend)) <- irdma_hw_modify_qp_callback
        wake_up(&iwqp->mod_qp_waitq)
    switch (attr->qp_state) -> QP状态机
    ...
    irdma_flush_wqes
        irdma_hw_flush_wqes
            cqp_request->callback_fcn = irdma_hw_flush_wqes_callback
            cqp_info->cqp_cmd = IRDMA_OP_QP_FLUSH_WQES
            irdma_handle_cqp_op -> cqp 控制QP
                irdma_process_cqp_cmd
                    irdma_exec_cqp_cmd
                        switch (pcmdinfo->cqp_cmd) -> 判断控制QP命令类型
                        case IRDMA_OP_CEQ_CREATE -> 驱动程序将特权命令发布到硬件管理队列（控制 QP 或 CQP），以请求硬件执行管理操作。 实现 CQP 的创建/销毁以及支持函数、数据结构和标头以处理不同的 CQP 命令
                            irdma_sc_cq_create
                                irdma_sc_cqp_get_next_send_wqe
                                set_64bit_val(wqe, 0, cq->cq_uk.cq_size) -> 按位设置其值(每8字节)
                                dma_wmb()
                                irdma_sc_cqp_post_sq -> no msleep(300)
                                    writel(IRDMA_RING_CURRENT_HEAD(cqp->sq_ring), cqp->dev->cqp_db) -> 写寄存器(往内存映射的 I/O 空间上写4字节数据)
                        ...
                        case IRDMA_OP_QP_CREATE
                            irdma_sc_qp_create -> 创建QP判断对应QP HMC是否已准备好
                        ...
    irdma_sc_qp_setctx_roce -> void irdma_sc_qp_setctx_roce(struct irdma_sc_qp *qp, __le64 *qp_ctx, -> 设置QP上下文
        set_64bit_val rq_wqe_size
        set_64bit_val(qp_ctx, 8, qp->sq_pa) -> set sendq pa
        set_64bit_val(qp_ctx, 16, qp->rq_pa)
        ...
        print_hex_dump_debug("WQE: QP_HOST CTX WQE", DUMP_PREFIX_OFFSET, 16, -> print_hex_dump
    if (attr_mask & IB_QP_STATE)
        irdma_hw_modify_qp
            cqp_request->callback_fcn = irdma_hw_modify_qp_callback -> no wait/async
        cqp_info->cqp_cmd = IRDMA_OP_QP_MODIFY
        irdma_handle_cqp_op(rf, cqp_request)

irdma_uk_rdma_write
irdma_uk_rdma_read
irdma_uk_cq_poll_cmpl


/proc/pid/pagemap - an array mapping virtual pages to pfns, 如果页面不存在但在交换中，则 PFN 包含交换文件号的编码以及页面在交换中的偏移量。 未映射的页面返回空 PFN。 这允许精确确定哪些页面被映射（或在交换中）并比较进程之间的映射页面。 * 此接口的高效用户将使用 /proc/pid/maps 来确定实际映射的内存区域，并使用 llseek 跳过未映射的区域
pagemap_read -> Maps4：添加 /proc/pid/pagemap 接口，该接口为地址空间中的每个页面提供到其物理页帧号的映射，允许精确确定哪些页面被映射以及哪些页面在进程之间共享。 此版本中的新增内容： - 标头再次消失（根据 Dave Hansen 和 Alan Cox 的建议） - 64 位条目（根据与 Andi Kleen 的讨论） - 交换导出的 pte 信息（来自 Dave Hansen） - 页面遍历器回调以查找漏洞（来自 Dave Hansen） - 直接 put_user I/O（根据 Rusty Russell 的建议）此补丁折叠在清理中并交换 Dave Hansen 的 PTE 支持
    file_ns_capable
    mmap_read_lock_killable
    untagged_addr_remote
    mmap_read_lock_killable
    walk_page_range(mm, start_vaddr, end, &pagemap_ops, &pm) -> walk_page_range - 使用回调遍历内存映射的页表：起始地址：结束地址：为树的每个级别调用的回调集递归地遍历 VMA 中内存区域的页表，调用提供的回调。 回调按顺序调用（第一个 PGD、第一个 PUD、第一个 PMD、第一个 PTE、第二个 PTE...第二个 PMD 等）。 如果省略较低级别的回调，则行走深度会减少。 每个回调接收一个入口指针以及关联范围的开始和结束，以及用于访问 ->private 或 ->mm 字段的原始 mm_walk 的副本。 通常不加锁，但分割透明大页可能会加页表锁。 如果需要，底层迭代器将从 highmem 映射 PTE 目录。 如果任何回调返回非零值，则遍历将中止并将返回值传播回调用者。 否则返回 0。 如果 walk->hugetlb_entry 为 !NULL，则 walk->mm->mmap_sem 必须至少保持读取状态
    copy_to_user

static const struct mm_walk_ops pagemap_ops = {
    .pmd_entry	= pagemap_pmd_range,
    .pte_hole	= pagemap_pte_hole,
    .hugetlb_entry	= pagemap_hugetlb_range,
    .walk_lock	= PGWALK_RDLOCK,
};

pagemap_pmd_range -> pagemap：将 mm 传递给 pagewalkers ，我们现在至少需要这个来检测大页，因为 powerpc 需要 vm_area_struct 来确定虚拟地址是否引用大页（它的 pmd_huge() 不起作用）。 对于其他一些用户来说它也可能派上用场
    pmd_trans_huge_lock
    ...
    for pte
        内核态实现pagemap proc接口的代码位于: fs/proc/task_mmu.c, 把PTE转换为pagemap_entry
        pte_to_pagemap_entry -> proc：报告 /proc/pid/pagemap 中的文件/匿名位，这是安德鲁提议的实现，扩展页面映射文件位以报告任务工作集缺少的内容。 工作集检测的问题是多方面的。 在 criu（检查点/恢复）项目中，我们将任务的内存转储到图像文件中，为了正确执行此操作，我们需要检测映射中的哪些页面真正在使用。 我虽然可以帮助解决这个问题，但 mincore 系统调用却没有。 首先，它不报告交换的页面，因此我们无法找出要转储的匿名映射的哪些部分。 接下来，它会报告页面缓存中存在的页面，即使它们没有被映射，但这并不意味着它们没有受到威胁。 请注意，交换页的问题至关重要——我们必须将交换页转储到映像文件。 但是文件页面的问题是优化 - 我们可以将所有文件页面进行映像，这是正确的，但是如果我们知道页面未映射或未受到限制，我们可以将它们从转储文件中删除。 转储仍然是自洽的，尽管大小明显较小（在实际应用程序中最多小 10 倍）。 Andrew 注意到，proc pagemap 文件解决了上述 3 个问题中的 2 个——它报告页面是否存在或交换，并且不报告未映射的页面缓存页面。 但是，它无法区分受限制的文件页面和不受限制的文件页面。 我想在此文件中最后一个未使用的位来报告映射到相应 pte 的页面是否为 PageAnon
            pte_present(pte)
            frame = pte_pfn(pte) -> 
            make_pme(frame, flags)
        add_to_pagemap
    ...




smap: 基于映射的扩展，显示每个映射的内存消耗以及与其关联的标志, scan page table
static const struct mm_walk_ops smaps_walk_ops = {
    .pmd_entry		= smaps_pte_range,
    .hugetlb_entry		= smaps_hugetlb_range,
    .walk_lock		= PGWALK_RDLOCK,
};
smaps_pte_range
    smaps_pte_entry


show_smap
    smap_gather_stats
    show_map_vma
    __show_smap
    seq_printf


irdma：为英特尔(R) 以太网控制器 E810 添加 RDMA 驱动程序，这是针对英特尔(R) 以太网控制器 E810 的 RDMA FreeBSD 驱动程序（称为 irdma）的初始提交。 以每 PF 方式支持 RoCEv2 和 iWARP 协议，RoCEv2 为默认协议。 测试已使用 krping 工具、perftest、ucmatose、rping、ud_pingpong、rc_pingpong 等完成, https://cgit.freebsd.org/src/commit/?id=42bad04a2156


struct ice_vsi




intel e810, drivers/infiniband/hw/irdma/main.c
irdma_probe
    ib_alloc_device -> #define ib_alloc_device(drv_struct, member) -> struct ib_device *_ib_alloc_device(size_t size)
        device = kzalloc(size, GFP_KERNEL)
        rdma_restrack_init
        rdma_init_coredev -> RDMA/core：在net命名空间中实现compat device/sysfs树，实现ib_core的兼容层sysfs条目，以便非init_net net命名空间也可以发现rdma设备。 每个非 init_net 网络命名空间都在其中创建了 ib_core_device。 这样的 ib_core_device sysfs 树类似于 init_net 命名空间中找到的 rdma 设备。 这允许通过 sysfs 条目在多个非 init_net 网络命名空间中发现 rdma 设备，并且对 rdma-core 用户空间很有帮助
            此 BUILD_BUG_ON 旨在捕获 ib_core_device 和 device 联合的布局更改。 dev 必须是第一个元素，因为 ib_core 和提供程序驱动程序使用它。 在 ib_core_device 中的 device 之前添加任何内容都会打破这个假设
            coredev->dev.class = &ib_class
            device_initialize -> device_initialize - 初始化设备结构。 @dev：设备。 * 这通过初始化其字段来准备设备以供其他层使用。 如果由该函数调用，则它是 device_register() 的前半部分，尽管它也可以单独调用，因此可以使用 @dev 的字段。 特别是，调用此函数后，可以使用 get_device()/put_device() 对 @dev 进行引用计数。 * @dev 中的所有字段都必须由调用者初始化为 0，除非那些明确设置为其他值的字段。 最简单的方法是使用 kzalloc() 来分配包含 @dev 的结构。 * 注意：调用此函数后，使用 put_device() 放弃引用，而不是直接释放 @dev
                INIT_LIST_HEAD(&dev->dma_pools)
                device_pm_init(dev)
                swiotlb_dev_init(dev)
                    dev->dma_io_tlb_mem = &io_tlb_default_mem
            INIT_LIST_HEAD(&coredev->port_list) -> RDMA/core：引入 ib_core_device 来保存设备，为了支持 rdma 设备的多个网络命名空间中的 sysfs 条目，引入一个 ib_core_device ，其范围仅限于保存核心设备和每个端口 sysfs 相关条目。 这是准备补丁，以便在后续补丁中可以在每个网络命名空间中创建多个 ib_core_device，这些设备都可以共享 ib_device。 (a) 将 sysfs 特定字段移至 ib_core_device。 (b) 使 sysfs 和设备生命周期相关例程在 ib_core_device 上工作。 (c) 引入并使用 rdma_init_coredev() 帮助程序来初始化 coredev 字段
            write_pnet(&coredev->rdma_net, net)
        INIT_LIST_HEAD(&device->event_handler_list)
        init_rwsem(&device->event_handler_rwsem) -> init_rwsem()的功能是初始化读写信号量，将信号量的count字段设置为0
        xa_init_flags(&device->client_data, XA_FLAGS_ALLOC) -> 初始化数组
        init_completion(&device->unreg_completion) -> RDMA/核心：使用 netlink 命令同步取消注册, -> 初始化动态分配的完成对象 -> 参考: https://zhuanlan.zhihu.com/p/504938832
        device->uverbs_cmd_mask = -> IB_USER_VERBS_CMD_ALLOC_MW... -> 设置命令掩码/比特位
    irdma_fill_device_info -> 填充设备信息,默认配置
        rf->gen_ops.register_qset = irdma_lan_register_qset;
            ice_add_rdma_qset(pf, &qset)
                ice_get_main_vsi
                ice_cfg_vsi_rdma
                    ice_cfg_vsi_qs
                        ice_sched_get_tc_node
                        ice_sched_cfg_vsi
                            ice_get_vsi_ctx
                            ice_sched_get_vsi_node
                            ...
                ice_ena_vsi_rdma_qset
                    ice_sched_get_free_qparent
        rf->gen_ops.unregister_qset = irdma_lan_unregister_qset
        rf->gen_ops.request_reset = irdma_request_reset
        rf->hw.hw_addr = pf->hw.hw_addr
        ...
    irdma_ctrl_init_hw -> 初始化硬件的控制部分
        irdma_setup_init_state
            status = irdma_save_msix_info(rf) -> 保存中断信息 -> 将 msix 矢量信息复制到 iwarp 设备 @rf: RDMA PCI 函数分配, iwdev msix 表并将 msix 信息复制到表中如果成功则返回 0，否则返回错误
				rf->iw_msixtbl = kzalloc(size, GFP_KERNEL)
				pmsix = rf->msix_entries
				for (i = 0, ceq_idx = 0; i < rf->msix_count; i++, iw_qvinfo++)
					rf->iw_msixtbl[i].idx = pmsix->entry;
					rf->iw_msixtbl[i].irq = pmsix->vector;
					rf->iw_msixtbl[i].cpu_affinity = ceq_idx;
                    if (!i) -> 如果是第一个中断向量(i = 0)
            rf->obj_mem.size = ALIGN(8192, IRDMA_HW_PAGE_SIZE) -> 以4096为上界对齐, 分配8KB
            rf->obj_mem.va = dma_alloc_coherent(rf->hw.device, rf->obj_mem.size, &rf->obj_mem.pa, GFP_KERNEL) -> 申请连续的大块内存(8KB)
            irdma_initialize_dev
                irdma_obj_aligned_mem -> irdma_obj_aligned_mem - 从设备分配的内存中获取对齐的内存，@rf：RDMA PCI函数@memptr：指向内存地址@size：所需的内存大小@mask：对齐内存的掩码获取请求大小的对齐内存并更新memptr 指向新对齐的内存成功则返回0，否则返回无内存错误 -> rf: RDMA PCI function
                    rf->obj_next.pa = memptr->pa + size
                irdma_obj_aligned_mem(rf, &mem, IRDMA_COMMIT_FPM_BUF_SIZE,
                info.bar0 = rf->hw.hw_addr
                irdma_sc_dev_init -> Initialize control part of device
                    dev->hw->hw_addr = info->bar0
                    irdma_sc_init_hw(dev) -> RDMA/irdma：实施硬件管理队列 OP，驱动程序将特权命令发布到硬件管理队列（控制 QP 或 CQP）以请求硬件的管理操作。 实现 CQP 的创建/销毁以及支持函数、数据结构和标头以处理不同的 CQP 命令
                        i40iw_init_hw(dev) -> V1
                            dev->hw_regs[i] = (u32 __iomem *)(i40iw_regs[i] + hw_addr)
                            dev->wqe_alloc_db = dev->hw_regs[IRDMA_WQEALLOC]
                            dev->hw_attrs.page_size_cap = SZ_4K | SZ_2M -> RDMA/irdma：不要为 x722 通告 1GB 页面大小，x722 不支持 1GB 页面大小，但 irdma 驱动程序错误地将 x722 设备的 1GB 页面大小支持通告给 ib_core，以计算在此 MR 上使用的最佳页面大小。 这可能会导致 MR 上的硬件计算出不正确的起始偏移量
                            dev->hw_attrs.max_qp_wr = I40IW_MAX_QP_WRS
                        or icrdma_init_hw(dev) -> V2
                            dev->hw_attrs.page_size_cap = SZ_4K | SZ_2M | SZ_1G;
                    irdma_wait_pe_ready(dev)
                        do {
                            statuscpu0 = readl(dev->hw_regs[IRDMA_GLPE_CPUSTATUS0])
                            mdelay(1000) -> mdelay是忙等待函数，在延迟过程中无法运行其他任务．这个延迟的时间是准确的．是需要等待多少时间就会真正等待多少时间
                        }
                    val = readl(dev->hw_regs[IRDMA_GLPCI_LBARCTRL])
                    dev->db_addr = dev->hw->hw_addr + (uintptr_t)dev->hw_regs[IRDMA_DB_ADDR_OFFSET]
        irdma_create_cqp(rf)
            cqp->sq.va = dma_alloc_coherent(dev->hw->device, cqp->sq.size,
            irdma_obj_aligned_mem(rf, &mem, sizeof(struct irdma_cqp_ctx),
            irdma_sc_cqp_init(dev->cqp, &cqp_init_info) -> Initialize buffers for a control Queue Pair
                irdma_get_encoded_wqe_size IRDMA_QUEUE_TYPE_CQP
                cqp->rocev2_rto_policy = info->rocev2_rto_policy
                memcpy(&cqp->dcqcn_params, &info->dcqcn_params, sizeof(cqp->dcqcn_params))
                IRDMA_RING_INIT(cqp->sq_ring, cqp->sq_size)
                INIT_LIST_HEAD(&cqp->dev->cqp_cmd_head)
                writel(0, cqp->dev->hw_regs[IRDMA_CQPTAIL]) -> db, register -> fpga, xt not call this db
                writel(0, cqp->dev->hw_regs[IRDMA_CQPDB])
                writel(0, cqp->dev->hw_regs[IRDMA_CCQPSTATUS])
            irdma_sc_cqp_create(dev->cqp, &maj_err, &min_err) -> create cqp during bringup
                cqp->sdbuf.size = ALIGN(IRDMA_UPDATE_SD_BUFF_SIZE * cqp->sq_size, IRDMA_SD_BUF_ALIGNMENT -> 128
                writel(p1, cqp->dev->hw_regs[IRDMA_CCQPHIGH])
                writel(p2, cqp->dev->hw_regs[IRDMA_CCQPLOW])
                udelay(cqp->dev->hw_attrs.max_sleep_count)
                readl(cqp->dev->hw_regs[IRDMA_CCQPSTATUS])
                cqp->process_cqp_sds = irdma_update_sds_noccq
            INIT_LIST_HEAD(&cqp->cqp_avail_reqs)
            INIT_LIST_HEAD(&cqp->cqp_pending_reqs)
            init_waitqueue_head(&cqp->cqp_requests[i].waitq) <- wake_up(&cqp_request->waitq)
        irdma_hmc_setup(rf)
            qpcnt = rsrc_limits_table[rf->limits_sel].qplimit
            rf->sd_type = IRDMA_SD_TYPE_DIRECT -> 默认段描述符为直接寻址
            irdma_cfg_fpm_val(&rf->sc_dev, qpcnt) -> 算法, 配置PCI功能私有内存 (FPM)
                irdma_sc_init_iw_hmc(dev, dev->hmc_fn_id)
                    wait_type = (u8)IRDMA_CQP_WAIT_POLL_REGS
                    irdma_sc_query_fpm_val(dev->cqp, 0, hmc_info->hmc_fn_id, &query_fpm_mem, true, wait_type)
                        wqe = irdma_sc_cqp_get_next_send_wqe(cqp, scratch)
                        set_64bit_val(wqe, 32, query_fpm_mem->pa) -> PA to HW
                        hdr = FIELD_PREP(IRDMA_CQPSQ_OPCODE, IRDMA_CQP_OP_QUERY_FPM_VAL) -> 查询CQP的FPM大小
                        irdma_sc_cqp_post_sq(cqp)
                        irdma_cqp_poll_registers(cqp, tail,
                            irdma_get_cqp_reg_info(cqp, &val, &newtail, &error)
                            error = readl(cqp->dev->hw_regs[IRDMA_CQPERRCODES])
                    irdma_sc_parse_fpm_query_buf(dev, query_fpm_mem.va, hmc_info,
                        hmc_info->first_sd_index = (u16)FIELD_GET(IRDMA_QUERY_FPM_FIRST_PE_SD_INDEX, temp)
                        hmc_info->sd_table.sd_cnt = max_pe_sds + hmc_info->first_sd_index;
                        ...
                        irdma_sc_decode_fpm_query(buf, 32, obj_info, IRDMA_HMC_IW_HTE)
                            get_64bit_val(buf, buf_idx, &temp)
                        obj_info[IRDMA_HMC_IW_APBVT_ENTRY].size = 
                        ...
                for (i = IRDMA_HMC_IW_QP; i < IRDMA_HMC_IW_MAX; i++)
                    hmc_info->hmc_obj[i].cnt = hmc_info->hmc_obj[i].max_cnt -> 初始化各 HMC 对象个数
                sd_needed = irdma_est_sd(dev, hmc_info) -> 返回 HMC 的 SD 的近似数量
                    size += round_up(hmc_info->hmc_obj[IRDMA_HMC_IW_PBLE].cnt * hmc_info->hmc_obj[IRDMA_HMC_IW_PBLE].size, 512)
                    sd = size >> 21 -> SD个数 = 各对象总大小 / 2MB(每个SD代表2MB)
                qpwanted = min(qp_count, hmc_info->hmc_obj[IRDMA_HMC_IW_QP].max_cnt)
                pblewanted = hmc_info->hmc_obj[IRDMA_HMC_IW_PBLE].max_cnt
                "HMC: req_qp=%d max_sd=%d, max_qp = %d, max_cq=%d, max_mr=%d, max_pble=%d, mc=%d, av=%d\n",
                while (irdma_q1_cnt(dev, hmc_info, qpwanted) > hmc_info->hmc_obj[IRDMA_HMC_IW_Q1].max_cnt)
                    roundup_pow_of_two
                cfg_fpm_value_gen_1
                or
                cfg_fpm_value_gen_2
                irdma_sc_cfg_iw_fpm(dev, dev->hmc_fn_id) -> warp
                    irdma_sc_commit_fpm_val -> IRDMA_COMMIT_FPM_BUF_SIZE | IRDMA_CQP_OP_COMMIT_FPM_VAL -> 提交FPM配置
                    irdma_sc_parse_fpm_commit_buf
                        irdma_sc_decode_fpm_commit
                            obj_info[rsrc_idx].cnt = (u32)FIELD_GET(IRDMA_COMMIT_FPM_QPCNT, temp) -> 初始化时获取支持的QP数量规格
                virt_mem.va = kzalloc(virt_mem.size, GFP_KERNEL)
                hmc_info->sd_table.sd_entry = virt_mem.va
            irdma_create_hmc_objs(rf, true, rf->rdma_ver) -> 初始化时驱动会按照支持QP数量规格把HMC准备好 -> create all hmc objects for the device
                for (i = 0; i < IW_HMC_OBJ_TYPE_NUM; i++)
                    irdma_create_hmc_obj_type(dev, &info) -> irdma_sc_create_hmc_obj
	                    irdma_find_sd_index_limit(info->hmc_info, info->rsrc_type, info->start_idx, info->count, &sd_idx, &sd_lmt) -> SD, 查找段描述符索引限制，@hmc_info：指向 HMC 配置信息结构的指针 @type：我们正在搜索的 HMC 资源类型 @idx：对象的起始索引 @cnt：我们尝试创建的对象数量 @ sd_idx：返回相关段描述符索引的指针 @sd_limit：返回段描述符最大数量的指针 该函数计算 irdma_hmc_rsrc_type 定义的资源的段描述符索引和索引限制
							fpm_addr = hmc_info->hmc_obj[(type)].base + hmc_info->hmc_obj[type].size * idx
							fpm_limit = fpm_addr + hmc_info->hmc_obj[type].size * cnt
							*sd_idx = (u32)(fpm_addr / IRDMA_HMC_DIRECT_BP_SIZE) -> fpm_addr / 2MB
							*sd_limit = (u32)((fpm_limit - 1) / IRDMA_HMC_DIRECT_BP_SIZE)
							*sd_limit += 1
						irdma_find_pd_index_limit(info->hmc_info, info->rsrc_type, info->start_idx, info->count, &pd_idx, &pd_lmt) -> PD, finds page descriptor index limit
                        for (j = sd_idx; j < sd_lmt; j++)
                            irdma_add_sd_table_entry(dev->hw, info->hmc_info, j, info->entry_type, IRDMA_HMC_DIRECT_BP_SIZE) -> 0x200000 (2MB) -> RDMA/irdma：添加 HMC 后备存储设置功能，HW 使用主机内存作为多个协议上下文对象和队列状态跟踪的后备存储。 主机内存缓存 (HMC) 是负责管理存储在主机内存中的这些对象的组件。 添加函数和数据结构来管理 HMC 为各种对象使用的支持页面的分配 -> Adds a segment descriptor to the table (SD)
                                allocate a 4K pd page or 2M backing page
                                dma_mem.size = ALIGN(alloc_len, IRDMA_HMC_PD_BP_BUF_ALIGNMENT) -> 4K
                                dma_mem.va = dma_alloc_coherent(hw->device, dma_mem.size, &dma_mem.pa, GFP_KERNEL) -> Alloc DMA Mem
                                if (type == IRDMA_SD_TYPE_PAGED)
                                    vmem->size = sizeof(struct irdma_hmc_pd_entry) * 512 -> 2MB = 512 * 4K
                                    vmem->va = kzalloc(vmem->size, GFP_KERNEL)
                                    sd_entry->u.pd_table.pd_entry = vmem->va
                                    memcpy(&sd_entry->u.pd_table.pd_page_addr, &dma_mem, sizeof(sd_entry->u.pd_table.pd_page_addr))
                                else
                                    memcpy(&sd_entry->u.bp.addr, &dma_mem, sizeof(sd_entry->u.bp.addr))
							if (sd_entry->entry_type == IRDMA_SD_TYPE_PAGED && (dev->hmc_info == info->hmc_info && info->rsrc_type != IRDMA_HMC_IW_PBLE))
								for (i = pd_idx1; i < pd_lmt1; i++)
									ret_code = irdma_add_pd_table_entry(dev, info->hmc_info, i, NULL) -> Adds page descriptor to the specified table -> 将页面描述符添加到指定的表中，@dev：指向我们的设备结构的指针@hmc_info：指向HMC配置信息结构的指针@pd_index：要操作的页面描述符索引@rsrc_pg：如果不为NULL，则使用预分配的页面而不是分配 新的一个。 该函数： 1. 初始化 pd 条目 2. 在 pd_table 中添加 pd_entry 3. 在 irdma_hmc_pd_entry 结构中标记该条目有效 4. 将 pd_entry 的引用计数初始化为 1 假设： 1. pd 的内存应该固定，物理上连续并且 在 4K 边界上对齐并将内存归零。 2.大小应为4K
										if (!pd_entry->valid)
											page->size = ALIGN(IRDMA_HMC_PAGED_BP_SIZE, IRDMA_HMC_PD_BP_BUF_ALIGNMENT)
											page->va = dma_alloc_coherent(dev->hw->device, page->size, &page->pa,GFP_KERNEL)
											memcpy(&pd_entry->bp.addr, page, sizeof(pd_entry->bp.addr))
											pd_entry->bp.entry_type = IRDMA_SD_TYPE_PAGED
											pd_entry->sd_index = sd_idx
											page_desc = page->pa | 0x1
											pd_entry->valid = true
											pd_table->use_cnt++
											irdma_invalidate_pf_hmc_pd(dev, sd_idx, rel_pd_idx) -> 使 PF 硬件中的 pd 缓存无效
												writel(val, dev->hw_regs[IRDMA_PFHMC_PDINV])
                        info->hmc_info->sd_indexes[info->add_sd_cnt] = (u16)j
						info->add_sd_cnt++
						sd_entry->valid = true
                        irdma_hmc_finish_add_sd_reg(dev, info) -> irdma_hmc_sd_grp(dev, info->hmc_info, info->hmc_info->sd_indexes[0], info->add_sd_cnt, true) -> 为 cqp 设置 sd 条目组
                            for (i = sd_index; i < sd_index + sd_cnt; i++)
                                irdma_set_sd_entry(pa, i, sd_entry->entry_type,
                                    entry->data = pa | IRDMA_HMC_MAX_BP_COUNT
                                    entry->cmd = idx | FIELD_PREP(IRDMA_PFHMC_SDCMD_PMSDWR, 1) | BIT(15)
                                or irdma_clr_sd_entry(i, sd_entry->entry_type,
                                if (sdinfo.cnt == IRDMA_MAX_SD_ENTRIES)
                                    ret_code = dev->cqp->process_cqp_sds(dev, &sdinfo)
                    irdma_sc_static_hmc_pages_allocated(dev->cqp, 0, dev->hmc_fn_id, true, true) -> cqp wqe to allocate hmc pages
                        IRDMA_SHMC_PAGE_ALLOCATED_HMC_FN_ID
                        hdr = FIELD_PREP(IRDMA_CQPSQ_OPCODE, IRDMA_CQP_OP_SHMC_PAGES_ALLOCATED) | FIELD_PREP(IRDMA_CQPSQ_WQEVALID, cqp->polarity)
        irdma_initialize_hw_rsrc -> 初始化硬件资源跟踪数组
            if (rf->rdma_ver != IRDMA_GEN_1)
                rf->allocated_ws_nodes = bitmap_zalloc(IRDMA_MAX_WS_NODES,
                set_bit(0, rf->allocated_ws_nodes)
            rsrc_size = irdma_calc_mem_rsrc_size(rf) -> 计算内存资源大小
                rsrc_size = sizeof(struct irdma_arp_entry) * rf->arp_table_size
                rsrc_size += sizeof(unsigned long) * BITS_TO_LONGS(rf->max_qp)
                ...
            rf->mem_rsrc = vzalloc(rsrc_size) -> 分配虚拟连续内存并用零填充
            rf->arp_table = (struct irdma_arp_entry *)rf->mem_rsrc
            irdma_set_hw_rsrc(rf) -> 按基址偏移和预留空间设置各物理资源(QP,CQ,MR,PD,AH,MCG,ARP,qp_table, cq_table)基地址
            rf->mr_stagmask = ~(((1 << mrdrvbits) - 1) << (32 - mrdrvbits))
        irdma_create_ccq -> 创建用于控制操作的完成队列
            ccq->shadow_area.size = sizeof(struct irdma_cq_shadow_area)
            ccq->mem_cq.va = dma_alloc_coherent
            info.num_elem = IW_CCQ_SIZE -> 2048
            irdma_sc_ccq_init(dev->ccq, &info)
                cq->cq_type = IRDMA_CQ_TYPE_CQP
                IRDMA_RING_INIT(cq->cq_uk.cq_ring, info->num_elem)
                cq->cq_uk.polarity = true -> 极性位/可反转/有效位
            irdma_sc_ccq_create(dev->ccq, 0, true, true)
                irdma_sc_cq_create(ccq, scratch, check_overflow, post_sq) -> 创建完成队列
                    ceq = cq->dev->ceq[cq->ceq_id]
                     irdma_sc_add_cq_ctx(ceq, cq) -> 为 ceq 添加 cq ctx 跟踪
                        ceq->reg_cq[ceq->reg_cq_size++] = cq
                     ...
                     set_64bit_val(wqe, 8, (uintptr_t)cq >> 1) -> tmp_cq << 1
                irdma_sc_ccq_create_done -> irdma_sc_poll_for_cqp_op_done(cqp, IRDMA_CQP_OP_CREATE_CQ, NULL) -> 等待 CQP SQ 中最后一次写入完成
                    while (1)
                        irdma_sc_ccq_get_cqe_info
                            polarity = (u8)FIELD_GET(IRDMA_CQ_VALID, temp)
                            IRDMA_RING_MOVE_HEAD ->  move the head for cq
                            IRDMA_RING_MOVE_TAIL ->  update cq tail in cq shadow memory also
                        udelay(cqp->dev->hw_attrs.max_sleep_count)
                ccq->dev->cqp->process_cqp_sds = irdma_cqp_sds_cmd
        irdma_setup_ceq_0 -> 创建CEQ 0及其中断资源 -> 为所有设备完成事件队列分配一个列表，创建ceq 0并配置其msix中断向量返回0，如果设置成功，否则返回错误
            num_ceqs = min(rf->msix_count, rf->sc_dev.hmc_fpm_misc.max_ceqs
            rf->ceqlist = kcalloc(num_ceqs, sizeof(*rf->ceqlist), GFP_KERNEL)
            irdma_create_ceq(rf, iwceq, 0, &rf->default_vsi) -> 创建完成事件队列
				ceq_size = min(rf->sc_dev.hmc_info->hmc_obj[IRDMA_HMC_IW_CQ].cnt, dev->hw_attrs.max_hw_ceq_size)
				iwceq->mem.size = ALIGN(sizeof(struct irdma_ceqe) * ceq_size, IRDMA_CEQ_ALIGNMENT)
				iwceq->mem.va = dma_alloc_coherent(dev->hw->device, iwceq->mem.size, &iwceq->mem.pa, GFP_KERNEL)
				info.ceqe_base = iwceq->mem.va
				info.ceqe_pa = iwceq->mem.pa
                irdma_sc_ceq_init(&iwceq->sc_ceq, &info)
					ceq->ceqe_base = (struct irdma_ceqe *)info->ceqe_base
					ceq->polarity = 1
					IRDMA_RING_INIT(ceq->ceq_ring, ceq->elem_cnt)
                irdma_cqp_ceq_cmd(&rf->sc_dev, &iwceq->sc_ceq, IRDMA_OP_CEQ_CREATE) -> irdma_sc_ceq_create
                or irdma_sc_cceq_create(&iwceq->sc_ceq, 0)
					ret_code = irdma_sc_ceq_create(ceq, scratch, true)
			i = rf->msix_shared ? 0 : 1 -> 共享模式取第0个索引
            irdma_cfg_ceq_vector(rf, iwceq, 0, msix_vec) -> 为完成事件队列设置中断, 传入ceq_id为0
                if (rf->msix_shared && !ceq_id)
                    tasklet_setup(&rf->dpc_tasklet, irdma_dpc) -> 处理AEQ和CEQ0的事件
                    request_irq(msix_vec->irq, irdma_irq_handler, 0,
						tasklet_schedule(&rf->dpc_tasklet)
                else
                    tasklet_setup(&iwceq->dpc_tasklet, irdma_ceq_dpc) -> 只处理CEQ事件
                        irdma_process_ceq(rf, iwceq)
                            do cq = irdma_sc_process_ceq(dev, sc_ceq)
								ceqe = IRDMA_GET_CURRENT_CEQ_ELEM(ceq)
                                temp_cq = (struct irdma_sc_cq *)(unsigned long)(temp << 1) -> how to get cq
                                ceq->polarity ^= 1
								cq_idx = irdma_sc_find_reg_cq(ceq, cq);
								IRDMA_RING_MOVE_TAIL(ceq->ceq_ring)
                                irdma_sc_cq_ack(cq)
									writel(cq->cq_uk.cq_id, cq->cq_uk.cq_ack_db)
							if (cq->cq_type == IRDMA_CQ_TYPE_IWARP)
								irdma_iwarp_ce_handler(cq)
									if (cq->ibcq.comp_handler)
										cq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context) -> ib_uverbs_comp_handler
                            queue_work(rf->cqp_cmpl_wq, &rf->cqp_cmpl_work)
                            irdma_puda_ce_handler(rf, cq)
                                do {
                                    irdma_puda_poll_cmpl(dev, cq, &compl_error)
                                        if (info.q_type == IRDMA_CQE_QTYPE_RQ)
                                            irdma_puda_poll_info(cq, &info)
                                                cqe = IRDMA_GET_CURRENT_CQ_ELEM(&cq->cq_uk)
                                                    (_cq)->cq_base[IRDMA_RING_CURRENT_HEAD((_cq)->cq_ring)].buf
                                                get_64bit_val(cqe, 24, &qword3)
                                                info->qp = (struct irdma_qp_uk *)(unsigned long)comp_ctx
                                                ...
                                            dma_sync_single_for_cpu -> 确保DMA缓冲区中的数据与物理内存中的数据同步。如果需要将数据从设备上读取到内存中，则应该使用 dma_sync_single_for_cpu()函数。如果需要将数据从内存中写入到设备上，则应该使用 dma_sync_single_for_device()函数
                                            irdma_puda_get_tcpip_info -> get tcpip info from puda buffer
                                                if (buf->vsi->dev->hw_attrs.uk_attrs.hw_rev == IRDMA_GEN_1)
                                                    irdma_gen1_puda_get_tcpip_info(info, buf)
                                                        if (ethh->h_proto == htons(0x8100))
                                                            buf->vlan_id = ntohs(((struct vlan_ethhdr *)ethh)->h_vlan_TCI) & VLAN_VID_MASK
                                                            ip6h = (struct ipv6hdr *)buf->iph
                                                buf->ipv4 = info->ipv4
                                                buf->seqnum = ntohl(tcph->seq)
                                                ether_addr_copy(buf->smac, info->smac)
                                            rsrc->receive(rsrc->vsi, buf) -> irdma_ieq_receive
                                                qp = irdma_ieq_get_qp(vsi->dev, buf)
                                                irdma_ieq_handle_exception(ieq, qp, buf)
                                                    irdma_ieq_check_first_buf(buf, fps)
                                                    irdma_send_ieq_ack(qp)
                                            irdma_ilq_putback_rcvbuf
                                            or irdma_puda_replenish_rq
                                                irdma_puda_get_bufpool
                                                    irdma_puda_get_listbuf
                                                irdma_puda_post_recvbuf
                                        else
                                            rsrc->xmit_complete(rsrc->vsi, buf -> irdma_ieq_tx_compl
                                                irdma_puda_ret_bufpool
                                                    list_add(&buf->list, &rsrc->bufpool)
                                            irdma_puda_send_buf
                                                irdma_puda_send
                                                    wqe = irdma_puda_get_next_send_wqe(&qp->qp_uk, &wqe_idx)
                                                    irdma_uk_qp_post_wr(&qp->qp_uk)
                                                        writel(qp->qp_id, qp->wqe_alloc_db)
                                    irdma_sc_ccq_arm(cq)
                                        writel(ccq->cq_uk.cq_id, ccq->dev->cq_arm_db)
                                }
                        irdma_ena_intr(&rf->sc_dev, iwceq->msix_idx) -> dev->irq_ops->irdma_en_irq(dev, msix_id) -> icrdma_ena_irq -> 启用中断
                            writel(val, dev->hw_regs[IRDMA_GLINT_DYN_CTL] + idx)
                    status = request_irq(msix_vec->irq, irdma_ceq_handler, 0, msix_vec->name, iwceq)
                        tasklet_schedule(&iwceq->dpc_tasklet)
                cpumask_clear(&msix_vec->mask)
                cpumask_set_cpu(msix_vec->cpu_affinity, &msix_vec->mask)
                irq_update_affinity_hint(msix_vec->irq, &msix_vec->mask)
                rf->sc_dev.irq_ops->irdma_cfg_ceq(&rf->sc_dev, ceq_id, msix_vec->idx, true) -> icrdma_cfg_ceq
                    writel(reg_val, dev->hw_regs[IRDMA_GLINT_CEQCTL] + ceq_id)
            irdma_ena_intr(&rf->sc_dev, msix_vec->idx)
        INIT_WORK(&rf->cqp_cmpl_work, cqp_compl_worker) -> irdma_cqp_ce_handler
            wake_up(&cqp_request->waitq)
            or cqp_request->callback_fcn(cqp_request)
			if (cqe_count)
				irdma_process_bh(dev);
					status = irdma_exec_cqp_cmd(dev, pcmdinfo)
				irdma_sc_ccq_arm(cq);
					get_64bit_val(ccq->cq_uk.shadow_area, 32, &temp_val)
					arm_seq_num++;
					writel(ccq->cq_uk.cq_id, ccq->dev->cq_arm_db)
        irdma_sc_ccq_arm
    ice_get_qos_params
    irdma_fill_qos_info
        l2params->vsi_prio_type = qos_info->vport_priority_type
        l2params->tc_info[i].rel_bw = qos_info->tc_info[i].rel_bw
        memcpy(l2params->dscp_map, qos_info->dscp_map, sizeof(l2params->dscp_map))
    irdma_rt_init_hw -> 初始化硬件运行时, ILQ, IEQ, CEQs and PBLEs
        irdma_sc_vsi_init
        irdma_setup_cm_core
        irdma_vsi_stats_init
        do {
            if (!iwdev->roce_mode) -> 非RoCE模式
            irdma_initialize_ilq -> create iwarp local queue for cm
            irdma_initialize_ieq
                irdma_puda_create_rsrc
                    irdma_puda_qp_create
                        irdma_cqp_qp_create_cmd
                            cqp_info->cqp_cmd = IRDMA_OP_QP_CREATE
            status = irdma_setup_ceqs(rf, &iwdev->vsi) -> 管理设备 ceq 及其中断资源，@rf：RDMA PCI 函数 @vsi：此 CEQ 的 VSI 结构 为所有设备完成事件队列分配列表 创建 ceq 并配置其 msix 中断向量 如果 ceq 成功
				for (ceq_id = 1; i < num_ceqs; i++, ceq_id++)
					iwceq = &rf->ceqlist[ceq_id] -> get ceq_addr
					status = irdma_create_ceq(rf, iwceq, ceq_id, vsi)
					status = irdma_cfg_ceq_vector(rf, iwceq, ceq_id, msix_vec)
						irdma_dpc
							irdma_process_aeq -> 处理异步事件
								...
								case IRDMA_AE_CQ_OPERATION_ERROR
									if (iwcq->ibcq.event_handler)
										iwcq->ibcq.event_handler(&ibevent, iwcq->ibcq.cq_context) -> ib_uverbs_cq_event_handler
                irdma_ena_intr
            iwdev->init_state = CEQS_CREATED
            status = irdma_hmc_init_pble(&rf->sc_dev, rf->pble_rsrc) -> Initialize pble resources during module load
                struct irdma_hmc_info *hmc_info
                relationship hmc_info and pble_rsrc
                if (pble_rsrc->fpm_base_addr & 0xfff) -> 0xFFF 4096
                    fpm_idx = (4096 - (pble_rsrc->fpm_base_addr & 0xfff)) >> 3
                pble_rsrc->fpm_base_addr = hmc_info->hmc_obj[IRDMA_HMC_IW_PBLE].base
                pble_rsrc->pinfo.pble_shift = PBLE_SHIFT
                INIT_LIST_HEAD(&pble_rsrc->pinfo.clist)
                add_pble_prm(pble_rsrc)
            irdma_setup_aeq
                irdma_create_aeq
                irdma_cfg_aeq_vector
                irdma_ena_intr
            irdma_alloc_set_mac -> set up a mac address table entry
                irdma_alloc_local_mac_entry
                    cqp_info->cqp_cmd = IRDMA_OP_ALLOC_LOCAL_MAC_ENTRY
                irdma_add_local_mac_entry
                    cqp_info->cqp_cmd = IRDMA_OP_ADD_LOCAL_MAC_ENTRY
            irdma_add_ip -> add ip addresses
                irdma_add_ipv4_addr
                    ip_addr = ntohl(ifa->ifa_address)
                    irdma_manage_arp_cache(iwdev->rf, dev->dev_addr, &ip_addr, true, IRDMA_ARP_ADD) -> manage hw arp cache
                        arp_index = irdma_arp_table(rf, ip_addr, ipv4, mac_addr, action)
                        cqp_request = irdma_alloc_and_get_cqp_request(&rf->cqp, false)
                        cqp_info->cqp_cmd = IRDMA_OP_ADD_ARP_CACHE_ENTRY
                irdma_add_ipv6_addr -> 将IPv6地址添加到硬件ARP表中
                    idev = __in6_dev_get(ip_dev)
                    irdma_copy_ip_ntohl(local_ipaddr6, ifp->addr.in6_u.u6_addr32)
                        *dst++ = ntohl(*src++);
                        ...
            iwdev->cleanup_wq = alloc_workqueue("irdma-cleanup-wq",
            irdma_get_used_rsrc -> 确定内部使用的资源
                iwdev->rf->used_pds
                ...
            init_waitqueue_head
        }
        irdma_rt_deinit_hw(iwdev) -> clean up the irdma device resources -> 根据状态机清理设备资源
            switch (iwdev->init_state)
    irdma_ib_register_device
    ice_rdma_update_vsi_filter -> update main VSI filters for RDMA -> get ice eth api
        vsi = ice_find_vsi(pf, vsi_id)
        ice_cfg_rdma_fltr(&pf->hw, vsi->idx, enable) -> enable/disable RDMA filtering on VSI
            cached_ctx = ice_get_vsi_ctx(hw, vsi_handle)
            ctx->info.valid_sections = cpu_to_le16(ICE_AQ_VSI_PROP_Q_OPT_VALID)
            ice_update_vsi
                ice_aq_update_vsi -> Ice：扩展 VSI 句柄的使用第 1/2 部分，VSI 句柄只是驱动程序维护的一个数字，用于唯一标识 VSI。 VSI 句柄由硬件中的 VSI 编号支持。 当与硬件交互时，VSI句柄被转换成VSI编号。 在提交 0f9d5027a749（“ice：重构 VSI 分配、删除和重建流程”）中，引入了 VSI 句柄，但仅在创建和删除 VSI 时使用。 该补丁是两个补丁之一，这两个补丁扩展了 VSI 句柄在驱动程序其余部分的使用。 此外，在此补丁中，必须重构代码的某些部分才能正确使用 VSI 句柄
                    ice_fill_dflt_direct_cmd_desc
                    cmd->vsi_num = cpu_to_le16(vsi_ctx->vsi_num | ICE_AQ_VSI_IS_VALID)
                    status = ice_aq_send_cmd(hw, &desc, &vsi_ctx->info, -> send FW Admin Queue command to FW Admin Queue
                        ice_sq_send_cmd_retry
                            ice_sq_send_cmd -> send command to Control Queue (ATQ)
                                desc_on_ring = ICE_CTL_Q_DESC(cq->sq, cq->sq.next_to_use)
                                wr32(hw, cq->sq.tail, cq->sq.next_to_use)
                                ice_flush(hw)
                                udelay(5)
    auxiliary_set_drvdata



/* types of hmc objects */
static enum irdma_hmc_rsrc_type iw_hmc_obj_types[] = {
	IRDMA_HMC_IW_QP,
	IRDMA_HMC_IW_CQ,
	IRDMA_HMC_IW_HTE,
	IRDMA_HMC_IW_ARP,
	IRDMA_HMC_IW_APBVT_ENTRY,
	IRDMA_HMC_IW_MR,
	IRDMA_HMC_IW_XF,
	IRDMA_HMC_IW_XFFL,
	IRDMA_HMC_IW_Q1,
	IRDMA_HMC_IW_Q1FL,
	IRDMA_HMC_IW_PBLE,
	IRDMA_HMC_IW_TIMER,
	IRDMA_HMC_IW_FSIMC,
	IRDMA_HMC_IW_FSIAV,
	IRDMA_HMC_IW_RRF,
	IRDMA_HMC_IW_RRFFL,
	IRDMA_HMC_IW_HDR,
	IRDMA_HMC_IW_MD,
	IRDMA_HMC_IW_OOISC,
	IRDMA_HMC_IW_OOISCFFL,
};

#define IW_HMC_OBJ_TYPE_NUM	ARRAY_SIZE(iw_hmc_obj_types)

static const struct irdma_irq_ops icrdma_irq_ops = {
	.irdma_cfg_aeq = irdma_cfg_aeq,
	.irdma_cfg_ceq = icrdma_cfg_ceq,
	.irdma_dis_irq = icrdma_disable_irq,
	.irdma_en_irq = icrdma_ena_irq,
};






drivers/dma/dmatest.c
late_initcall(dmatest_init);



mlx5_ib_post_send_nodrain -> mlx5_ib_post_send
    begin_wqe
    ...



cm状态机:
enum rdma_cm_state {
    RDMA_CM_IDLE,
    RDMA_CM_ADDR_QUERY,
    RDMA_CM_ADDR_RESOLVED,
    RDMA_CM_ROUTE_QUERY,
    RDMA_CM_ROUTE_RESOLVED,
    RDMA_CM_CONNECT,
    RDMA_CM_DISCONNECT,
    RDMA_CM_ADDR_BOUND,
    RDMA_CM_LISTEN,
    RDMA_CM_DEVICE_REMOVAL,
    RDMA_CM_DESTROYING
};





mm/hugetlb.c
free_hugepages



const struct file_operations hugetlbfs_file_operations = {
    .read_iter		= hugetlbfs_read_iter,
    .mmap			= hugetlbfs_file_mmap,
    .fsync			= noop_fsync,
    .get_unmapped_area	= hugetlb_get_unmapped_area,
    .llseek			= default_llseek,
    .fallocate		= hugetlbfs_fallocate,
};





samples/trace_events
samples/trace_events/trace-events-sample.c







build kernel, https://zhuanlan.zhihu.com/p/105069730
apt-get install libncurses5-dev libssl-dev bison flex libelf-dev gcc make openssl libc6-dev -y

qemu:
apt-get install qemu -y
apt-get update
apt install qemu-kvm virt-manager virtinst libvirt-clients bridge-utils libvirt-daemon-system -y
apt install dwarves
buid:
gcc --static -o helloworld main.c
echo helloworld | cpio -o --format=newc > rootfs


此时将断点设在 init/main.c 中的start_kernel函数中，然后Qemu 开启GDB调试，vscode start debug即可开始调试内核

using make menuconfig it's going to be under "Kernel hacking" -> "Compile-time checks and compiler options" -> "Compiler a kernel with debug info"


source code:
wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.4.18.tar.gz


set key to ""
CONFIG_SYSTEM_TRUSTED_KEYS
CONFIG_SYSTEM_REVOCATION_KEYS

[iproute2-next,4/8] rdma：添加 rdma 统计计数器每端口自动模式支持，通过每 QP 统计计数器支持，用户可以监控特定的 QP 类别，这些类别与动态分配的计数器绑定/解除绑定 /解除分配。 在每端口“自动”模式下，QP 根据通用标准自动绑定到计数器。 例如，每个“类型”（qp 类型）方案，其中每个进程中所有具有相同 qp 类型的 QP 都会自动绑定到单个计数器。 目前仅支持“type”（qp 类型）。 示例: $ rdma statistic qp set link mlx5_2/1 auto type on $ rdma statistic qp set link mlx5_2/1 auto off, https://patchwork.kernel.org/project/linux-rdma/patch/20190410112121.6790-5-leon@kernel.org/



SET_PORT_SNIFFER



irdma register: -> RDMA/irdma：实施硬件管理队列 OP，驱动程序将特权命令发布到硬件管理队列（控制 QP 或 CQP）以请求硬件的管理操作。 实现 CQP 的创建/销毁以及支持函数、数据结构和标头以处理不同的 CQP 命令
enum irdma_registers {
	IRDMA_CQPTAIL,
	IRDMA_CQPDB,
	IRDMA_CCQPSTATUS,
	IRDMA_CCQPHIGH,
	IRDMA_CCQPLOW,
	IRDMA_CQARM,
	IRDMA_CQACK,
	IRDMA_AEQALLOC,
	IRDMA_CQPERRCODES,
	IRDMA_WQEALLOC,
	IRDMA_GLINT_DYN_CTL,
	IRDMA_DB_ADDR_OFFSET,
	IRDMA_GLPCI_LBARCTRL,
	IRDMA_GLPE_CPUSTATUS0,
	IRDMA_GLPE_CPUSTATUS1,
	IRDMA_GLPE_CPUSTATUS2,
	IRDMA_PFINT_AEQCTL,
	IRDMA_GLINT_CEQCTL,
	IRDMA_VSIQF_PE_CTL1,
	IRDMA_PFHMC_PDINV,
	IRDMA_GLHMC_VFPDINV,
	IRDMA_GLPE_CRITERR,
	IRDMA_GLINT_RATE,
	IRDMA_MAX_REGS, /* Must be last entry */
};



get_user_pages_fast:
进行参数检查，确保传入的gup_flags是合法的组合。
如果设置了FOLL_PIN标记，将当前进程的内存管理结构中的has_pinned原子变量设为1。
如果没有设置FOLL_FAST_ONLY标记，使用might_lock_read宏提示可能会对mmap_lock读锁进行加锁。
计算起始地址、结束地址和地址范围长度，确保它们是有效的。
禁用中断，阻止页面表页面在操作期间被释放。
如果启用了CONFIG_HAVE_FAST_GUP配置，并且允许快速GUP（Get User Pages）操作，则调用gup_pgd_range函数获取页面，将获取到的页面数量存储在nr_pinned变量中。
如果没有获取到所有请求的页面并且没有设置FOLL_FAST_ONLY标记，则尝试使用__gup_longterm_unlocked函数获取剩余的页面。
根据已获取页面的数量和剩余页面尝试的结果，返回最终结果。




rxe:
int rxe_mr_init_user
    rxe_mr_init
    xa_init -> struct xarray		page_list
    ib_umem_get
    rxe_mr_fill_pages_from_sgt
        __sg_page_iter_start
        __sg_page_iter_next
        while
            sg_page_iter_page
            xas_store
            xas_next


vaddr = page_address(sg_page_iter_page(&sg_iter))
page_address( )宏的功能是获得物理页的逻辑地址, 根据给定的struct page 结构体返回该物理页面的内核起始虚拟地址
buf->addr = (uintptr_t)vaddr


#define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
在x86_64等64位架构中，因为内核虚拟空间较大，所以直接把所有物理内存直接线性映射到内核虚拟地址当中，物理地址和内核虚拟地址仅仅一个PAGE_OFFSET的偏移, __va() 宏用于将给定的物理地址转换为对应的内核虚拟地址


ibv_create_flow -> IB_USER_VERBS_EX_CMD_CREATE_FLOW -> ib_uverbs_ex_create_flow
uverbs_request_start
rdma_is_port_valid
uobj_get_obj_read
flow_resources_alloc
    kcalloc
kern_spec_to_ib_spec
flow_id = qp->device->ops.create_flow(qp, flow_attr, -> mlx5_ib_create_flow
    get_flow_table
        mlx5_eswitch_get_encap_mode
    _create_flow_rule
        parse_flow_attr
        flow_is_multicast_only
            ipv4_is_multicast -> 用于检查地址是否是多播地址，即224.x.x.x的D类地址 -> static inline bool ipv4_is_multicast(__be32 addr) -> return (addr & htonl(0xf0000000)) == htonl(0xe0000000)
            is_multicast_ether_addr -> 如果一个流可以同时捕获组播和单播数据包，则它不会落入组播流转向表中，并且该规则可以窃取其他组播数据包
                return 0x01 & (a >> ((sizeof(a) * 8) - 8));
        set_underlay_qp
        mlx5_ib_set_rule_source_port
        get_match_criteria_enable
        mlx5_ib_flow_counters_set_data
        mlx5_add_flow_rules -> EXPORT_SYMBOL(mlx5_add_flow_rules)
            _mlx5_add_flow_rules
    create_leftovers_rule
        create_flow_rule
    create_sniffer_rule
        .type = IB_FLOW_ATTR_SNIFFER
        create_flow_rule(dev, ft_rx, &flow_attr, dst)
ib_set_flow
rdma_lookup_put_uobject
uobj_finalize_uobj_create
uverbs_response

qemu-system-x86_64 -enable-kvm -m 512 -smp 2 -boot order=c -hda ubuntu14.04.img -vnc :1 -monitor stdio
--graphics vnc,port=5900,listen=0.0.0.0


https://blog.csdn.net/qq_26907291/article/details/129403678?spm=1001.2014.3001.5502
qemu-system-x86_64 -m 2048 \
-drive file=ubuntu22.04.img,if=virtio \
-nic user,hostfwd=tcp::5557-:22 \
-vnc :1 -monitor stdio

# vnc可以查看开机情况，ssh可以登录VM与host主机共享网络
ssh -p 5557 username@localhost

# 往VM主机发送文件
scp -P 5557 filename username@localhost:/directory

# sharefolder方案，需要qemu安装的时候打开--enable-virtfs, # 开启virtfs方便后续切换vm的kernel版本
# host端启动qemu
qemu-system-x86_64 -m 2048 \
-drive file=ubuntu22.04.img,if=virtio \
-nic user,hostfwd=tcp::5557-:22 \
-fsdev local,security_model=passthrough,id=fsdev0,path=/host_directory \
-device virtio-9p-pci,fsdev=fsdev0,mount_tag=kernelcode \
-vnc :1 -monitor stdio

# 共享文件夹靠id和tag标识，可以加载多个
# VM启动后加载共享文件夹
mount -t 9p -o trans=virtio kernelcode /vm_directory

# 创建镜像
qemu-img create -f qcow2 ubuntu22.04.img 16G
# 安装系统
qemu-system-x86_64 -m 1024 -enable-kvm \
-drive if=virtio,file=ubuntu22.04.img,cache=none \
-cdrom ubuntu-22.04-live-server-amd64.iso \
-vnc :1


# 注意自己改路径
qemu-system-x86_64 -m 2048 -enable-kvm -cpu host \
-smp cores=16,sockets=1 \
-drive file=ubuntu-22.04.img,if=virtio \
-nic user,hostfwd=tcp::5557-:22 \
-fsdev local,security_model=passthrough,id=fsdev0,path=/kernel_code_directory \
-device virtio-9p-pci,fsdev=fsdev0,mount_tag=kernelmake \
-S -s \
-vnc :1 -monitor stdio


mounse, pointer, 
qemu/build/qemu-system-aarch64 \
-m 1024 \
-M raspi3b \
-kernel $TMP/boot/kernel8.img \
-dtb "$TMP/boot/bcm2710-rpi-3-b-plus.dtb" \
-drive file="$IMAGE_FILE",if=sd,format=raw \
-append "console=ttyAMA0 root=/dev/mmcblk0p2 rw rootwait rootfstype=ext4" \
-device usb-net,netdev=net0 \
-netdev user,id=net0,hostfwd=tcp::5555-:22 \
-device usb-mouse -device usb-tablet -device usb-kbd

-device nec-usb-xhci,id=usb,bus=pci.0,addr=0x4 for usb 3.0 or
-device usb-ehci,id=usb,bus=pci.0,addr=0x4 for usb 2.
Then you can pass -device usb-tablet.


qemu-system-x86_64 -m 2048 \
-drive file=ubuntu22.04.img,if=virtio \
-nic user,hostfwd=tcp::5557-:22 \
-vnc :1 -monitor stdio

# vnc可以查看开机情况，ssh可以登录VM与host主机共享网络
ssh -p 5557 username@localhost

# 往VM主机发送文件
scp -P 5557 filename username@localhost:/directory

# sharefolder方案，需要qemu安装的时候打开--enable-virtfs
# host端启动qemu
qemu-system-x86_64 -m 2048 \
-drive file=ubuntu22.04.img,if=virtio \
-nic user,hostfwd=tcp::5557-:22 \
-fsdev local,security_model=passthrough,id=fsdev0,path=/host_directory \
-device virtio-9p-pci,fsdev=fsdev0,mount_tag=kernelcode \
-vnc :1 -monitor stdio

# 共享文件夹靠id和tag标识，可以加载多个
# VM启动后加载共享文件夹
mount -t 9p -o trans=virtio kernelcode /vm_directory


enable-kvm
debug guest: qemu,
#ifdef KVM_CAP_SET_GUEST_DEBUG
    kvm_has_guest_debug =
        (kvm_check_extension(s, KVM_CAP_SET_GUEST_DEBUG) > 0);
            kvm_ioctl(s, KVM_CHECK_EXTENSION, extension)
KVM_CAP_SET_GUEST_DEBUG


problem: qemu-system-x86_64 -m 1024 -enable-kvm -S -s ...启动虚机的时候, 提示不支持kvm debug
查阅代码发现, 当前内核默认支持debug guest, 咱们使用的centos7, 内核是3.10的, 太老旧了

kvm_init
    kmem_cache_create_usercopy(const char *name,
    for_each_possible_cpu alloc_cpumask_var_node
    kvm_irqfd_init -> KVM：在架构硬件设置后初始化 IRQ FD，将 KVM 的 IRQ FD 工作队列的初始化移至架构硬件设置下方，作为巩固架构“init”和“硬件设置”的一步，并最终完全放弃挂钩。 不依赖于在硬件设置之前创建的工作队列，工作队列仅在销毁虚拟机时使用，即只需要在 /dev/kvm 暴露给用户空间之前创建。 将工作队列的销毁移到 arch 钩子之前以保持对称性，这样 arch 代码就可以远离钩子而不必担心顺序更改。 重写有关 kvm_irqfd_init() 需要在 kvm_arch_init() 之后出现的注释，以指出 kvm_arch_init() 必须在常见 KVM 执行任何操作之前出现，因为 x86 非常巧妙地依赖该行为来处理对 kvm_init() 的多次调用，例如 如果用户空间尝试加载 kvm_amd.ko 和 kvm_intel.ko。 使用 FIXME 标记代码，因为 x86 的微妙要求很粗暴，并且调用 arch 回调作为仅从 arch 代码调用的帮助程序中的第一个操作是愚蠢的
    kvm_async_pf_init
    kvm_init_debug
        debugfs_create_dir("kvm", NULL)
        debugfs_create_file
    kvm_vfio_ops_init
         kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO)
    kvm_gmem_init -> KVM：为特定于客户的后备内存添加 KVM_CREATE_GUEST_MEMFD ioctl()，引入 ioctl()、KVM_CREATE_GUEST_MEMFD，以允许创建与特定 KVM 虚拟机绑定的基于文件的内存，其主要目的是为客户内存提供服务。 客户优先内存子系统允许进行优化和增强，而这些优化和增强在通用内存子系统中实现/支持是笨拙或完全不可行的。 使用 guest_memfd，来宾保护和映射大小与主机用户空间映射完全解耦。 例如。 KVM 目前不支持将内存映射为在来宾中可写，而不在主机用户空间中也可写，因为 KVM 的 ABI 使用 VMA 保护来定义允许来宾保护。 用户空间可以通过建立两个映射来弥补这一点，一个是客户可写的映射，一个是自身可读的映射，但这在多个方面都不是最佳的。 类似地，KVM 目前要求来宾映射大小是主机用户空间映射大小的严格子集，例如 KVM 不支持创建 1GiB 访客映射，除非用户空间也有 1GiB 访客映射。 解耦映射大小将允许用户空间仅精确映射所需的内容，而不会影响来宾性能，例如 加强对来宾内存的无意访问。 解耦来宾和用户空间映射还可以为 HugeTLB 的高粒度映射提供更清晰的替代方案，而 HugeTLB 已经陷入了僵局，并且不太可能被合并。 客户优先的内存子系统还为诸如专用内存池（用于硬件虚拟机切片）和消除“结构页面”（用于用户空间永远不需要 mmap() 客户内存的卸载设置）等提供了更清晰的视线。 。 更直接地说，能够将内存映射到 KVM 来宾而不将所述内存映射到主机对于机密 VM（CoCo VM）（guest_memfd 的初始用例）至关重要。 虽然 AMD 的 SEV 和英特尔的 TDX 通过使用不受信任的主机无法使用的密钥加密来宾内存来防止不受信任的软件读取来宾私有数据，但受保护的 KVM (pKVM) 等项目*无需*依赖内存加密即可提供机密性和完整性 。 对于 SEV-SNP 和 TDX，访问来宾私有内存对于主机来说可能是致命的，即无论硬件行为如何，KVM 都必须阻止主机用户空间访问来宾内存。 支持 CoCo VM 的尝试#1 是添加一个 VMA 标志来将内存标记为只能由 KVM（或类似的启发式内核子系统）映射。 这种方法被放弃主要是因为它需要使用 PROT_NONE 来玩游戏以防止用户空间访问来宾内存。 尝试 #2 是篡夺 PG_hwpoison 以阻止主机将来宾私有内存映射到用户空间，但该方法无法满足基于软件的 CoCo VM 的几个要求，例如 pKVM，因为内核无法轻松强制执行 1:1 的页面：来宾关联，更不用说 1:1 的 pfn:gfn 映射了。 并且使用 PG_hwpoison 不适用于不受“struct page”支持的内存，例如 如果设备获得向访客公开加密内存区域的支持。 尝试 #3 是扩展 memfd() 系统调用并包装 shmem 以提供基于文件的专用来宾内存。 在 Hugh Dickins 和 Christian Brauner（以及其他人）的反馈导致其消亡之前，这种方法已经发展到了 v10。 Hugh 的反对意见是，搭载 shmem 对于 KVM 的用例来说没有任何意义，因为 KVM 实际上并不“想要”shmem 提供的功能。 IE。 KVM 使用 memfd() 和 shmem 来避免直接管理内存，并不是因为 memfd() 和 shmem 是最佳解决方案，例如 shmem 中的读/写/mmap 之类的东西很重。 Christian 指出了实现部分覆盖（仅包裹 shmem 的一些）的缺陷，例如 戳 inode_operations 或 super_operations 会显示 shmem 内容，但 address_space_operations 和 file_operations 会显示 KVM 的覆盖。 Christian 重重地解释了一下，建议 KVM 不要再偷懒了，创建一个合适的 API
    kvm_chardev_ops.owner = module
    r = misc_register(&kvm_dev)


static struct file_operations kvm_chardev_ops = {
	.unlocked_ioctl = kvm_dev_ioctl,
	.compat_ioctl   = kvm_dev_ioctl,
	.llseek		= noop_llseek,
};

static struct miscdevice kvm_dev = {
	KVM_MINOR,
	"kvm",
	&kvm_chardev_ops,
};



kvm debug guest:
KVM_CAP_SET_GUEST_DEBUG


tools/testing/selftests/kvm/x86_64/debug_regs.c
    int main(void)
    TEST_REQUIRE(kvm_has_cap(KVM_CAP_SET_GUEST_DEBUG)) -> unsigned int kvm_check_cap(long cap)
        open_kvm_dev_path_or_exit
        __kvm_ioctl(kvm_fd, KVM_CHECK_EXTENSION, (void *)cap) -> kvm_do_ioctl -> ioctl(fd, cmd, arg)
            ...
            kvm_dev_ioctl
                kvm_vm_ioctl_check_extension_generic
                    kvm_vm_ioctl_check_extension
                        case KVM_CAP_SET_GUEST_DEBUG -> KVM：X86：正确声明 KVM_CAP_SET_GUEST_DEBUG，x86 应该支持 KVM_CAP_SET_GUEST_DEBUG，但未声明为受支持。 我的大胆猜测是，像 QEMU 这样的用户空间正在使用“#ifdef KVM_CAP_SET_GUEST_DEBUG”来检查功能，但这可能是错误的，因为编译主机可能不是运行时主机。 用户空间可能仍然希望保留旧的“#ifdef”，但不要破坏旧内核上的来宾调试
                            r = 1
        return (unsigned int)ret
    vm_create_with_one_vcpu
    vcpu_guest_debug_set
    vcpu_run
    vcpu_skip_insn
    cmd = get_ucall(vcpu, &uc)
    kvm_vm_free



debug module:
cat /sys/module/{这个modulede的名字}/sections/.text
cat /sys/module/{这个modulede的名字}/sections/.data
cat /sys/module/{这个modulede的名字}/sections/.bss
获取三个地址，在gdb里输入
add-symbol-file {这个module}.ko <text_addr> -s .data <data_addr> -s .bss <bss_addr>


~/.gdbinit file added
add-auto-load-safe-path path/to/linux/kernel/tree/scripts/gdb/vmlinux-gdb.py
apropos lx
lx-symbol
                        
原文链接：https://blog.csdn.net/qq_26907291/article/details/129403236


info proc map


禁用 CONFIG_RANDOMIZE_BASE
开启 CONFIG_GDB_SCRIPTS
开启 CONFIG_DEBUG_INFO_REDUCED
开启 CONFIG_FRAME_POINTER



apt-get install libncurses5-dev libssl-dev bison flex libelf-dev gcc make openssl libc6-dev -y

make menuconfig

it's going to be under "Kernel hacking" -> "Compile-time checks and compiler options" -> "Compiler a kernel with debug info"


source code:
wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.4.18.tar.gz


set key to ""
CONFIG_SYSTEM_TRUSTED_KEYS
CONFIG_SYSTEM_REVOCATION_KEYS


apt install qemu qemu-system qemu-kvm -y

Failed to generate BTF for vmlinux
Try to disable CONFIG_DEBUG_INFO_BTF
make: *** [Makefile:1077: vmlinux] Error 1

apt install dwarves

touch main.c
#include <stdio>
int main()
{
    printf("hello world!");
    printf("hello world!");
    printf("hello world!");
    printf("hello world!");
    fflush(stdout);
    while(1);
    return 0;
}

gcc --static -o helloworld main.c
echo helloworld | cpio -o --format=newc > rootfs

qemu-system-x86_64 \
    -kernel ./arch/x86/boot/bzImage \
    -initrd ./rootfs \
    -append "root=/dev/ram rdinit=/helloworld"

qemu-system-x86_64  \
 -kernel ./arch/x86/boot/bzImage  \
 -initrd ./rootfs  \
 -append "root=/dev/ram rdinit=/helloworld" \
 -smp 2  \
 -s -S --nographic


qemu-system-x86_64 -s -S -kernel /root/project/linux-5.4.18/arch/x86/boot/bzImage -initrd /root/project/linux-5.4.18/rootfs -nographic -append "root=/dev/ram rdinit=/helloworld console=ttyS0" -serial mon:stdio -device e1000,netdev=net0 -netdev user,id=net0,hostfwd=tcp::2222-:22

gdb vmLinux
#以下进行调试
target remote:1234
lx-symbol
b start_kernel
c


{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "kernel-debug",
            "type": "cppdbg",
            "request": "launch",
            "miDebuggerServerAddress": "127.0.0.1:1234",
            "program": "${workspaceFolder}/vmlinux",
            "args": [],
            "stopAtEntry": false,
            "cwd": "${workspaceFolder}",
            "environment": [],
            "externalConsole": false,
            "logging": {
                "engineLogging": false
            },
            "MIMode": "gdb",
        }
    ]
}

qemu-system-x86_64 -s -S -kernel ~/linux-5.3.7/arch/x86/boot/bzImage -initrd ~/busybox-1.31.0/initramfs.cpio.gz -nographic -append "nokaslr console=ttyS0" -serial mon:stdio -device e1000,netdev=net0 -netdev user,id=net0,hostfwd=tcp::5555-:22



(gdb) target remote:1234
Remote debugging using :1234
0x000000000000fff0 in entry_stack_storage ()
(gdb) hb start_kernel
Breakpoint 1 at 0xffffffff8302c118: file init/main.c, line 577.
(gdb) c
Continuing.

Thread 1 hit Breakpoint 1, start_kernel () at init/main.c:577
577     {
(gdb) bt
#0  start_kernel () at init/main.c:577
#1  0xffffffff8302b583 in x86_64_start_reservations (real_mode_data=<optimized out>) at arch/x86/kernel/head64.c:490
#2  0xffffffff8302b675 in x86_64_start_kernel (real_mode_data=0x14970 <bts_ctx+10608> <error: Cannot access memory at address 0x14970>) at arch/x86/kernel/head64.c:471
#3  0xffffffff810000e6 in secondary_startup_64 () at arch/x86/kernel/head_64.S:241


yum install flex bison openssl libssl-dev libelf-dev -y


rdma link add rxe_ens3 type rxe netdev ens3

./drivers/infiniband/sw/rxe/rdma_rxe.ko
cat ./drivers/infiniband/sw/rxe/rdma_rxe.ko
root@vm:~/project/linux/linux-5.4.18# cat /sys/module/rdma_rxe/sections/.text
0xffffffffc0be3000
root@vm:~/project/linux/linux-5.4.18# cat /sys/module/rdma_rxe/sections/.data
0xffffffffc0bf90a0
root@vm:~/project/linux/linux-5.4.18# cat /sys/module/rdma_rxe/sections/.bss 
0xffffffffc0bffa40

add-symbol-file ./drivers/infiniband/sw/rxe/rdma_rxe.ko 0xffffffffc0be3000 -s .data 0xffffffffc0bf90a0 -s .bss 0xffffffffc0bffa40
add-symbol-file ./drivers/infiniband/core/ib_uverbs.ko 0xffffffffc0be3000 -s .data 0xffffffffc0bf90a0 -s .bss 0xffffffffc0bffa40
add-symbol-file ./drivers/infiniband/core/ib_core.ko 0xffffffffc0be3000 -s .data 0xffffffffc0bf90a0 -s .bss 0xffffffffc0bffa40

echo "add-symbol-file ./drivers/infiniband/sw/rxe/rdma_rxe.ko `cat /sys/module/rdma_rxe/sections/.text` -s .data `cat /sys/module/rdma_rxe/sections/.data` -s .bss `cat /sys/module/rdma_rxe/sections/.bss`"
echo "add-symbol-file ./drivers/infiniband/core/ib_core.ko `cat /sys/module/ib_core/sections/.text` -s .data `cat /sys/module/ib_core/sections/.data` -s .bss `cat /sys/module/ib_core/sections/.bss`"
echo "add-symbol-file ./drivers/infiniband/core/ib_uverbs.ko `cat /sys/module/ib_uverbs/sections/.text` -s .data `cat /sys/module/ib_uverbs/sections/.data` -s .bss `cat /sys/module/ib_uverbs/sections/.bss`"




ibv_reg_mr:
.reg_user_mr = rxe_reg_user_mr,

ibv_alloc_pd
execute_cmd_write(context, IB_USER_VERBS_CMD_ALLOC_PD
.alloc_pd = rxe_alloc_pd, -> kernel IB_USER_VERBS_CMD_ALLOC_PD -> ib_uverbs_alloc_pd
    uverbs_request
    rdma_zalloc_drv_obj
    ret = ib_dev->ops.alloc_pd(pd, &attrs->driver_udata) -> rxe_alloc_pd -> rxe_add_to_pool
        might_sleep_if
        elem->pool = pool
    uverbs_response
    uobj_alloc_commit


lsmod|grep rdma
rpcrdma               299008  0
sunrpc                581632  1 rpcrdma
rdma_ucm               28672  0
rdma_cm               118784  3 rpcrdma,ib_iser,rdma_ucm
iw_cm                  53248  1 rdma_cm
ib_cm                 131072  1 rdma_cm
rdma_rxe              135168  0
ib_uverbs             163840  2 rdma_rxe,rdma_ucm
ip6_udp_tunnel         16384  1 rdma_rxe
udp_tunnel             20480  1 rdma_rxe
ib_core               393216  8 rdma_cm,rdma_rxe,rpcrdma,iw_cm,ib_iser,rdma_ucm,ib_uverbs,ib_cm
root@vm:~/project/linux/linux-5.4.18/drivers/infiniband/core# modinfo ib_core 
filename:       /lib/modules/5.15.0-67-generic/kernel/drivers/infiniband/core/ib_core.ko



root@vm:~/project/rdma/rdma-core/build/bin# ./ibv_devices 
    device                 node GUID
    ------              ----------------
    rxe_ens3            505400fffe123456
root@vm:~/project/rdma/rdma-core/build/bin# ./ibv_rc_pingpong -d rxe_ens3 -g 0
  local address:  LID 0x0000, QPN 0x000011, PSN 0x1580f2, GID fe80::5054:ff:fe12:3456
root@vm:~/project/rdma/rdma-core/build/bin# gdb --args ./ibv_rc_pingpong -d rxe_ens3 -g 0



add-auto-load-safe-path /root/project/linux/linux-5.4.18/scripts/gdb/vmlinux-gdb.py


b cmdline_proc_show


readelf -h vmlinux


boot:
vim /boot/grub/grub.cfg

vim /etc/default/grub
GRUB_DEFAULT=1> 2
GRUB_TIMEOUT_STYLE=hidden
GRUB_TIMEOUT=0
GRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash"
GRUB_CMDLINE_LINUX="

update-grub
gnulinux-5.4.18+-advanced-64471880-f885-4431-a067-a2d6d996387c

dpkg --get-selections|grep linux-image

kvm:
kvm.ko, commit: https://github.com/ssbandjl/linux/commit/6aa8b732ca01c3d7a54e93f4d701b8aabbe60fb7,  
virt/kvm/vfio.c
    kvm_vfio_ops_init


kvm_main.c -> kvm_init KVM模块初始化

arch/x86/kvm


svm_init, amd
static int __init vmx_init(void) -> intel -> [PATCH] kvm：用户空间界面，网站：http://kvm.sourceforge.net 邮件列表：kvm-devel@lists.sourceforge.net (http://lists.sourceforge.net/lists/listinfo/kvm-devel ）以下补丁集将英特尔硬件虚拟化扩展的驱动程序添加到 x86 架构。 该驱动程序添加了一个向用户空间公开虚拟化功能的字符设备 (/dev/kvm)。 使用此驱动程序，进程可以在包含其自己的虚拟硬盘、网络适配器和显示器的完全虚拟化 PC 中运行虚拟机（“来宾”）。 使用此驱动程序，可以在一台主机上启动多个虚拟机。 每个虚拟机都是主机上的一个进程； 虚拟CPU是该进程中的一个线程。 Kill(1)、nice(1)、top(1) 按预期工作。 实际上，驱动程序在现有两种执行模式的基础上添加了第三种执行模式：现在有内核模式、用户模式和来宾模式。 访客模式有自己的地址空间映射访客物理内存（用户模式可通过 mmap()ing /dev/kvm 访问）。 访客模式无法访问任何I/O设备； 任何此类访问都会被拦截并定向到用户模式进行模拟。 该驱动程序支持 i386 和 x86_64 主机和来宾。 除了 i386 主机上的 x86_64 guest 以外，所有组合均被允许。 对于 i386 来宾和主机，支持 pae 和 non-pae 分页模式。 支持 SMP 主机和 UP 来宾。 目前仅支持 Intel 硬件，但正在开发 AMD 虚拟化支持。 由于 mmu 虚拟化的简单实现，每次上下文切换都会丢弃大部分影子页表条目，因此目前的性能并不理想。 我们计划通过两种方式解决这个问题： - 跨 tlb 刷新缓存影子页表 - 等到 AMD 和 Intel 发布具有嵌套页表的处理器 目前，虚拟桌面响应良好，但会消耗大量 CPU。 在Windows下我尝试玩弹球游戏和看一些Flash电影； 使用最新的 CPU 几乎感觉不到虚拟化。 Linux/X 较慢，可能是因为 X 位于单独的进程中。 除了驱动程序之外，您还需要稍微修改一下的 qemu 来提供 I/O 设备模拟和 BIOS。 注意事项（akpm：可能不再正确）： - 由于虚拟 APIC 问题，Windows 安装当前出现蓝屏。 我们正在努力修复。 临时解决方法是使用现有映像或通过 qemu 安装 - Windows 64 位不起作用。 qemu也是如此，所以很可能是设备型号的问题
    kvm_is_vmx_supported
        smp_processor_id
        if (!(cpuid_ecx(1) & feature_bit(VMX)))
    hv_init_evmcs
    kvm_x86_vendor_init
    vmx_setup_l1d_flush
    pi_init_cpu
    cpu_emergency_register_virt_callback
    vmx_check_vmcs12_offsets
    kvm_init THIS_MODULE ->  这是一个表示当前模块的标识符，用于标记KVM模块的所属

总结一下kvm模块初始化到底做了什么：  硬件检查： 初始化过程首先会进行硬件检查，确保主机的硬件支持虚拟化扩展，如 Intel VT-x 或 AMD-V。这些硬件扩展允许虚拟机在更加隔离的环境中运行，提高性能和安全性。 分配结构缓存： KVM 初始化过程分配了一些常用的数据结构的缓存，这些结构将用于管理虚拟机和虚拟 CPU 的状态。 创建设备节点： 通过创建 /dev/kvm 设备节点，用户空间程序可以通过这个设备与 KVM 内核模块进行通信，发起虚拟化请求和操作。 获取 VMCS 配置： 在初始化过程中，KVM 模块会获取 VMCS（Virtual Machine Control Structure）的配置信息，这些信息用于初始化 VMCS 结构。VMCS 是一个关键的数据结构，用于控制虚拟机运行的各个方面。 设置全局变量： 根据主机 CPU 的特性和支持，KVM 模块会设置一些全局变量，以适应不同的硬件环境。例如，根据 CPU 是否支持 EPT（Extended Page Tables）等特性，设置相应的全局标志。 为每个物理 CPU 分配 VMCS： KVM 初始化过程会为每个物理 CPU 分配一个 VMCS 结构，并将这些结构保存在 percpu 变量中。这为虚拟机在不同的物理 CPU 上切换和调度提供了支持。 进入 VMX 模式： 在初始化过程中，并没有将 CPU 设置为 VMX 模式。VMX 模式是虚拟机扩展的一种硬件虚拟化模式，需要通过设置 CR4 寄存器的 VMXE 位并分配 VMXON 区域来开启。然而，在创建第一个虚拟机之前，这些步骤不会执行。这是一种惰性策略，只有在实际需要创建虚拟机时才会启用 VMX 模式，以避免不必要的开销。 综上所述，KVM 模块的初始化过程主要包括硬件检查、资源分配、设备节点创建、全局变量设置、VMCS 配置等步骤。该过程确保了在虚拟化环境下能够准备好必要的数据结构和配置，以便在创建和管理虚拟机时进行有效的虚拟化操作。真正的 VMX 模式开启是在创建第一个虚拟机时才会执行，以避免不必要的性能损耗





核心驱动:
static struct pci_driver mlx5_core_driver = {
    .name           = KBUILD_MODNAME,
    .id_table       = mlx5_core_pci_table,
    .probe          = probe_one,
    .remove         = remove_one,
    .suspend        = mlx5_suspend,
    .resume         = mlx5_resume,
    .shutdown	= shutdown,
    .err_handler	= &mlx5_err_handler,
    .sriov_configure   = mlx5_core_sriov_configure, / 支持sriov功能需要实现的函数 /
        mlx5_sriov_enable(pdev, num_vfs)
            mlx5_device_enable_sriov
                mlx5_eswitch_enable
                mlx5_core_enable_hca
                mlx5_set_msix_vec_count
                vport_num = mlx5_core_ec_sriov_enabled(dev)
                mlx5_core_ec_vf_vport_base
                sriov_restore_guids
                    struct mlx5_hca_vport_context *in
                    in->node_guid = sriov->vfs_ctx[vf].node_guid
                    in->port_guid = sriov->vfs_ctx[vf].port_guid
                    mlx5_core_modify_hca_vport_context(dev, 1, 1, func_id, in)
            pci_enable_sriov
    .sriov_get_vf_total_msix = mlx5_sriov_get_vf_total_msix, //  PF 驱动程序回调以获取可用于分发到VF 的MSI-X 向量的总数
    .sriov_set_msix_vec_count = mlx5_core_sriov_set_msix_vec_count, // PF 驱动程序回调以更改 VF 上的 MSI-X 向量的数量
};

static struct auxiliary_driver mlx5_sf_driver = {
    .name = MLX5_SF_DEV_ID_NAME,
    .probe = mlx5_sf_dev_probe,
    .remove = mlx5_sf_dev_remove,
    .shutdown = mlx5_sf_dev_shutdown,
    .id_table = mlx5_sf_dev_id_table,
};



static int mlx5_sf_dev_probe(struct auxiliary_device *adev, const struct auxiliary_device_id *id)
    mlx5_devlink_alloc -> devlink：尽早设置设备 所有内核 devlink 实现都会在特定设备的初始化例程期间调用 devlink_alloc()，该设备稍后用作 devlink_register() 的父设备。 这种较晚的设备分配会导致需要我们在设置其他参数之前调用 device_register() 的情况，但该调用向世界打开了 devlink 并可供 netlink 用户访问。 由于访问 devlink->dev 指针，任何将 devlink_register() 移至最后一次调用的尝试都会生成以下错误。
    mlx5_dev_set_lightweight -> net/mlx5：轻探测本地 SF 如果用户想要配置 SF，例如：仅使用 vdpa 功能，则他需要完全探测 SF，配置他想要的内容，然后重新加载 SF。 为了节省重新加载的时间，本地SF将在没有任何辅助子设备的情况下进行探测，从而可以在其完全探测之前对SF进行配置。 这些 SF 的 enable_* devlink 参数的默认值设置为 false。
    mlx5_mdev_init -> net/mlx5：为 SF 创建新的配置文件 为 SF 创建新的配置文件以禁用命令缓存。 每个功能命令缓存消耗约 500KB 的内存，当使用大量 SF 时，这种节省在内存受限的系统上非常显着。 使用新的配置文件来提供 SF 和 PF 之间未来的差异。 mr_cache 不用于非 PF 函数，因此它被排除在新配置文件之外。
    ioremap
    mlx5_init_one_light
    or
    mlx5_init_one
    mlx5_core_peer_devlink_set
    devlink_register


net/mlx5: SF，添加辅助设备驱动程序
为mlx5子功能辅助设备添加辅助设备驱动程序。 mlx5 子功能类似于 PCI PF 和 VF。 对于子功能，创建辅助设备。 因此，当 mlx5 SF 辅助设备绑定到驱动程序时，会创建其 netdev 和 rdma 设备，它们显示为
$ ls -l /sys/bus/auxiliary/devices/
mlx5_core.sf.4 -> ../../../devices/pci0000:00/0000:00:03.0/0000:06:00.0/mlx5_core.sf.4
$ ls -l /sys/class/net/eth1/device
/sys/class/net/eth1/device -> ../../../mlx5_core.sf.4
$ cat /sys/bus/auxiliary/devices/mlx5_core.sf.4/sfnum
$ devlink dev show
pci/0000:06:00.0
auxiliary/mlx5_core.sf.4
$ devlink port show auxiliary/mlx5_core.sf.4/1
auxiliary/mlx5_core.sf.4/1: type eth netdev p0sf88 flavour virtual port 0 splittable false
$ rdma link show mlx5_0/1
link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev p0sf88
$ rdma dev show
8: rocep6s0f1: node_type ca fw 16.29.0550 node_guid 248a:0703:00b3:d113 sys_image_guid 248a:0703:00b3:d112
13: mlx5_0: node_type ca fw 16.29.0550 node_guid 0000:00ff:fe00:8888 sys_image_guid 248a:0703:00b3:d112
将来，devlink 设备实例名称将适应使用别名或 RFC [1] 中描述的 devlink 实例名称的 sfnum 注释。










default config profile:
static struct mlx5_profile profile[]




git remote add origin git@gitlab.nsv6.b122.top:bin/linux_kernel.git


register_chrdev

pci_enable_device

filter: ./drivers/net/ethernet/intel/


get info:
System with RoCE SR-IOV card with 4 VFs:
[leonro@vm ~]$ lspci |grep nox
01:00.0 Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6]
01:00.1 Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6 Virtual Function]
01:00.2 Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6 Virtual Function]
01:00.3 Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6 Virtual Function]
01:00.4 Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6 Virtual Function]
[leonro@vm ~]$ ls -l /sys/bus/auxiliary/devices/
mlx5_core.eth.0 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.0/mlx5_core.eth.0
mlx5_core.eth.1 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.1/mlx5_core.eth.1
mlx5_core.eth.2 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.2/mlx5_core.eth.2
mlx5_core.eth.3 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.3/mlx5_core.eth.3
mlx5_core.eth.4 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.4/mlx5_core.eth.4
mlx5_core.rdma.0 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.0/mlx5_core.rdma.0
mlx5_core.rdma.1 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.1/mlx5_core.rdma.1
mlx5_core.rdma.2 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.2/mlx5_core.rdma.2
mlx5_core.rdma.3 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.3/mlx5_core.rdma.3
mlx5_core.rdma.4 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.4/mlx5_core.rdma.4
mlx5_core.vdpa.1 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.1/mlx5_core.vdpa.1
mlx5_core.vdpa.2 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.2/mlx5_core.vdpa.2
mlx5_core.vdpa.3 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.3/mlx5_core.vdpa.3
mlx5_core.vdpa.4 -> ../../../devices/pci0000:00/0000:00:09.0/0000:01:00.4/mlx5_core.vdpa.4
[leonro@vm ~]$ rdma dev
0: rocep1s0f0: node_type ca fw 4.6.9999 node_guid 5254:00c0:fe12:3455 sys_image_guid 5254:00c0:fe12:3455
1: rocep1s0f0v0: node_type ca fw 4.6.9999 node_guid 0000:0000:0000:0000 sys_image_guid 5254:00c0:fe12:3456
2: rocep1s0f0v1: node_type ca fw 4.6.9999 node_guid 0000:0000:0000:0000 sys_image_guid 5254:00c0:fe12:3457
3: rocep1s0f0v2: node_type ca fw 4.6.9999 node_guid 0000:0000:0000:0000 sys_image_guid 5254:00c0:fe12:3458
4: rocep1s0f0v3: node_type ca fw 4.6.9999 node_guid 0000:0000:0000:0000 sys_image_guid 5254:00c0:fe12:3459



static const struct mlx5_adev_device {
    const char *suffix;
    bool (*is_supported)(struct mlx5_core_dev *dev);
    bool (*is_enabled)(struct mlx5_core_dev *dev);
} mlx5_adev_devices[] = {
    [MLX5_INTERFACE_PROTOCOL_VNET] = { .suffix = "vnet",
                       .is_supported = &mlx5_vnet_supported,
                       .is_enabled = &is_vnet_enabled },
    [MLX5_INTERFACE_PROTOCOL_IB] = { .suffix = "rdma",
                     .is_supported = &mlx5_rdma_supported,
                     .is_enabled = &is_ib_enabled },
    [MLX5_INTERFACE_PROTOCOL_ETH] = { .suffix = "eth",
                      .is_supported = &mlx5_eth_supported,
                      .is_enabled = &mlx5_core_is_eth_enabled },
    [MLX5_INTERFACE_PROTOCOL_ETH_REP] = { .suffix = "eth-rep",
                       .is_supported = &is_eth_rep_supported },
    [MLX5_INTERFACE_PROTOCOL_IB_REP] = { .suffix = "rdma-rep",
                       .is_supported = &is_ib_rep_supported },
    [MLX5_INTERFACE_PROTOCOL_MPIB] = { .suffix = "multiport",
                       .is_supported = &is_mp_supported },
    [MLX5_INTERFACE_PROTOCOL_DPLL] = { .suffix = "dpll",
                       .is_supported = &is_dpll_supported },
};


capability types:
static const int types[] = {
    MLX5_CAP_GENERAL,
    MLX5_CAP_GENERAL_2,
    MLX5_CAP_ETHERNET_OFFLOADS,
    MLX5_CAP_IPOIB_ENHANCED_OFFLOADS,
    MLX5_CAP_ODP,
    MLX5_CAP_ATOMIC,
    MLX5_CAP_ROCE,
    MLX5_CAP_IPOIB_OFFLOADS,
    MLX5_CAP_FLOW_TABLE,
    MLX5_CAP_ESWITCH_FLOW_TABLE,
    MLX5_CAP_ESWITCH,
    MLX5_CAP_QOS,
    MLX5_CAP_DEBUG,
    MLX5_CAP_DEV_MEM,
    MLX5_CAP_DEV_EVENT,
    MLX5_CAP_TLS,
    MLX5_CAP_VDPA_EMULATION,
    MLX5_CAP_IPSEC,
    MLX5_CAP_PORT_SELECTION,
    MLX5_CAP_MACSEC,
    MLX5_CAP_ADV_VIRTUALIZATION,
    MLX5_CAP_CRYPTO,
};



module_init(mlx5_dpll_init); -> mlx5：使用 DPLL 基础设施实现 SyncE 支持 使用新引入的 DPLL 支持实现 SyncE 支持。 确保使用适当功能探测的每个 PF/VF/SF 将生成 dpll 辅助设备并注册适当的 dpll 设备和引脚实例



该产品组合包括符合 ITU-T/IEEE 标准的数字锁相环 (DPLL)，用于网络同步，其性能满足 SyncE 和 IEEE 1588 严格的 10G/40G/100G 接口要求。


mlx5_tout_init


The defaults of the enable_* devlink params of these SFs are set to
false.

Usage example:
Create SF:
$ devlink port add pci/0000:08:00.0 flavour pcisf pfnum 0 sfnum 11
$ devlink port function set pci/0000:08:00.0/32768 \
hw_addr 00:00:00:00:00:11 state active

Enable ETH auxiliary device:
$ devlink dev param set auxiliary/mlx5_core.sf.1 \
name enable_eth value true cmode driverinit

Now, in order to fully probe the SF, use devlink reload:
$ devlink dev reload auxiliary/mlx5_core.sf.1
此时，用户拥有仅用于以太网功能的带有辅助设备的 SF devlink 实例


命令操作:
const char *mlx5_command_str(int command)
{
#define MLX5_COMMAND_STR_CASE(__cmd) case MLX5_CMD_OP_ ## __cmd: return #__cmd

    switch (command) {
    MLX5_COMMAND_STR_CASE(QUERY_HCA_CAP);
    MLX5_COMMAND_STR_CASE(QUERY_ADAPTER);
    MLX5_COMMAND_STR_CASE(INIT_HCA);
    MLX5_COMMAND_STR_CASE(TEARDOWN_HCA);
    MLX5_COMMAND_STR_CASE(ENABLE_HCA);
    MLX5_COMMAND_STR_CASE(DISABLE_HCA);
    MLX5_COMMAND_STR_CASE(QUERY_PAGES);
    MLX5_COMMAND_STR_CASE(MANAGE_PAGES);
    MLX5_COMMAND_STR_CASE(SET_HCA_CAP);
    MLX5_COMMAND_STR_CASE(QUERY_ISSI);
    MLX5_COMMAND_STR_CASE(SET_ISSI);
    MLX5_COMMAND_STR_CASE(SET_DRIVER_VERSION);
    MLX5_COMMAND_STR_CASE(CREATE_MKEY);
    MLX5_COMMAND_STR_CASE(QUERY_MKEY);
    MLX5_COMMAND_STR_CASE(DESTROY_MKEY);
    MLX5_COMMAND_STR_CASE(QUERY_SPECIAL_CONTEXTS);
    MLX5_COMMAND_STR_CASE(PAGE_FAULT_RESUME);
    MLX5_COMMAND_STR_CASE(CREATE_EQ);
    MLX5_COMMAND_STR_CASE(DESTROY_EQ);
    MLX5_COMMAND_STR_CASE(QUERY_EQ);
    MLX5_COMMAND_STR_CASE(GEN_EQE);
    MLX5_COMMAND_STR_CASE(CREATE_CQ);
    MLX5_COMMAND_STR_CASE(DESTROY_CQ);
    MLX5_COMMAND_STR_CASE(QUERY_CQ);
    MLX5_COMMAND_STR_CASE(MODIFY_CQ);
    MLX5_COMMAND_STR_CASE(CREATE_QP);
    MLX5_COMMAND_STR_CASE(DESTROY_QP);
    MLX5_COMMAND_STR_CASE(RST2INIT_QP);
    MLX5_COMMAND_STR_CASE(INIT2RTR_QP);
    MLX5_COMMAND_STR_CASE(RTR2RTS_QP);
    MLX5_COMMAND_STR_CASE(RTS2RTS_QP);
    MLX5_COMMAND_STR_CASE(SQERR2RTS_QP);
    MLX5_COMMAND_STR_CASE(2ERR_QP);
    MLX5_COMMAND_STR_CASE(2RST_QP);
    MLX5_COMMAND_STR_CASE(QUERY_QP);
    MLX5_COMMAND_STR_CASE(SQD_RTS_QP);
    MLX5_COMMAND_STR_CASE(INIT2INIT_QP);
    MLX5_COMMAND_STR_CASE(CREATE_PSV);
    MLX5_COMMAND_STR_CASE(DESTROY_PSV);
    MLX5_COMMAND_STR_CASE(CREATE_SRQ);
    MLX5_COMMAND_STR_CASE(DESTROY_SRQ);
    MLX5_COMMAND_STR_CASE(QUERY_SRQ);
    MLX5_COMMAND_STR_CASE(ARM_RQ);
    MLX5_COMMAND_STR_CASE(CREATE_XRC_SRQ);
    MLX5_COMMAND_STR_CASE(DESTROY_XRC_SRQ);
    MLX5_COMMAND_STR_CASE(QUERY_XRC_SRQ);
    MLX5_COMMAND_STR_CASE(ARM_XRC_SRQ);
    MLX5_COMMAND_STR_CASE(CREATE_DCT);
    MLX5_COMMAND_STR_CASE(DESTROY_DCT);
    MLX5_COMMAND_STR_CASE(DRAIN_DCT);
    MLX5_COMMAND_STR_CASE(QUERY_DCT);
    MLX5_COMMAND_STR_CASE(ARM_DCT_FOR_KEY_VIOLATION);
    MLX5_COMMAND_STR_CASE(QUERY_VPORT_STATE);
    MLX5_COMMAND_STR_CASE(MODIFY_VPORT_STATE);
    MLX5_COMMAND_STR_CASE(QUERY_ESW_VPORT_CONTEXT);
    MLX5_COMMAND_STR_CASE(MODIFY_ESW_VPORT_CONTEXT);
    MLX5_COMMAND_STR_CASE(QUERY_NIC_VPORT_CONTEXT);
    MLX5_COMMAND_STR_CASE(MODIFY_NIC_VPORT_CONTEXT);
    MLX5_COMMAND_STR_CASE(QUERY_ROCE_ADDRESS);
    MLX5_COMMAND_STR_CASE(SET_ROCE_ADDRESS);
    MLX5_COMMAND_STR_CASE(QUERY_HCA_VPORT_CONTEXT);
    MLX5_COMMAND_STR_CASE(MODIFY_HCA_VPORT_CONTEXT);
    MLX5_COMMAND_STR_CASE(QUERY_HCA_VPORT_GID);
    MLX5_COMMAND_STR_CASE(QUERY_HCA_VPORT_PKEY);
    MLX5_COMMAND_STR_CASE(QUERY_VNIC_ENV);
    MLX5_COMMAND_STR_CASE(QUERY_VPORT_COUNTER);
    MLX5_COMMAND_STR_CASE(ALLOC_Q_COUNTER);
    MLX5_COMMAND_STR_CASE(DEALLOC_Q_COUNTER);
    MLX5_COMMAND_STR_CASE(QUERY_Q_COUNTER);
    MLX5_COMMAND_STR_CASE(SET_MONITOR_COUNTER);
    MLX5_COMMAND_STR_CASE(ARM_MONITOR_COUNTER);
    MLX5_COMMAND_STR_CASE(SET_PP_RATE_LIMIT);
    MLX5_COMMAND_STR_CASE(QUERY_RATE_LIMIT);
    MLX5_COMMAND_STR_CASE(CREATE_SCHEDULING_ELEMENT);
    MLX5_COMMAND_STR_CASE(DESTROY_SCHEDULING_ELEMENT);
    MLX5_COMMAND_STR_CASE(QUERY_SCHEDULING_ELEMENT);
    MLX5_COMMAND_STR_CASE(MODIFY_SCHEDULING_ELEMENT);
    MLX5_COMMAND_STR_CASE(CREATE_QOS_PARA_VPORT);
    MLX5_COMMAND_STR_CASE(DESTROY_QOS_PARA_VPORT);
    MLX5_COMMAND_STR_CASE(ALLOC_PD);
    MLX5_COMMAND_STR_CASE(DEALLOC_PD);
    MLX5_COMMAND_STR_CASE(ALLOC_UAR);
    MLX5_COMMAND_STR_CASE(DEALLOC_UAR);
    MLX5_COMMAND_STR_CASE(CONFIG_INT_MODERATION);
    MLX5_COMMAND_STR_CASE(ACCESS_REG);
    MLX5_COMMAND_STR_CASE(ATTACH_TO_MCG);
    MLX5_COMMAND_STR_CASE(DETACH_FROM_MCG);
    MLX5_COMMAND_STR_CASE(GET_DROPPED_PACKET_LOG);
    MLX5_COMMAND_STR_CASE(MAD_IFC);
    MLX5_COMMAND_STR_CASE(QUERY_MAD_DEMUX);
    MLX5_COMMAND_STR_CASE(SET_MAD_DEMUX);
    MLX5_COMMAND_STR_CASE(NOP);
    MLX5_COMMAND_STR_CASE(ALLOC_XRCD);
    MLX5_COMMAND_STR_CASE(DEALLOC_XRCD);
    MLX5_COMMAND_STR_CASE(ALLOC_TRANSPORT_DOMAIN);
    MLX5_COMMAND_STR_CASE(DEALLOC_TRANSPORT_DOMAIN);
    MLX5_COMMAND_STR_CASE(QUERY_CONG_STATUS);
    MLX5_COMMAND_STR_CASE(MODIFY_CONG_STATUS);
    MLX5_COMMAND_STR_CASE(QUERY_CONG_PARAMS);
    MLX5_COMMAND_STR_CASE(MODIFY_CONG_PARAMS);
    MLX5_COMMAND_STR_CASE(QUERY_CONG_STATISTICS);
    MLX5_COMMAND_STR_CASE(ADD_VXLAN_UDP_DPORT);
    MLX5_COMMAND_STR_CASE(DELETE_VXLAN_UDP_DPORT);
    MLX5_COMMAND_STR_CASE(SET_L2_TABLE_ENTRY);
    MLX5_COMMAND_STR_CASE(QUERY_L2_TABLE_ENTRY);
    MLX5_COMMAND_STR_CASE(DELETE_L2_TABLE_ENTRY);
    MLX5_COMMAND_STR_CASE(SET_WOL_ROL);
    MLX5_COMMAND_STR_CASE(QUERY_WOL_ROL);
    MLX5_COMMAND_STR_CASE(CREATE_LAG);
    MLX5_COMMAND_STR_CASE(MODIFY_LAG);
    MLX5_COMMAND_STR_CASE(QUERY_LAG);
    MLX5_COMMAND_STR_CASE(DESTROY_LAG);
    MLX5_COMMAND_STR_CASE(CREATE_VPORT_LAG);
    MLX5_COMMAND_STR_CASE(DESTROY_VPORT_LAG);
    MLX5_COMMAND_STR_CASE(CREATE_TIR);
    MLX5_COMMAND_STR_CASE(MODIFY_TIR);
    MLX5_COMMAND_STR_CASE(DESTROY_TIR);
    MLX5_COMMAND_STR_CASE(QUERY_TIR);
    MLX5_COMMAND_STR_CASE(CREATE_SQ);
    MLX5_COMMAND_STR_CASE(MODIFY_SQ);
    MLX5_COMMAND_STR_CASE(DESTROY_SQ);
    MLX5_COMMAND_STR_CASE(QUERY_SQ);
    MLX5_COMMAND_STR_CASE(CREATE_RQ);
    MLX5_COMMAND_STR_CASE(MODIFY_RQ);
    MLX5_COMMAND_STR_CASE(DESTROY_RQ);
    MLX5_COMMAND_STR_CASE(QUERY_RQ);
    MLX5_COMMAND_STR_CASE(CREATE_RMP);
    MLX5_COMMAND_STR_CASE(MODIFY_RMP);
    MLX5_COMMAND_STR_CASE(DESTROY_RMP);
    MLX5_COMMAND_STR_CASE(QUERY_RMP);
    MLX5_COMMAND_STR_CASE(CREATE_TIS);
    MLX5_COMMAND_STR_CASE(MODIFY_TIS);
    MLX5_COMMAND_STR_CASE(DESTROY_TIS);
    MLX5_COMMAND_STR_CASE(QUERY_TIS);
    MLX5_COMMAND_STR_CASE(CREATE_RQT);
    MLX5_COMMAND_STR_CASE(MODIFY_RQT);
    MLX5_COMMAND_STR_CASE(DESTROY_RQT);
    MLX5_COMMAND_STR_CASE(QUERY_RQT);
    MLX5_COMMAND_STR_CASE(SET_FLOW_TABLE_ROOT);
    MLX5_COMMAND_STR_CASE(CREATE_FLOW_TABLE);
    MLX5_COMMAND_STR_CASE(DESTROY_FLOW_TABLE);
    MLX5_COMMAND_STR_CASE(QUERY_FLOW_TABLE);
    MLX5_COMMAND_STR_CASE(CREATE_FLOW_GROUP);
    MLX5_COMMAND_STR_CASE(DESTROY_FLOW_GROUP);
    MLX5_COMMAND_STR_CASE(QUERY_FLOW_GROUP);
    MLX5_COMMAND_STR_CASE(SET_FLOW_TABLE_ENTRY);
    MLX5_COMMAND_STR_CASE(QUERY_FLOW_TABLE_ENTRY);
    MLX5_COMMAND_STR_CASE(DELETE_FLOW_TABLE_ENTRY);
    MLX5_COMMAND_STR_CASE(ALLOC_FLOW_COUNTER);
    MLX5_COMMAND_STR_CASE(DEALLOC_FLOW_COUNTER);
    MLX5_COMMAND_STR_CASE(QUERY_FLOW_COUNTER);
    MLX5_COMMAND_STR_CASE(MODIFY_FLOW_TABLE);
    MLX5_COMMAND_STR_CASE(ALLOC_PACKET_REFORMAT_CONTEXT);
    MLX5_COMMAND_STR_CASE(DEALLOC_PACKET_REFORMAT_CONTEXT);
    MLX5_COMMAND_STR_CASE(ALLOC_MODIFY_HEADER_CONTEXT);
    MLX5_COMMAND_STR_CASE(DEALLOC_MODIFY_HEADER_CONTEXT);
    MLX5_COMMAND_STR_CASE(FPGA_CREATE_QP);
    MLX5_COMMAND_STR_CASE(FPGA_MODIFY_QP);
    MLX5_COMMAND_STR_CASE(FPGA_QUERY_QP);
    MLX5_COMMAND_STR_CASE(FPGA_QUERY_QP_COUNTERS);
    MLX5_COMMAND_STR_CASE(FPGA_DESTROY_QP);
    MLX5_COMMAND_STR_CASE(CREATE_XRQ);
    MLX5_COMMAND_STR_CASE(DESTROY_XRQ);
    MLX5_COMMAND_STR_CASE(QUERY_XRQ);
    MLX5_COMMAND_STR_CASE(ARM_XRQ);
    MLX5_COMMAND_STR_CASE(CREATE_GENERAL_OBJECT);
    MLX5_COMMAND_STR_CASE(DESTROY_GENERAL_OBJECT);
    MLX5_COMMAND_STR_CASE(MODIFY_GENERAL_OBJECT);
    MLX5_COMMAND_STR_CASE(QUERY_GENERAL_OBJECT);
    MLX5_COMMAND_STR_CASE(QUERY_MODIFY_HEADER_CONTEXT);
    MLX5_COMMAND_STR_CASE(ALLOC_MEMIC);
    MLX5_COMMAND_STR_CASE(DEALLOC_MEMIC);
    MLX5_COMMAND_STR_CASE(QUERY_ESW_FUNCTIONS);
    MLX5_COMMAND_STR_CASE(CREATE_UCTX);
    MLX5_COMMAND_STR_CASE(DESTROY_UCTX);
    MLX5_COMMAND_STR_CASE(CREATE_UMEM);
    MLX5_COMMAND_STR_CASE(DESTROY_UMEM);
    MLX5_COMMAND_STR_CASE(RELEASE_XRQ_ERROR);
    MLX5_COMMAND_STR_CASE(MODIFY_XRQ);
    MLX5_COMMAND_STR_CASE(QUERY_VHCA_STATE);
    MLX5_COMMAND_STR_CASE(MODIFY_VHCA_STATE);
    MLX5_COMMAND_STR_CASE(ALLOC_SF);
    MLX5_COMMAND_STR_CASE(DEALLOC_SF);
    MLX5_COMMAND_STR_CASE(SUSPEND_VHCA);
    MLX5_COMMAND_STR_CASE(RESUME_VHCA);
    MLX5_COMMAND_STR_CASE(QUERY_VHCA_MIGRATION_STATE);
    MLX5_COMMAND_STR_CASE(SAVE_VHCA_STATE);
    MLX5_COMMAND_STR_CASE(LOAD_VHCA_STATE);
    MLX5_COMMAND_STR_CASE(SYNC_CRYPTO);
    MLX5_COMMAND_STR_CASE(ALLOW_OTHER_VHCA_ACCESS);
    default: return "unknown command opcode";
    }
}



vlan header:
#define IANA_VXLAN_UDP_PORT     4789
#define IANA_VXLAN_GPE_UDP_PORT 4790

/* VXLAN protocol (RFC 7348) header:
 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
 * |R|R|R|R|I|R|R|R|               Reserved                        |
 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
 * |                VXLAN Network Identifier (VNI) |   Reserved    |
 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
 *
 * I = VXLAN Network Identifier (VNI) present.
 */



.unlocked_ioctl = ib_uverbs_ioctl
...
user space
ioctl(context->cmd_fd, RDMA_VERBS_IOCTL, &cmd->hdr)
...
ib_uverbs_ioctl
    unlikely(cmd != RDMA_VERBS_IOCTL)
    copy_from_user
    srcu_read_lock
    ib_uverbs_cmd_verbs -> IB/uverbs：使用 uverbs_api 解组 ioctl 命令 转换 ioctl 方法系统调用路径以使用 uverbs_api 数据结构。 新的 uapi 结构包含所有相同的信息，只是以不同且更优化的方式。 - 对于与属性相关的所有内容，使用 attr_bkey 而不是 2 级基数树。 这包括属性存储、存在以及缺失强制属性的检测。 - 避免在完成时迭代所有属性存储，而是使用 find_first_bit 和 attr_bkey 来仅定位那些需要清理的属性。 - 组织事物始终运行并始终依赖清理。 这避免了一堆棘手的错误展开情况。 - 使用基数树定位方法，并使用非常高效的增量基数树查找来定位属性 - 使用预先计算的 destroy_bkey 来处理 uobject 销毁 - 使用预先计算的分配大小和预先计算的“need_stack”以避免快速路径中的数学运算。 如果用户空间不传递（许多）不支持的属性，这是最佳的。 总的来说，这为属性访问器带来了更好的代码生成，所有内容现在都存储在由 attr_bkey 索引的位图或线性数组中。 编译器可以在编译时计算所有方法属性的 attr_bkey 值，这意味着像 uverbs_attr_is_valid() 这样的东西现在编译成单指令位测试
        radix_tree_iter_lookup <- uapi_add_get_elm
            uapi_key_ioctl_method(hdr->method_id)
        method_elm = rcu_dereference_protected(*slot, true)
        ib_uverbs_run_method
            handler = srcu_dereference
            ret = handler(&pbundle->bundle)
            ...
            ib_uverbs_alloc_pd -> IB_USER_VERBS_CMD_ALLOC_PD
                uverbs_request
                    copy_from_user
                uobj_alloc(UVERBS_OBJECT_PD, attrs, &ib_dev)
                    ...
                    rdma_alloc_begin_uobject
                        obj->type_class->alloc_begin
                rdma_zalloc_drv_obj
                rdma_restrack_new
                rdma_restrack_set_name
                ib_dev->ops.alloc_pd(pd, &attrs->driver_udata) -> .alloc_pd =  -> mlx5_ib_alloc_pd
                    rdma_udata_to_drv_context
                    mlx5_cmd_exec_inout(to_mdev(ibdev)->mdev, alloc_pd, in, out) -> 命令框架
                        ...
                    ib_copy_to_udata
                rdma_restrack_add
                    xa_insert
                    or xa_alloc_cyclic
                uobj_finalize_uobj_create
                uverbs_response
        bundle_destroy
    srcu_read_unlock


ioctl, cmd, 
uverbs_core_api
uverbs_def_write_intf
static const struct uapi_definition uverbs_core_api[] = {
	UAPI_DEF_CHAIN(uverbs_def_obj_async_fd),
	UAPI_DEF_CHAIN(uverbs_def_obj_counters),
	UAPI_DEF_CHAIN(uverbs_def_obj_cq),
	UAPI_DEF_CHAIN(uverbs_def_obj_device),
	UAPI_DEF_CHAIN(uverbs_def_obj_dm),
	UAPI_DEF_CHAIN(uverbs_def_obj_flow_action),
	UAPI_DEF_CHAIN(uverbs_def_obj_intf),
	UAPI_DEF_CHAIN(uverbs_def_obj_mr),
	UAPI_DEF_CHAIN(uverbs_def_obj_qp),
	UAPI_DEF_CHAIN(uverbs_def_obj_srq),
	UAPI_DEF_CHAIN(uverbs_def_obj_wq),
	UAPI_DEF_CHAIN(uverbs_def_write_intf),
	{},
};





ib_uverbs_add_one  -> RDMA：允许 ib_client 在调用 add() 时失败，添加客户端时不允许失败，但所有客户端在其添加例程中都有各种失败路径。 这会产生一种非常边缘的情况：添加客户端后，在添加过程中失败并且未设置 client_data。 然后，核心代码仍然会使用 NULL client_data 调用其他以 client_data 为中心的操作，例如 remove()、rename()、get_nl_info() 和 get_net_dev_by_params() - 这是令人困惑和意外的。 如果 add() 回调失败，则不要再为设备调用任何客户端操作，甚至删除。 删除操作回调中现在对 NULL client_data 的所有冗余检查。 更新所有 add() 回调以正确返回错误代码。 EOPNOTSUPP 用于 ULP 不支持 ib_device 的情况 - 例如，因为它仅适用于 IB
参考: https://www.cnblogs.com/vlhn/p/8301427.html
    ib_uverbs_create_uapi
        uverbs_alloc_api
            uapi_merge_def(uapi, ibdev, uverbs_core_api, false)
                uapi_merge_obj_tree
                    uapi_merge_method
    dev_set_name
    cdev_init
    cdev_device_add
    ib_set_client_data



Mellanox ConnectX-6-dx智能网卡 openvswitch 流表卸载源码分析: https://blog.csdn.net/qq_20679687/article/details/131632198
// -------- ovs -----------------
// --- ovs vswitchd 主线程 -----------------
// 首包触发卸载，后续包走datapath转发或硬件转发
dp_netdev_flow_add
    dpcls_insert(cls, &flow->cr, &mask);  // datapath缓存流转发规则
    // 入队 卸载任务队列
    queue_netdev_flow_put(pmd, flow, match, actions, actions_len, DP_NETDEV_FLOW_OFFLOAD_OP_ADD);

// --- ovs offload_main 卸载线程 -----
dp_netdev_flow_offload_main    // 通过队列+poll的方式，该线程 异步处理 来自ovs主线程的卸载任务队列。
    dp_offload_flow(offload);
        // netdev offload
        dp_netdev_flow_offload_put  // 单独的一个线程 dp_netdev_flow_offload_main
        netdev_flow_put->flow_ flow_api->flow_put
            netdev_offload_dpdk_flow_put
                netdev_offload_dpdk_add_flow
                    netdev_offload_dpdk_action
                        netdev_offload_dpdk_flow_create
                            netdev_dpdk_rte_flow_create
                                // ---- dpdk rte_flow ----------------------
                                rte_flow_create()
                                 ==>mlx5_flow_create    // dpdk: drivers/net/mlx5/mlx5_flow.c
                                        mlx5_flow_list_create
                                            flow_idx = flow_list_create(dev, type, attr, items, original_actions, 
                                                                        external, wks, error);
                                                flow_drv_translate(dev, dev_flow,&attr_tx, items_tx.items,
                                                                   actions_hairpin_tx.actions, error);
                                                    fops->translate(dev, dev_flow, attr, items, actions, error);
                                                     ==>flow_dv_translate(dev, dev_flow, attr, items, actions, error)  # 详见下方
                                                flow_drv_validate()
                                                flow_drv_apply(dev, flow, error);
                                                    fops->apply(dev, flow, error);
                                                     ==>flow_dv_apply     # 详见下方

// 分析 flow_dv_translate 
    for (; !actions_end ; actions++) {
        // parse action
    }
    dev_flow->act_flags = action_flags;
    flow_dv_matcher_register(dev, &matcher, &tbl_key, dev_flow, tunnel, attr->group, error)

// 分析 flow_dv_apply
flow_dv_apply
     mlx5_flow_os_create_flow(dv_h->matcher->matcher_object,(void *)&dv->value, n,dv->actions, &dh->drv_flow);
        *flow = mlx5_glue->dv_create_flow(matcher, match_value,num_actions, actions);
            mlx5_glue_dv_create_flow
                mlx5dv_dr_rule_create(matcher, match_value, num_actions,(struct mlx5dv_dr_action **)actions);
                    ==>用户态驱动 详见下小节
                    mlx5dv_dr_rule_create(matcher, match_value, num_actions,(struct mlx5dv_dr_action **)actions);
                    ==>内核态驱动 详见下下小节
                    __mlx5_glue_dv_create_flow(matcher, match_value,num_actions, actions_attr);



// --- rdma-core --------
mlx5dv_dr_rule_create(matcher, match_value, num_actions,(struct mlx5dv_dr_action **)actions);
    dr_rule_create_rule(matcher, value, num_actions, actions);
        dr_rule_create_rule_fdb(rule, &param,num_actions, actions);
            dr_rule_create_rule_nic(rule, &rule->tx, &copy_param,num_actions, actions);
                dr_rule_send_update_list(&send_ste_list, dmn, true, nic_rule->lock_index);
                    dr_rule_handle_one_ste_in_update_list(ste_info,dmn,send_ring_idx);
                        dr_send_postsend_ste(dmn, ste_info->ste, ste_info->data,ste_info->size,
                                             ste_info->offset,send_ring_idx);
                            dr_postsend_icm_data(dmn, &send_info, ring_idx);	
                                dr_post_send(send_ring->qp, send_info);   // 走rdma通道 
                                    /* Write. false, because we delay the post_send_db till the coming READ */
                                    dr_rdma_segments(dr_qp, send_info->remote_addr, send_info->rkey,
                                             &send_info->write, MLX5_OPCODE_RDMA_WRITE, false);
                                    /* Read. true, because we send WRITE + READ together */
                                    dr_rdma_segments(dr_qp, send_info->remote_addr, send_info->rkey,
                                             &send_info->read, MLX5_OPCODE_RDMA_READ, true);

// --- dpdk ---------------
mlx5_glue_dv_create_flow()
    __mlx5_glue_dv_create_flow(matcher, match_value,num_actions, actions_attr);
// --- rdma mlx5dv_create_flow in rdma-core providers/mlx5/verbs.c ---
    dvops->create_flow(flow_matcher,match_value,num_actions,actions_attr,NULL);
        _mlx5dv_create_flow
            fill_attr_in_uint32(cmd,MLX5_IB_ATTR_CREATE_FLOW_FLAGS,MLX5_IB_ATTR_CREATE_FLOW_FLAGS_DEFAULT_MISS);
            execute_ioctl(flow_matcher->context, cmd); // call kernel ioctl
// --- kernel space -- mlx5_ib.ko mlx_core.ko -------------
// ib_core dispatch message to mlx5_ib
UVERBS_HANDLER(MLX5_IB_METHOD_CREATE_FLOW)   // drivers/infiniband/hw/mlx5/fs.c
    get_dests
        uverbs_get_flags32
            uverbs_get_flags64(&flags, attrs_bundle, idx, allowed_bits);
            raw_fs_rule_add(dev, fs_matcher, &flow_context, &flow_act,counter_id, cmd_in, inlen, dest_id, dest_type);
                _create_raw_flow_rule
                    mlx5_add_flow_rules(ft, spec,flow_act, dst, dst_num);  // drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
                        rule = add_rule_fg(g, spec, flow_act, dest, dest_num, fte);
                            handle = add_rule_fte(fte, fg, dest, dest_num,old_action != flow_act->action);
                                root->cmds->update_fte(root, ft, fg, modify_mask, fte)
                                    mlx5_cmd_update_fte
                                        mlx5_cmd_set_fte(dev, opmod, modify_mask, ft, fg->id, fte);
                                            mlx5_cmd_exec(dev, in, inlen, out, sizeof(out));
                                                mlx5_cmd_do(dev, in, in_size, out, out_size);
                                                    cmd_exec(dev, in, in_size, out, out_size, NULL, NULL, false);
                                                        cmd_work_handler
                                                            down(sem)
                                                            lay = get_inst(cmd, ent->idx);
                                                                return cmd->cmd_buf + (idx << cmd->log_stride);
                                                            memset(lay, 0, sizeof(*lay));
                                                            memcpy(lay->in, ent->in->first.data, sizeof(lay->in));
                                                            lay->type = MLX5_PCI_CMD_XPORT
                                                            lay->status_own = CMD_OWNER_HW; // transfer ownership to hardware
                                                            wmb()
                                                            iowrite32be(1 << ent->idx, &dev->iseg->cmd_dbell);
                                                            // polling reg for completion
                                                            mlx5_cmd_comp_handler(dev, 1ULL << ent->idx, ent->ret == -ETIMEDOUT ?
                                                                      MLX5_CMD_COMP_TYPE_FORCED : MLX5_CMD_COMP_TYPE_POLLING);
                                                                callback or
                                                                complete

// ovs kernel datapath 路径
// -- user space ---
dpif_netlink_operate
 -->try_send_to_netdev   // 先卸载。这里会卡住。
    dpif_netlink_operate_chunks // 再转发
        dpif_netlink_operate__
            dpif_netlink_init_flow_put  // netlink
// -- kernel space ---
    ovs_flow_cmd_new()
        ovs_flow_tbl_lookup(&dp->table, &new_flow->key);
        ovs_flow_tbl_insert(&dp->table, new_flow, &mask);

// ovs卸载路径
// --- user space ---
dpif_netlink_operate
    try_send_to_netdev
        parse_flow_put(dpif, put);
            netdev_flow_put(dev, &match,...)
            flow_api->flow_put(netdev,...）
                netdev_tc_flow_put()
                    tc_replace_flower(&id, &flower);   // netlink 通信。组装tc-flow指令给tc内核模块
// --- kernel space ------
tc_ctl_tfilter // tc module。不属于ovs代码
    // 代表口卸载
    mlx5e_rep_indr_setup_tc_cb  // mlx5_core.ko
        mlx5e_rep_indr_offload(priv->netdev, type_data, priv,flags);
            mlx5e_configure_flower(netdev, priv, flower, flags)
                mlx5e_tc_add_flow(priv, f, flags, dev, &flow);
                    mlx5e_add_fdb_flow(priv, f, flow_flags,filter_dev, flow);
                        mlx5e_tc_offload_fdb_rules
                            mlx5_eswitch_add_offloaded_rule
                             ==>mlx5_add_flow_rules   //与 rte_flow/ib一样的底层调用接口
                               
// data structure
const struct net_device_ops mlx5e_netdev_ops = {
    .ndo_setup_tc            = mlx5e_setup_tc,
}

udpif_revalidator()
    revalidate(revalidator);
        dpif_flow_dump_next
            dpif->dpif_class->flow_dump_next(thread, flows, max_flows);
                dpif_netdev_flow_dump_next
                    dp_netdev_flow_to_dpif_flow
                        get_dpif_flow_status()
                            dpif_netdev_get_flow_offload_status()
                                netdev_offload_dpdk_flow_get
                                    netdev_dpdk_rte_flow_query_count
                                     ==>rte_flow_query()
// --- rte_flow_query in dpdk -----------
rte_flow_query
    flow_dv_query
        flow_dv_query_count
            _flow_dv_query_count(dev, cnt_idx, &pkts, &bytes);
                cnt = flow_dv_counter_get_by_idx(dev, counter, NULL);
                    MLX5_POOL_GET_CNT(pool, idx % MLX5_COUNTERS_PER_POOL);  // 直接读硬件统计寄存器


mellanox 命令操作码, 寄存器:
enum {
    MLX5_CMD_OP_QUERY_HCA_CAP                 = 0x100,
    MLX5_CMD_OP_QUERY_ADAPTER                 = 0x101,
    MLX5_CMD_OP_INIT_HCA                      = 0x102,
    MLX5_CMD_OP_TEARDOWN_HCA                  = 0x103,
    MLX5_CMD_OP_ENABLE_HCA                    = 0x104,
    MLX5_CMD_OP_DISABLE_HCA                   = 0x105,
    MLX5_CMD_OP_QUERY_PAGES                   = 0x107,
    MLX5_CMD_OP_MANAGE_PAGES                  = 0x108,
    MLX5_CMD_OP_SET_HCA_CAP                   = 0x109,
    MLX5_CMD_OP_QUERY_ISSI                    = 0x10a,
    MLX5_CMD_OP_SET_ISSI                      = 0x10b,
    MLX5_CMD_OP_SET_DRIVER_VERSION            = 0x10d,
    MLX5_CMD_OP_QUERY_SF_PARTITION            = 0x111,
    MLX5_CMD_OP_ALLOC_SF                      = 0x113,
    MLX5_CMD_OP_DEALLOC_SF                    = 0x114,
    MLX5_CMD_OP_SUSPEND_VHCA                  = 0x115,
    MLX5_CMD_OP_RESUME_VHCA                   = 0x116,
    MLX5_CMD_OP_QUERY_VHCA_MIGRATION_STATE    = 0x117,
    MLX5_CMD_OP_SAVE_VHCA_STATE               = 0x118,
    MLX5_CMD_OP_LOAD_VHCA_STATE               = 0x119,
    MLX5_CMD_OP_CREATE_MKEY                   = 0x200,
    MLX5_CMD_OP_QUERY_MKEY                    = 0x201,
    MLX5_CMD_OP_DESTROY_MKEY                  = 0x202,
    MLX5_CMD_OP_QUERY_SPECIAL_CONTEXTS        = 0x203,
    MLX5_CMD_OP_PAGE_FAULT_RESUME             = 0x204,
    MLX5_CMD_OP_ALLOC_MEMIC                   = 0x205,
    MLX5_CMD_OP_DEALLOC_MEMIC                 = 0x206,
    MLX5_CMD_OP_MODIFY_MEMIC                  = 0x207,
    MLX5_CMD_OP_CREATE_EQ                     = 0x301,
    MLX5_CMD_OP_DESTROY_EQ                    = 0x302,
    MLX5_CMD_OP_QUERY_EQ                      = 0x303,
    MLX5_CMD_OP_GEN_EQE                       = 0x304,
    MLX5_CMD_OP_CREATE_CQ                     = 0x400,
    MLX5_CMD_OP_DESTROY_CQ                    = 0x401,
    MLX5_CMD_OP_QUERY_CQ                      = 0x402,
    MLX5_CMD_OP_MODIFY_CQ                     = 0x403,
    MLX5_CMD_OP_CREATE_QP                     = 0x500,
    MLX5_CMD_OP_DESTROY_QP                    = 0x501,
    MLX5_CMD_OP_RST2INIT_QP                   = 0x502,
    MLX5_CMD_OP_INIT2RTR_QP                   = 0x503,
    MLX5_CMD_OP_RTR2RTS_QP                    = 0x504,
    MLX5_CMD_OP_RTS2RTS_QP                    = 0x505,
    MLX5_CMD_OP_SQERR2RTS_QP                  = 0x506,
    MLX5_CMD_OP_2ERR_QP                       = 0x507,
    MLX5_CMD_OP_2RST_QP                       = 0x50a,
    MLX5_CMD_OP_QUERY_QP                      = 0x50b,
    MLX5_CMD_OP_SQD_RTS_QP                    = 0x50c,
    MLX5_CMD_OP_INIT2INIT_QP                  = 0x50e,
    MLX5_CMD_OP_CREATE_PSV                    = 0x600,
    MLX5_CMD_OP_DESTROY_PSV                   = 0x601,
    MLX5_CMD_OP_CREATE_SRQ                    = 0x700,
    MLX5_CMD_OP_DESTROY_SRQ                   = 0x701,
    MLX5_CMD_OP_QUERY_SRQ                     = 0x702,
    MLX5_CMD_OP_ARM_RQ                        = 0x703,
    MLX5_CMD_OP_CREATE_XRC_SRQ                = 0x705,
    MLX5_CMD_OP_DESTROY_XRC_SRQ               = 0x706,
    MLX5_CMD_OP_QUERY_XRC_SRQ                 = 0x707,
    MLX5_CMD_OP_ARM_XRC_SRQ                   = 0x708,
    MLX5_CMD_OP_CREATE_DCT                    = 0x710,
    MLX5_CMD_OP_DESTROY_DCT                   = 0x711,
    MLX5_CMD_OP_DRAIN_DCT                     = 0x712,
    MLX5_CMD_OP_QUERY_DCT                     = 0x713,
    MLX5_CMD_OP_ARM_DCT_FOR_KEY_VIOLATION     = 0x714,
    MLX5_CMD_OP_CREATE_XRQ                    = 0x717,
    MLX5_CMD_OP_DESTROY_XRQ                   = 0x718,
    MLX5_CMD_OP_QUERY_XRQ                     = 0x719,
    MLX5_CMD_OP_ARM_XRQ                       = 0x71a,
    MLX5_CMD_OP_QUERY_XRQ_DC_PARAMS_ENTRY     = 0x725,
    MLX5_CMD_OP_SET_XRQ_DC_PARAMS_ENTRY       = 0x726,
    MLX5_CMD_OP_QUERY_XRQ_ERROR_PARAMS        = 0x727,
    MLX5_CMD_OP_RELEASE_XRQ_ERROR             = 0x729,
    MLX5_CMD_OP_MODIFY_XRQ                    = 0x72a,
    MLX5_CMD_OP_QUERY_ESW_FUNCTIONS           = 0x740,
    MLX5_CMD_OP_QUERY_VPORT_STATE             = 0x750,
    MLX5_CMD_OP_MODIFY_VPORT_STATE            = 0x751,
    MLX5_CMD_OP_QUERY_ESW_VPORT_CONTEXT       = 0x752,
    MLX5_CMD_OP_MODIFY_ESW_VPORT_CONTEXT      = 0x753,
    MLX5_CMD_OP_QUERY_NIC_VPORT_CONTEXT       = 0x754,
    MLX5_CMD_OP_MODIFY_NIC_VPORT_CONTEXT      = 0x755,
    MLX5_CMD_OP_QUERY_ROCE_ADDRESS            = 0x760,
    MLX5_CMD_OP_SET_ROCE_ADDRESS              = 0x761,
    MLX5_CMD_OP_QUERY_HCA_VPORT_CONTEXT       = 0x762,
    MLX5_CMD_OP_MODIFY_HCA_VPORT_CONTEXT      = 0x763,
    MLX5_CMD_OP_QUERY_HCA_VPORT_GID           = 0x764,
    MLX5_CMD_OP_QUERY_HCA_VPORT_PKEY          = 0x765,
    MLX5_CMD_OP_QUERY_VNIC_ENV                = 0x76f,
    MLX5_CMD_OP_QUERY_VPORT_COUNTER           = 0x770,
    MLX5_CMD_OP_ALLOC_Q_COUNTER               = 0x771,
    MLX5_CMD_OP_DEALLOC_Q_COUNTER             = 0x772,
    MLX5_CMD_OP_QUERY_Q_COUNTER               = 0x773,
    MLX5_CMD_OP_SET_MONITOR_COUNTER           = 0x774,
    MLX5_CMD_OP_ARM_MONITOR_COUNTER           = 0x775,
    MLX5_CMD_OP_SET_PP_RATE_LIMIT             = 0x780,
    MLX5_CMD_OP_QUERY_RATE_LIMIT              = 0x781,
    MLX5_CMD_OP_CREATE_SCHEDULING_ELEMENT      = 0x782,
    MLX5_CMD_OP_DESTROY_SCHEDULING_ELEMENT     = 0x783,
    MLX5_CMD_OP_QUERY_SCHEDULING_ELEMENT       = 0x784,
    MLX5_CMD_OP_MODIFY_SCHEDULING_ELEMENT      = 0x785,
    MLX5_CMD_OP_CREATE_QOS_PARA_VPORT         = 0x786,
    MLX5_CMD_OP_DESTROY_QOS_PARA_VPORT        = 0x787,
    MLX5_CMD_OP_ALLOC_PD                      = 0x800,
    MLX5_CMD_OP_DEALLOC_PD                    = 0x801,
    MLX5_CMD_OP_ALLOC_UAR                     = 0x802,
    MLX5_CMD_OP_DEALLOC_UAR                   = 0x803,
    MLX5_CMD_OP_CONFIG_INT_MODERATION         = 0x804,
    MLX5_CMD_OP_ACCESS_REG                    = 0x805,
    MLX5_CMD_OP_ATTACH_TO_MCG                 = 0x806,
    MLX5_CMD_OP_DETACH_FROM_MCG               = 0x807,
    MLX5_CMD_OP_GET_DROPPED_PACKET_LOG        = 0x80a,
    MLX5_CMD_OP_MAD_IFC                       = 0x50d,
    MLX5_CMD_OP_QUERY_MAD_DEMUX               = 0x80b,
    MLX5_CMD_OP_SET_MAD_DEMUX                 = 0x80c,
    MLX5_CMD_OP_NOP                           = 0x80d,
    MLX5_CMD_OP_ALLOC_XRCD                    = 0x80e,
    MLX5_CMD_OP_DEALLOC_XRCD                  = 0x80f,
    MLX5_CMD_OP_ALLOC_TRANSPORT_DOMAIN        = 0x816,
    MLX5_CMD_OP_DEALLOC_TRANSPORT_DOMAIN      = 0x817,
    MLX5_CMD_OP_QUERY_CONG_STATUS             = 0x822,
    MLX5_CMD_OP_MODIFY_CONG_STATUS            = 0x823,
    MLX5_CMD_OP_QUERY_CONG_PARAMS             = 0x824,
    MLX5_CMD_OP_MODIFY_CONG_PARAMS            = 0x825,
    MLX5_CMD_OP_QUERY_CONG_STATISTICS         = 0x826,
    MLX5_CMD_OP_ADD_VXLAN_UDP_DPORT           = 0x827,
    MLX5_CMD_OP_DELETE_VXLAN_UDP_DPORT        = 0x828,
    MLX5_CMD_OP_SET_L2_TABLE_ENTRY            = 0x829,
    MLX5_CMD_OP_QUERY_L2_TABLE_ENTRY          = 0x82a,
    MLX5_CMD_OP_DELETE_L2_TABLE_ENTRY         = 0x82b,
    MLX5_CMD_OP_SET_WOL_ROL                   = 0x830,
    MLX5_CMD_OP_QUERY_WOL_ROL                 = 0x831,
    MLX5_CMD_OP_CREATE_LAG                    = 0x840,
    MLX5_CMD_OP_MODIFY_LAG                    = 0x841,
    MLX5_CMD_OP_QUERY_LAG                     = 0x842,
    MLX5_CMD_OP_DESTROY_LAG                   = 0x843,
    MLX5_CMD_OP_CREATE_VPORT_LAG              = 0x844,
    MLX5_CMD_OP_DESTROY_VPORT_LAG             = 0x845,
    MLX5_CMD_OP_CREATE_TIR                    = 0x900,
    MLX5_CMD_OP_MODIFY_TIR                    = 0x901,
    MLX5_CMD_OP_DESTROY_TIR                   = 0x902,
    MLX5_CMD_OP_QUERY_TIR                     = 0x903,
    MLX5_CMD_OP_CREATE_SQ                     = 0x904,
    MLX5_CMD_OP_MODIFY_SQ                     = 0x905,
    MLX5_CMD_OP_DESTROY_SQ                    = 0x906,
    MLX5_CMD_OP_QUERY_SQ                      = 0x907,
    MLX5_CMD_OP_CREATE_RQ                     = 0x908,
    MLX5_CMD_OP_MODIFY_RQ                     = 0x909,
    MLX5_CMD_OP_SET_DELAY_DROP_PARAMS         = 0x910,
    MLX5_CMD_OP_DESTROY_RQ                    = 0x90a,
    MLX5_CMD_OP_QUERY_RQ                      = 0x90b,
    MLX5_CMD_OP_CREATE_RMP                    = 0x90c,
    MLX5_CMD_OP_MODIFY_RMP                    = 0x90d,
    MLX5_CMD_OP_DESTROY_RMP                   = 0x90e,
    MLX5_CMD_OP_QUERY_RMP                     = 0x90f,
    MLX5_CMD_OP_CREATE_TIS                    = 0x912,
    MLX5_CMD_OP_MODIFY_TIS                    = 0x913,
    MLX5_CMD_OP_DESTROY_TIS                   = 0x914,
    MLX5_CMD_OP_QUERY_TIS                     = 0x915,
    MLX5_CMD_OP_CREATE_RQT                    = 0x916,
    MLX5_CMD_OP_MODIFY_RQT                    = 0x917,
    MLX5_CMD_OP_DESTROY_RQT                   = 0x918,
    MLX5_CMD_OP_QUERY_RQT                     = 0x919,
    MLX5_CMD_OP_SET_FLOW_TABLE_ROOT		  = 0x92f,
    MLX5_CMD_OP_CREATE_FLOW_TABLE             = 0x930,
    MLX5_CMD_OP_DESTROY_FLOW_TABLE            = 0x931,
    MLX5_CMD_OP_QUERY_FLOW_TABLE              = 0x932,
    MLX5_CMD_OP_CREATE_FLOW_GROUP             = 0x933,
    MLX5_CMD_OP_DESTROY_FLOW_GROUP            = 0x934,
    MLX5_CMD_OP_QUERY_FLOW_GROUP              = 0x935,
    MLX5_CMD_OP_SET_FLOW_TABLE_ENTRY          = 0x936,
    MLX5_CMD_OP_QUERY_FLOW_TABLE_ENTRY        = 0x937,
    MLX5_CMD_OP_DELETE_FLOW_TABLE_ENTRY       = 0x938,
    MLX5_CMD_OP_ALLOC_FLOW_COUNTER            = 0x939,
    MLX5_CMD_OP_DEALLOC_FLOW_COUNTER          = 0x93a,
    MLX5_CMD_OP_QUERY_FLOW_COUNTER            = 0x93b,
    MLX5_CMD_OP_MODIFY_FLOW_TABLE             = 0x93c,
    MLX5_CMD_OP_ALLOC_PACKET_REFORMAT_CONTEXT = 0x93d,
    MLX5_CMD_OP_DEALLOC_PACKET_REFORMAT_CONTEXT = 0x93e,
    MLX5_CMD_OP_QUERY_PACKET_REFORMAT_CONTEXT = 0x93f,
    MLX5_CMD_OP_ALLOC_MODIFY_HEADER_CONTEXT   = 0x940,
    MLX5_CMD_OP_DEALLOC_MODIFY_HEADER_CONTEXT = 0x941,
    MLX5_CMD_OP_QUERY_MODIFY_HEADER_CONTEXT   = 0x942,
    MLX5_CMD_OP_FPGA_CREATE_QP                = 0x960,
    MLX5_CMD_OP_FPGA_MODIFY_QP                = 0x961,
    MLX5_CMD_OP_FPGA_QUERY_QP                 = 0x962,
    MLX5_CMD_OP_FPGA_DESTROY_QP               = 0x963,
    MLX5_CMD_OP_FPGA_QUERY_QP_COUNTERS        = 0x964,
    MLX5_CMD_OP_CREATE_GENERAL_OBJECT         = 0xa00,
    MLX5_CMD_OP_MODIFY_GENERAL_OBJECT         = 0xa01,
    MLX5_CMD_OP_QUERY_GENERAL_OBJECT          = 0xa02,
    MLX5_CMD_OP_DESTROY_GENERAL_OBJECT        = 0xa03,
    MLX5_CMD_OP_CREATE_UCTX                   = 0xa04,
    MLX5_CMD_OP_DESTROY_UCTX                  = 0xa06,
    MLX5_CMD_OP_CREATE_UMEM                   = 0xa08,
    MLX5_CMD_OP_DESTROY_UMEM                  = 0xa0a,
    MLX5_CMD_OP_SYNC_STEERING                 = 0xb00,
    MLX5_CMD_OP_QUERY_VHCA_STATE              = 0xb0d,
    MLX5_CMD_OP_MODIFY_VHCA_STATE             = 0xb0e,
    MLX5_CMD_OP_SYNC_CRYPTO                   = 0xb12,
    MLX5_CMD_OP_ALLOW_OTHER_VHCA_ACCESS       = 0xb16,
    MLX5_CMD_OP_MAX
};
net/mlx5_core：使用硬件寄存器描述头文件，添加自动生成的头文件来描述硬件寄存器以及设置/获取值的宏集。 这些宏进行静态检查以避免溢出、处理字节顺序，并总体上提供了一种干净的命令编码方式。 目前头文件很小，我们将在使用宏时添加结构。 一些命令已从命令枚举中删除，因为当前不支持它们，将在支持可用时添加



static const struct rdma_nl_cbs nldev_cb_table[RDMA_NLDEV_NUM_OPS] = {
    [RDMA_NLDEV_CMD_GET] = {
        .doit = nldev_get_doit,
        .dump = nldev_get_dumpit,
    },
如果用户提供特定索引，我们可以使用 .doit 回调加速查询，并在之后保存完整转储和过滤
nldev_get_dumpit _nldev_get_dumpit
    rdma_dev_access_netns
    _nldev_get_dumpit
        nlmsg_end



ice intel debug logging 配置固件调试日志:
The format to set the log levels for a module are:
  # echo <log level> > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/<module>
The supported log levels are:
      *	none
      *	error
      *	warning
      *	normal
      *	verbose
Each level includes the messages from the previous/lower level
The modules that are supported are:
      *	general
      *	ctrl
      *	link
      *	link_topo
      *	dnl
      *	i2c
      *	sdp
      *	mdio
      *	adminq
      *	hdma
      *	lldp
      *	dcbx
      *	dcb
      *	xlr
      *	nvm
      *	auth
      *	vpd
      *	iosf
      *	parser
      *	sw
      *	scheduler
      *	txq
      *	rsvd
      *	post
      *	watchdog
      *	task_dispatch
      *	mng
      *	synce
      *	health
      *	tsdrv
      *	pfreg
      *	mdlver
      *	all
The module 'all' is a special module which allows the user to read or
write to all of the modules.
The following example command would set the DCB module to the 'normal'
log level:
  # echo normal > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/dcb
If the user wants to set the DCB, Link, and the AdminQ modules to
'verbose' then the commands are:
  # echo verbose > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/dcb
  # echo verbose > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/link
  # echo verbose > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/adminq
If the user wants to set all modules to the 'warning' level then the
command is:
  # echo warning > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/all
If the user wants to disable logging for a module then they can set the
level to 'none'. An example setting the 'watchdog' module is:
  # echo none > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/watchdog
If the user wants to see what the log level is for a specific module
then the command is:
  # cat /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/dcb
This will return the log level for the DCB module. If the user wants to
see the log level for all the modules then the command is:
  # cat /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/modules/all
Writing to the module file will update the configuration, but NOT enable the
configuration (that is a separate command).
In addition to configuring the modules, the user can also configure the
number of log messages (nr_messages) to include in a single Admin Receive
Queue (ARQ) event.The range is 1-128 (1 means push every log message, 128
means push only when the max AQ command buffer is full). The suggested
value is 10.
To see/change the resolution the user can read/write the
'fwlog/nr_messages' file. An example changing the value to 50 is
  # echo 50 > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/nr_messages
To see the current value of 'nr_messages' then the command is:
  # cat /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/nr_messages

ice: add ability to read and configure FW log data
Once logging is enabled the user should read the data from the 'data'
file. The data is in the form of a binary blob that can be sent to Intel
for decoding. To read the data use a command like:
  # cat /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/data > log_data.bin
If the user wants to clear the FW log data that has been stored in the
driver then they can write any value to the 'data' file and that will clear
the data. An example is:
  # echo 34 > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/data
In addition to being able to read the data the user can configure how
much memory is used to store FW log data. This allows the user to
increase/decrease the amount of memory based on the users situation.
The data is stored such that if the memory fills up then the oldest data
will get overwritten in a circular manner. To change the amount of
memory the user can write to the 'log_size' file like this:
  # echo <value> > /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/log_size
Where <value> is one of 128K, 256K, 512K, 1M, and 2M. The default value
is 1M.
The user can see the current value of 'log_size' by reading the file:
  # cat /sys/kernel/debug/ice/0000\:18\:00.0/fwlog/log_size
At this point the user have SF devlink instance with auxiliary device
for the Ethernet functionality only.



timeout type:
enum mlx5_timeouts_types {
    /* pre init timeouts (not read from FW) */
    MLX5_TO_FW_PRE_INIT_TIMEOUT_MS,
    MLX5_TO_FW_PRE_INIT_ON_RECOVERY_TIMEOUT_MS,
    MLX5_TO_FW_PRE_INIT_WARN_MESSAGE_INTERVAL_MS,
    MLX5_TO_FW_PRE_INIT_WAIT_MS,

    /* init segment timeouts */
    MLX5_TO_FW_INIT_MS,
    MLX5_TO_CMD_MS,

    /* DTOR timeouts */
    MLX5_TO_PCI_TOGGLE_MS,
    MLX5_TO_HEALTH_POLL_INTERVAL_MS,
    MLX5_TO_FULL_CRDUMP_MS,
    MLX5_TO_FW_RESET_MS,
    MLX5_TO_FLUSH_ON_ERROR_MS,
    MLX5_TO_PCI_SYNC_UPDATE_MS,
    MLX5_TO_TEARDOWN_MS,
    MLX5_TO_FSM_REACTIVATE_MS,
    MLX5_TO_RECLAIM_PAGES_MS,
    MLX5_TO_RECLAIM_VFS_PAGES_MS,
    MLX5_TO_RESET_UNLOAD_MS,

    MAX_TIMEOUT_TYPES
};

default timeout:
static const u32 tout_def_sw_val[MAX_TIMEOUT_TYPES] = {
    [MLX5_TO_FW_PRE_INIT_TIMEOUT_MS] = 120000,
    [MLX5_TO_FW_PRE_INIT_ON_RECOVERY_TIMEOUT_MS] = 7200000,
    [MLX5_TO_FW_PRE_INIT_WARN_MESSAGE_INTERVAL_MS] = 20000,
    [MLX5_TO_FW_PRE_INIT_WAIT_MS] = 2,
    [MLX5_TO_FW_INIT_MS] = 2000,
    [MLX5_TO_CMD_MS] = 60000,
    [MLX5_TO_PCI_TOGGLE_MS] =  2000,
    [MLX5_TO_HEALTH_POLL_INTERVAL_MS] =  2000,
    [MLX5_TO_FULL_CRDUMP_MS] = 60000,
    [MLX5_TO_FW_RESET_MS] = 60000,
    [MLX5_TO_FLUSH_ON_ERROR_MS] = 2000,
    [MLX5_TO_PCI_SYNC_UPDATE_MS] = 5000,
    [MLX5_TO_TEARDOWN_MS] = 3000,
    [MLX5_TO_FSM_REACTIVATE_MS] = 5000,
    [MLX5_TO_RECLAIM_PAGES_MS] = 5000,
    [MLX5_TO_RECLAIM_VFS_PAGES_MS] = 120000,
    [MLX5_TO_RESET_UNLOAD_MS] = 300000
};


mlx5_function_setup



ip分片:
net/ipv4/ip_output.c
int ip_do_fragment



dev_alloc_pages 函数，用于分配多个连续的物理内存页，分配数量只能是2的非负整数次幂




.port_new = mlx5_devlink_sf_port_new
mlx5_sf_add
    mlx5_eswitch_load_sf_vport


struct devlink_port {
    ...
}
引入 devlink 基础设施，为驱动程序引入 devlink 基础设施，以通过通用 Netlink 接口注册并公开给用户空间。 定义了两个基本对象： 
devlink - 每个“父设备”都有一个实例，例如交换机 ASIC (芯片)
devlink_port - 设备的每个物理端口都有一个实例。 这个初始部分实现了将对象基本获取/转储到用户空间。 此外，还实现了端口分配器和端口类型设置

makefile:
obj-$(CONFIG_NET_DEVLINK)	+= devlink/

code_style:
devlink：将代码移动到专用目录，devlink 代码很难在一个文件中使用 13kLoC 进行导航。 我真的很喜欢 Michal 将 ethtool 分成每个命令文件和核心的方式。 将其全部拆分可能太多了，但我们至少可以将核心部分从每个 cmd 实现中分离出来，并将其放在一个目录中，以便新命令可以是单独的文件。 移动代码，后续commit会做部分分割

core_code:
drivers/net/ethernet/mellanox/mlx5/core/devlink.c


mlx5_devlink_ops



create_cq:
ibv_create_comp_channel
IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL -> ib_uverbs_create_comp_channel
    struct ib_uverbs_completion_event_file	  *ev_file
    ib_uverbs_init_event_queue(&ev_file->ev_queue)
        spin_lock_init(&ev_queue->lock)
        INIT_LIST_HEAD(&ev_queue->event_list);
        init_waitqueue_head(&ev_queue->poll_wait);
        ev_queue->is_closed   = 0;
        ev_queue->async_queue = NULL;
    resp.fd = uobj->id



ovs:
module_init(dp_init) -> net：添加 Open vSwitch 内核组件。 Open vSwitch 是一款针对虚拟化环境的多层以太网交换机。 除了支持传统硬件交换机所期望的各种功能之外，它还支持细粒度的编程扩展和基于流的网络控制。 这种控制在各种应用程序中都很有用，但在多服务器虚拟化部署中尤其重要，多服务器虚拟化部署的特点通常是高度动态的端点以及需要维护多个租户的逻辑抽象。 Open vSwitch 数据路径为数据包转发提供了内核内快速路径。 它由用户空间守护进程 ovs-vswitchd 进行补充，该守护进程能够接受来自各种来源的配置并将其转换为数据包处理规则。 请参阅 http://openvswitch.org 了解更多信息和用户空间实用程序
action_fifos_init
ovs_internal_dev_rtnl_link_register
ovs_flow_init
    static struct kmem_cache *flow_cache;
    struct kmem_cache *flow_stats_cache __read_mostly;
ovs_vport_init
    static struct hlist_head *dev_table;
    #define VPORT_HASH_BUCKETS 1024
register_pernet_device(&ovs_net_ops)
    ...
    ovs_ct_init -> openvswitch：支持conntrack区域限制，目前，nf_conntrack_max用于限制每个网络命名空间的conntrack表中的最大conntrack条目数。 对于驻留在同一命名空间中的虚拟机和容器，它们共享相同的 conntrack 表，并且所有虚拟机和容器的 conntrack 条目总数受 nf_conntrack_max 限制。 在这种情况下，如果 VM/容器之一滥用 conntrack 条目，它会阻止其他 VM/容器将有效的 conntrack 条目提交到 conntrack 表中。 即使我们可以将虚拟机放在不同的网络命名空间中，当前的 nf_conntrack_max 配置有点僵化，我们无法限制不同的虚拟机/容器具有不同的 # conntrack 条目。 为了解决上述问题，该补丁提出了一种细粒度的机制，可以进一步限制每个区域的 conntrack 条目数量。 例如，我们可以为不同的VM指定不同的zone，并为每个zone设置conntrack限制。 通过提供这种隔离，行为不当的虚拟机仅消耗其自己区域中的 conntrack 条目，并且不会影响其他行为良好的虚拟机。 此外，用户可以根据自己的喜好对不同区域设置各种连接跟踪限制。 建议的实现利用 Netfilter 的 nf_conncount 后端来计算特定区域中的连接数。 如果连接数超出配置的限制，ovs 会将 ENOMEM 返回到用户空间。 如果用户空间没有配置区域限制，则限制默认为零，即没有限制，这向后兼容没有此补丁的行为。 向用户空间提供以下高级 API： - OVS_CT_LIMIT_CMD_SET：* 设置所有区域的默认连接限制 * 设置特定区域的连接限制 - OVS_CT_LIMIT_CMD_DEL：* 删除特定区域的连接限制 - OVS_CT_LIMIT_CMD_GET：* 获取默认值 所有区域的连接限制 * 获取特定区域的连接限制
register_netdevice_notifier(&ovs_dp_device_notifier)
ovs_netdev_init -> ovs：将具有依赖关系的 vport 转换为单独的模块，内部和 netdev vport 仍然是 openvswitch.ko 的一部分。 Encap vports（包括 vxlan、gre 和geneve）可以构建为单独的模块并按需加载。 模块使用后可以卸载。 数据路径端口在其生命周期内保留对 vport 模块的引用。 允许删除全局列表 vport_ops_list 的容易出错的维护
    ovs_vport_ops_register(&ovs_netdev_vport_ops)
        ops)->owner = THIS_MODULE;
dp_register_genl
    genl_register_family(dp_genl_families[i]) -> 注册通用netlink系列，@family：通用netlink系列首先验证后注册指定的系列。 只有一个家庭可以使用相同的姓氏或标识符进行登记。 该系列的操作、多播组和模块指针必须已分配。 成功返回 0 或负错误代码
        genl_validate_ops
        genl_lock_all
drop_reasons_register_subsys

int netif_rx(struct sk_buff *skb)

skeleton.c


set_64bit_val





...
irdma_init_roce_device
    ib_set_device_ops(&iwdev->ibdev, &irdma_roce_dev_ops);

static const struct ib_device_ops irdma_roce_dev_ops = {
    .attach_mcast = irdma_attach_mcast,
    .create_ah = irdma_create_ah,
    .create_user_ah = irdma_create_user_ah,
    .destroy_ah = irdma_destroy_ah,
    .detach_mcast = irdma_detach_mcast,
    .get_link_layer = irdma_get_link_layer,
    .get_port_immutable = irdma_roce_port_immutable,
    .modify_qp = irdma_modify_qp_roce,
    .query_ah = irdma_query_ah,
    .query_pkey = irdma_query_pkey,
};



ibv_modify_qp -> IB_USER_VERBS_EX_CMD_MODIFY_QP -> ib_uverbs_ex_modify_qp
    modify_qp
        uobj_get_obj_read UVERBS_OBJECT_QP
        校验参数
        ...
        ib_modify_qp_with_udata
            _ib_modify_qp -> IB/core：将 DMAC 解析限制为用户空间 QP，当前 ah_attr 由 ib_cm 层针对基于 rdma_cm 的应用程序进行初始化。 对于 RoCE 传输，ah_attr.roce.dmac 已由 ib_cm、rdma_cm 从 wc、路径记录、路由解析、显式路径记录设置（取决于主动方或被动方 QP）初始化。 因此避免为内核消费者的 QP 解析 DMAC
                attr_mask & IB_QP_AV -> IBV_QP_AV，主要用来指示内核做地址解析，对于RoCE，则进行L3到MAC地址的转换
                rdma_fill_sgid_attr
                    rdma_check_ah_attr
                    rdma_ah_retrieve_grh
                    rdma_get_gid_attr
                        rdma_gid_table
                        get_gid_entry
                ib_resolve_eth_dmac -> 调用ib_resolve_eth_dmac解析remote gid对应的MAC地址
                    ...
                    rdma_resolve_ip resolve_cb
                        complete -> IB/core：verbs/cm 结构中的以太网 L2 属性 ，此补丁添加了对 verbs/cm/cma 结构中的以太网 L2 属性的支持。 在处理 L2 以太网时，我们应该以与使用 IB L2（和 L4 PKEY）属性类似的方式使用 smac、dmac、vlan ID 和优先级。 因此，这些属性被添加到以下结构中： * ib_ah_attr - 添加了 dmac * ib_qp_attr - 添加了 smac 和 vlan_id，（sl 保留 vlan 优先级） * ib_wc - 添加了 smac、vlan_id * ib_sa_path_rec - 添加了 smac、dmac、vlan_id * cm_av - 添加了 smac 和 vlan_id 对于路径记录结构，在将其打包为有线格式时特别注意避免新字段，因此我们不会破坏 IB CM 和 SA 有线协议。 在主动侧，CM 被填充。 其内部结构来自 ULP 提供的路径。 我们添加了 ETH L2 属性并将它们放入 CM 地址句柄（struct cm_av）中。 在被动侧，CM 从与 REQ 消息关联的 WC 中填充其内部结构。 我们添加了从 WC 获取 ETH L2 属性的内容。 当硬件驱动程序在 WC 中提供所需的 ETH L2 属性时，它们会设置 IB_WC_WITH_SMAC 和 IB_WC_WITH_VLAN 标志。 IB 核心代码检查这些标志是否存在，如果没有，则从 ib_init_ah_from_wc() 辅助函数进行地址解析。 ib_modify_qp_is_ok 也被更新以考虑链路层。 有些参数对于以太网链路层是必需的，而对于IB来说则无关。 修改供应商驱动程序以支持新的函数签名
                rdma_lag_get_ah_roce_slave
                    rdma_read_gid_attr_ndev_rcu
                    rdma_get_xmit_slave_udp
                        rdma_build_skb
                        netdev_get_xmit_slave RDMA_LAG_FLAGS_HASH_ALL_SLAVES
                rdma_counter_bind_qp_auto
                ib_security_modify_qp -> IB/核心：在 QP 上强制执行 PKey 安全性，添加新的 LSM 挂钩以分配和释放安全上下文并检查访问 PKey 的权限。 创建和销毁 QP 时分配和释放安全上下文。 此上下文用于控制对 PKey 的访问。 当请求修改 QP 来更改端口、PKey 索引或备用路径时，请检查 QP 是否具有对该端口子网前缀上的 PKey 表索引中的 PKey 的权限。 如果 QP 是共享的，请确保 QP 的所有句柄也具有访问权限。 存储 QP 正在使用的端口和 PKey 索引。 重置到初始化转换后，用户可以独立修改端口、PKey 索引和备用路径。 因此，端口和 PKey 设置更改可以是先前设置和新设置的合并。 为了在 PKey 表或子网前缀更改时维持访问控制，请保留每个端口上使用每个 PKey 索引的所有 QP 的列表。 如果发生更改，则使用该设备和端口的所有 QP 都必须强制执行新缓存设置的访问权限。 这些更改将事务添加到 QP 修改过程中。 如果修改失败，则必须保持与旧端口和 PKey 索引的关联；如果修改成功，则必须将其删除。 必须在修改之前建立与新端口和 PKey 索引的关联，如果修改失败则将其删除。 1. 当 QP 被修改为特定端口时，PKey 索引或备用路径将该 QP 插入到适当的列表中。 2. 检查访问新设置的权限。 3. 如果步骤 2 授予访问权限，则尝试修改 QP。 4a. 如果步骤 2 和 3 成功，则删除任何先前的关联。 4b. 如果以太失败，请删除新的设置关联。 如果 PKey 表或子网前缀发生更改，则遍历 QP 列表并检查它们是否具有权限。 如果没有，则将 QP 发送到错误状态并引发致命错误事件。 如果它是共享 QP，请确保共享 real_qp 的所有 QP 也具有权限。 如果拥有安全结构的 QP 被拒绝访问，则安全结构将被标记为此类，并且 QP 将被添加到 error_list 中。 一旦将 QP 移至错误完成，安全结构标记就会被清除。 正确维护列表会将 QP 销毁转变为事务。 设备的硬件驱动程序释放 ib_qp 结构，因此当销毁正在进行时，ib_qp_security 结构中的 ib_qp 指针未定义。 当销毁过程开始时，ib_qp_security 结构被标记为正在销毁。 这可以防止对 QP 指针采取任何操作。 QP 成功销毁后，它仍然可以列在 error_list 上，等待该流处理它，然后再清理结构。 如果销毁失败，则 QP 端口和 PKey 设置将重新插入到适当的列表中，销毁标志将被清除，并强制执行访问控制，以防在销毁流程期间发生任何缓存更改。 为了保持安全更改隔离，使用新文件来保存与安全相关的功能
                    port_pkey_list_insert -> 在检查权限之前，将此 QP 添加到新端口和 pkey 设置的列表中，以防发生并发缓存更新。 遍历列表进行缓存更改不会获取安全互斥体，除非将 QP 发送到错误
                    check_qp_port_pkey_settings
                        get_pkey_and_subnet_prefix
                            ib_get_cached_pkey
                            ib_get_cached_subnet_prefix
                        enforce_qp_pkey_security
                    modify_qp -> .modify_qp = irdma_modify_qp_roce
                rdma_lag_put_ah_roce_slave
        release_qp:
        rdma_lookup_put_uobject

irdma_uk_rdma_write
irdma_uk_rdma_read
irdma_uk_cq_poll_cmpl


/proc/pid/pagemap - an array mapping virtual pages to pfns, 如果页面不存在但在交换中，则 PFN 包含交换文件号的编码以及页面在交换中的偏移量。 未映射的页面返回空 PFN。 这允许精确确定哪些页面被映射（或在交换中）并比较进程之间的映射页面。 * 此接口的高效用户将使用 /proc/pid/maps 来确定实际映射的内存区域，并使用 llseek 跳过未映射的区域
pagemap_read -> Maps4：添加 /proc/pid/pagemap 接口，该接口为地址空间中的每个页面提供到其物理页帧号的映射，允许精确确定哪些页面被映射以及哪些页面在进程之间共享。 此版本中的新增内容： - 标头再次消失（根据 Dave Hansen 和 Alan Cox 的建议） - 64 位条目（根据与 Andi Kleen 的讨论） - 交换导出的 pte 信息（来自 Dave Hansen） - 页面遍历器回调以查找漏洞（来自 Dave Hansen） - 直接 put_user I/O（根据 Rusty Russell 的建议）此补丁折叠在清理中并交换 Dave Hansen 的 PTE 支持
    file_ns_capable
    mmap_read_lock_killable
    untagged_addr_remote
    mmap_read_lock_killable
    walk_page_range(mm, start_vaddr, end, &pagemap_ops, &pm) -> walk_page_range - 使用回调遍历内存映射的页表：起始地址：结束地址：为树的每个级别调用的回调集递归地遍历 VMA 中内存区域的页表，调用提供的回调。 回调按顺序调用（第一个 PGD、第一个 PUD、第一个 PMD、第一个 PTE、第二个 PTE...第二个 PMD 等）。 如果省略较低级别的回调，则行走深度会减少。 每个回调接收一个入口指针以及关联范围的开始和结束，以及用于访问 ->private 或 ->mm 字段的原始 mm_walk 的副本。 通常不加锁，但分割透明大页可能会加页表锁。 如果需要，底层迭代器将从 highmem 映射 PTE 目录。 如果任何回调返回非零值，则遍历将中止并将返回值传播回调用者。 否则返回 0。 如果 walk->hugetlb_entry 为 !NULL，则 walk->mm->mmap_sem 必须至少保持读取状态
    copy_to_user

static const struct mm_walk_ops pagemap_ops = {
    .pmd_entry	= pagemap_pmd_range,
    .pte_hole	= pagemap_pte_hole,
    .hugetlb_entry	= pagemap_hugetlb_range,
    .walk_lock	= PGWALK_RDLOCK,
};

pagemap_pmd_range -> pagemap：将 mm 传递给 pagewalkers ，我们现在至少需要这个来检测大页，因为 powerpc 需要 vm_area_struct 来确定虚拟地址是否引用大页（它的 pmd_huge() 不起作用）。 对于其他一些用户来说它也可能派上用场
    pmd_trans_huge_lock
    ...
    for pte
        内核态实现pagemap proc接口的代码位于: fs/proc/task_mmu.c, 把PTE转换为pagemap_entry
        pte_to_pagemap_entry -> proc：报告 /proc/pid/pagemap 中的文件/匿名位，这是安德鲁提议的实现，扩展页面映射文件位以报告任务工作集缺少的内容。 工作集检测的问题是多方面的。 在 criu（检查点/恢复）项目中，我们将任务的内存转储到图像文件中，为了正确执行此操作，我们需要检测映射中的哪些页面真正在使用。 我虽然可以帮助解决这个问题，但 mincore 系统调用却没有。 首先，它不报告交换的页面，因此我们无法找出要转储的匿名映射的哪些部分。 接下来，它会报告页面缓存中存在的页面，即使它们没有被映射，但这并不意味着它们没有受到威胁。 请注意，交换页的问题至关重要——我们必须将交换页转储到映像文件。 但是文件页面的问题是优化 - 我们可以将所有文件页面进行映像，这是正确的，但是如果我们知道页面未映射或未受到限制，我们可以将它们从转储文件中删除。 转储仍然是自洽的，尽管大小明显较小（在实际应用程序中最多小 10 倍）。 Andrew 注意到，proc pagemap 文件解决了上述 3 个问题中的 2 个——它报告页面是否存在或交换，并且不报告未映射的页面缓存页面。 但是，它无法区分受限制的文件页面和不受限制的文件页面。 我想在此文件中最后一个未使用的位来报告映射到相应 pte 的页面是否为 PageAnon
            pte_present(pte)
            frame = pte_pfn(pte) -> 
            make_pme(frame, flags)
        add_to_pagemap
    ...




smap: 基于映射的扩展，显示每个映射的内存消耗以及与其关联的标志, scan page table
static const struct mm_walk_ops smaps_walk_ops = {
    .pmd_entry		= smaps_pte_range,
    .hugetlb_entry		= smaps_hugetlb_range,
    .walk_lock		= PGWALK_RDLOCK,
};
smaps_pte_range
    smaps_pte_entry


show_smap
    smap_gather_stats
    show_map_vma
    __show_smap
    seq_printf


irdma：为英特尔(R) 以太网控制器 E810 添加 RDMA 驱动程序，这是针对英特尔(R) 以太网控制器 E810 的 RDMA FreeBSD 驱动程序（称为 irdma）的初始提交。 以每 PF 方式支持 RoCEv2 和 iWARP 协议，RoCEv2 为默认协议。 测试已使用 krping 工具、perftest、ucmatose、rping、ud_pingpong、rc_pingpong 等完成, https://cgit.freebsd.org/src/commit/?id=42bad04a2156


struct ice_vsi




iavf_register_client


drivers/net/ethernet/hisilicon/Makefile
net：添加海思网络子系统hnae框架支持，HNAE（海思网络加速引擎）是为海思网络加速引擎提供统一环形缓冲区接口的框架。 通过该接口，上层可以有意地充当以太网驱动程序、ODP驱动程序或其他服务驱动程序


capability:
pci_find_capability

read pcie config space of device:
u32 read_pci_config
    outl(0x80000000 | (bus<<16) | (slot<<11) | (func<<8) | offset, 0xcf8);





三星:
drivers/dma/pl330.c
pd->device_issue_pending = pl330_issue_pending;
dma_async_issue_pending
    device_issue_pending
xdev->common.device_issue_pending = xilinx_dma_issue_pending;
chan->start_transfer = xilinx_dma_start_transfer

struct dma_device

desc->txd.tx_submit = pl330_tx_submit;


module_amba_driver(pl330_driver);
dma_async_device_register


Xilinx media platform drivers
drivers/media/platform/xilinx/xilinx-dma.c
dmaengine_prep_interleaved_dma


dmaengine_prep_interleaved_dma
dmaengine_submit


https://www.kernel.org/doc/html/v4.9/media/kapi/v4l2-videobuf2.html
static const struct vb2_ops xvip_dma_queue_qops = {
    .queue_setup = xvip_dma_queue_setup,
    .buf_prepare = xvip_dma_buffer_prepare,
    .buf_queue = xvip_dma_buffer_queue,
    .wait_prepare = vb2_ops_wait_prepare,
    .wait_finish = vb2_ops_wait_finish,
    .start_streaming = xvip_dma_start_streaming,
    .stop_streaming = xvip_dma_stop_streaming,
};

xvip_dma_buffer_queue


static struct platform_driver xvip_composite_driver = {
    .driver = {
        .name = "xilinx-video",
        .of_match_table = xvip_composite_of_id_table,
    },
    .probe = xvip_composite_probe,
    .remove_new = xvip_composite_remove,
};

module_platform_driver(xvip_composite_driver);
xvip_graph_init
xvip_graph_dma_init
xvip_graph_dma_init_one
    xvip_dma_init
        ...
        dma->queue.ops = &xvip_dma_queue_qops;
        dma->dma = dma_request_chan(dma->xdev->dev, name);


https://github.com/Xilinx/linux-xlnx/tree/xilinx-v14.4



drivers/infiniband/core/ucma.c
static ssize_t (*ucma_cmd_table[])(struct ucma_file *file,
                   const char __user *inbuf,
                   int in_len, int out_len) = {
    [RDMA_USER_CM_CMD_CREATE_ID] 	 = ucma_create_id,
    [RDMA_USER_CM_CMD_DESTROY_ID]	 = ucma_destroy_id,
    [RDMA_USER_CM_CMD_BIND_IP]	 = ucma_bind_ip,
    [RDMA_USER_CM_CMD_RESOLVE_IP]	 = ucma_resolve_ip,
    [RDMA_USER_CM_CMD_RESOLVE_ROUTE] = ucma_resolve_route,
    [RDMA_USER_CM_CMD_QUERY_ROUTE]	 = ucma_query_route,
    [RDMA_USER_CM_CMD_CONNECT]	 = ucma_connect,
    [RDMA_USER_CM_CMD_LISTEN]	 = ucma_listen,
    [RDMA_USER_CM_CMD_ACCEPT]	 = ucma_accept,
    [RDMA_USER_CM_CMD_REJECT]	 = ucma_reject,
    [RDMA_USER_CM_CMD_DISCONNECT]	 = ucma_disconnect,
    [RDMA_USER_CM_CMD_INIT_QP_ATTR]	 = ucma_init_qp_attr,
    [RDMA_USER_CM_CMD_GET_EVENT]	 = ucma_get_event,
    [RDMA_USER_CM_CMD_GET_OPTION]	 = NULL,
    [RDMA_USER_CM_CMD_SET_OPTION]	 = ucma_set_option,
    [RDMA_USER_CM_CMD_NOTIFY]	 = ucma_notify,
    [RDMA_USER_CM_CMD_JOIN_IP_MCAST] = ucma_join_ip_multicast,
    [RDMA_USER_CM_CMD_LEAVE_MCAST]	 = ucma_leave_multicast,
    [RDMA_USER_CM_CMD_MIGRATE_ID]	 = ucma_migrate_id,
    [RDMA_USER_CM_CMD_QUERY]	 = ucma_query,
    [RDMA_USER_CM_CMD_BIND]		 = ucma_bind,
    [RDMA_USER_CM_CMD_RESOLVE_ADDR]	 = ucma_resolve_addr,
    [RDMA_USER_CM_CMD_JOIN_MCAST]	 = ucma_join_multicast
};

UCMA_CMD_CONNECT -> static ssize_t (*ucma_cmd_table[]) -> static ssize_t ucma_connect
    copy_from_user
    ucma_get_ctx_dev
    ucma_copy_conn_param -> RDMA/cma：为AF_IB设置qkey，允许用户在使用AF_IB时指定qkey。 qkey 被添加到 struct rdma_ucm_conn_param 中代替保留字段，但为了向后兼容，仅当关联的 rdma_cm_id 使用 AF_IB 时才可访问
        ...
        dst->qkey = (id->route.addr.src_addr.ss_family == AF_IB) ? src->qkey : 0;
    rdma_connect_ece -> RDMA/ucma：扩展ucma_connect以接收ECE参数，CMID的主动方通过librdmacm的rdma_connect()和内核的ucma_connect()发起连接。 扩展 UCMA 接口来处理这些新参数
        rdma_connect(id, conn_param) -> rdma_connect_locked
            cma_comp_exch(id_priv, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_CONNECT)
            rdma_cap_ib_cm(id->device, id->port_num) -> rdma_cap_ib_cm - 检查设备端口是否具有 Infiniband Communication Manager 功能。 @device：要检查的设备 @port_num：要检查的端口号 InfiniBand 通信管理器是通过通用服务接口 (GSI) 访问的许多预定义通用服务代理 (GSA) 之一。 它的作用是促进节点之间连接的建立以及已建立的连接的其他管理相关任务。 返回：如果端口支持 IB CM，则返回 true（但这并不能保证 CM 实际正在运行）
                RDMA_CORE_CAP_IB_CM
            if (id->qp_type == IB_QPT_UD) -> cma_resolve_ib_udp -> RDMA/cma：添加对 RDMA_PS_UDP 的支持，允许通过 rdma_cm 使用 UD QP，以便为使用 SIDR 解析数据报消息的 IB 地址提供地址转换服务
            or cma_connect_ib
                check_add_overflow(offset, conn_param->private_data_len, &req.private_data_len) -> 为了简单性和代码卫生，下面的后备代码坚持 a、b 和 *d 具有相同的类型（类似于 min() 和 max() 宏），而 gcc 的类型通用溢出检查器接受不同的类型。 因此，我们不只是将 check_add_overflow 设置为 __builtin_add_overflow 的别名，而是添加类似于下面的类型检查
                ib_create_cm_id(id_priv->id.device, cma_ib_handler, id_priv)
                    cm_alloc_id_priv -> RDMA/cm：简化建立监听cm_id
                        cm_id_priv->id.cm_handler = cm_handler
                        RB_CLEAR_NODE(&cm_id_priv->service_node) -> rb_tree
                        init_completion(&cm_id_priv->comp)
                        xa_alloc_cyclic -> 在 XArray 中找到存储此条目的位置 -> 在 xa 中查找 limit.min 和 limit.max 之间的空条目，将索引存储到 id 指针中，然后将条目存储在该索引处。 并发查找不会看到未初始化的 id。 对空条目的搜索将从下一个开始，并在必要时回绕, https://docs.kernel.org/core-api/xarray.html#c.xa_alloc_cyclic
                        cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand
                    cm_finalize_id(cm_id_priv)
                        xa_store(&cm.local_id_table,
                trace_cm_send_req
                ib_send_cm_req(id_priv->cm_id.ib, &req)
                    struct ib_mad_send_buf *msg -> ib_mad_send_buf - MAD 数据缓冲区和发送的工作请求。@next：用于将 MAD 链接在一起以进行发布的指针。 @mad：为没有激活 RMPP 的 MAD 引用分配的 MAD 数据缓冲区。 对于使用 RMPP 的 MAD，引用公共和管理类特定标头。 @mad_agent：分配缓冲区的 MAD 代理。 @ah：发送 MAD 时使用的地址句柄。 @context：用户控制的上下文字段。 @hdr_len：表示MAD的数据头的大小。 此长度包括常见的 MAD、RMPP 和特定于类的标头。 @data_len：表示用户传输的数据的总大小。 @seg_count：为此发送分配的 RMPP 段数。 @seg_size：每个 RMPP 段中数据的大小。 这不包括特定于类的标头。 @seg_rmpp_size：每个 RMPP 段的大小，包括类特定标头。 @timeout_ms：等待响应的时间。 @retries：重试响应请求的次数。 对于使用 RMPP 的 MAD，这适用于每个窗口。 完成后，返回完成传输所需的重试次数。 用户负责初始化 MAD 缓冲区本身，任何 RMPP 标头除外。 超出 data_len 分配的额外段缓冲区空间是填充
                    cm_validate_req_param(param)
                    cm_id_priv->timewait_info = cm_create_timewait_info(cm_id_priv->
                        INIT_DELAYED_WORK(&timewait_info->work.work, cm_work_handler)
                    cm_init_av_by_path(param->primary_path, param->ppath_sgid_attr, &av) -> IB/cm：将 sa_path_rec 的成员替换为“struct sgid_attr *”，在处理 CM 消息中的路径记录条目时，现在还提供关联的 GID 属性。 目前，对于 RoCE，网络设备的网络命名空间指针和 ifindex 存储在路径记录条目中。 在处理 CM 消息时，netdev 的这两个字段都可以随时更改。 另外，存储网络命名空间而不保留引用将导致释放后使用崩溃。 因此将其删除。 RoCE 的网络设备信息是通过 ib_cm 请求中引用的 gid 属性提供的。 这样的设计会导致当网络指针无效时内核可能崩溃的情况。 然而今天它总是被初始化为init_net，它不会变得无效。 为了支持处理接收到的数据包的任意命名空间中的数据包，有必要避免这种情况。 该补丁消除了对网络指针和 ifindex 的依赖； 相反，它将依赖于包含指向 netdev 的指针的 SGID 属性
                        port = get_cm_port_from_path(path, sgid_attr)
                            rdma_find_gid
                                index = find_gid(table, gid, &gid_attr_val, false, mask, NULL)
                        ib_find_cached_pkey(cm_dev->ib_device, port->port_num,
                            cache = device->port_data[port_num].cache.pkey
                            if ((cache->table[i] & 0x7fff) == (pkey & 0x7fff))
                                if (cache->table[i] & 0x8000)
                        cm_set_av_port(av, port)
                        ib_init_ah_attr_from_path(cm_dev->ib_device, port->port_num, path, &new_ah_attr, sgid_attr) -> av->ah_attr 可能会根据 wc 或请求处理时间进行初始化，这可能会引用 sgid_attr。 因此在堆栈上初始化一个新的ah_attr。 如果初始化失败，则使用旧的 ah_attr 来发送任何响应。 如果初始化成功，则使用新的 ah_attr 覆盖旧的。 这样就可以使用 ah_attr 来返回错误响应 -> ib_init_ah_attr_from_path - 根据 SA 路径记录初始化地址句柄属性。 @device：设备关联啊属性初始化。 @port_num：指定设备上的端口。 @rec：用于 ah 属性初始化的路径记录条目。 @ah_attr：从路径记录初始化地址句柄属性。 @gid_attr：初始化期间要考虑的SGID属性。 当 ib_init_ah_attr_from_path() 返回成功时，(a) 对于 IB 链路层，当 IB 链路层存在 GRH 时，它可选地包含对 SGID 属性的引用。 (b) 对于 RoCE 链路层，它包含对 SGID 属性的引用。 用户必须调用 rdma_destroy_ah_attr() 来释放对使用 ib_init_ah_attr_from_path() 初始化的 SGID 属性的引用
                            rdma_ah_find_type
                            rdma_ah_set_sl
                            rdma_ah_set_port_num(ah_attr, port_num)
                            rdma_ah_set_static_rate(ah_attr, rec->rate)
                            roce_resolve_route_from_path(rec, gid_attr)
                                might_sleep -> RDMA/addr：将 addr_resolve 标记为 might_sleep()，在通过 ib_nl_fetch_ha() 的一条路径下，这会调用 nlmsg_new(GFP_KERNEL)，这是一个睡眠调用。 这是一条非常罕见的路径，因此将 fetch_ha() 和有条件调用 fetch_ha() 的模块外部入口点标记为 might_sleep() -> 在当前CONFIG_DEBUG_ATOMIC_SLEEP选项使能的前提下， 可以看到__might_sleep还是干了不少事情的，最主要的工作是在第一个if语句那里，尤其是preempt_count_equals和 irqs_disabled，都是用来判断当前的上下文是否是一个atomic context，因为我们知道，只要进程获得了spin_lock的任一个变种形式的lock，那么无论是单处理器系统还是多处理器系统，都会导致 preempt_count发生变化，而irq_disabled则是用来判断当前中断是否开启。__might_sleep正是根据这些信息来判断当前正在执行的代码上下文是否是个atomic，如果不是，那么函数就直接返回了，因为一切正常。如果是，那么代码下行
                                rdma_gid2ip((struct sockaddr *)&sgid, &rec->sgid)
                                    if (ipv6_addr_v4mapped((struct in6_addr *)gid))
                                        memcpy(&out_in->sin_addr.s_addr, gid->raw + 12, 4)
                                    else
                                        memcpy(&out_in->sin6_addr.s6_addr, gid->raw, 16)
                                rdma_gid2ip((struct sockaddr *)&dgid, &rec->dgid)
                                addr_resolve((struct sockaddr *)&sgid, (struct sockaddr *)&dgid, &dev_addr, false, true, 0)
                                    if (resolve_by_gid_attr) -> RDMA/core：RoCE考虑gid属性的net ns，解析目的地址或路由时，当netnamespace不可用时，参考SGID属性的netdevice的netnamespace。 这通常是从网络到达 RoCE 端口的请求的情况
                                        set_addr_netns_by_gid_rcu(addr)
                                    ret = addr4_resolve(src_in, dst_in, addr, &rt) -> https://hustcat.github.io/queue-pair-in-rdma/
                                        ip_route_output_key(addr->net, &fl4) -> ip_route_output_flow(net, flp, NULL) -> 查找路由表：为套接字缓冲区设置路由出口信息。如果已有TCP连接，则套接字缓冲区保存了路由信息
                                        addr->hoplimit = ip4_dst_hoplimit(&rt->dst) -> 访问到dst metric的RTAX_HOPLIMIT字段，此字段并未做初始化，其值为零，ip4_dst_hoplimit会采用系统默认的hoplimit
                                    or ret = addr6_resolve(src_in, dst_in, addr, &dst)
                                        ipv6_dst_lookup_flow
                                        ip6_dst_hoplimit
                                    rdma_set_src_addr_rcu
                                        rdma_ah_set_grh(attr, dgid, flow_label, sgid_attr->index, hop_limit, traffic_class)
                                            grh->flow_label = flow_label
                                            grh->sgid_index = sgid_index -> grh.sgid_index 源端口 GID 表中的索引，用于识别数据包的发起者
                                            grh->hop_limit = hop_limit
                                            grh->traffic_class = traffic_class
                                    addr_resolve_neigh -> arp
                                        memcpy(addr->dst_dev_addr, addr->src_dev_addr, MAX_ADDR_LEN)
                                        or ret = fetch_ha(dst, addr, dst_in, seq)
                                    rdma_addr_set_net_defaults
                                rec->roce.route_resolved = true
                            else rdma_ah_set_dlid(ah_attr, be32_to_cpu(sa_path_get_dlid(rec))) -> attr->ib.dlid = (u16)dlid
                            init_ah_attr_grh_fields(device, port_num,
                                sa_conv_pathrec_to_gid_type(rec) -> IB_GID_TYPE_ROCE
                                gid_attr = rdma_find_gid_by_port(device, &rec->sgid, type,
                                    local_index = find_gid(table, gid, &val, false, mask, NULL)
                                or rdma_hold_gid_attr
                                rdma_move_grh_sgid_attr
                        rdma_move_ah_attr(&av->ah_attr, &new_ah_attr)
                    cm_move_av_from_path(&cm_id_priv->av, &av)
                    msg = cm_alloc_priv_msg(cm_id_priv) -> cm_alloc_msg
                        mad_agent = cm_id_priv->av.port->mad_agent
                        ah = rdma_create_ah(mad_agent->qp->pd, &cm_id_priv->av.ah_attr, 0)
                            rdma_fill_sgid_attr
                            rdma_lag_get_ah_roce_slave
                            _rdma_create_ah -> IB/核心：引入和使用 rdma_create_user_ah，引入 rdma_create_user_ah API，该 API 允许将 udata 传递给提供程序驱动程序，并另外解析 RoCE 的 DMAC。 ib_resolve_eth_dmac() 解析单播、多播、链接本地 ipv4 映射的 ipv6 和 ipv6 目标 gid 条目的目标 mac 地址。 这允许所有 RoCE 提供程序驱动程序避免重复此类代码。 这种更改带来了一致性，其中 IB 核心始终解析 dmac 并将其传递给用户和内核使用者的 RoCE 提供程序驱动程序，并且 ah_attr->roce.dmac 始终是提供程序驱动程序的输入字段。 这种一致性避免了将 ib_resolve_eth_dmac 符号导出到提供程序或其他模块。 因此，它会在补丁系列的后面部分作为导出符号被删除。 现在 uverbs 和 umad 都使用 rdma_create_user_ah API，它修复了 umad 地址的 DMAC 无效的问题
                                rdma_zalloc_drv_obj_gfp -> kzalloc_node(size, gfp, dev->ops.get_numa_node(dev)
                                device->ops.create_user_ah(ah, &init_attr, udata) -> irdma_create_user_ah
                                    irdma_setup_ah
                                    irdma_create_hw_ah -> IRDMA_OP_AH_CREATE
                                    hash_add(iwdev->ah_hash_tbl, &parent_ah->list, key)
                                or device->ops.create_ah(ah, &init_attr, NULL)
                            rdma_lag_put_ah_roce_slave
                            rdma_unfill_sgid_attr
                        m = ib_create_send_mad(mad_agent, cm_id_priv->id.remote_cm_qpn, cm_id_priv->av.pkey_index, 0, IB_MGMT_MAD_HDR, IB_MGMT_MAD_DATA, GFP_ATOMIC, IB_MGMT_BASE_VERSION)
                            opa = rdma_cap_opa_mad(mad_agent->device, mad_agent->port_num) -> IB/mad：添加部分英特尔 OPA MAD 支持，此补丁是 3 个补丁中的第一个，添加了 OPA MAD 处理 1) 添加英特尔 Omni-Path 架构定义 2) 增加最大管理版本以适应 OPA 3) 更新 ib_create_send_mad 如果设备支持 OPA MAD 和发送的 MAD 是 OPA 基本版本，根据需要更改 MAD 大小和 sg 长度
                            if (ib_mad_kernel_rmpp_agent(mad_agent))
                            INIT_LIST_HEAD(&mad_send_wr->rmpp_list)
                            mad_send_wr->mad_list.cqe.done = ib_mad_send_done;
                            mad_send_wr->send_wr.wr.num_sge = 2
                            mad_send_wr->send_wr.wr.opcode = IB_WR_SEND
                            ret = alloc_send_rmpp_list(mad_send_wr, mad_size, gfp_mask)
                        m->context[0] = cm_id_priv
                    msg->context[1] = (void *)(unsigned long)IB_CM_REQ_SENT
                    ib_post_send_mad -> [IB] 修复 MAD 层 DMA 映射，以避免在映射后触及数据缓冲区。MAD 层在 DMA 映射完成后触及用于发送的数据缓冲区，从而违反了 DMA API。 这会导致非缓存一致性架构出现问题，因为执行 DMA 的设备不会看到仅存在于 CPU 缓存中的有效负载缓冲区的更新。 通过让所有 MAD 使用者使用 ib_create_send_mad() 分配其发送缓冲区，并将 DMA 映射移动到 MAD 层，以便可以在调用 send 之前（以及 MAD 层对发送缓冲区进行任何修改之后）完成此操作，可以解决此问题。 在非缓存一致性 PowerPC 440SPe 系统上进行测试
                        ib_mad_enforce_security -> IB/核心：在管理数据报上强制执行安全性，在创建和销毁 MAD 代理时分配和释放安全上下文。 该上下文用于控制对 PKey 的访问以及发送和接收 SMP。 发送或接收 MAD 时，检查代理是否有权访问端口子网前缀的 PKey。 在 SMI QP 的 MAD 和监听代理注册期间，检查调用进程是否有权访问管理子网并向 LSM 注册回调以获取策略更改通知。 当发生策略更改通知时，重新检查权限并设置一个标志，指示允许发送和接收 SMP。 发送和接收 MAD 时，如果代理位于 SMI QP 上，请检查代理是否有权访问 SMI。 由于安全策略可以更改，因此在创建代理时可能允许许可，但不再允许
                            rdma_protocol_ib
                            ib_security_pkey_access
                        ib_is_mad_class_rmpp
                        handle_outgoing_dr_smp
                        ib_mad_kernel_rmpp_agent
                        ib_send_rmpp_mad
                        ib_send_mad
                            ib_dma_map_single
                                ib_uses_virt_dma
                                dma_map_single
                            ib_post_send
                                .post_send = mlx5_ib_post_send_nodrain,
            or cma_connect_iw
    ucma_put_ctx



mlx5_ib_post_send_nodrain -> mlx5_ib_post_send
    begin_wqe
    ...




cma_ib_handler
switch (ib_event->event)
case IB_CM_REP_RECEIVED
    ib_send_cm_mra(cm_id, CMA_CM_MRA_SETTING, NULL, 0) -> IB/cma：为回复消息发送MRA，RDMA_CM的当前实现仅针对请求消息发送MRA（消息接收确认），而不针对响应消息发送MRA。 因此，连接的缓慢主动方可能会在延迟太长的情况下向被动方发送就绪消息，而被动方无法等待。 该补丁在收到响应消息时添加了对 ib_send_cm_mra() 的调用，从而告诉对方将服务超时修改为更大的值，是之前的 16 倍。 与请求情况一样，仅当重复响应到达时才会发送用于回复的 MRA
        ...


大页:
配置内核大页:
echo 16 >/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages





ibv_create_qp -> IB_USER_VERBS_CMD_CREATE_QP -> static int ib_uverbs_create_qp
    ...
    uobj_get_obj_read
    attr.event_handler = ib_uverbs_qp_event_handler
    ib_create_qp_user
        create_qp
            rdma_zalloc_drv_obj_numa
            rdma_restrack_new(&qp->res, RDMA_RESTRACK_QP)
             dev->ops.create_qp(qp, attr, udata) -> .create_qp = irdma_create_qp,
                ...
             ib_create_qp_security
             rdma_restrack_add(&qp->res)
        create_xrc_qp_user
    ib_qp_usecnt_inc
    uverbs_response


example of e810:
irdma_create_qp
    ...
    iwqp->q2_ctx_mem.va = dma_alloc_coherent
    irdma_alloc_rsrc
    rdma_protocol_roce
    irdma_sc_qp_init
        irdma_uk_qp_init
        irdma_get_encoded_wqe_size
    irdma_cqp_create_qp_cmd
        irdma_alloc_and_get_cqp_request
        cqp_info->cqp_cmd = IRDMA_OP_QP_CREATE
        status = irdma_handle_cqp_op(rf, cqp_request) -> irdma_sc_qp_create
            irdma_sc_cqp_get_next_send_wqe
            dma_wmb();
            set_64bit_val(wqe, 24, hdr)
            print_hex_dump_debug
            irdma_sc_cqp_post_sq
        irdma_put_cqp_request(&rf->cqp, cqp_request)
    iwqp->sig_all = init_attr->sq_sig_type == IB_SIGNAL_ALL_WR
    rdma_protocol_roce
    init_completion(&iwqp->free_qp)



l2b:
include/linux/byteorder/generic.h
cpu_to_be64 -> #define cpu_to_be64 __cpu_to_be64




samples/trace_events
samples/trace_events/trace-events-sample.c



ib_uverbs_mmap
    ret = ucontext->device->ops.mmap(ucontext, vma) ->  irdma_mmap

static const struct ib_device_ops irdma_dev_ops = {
	.owner = THIS_MODULE,
	.driver_id = RDMA_DRIVER_IRDMA,
	.uverbs_abi_ver = IRDMA_ABI_VER,

	.alloc_hw_port_stats = irdma_alloc_hw_port_stats, -> struct rdma_hw_stats
	.alloc_mr = irdma_alloc_mr, -> 注册 stag 以进行快速内存注册
	.alloc_mw = irdma_alloc_mw,
        stag = irdma_create_stag(iwdev)
            ret = irdma_alloc_rsrc(iwdev->rf, iwdev->rf->allocated_mrs, iwdev->rf->max_mr, &stag_index, &next_stag_index)
        err_code = irdma_hw_alloc_mw(iwdev, iwmr)
            cqp_info->cqp_cmd = IRDMA_OP_MW_ALLOC
	.alloc_pd = irdma_alloc_pd,
	.alloc_ucontext = irdma_alloc_ucontext,
	.create_cq = irdma_create_cq,
	.create_qp = irdma_create_qp,
	.dealloc_driver = irdma_ib_dealloc_device,
	.dealloc_mw = irdma_dealloc_mw,
	.dealloc_pd = irdma_dealloc_pd,
	.dealloc_ucontext = irdma_dealloc_ucontext,
	.dereg_mr = irdma_dereg_mr,
	.destroy_cq = irdma_destroy_cq,
		irdma_cq_wq_destroy(iwdev->rf, cq)
			cqp_info->cqp_cmd = IRDMA_OP_CQ_DESTROY
				irdma_sc_cq_destroy
					set_64bit_val(wqe, 8, (uintptr_t)cq >> 1)
					...
		irdma_sc_cleanup_ceqes(cq, ceq)
		irdma_cq_free_rsrc(iwdev->rf, iwcq)
	.destroy_qp = irdma_destroy_qp,
        if (iwqp->iwarp_state == IRDMA_QP_STATE_RTS)
            irdma_modify_qp_to_err(&iwqp->sc_qp)
        if (!iwqp->user_mode)
            if (iwqp->iwscq)
                irdma_clean_cqes(iwqp, iwqp->iwscq)
                    irdma_uk_clean_cq(&iwqp->sc_qp.qp_uk, ukcq)
                        do
                            cqe = cq->cq_base[cq_head].buf
                            if (polarity != temp)
                                break
                            dma_rmb()
                            if ((void *)(unsigned long)comp_ctx == q)
                                set_64bit_val(cqe, 8, 0)
                            cq_head = (cq_head + 1) % cq->cq_ring.size
                            if (!cq_head)
                                temp ^= 1;
                        while (true)
        wait_for_completion(&iwqp->free_qp)
        irdma_free_lsmm_rsrc(iwqp);
        irdma_cqp_qp_destroy_cmd(&iwdev->rf->sc_dev, &iwqp->sc_qp);
        irdma_remove_push_mmap_entries(iwqp);
        irdma_free_qp_rsrc(iwqp);
	.disassociate_ucontext = irdma_disassociate_ucontext,
	.get_dev_fw_str = irdma_get_dev_fw_str,
	.get_dma_mr = irdma_get_dma_mr,
	.get_hw_stats = irdma_get_hw_stats,
	.map_mr_sg = irdma_map_mr_sg,
        ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, irdma_set_page)
	.mmap = irdma_mmap,
	.mmap_free = irdma_mmap_free,
	.poll_cq = irdma_poll_cq,
	.post_recv = irdma_post_recv,
        
	.post_send = irdma_post_send,
        if (ib_wr->send_flags & IB_SEND_INLINE)
            err = irdma_uk_inline_send(ukqp, &info, false)
                quanta = qp->wqe_ops.iw_inline_data_size_to_quanta(total_size)
                wqe = irdma_qp_get_next_send_wqe(qp, &wqe_idx, quanta, total_size, info)
                irdma_clr_wqes(qp, wqe_idx)
                qp->wqe_ops.iw_copy_inline_data((u8 *)wqe, op_info->sg_list, op_info->num_sges, qp->swqe_polarity)
        else
            err = irdma_uk_send(ukqp, &info, false)
	.query_device = irdma_query_device,
	.query_port = irdma_query_port,
	.query_qp = irdma_query_qp,
	.reg_user_mr = irdma_reg_user_mr,
	.reg_user_mr_dmabuf = irdma_reg_user_mr_dmabuf,
        struct ib_umem_dmabuf *umem_dmabuf
        umem_dmabuf = ib_umem_dmabuf_get_pinned(pd->device, start, len, fd, access)
            umem_dmabuf = ib_umem_dmabuf_get(device, offset, size, fd, access, &ib_umem_dmabuf_attach_pinned_ops)
                dmabuf = dma_buf_get(fd)
                umem_dmabuf = kzalloc(sizeof(*umem_dmabuf), GFP_KERNEL)
                umem_dmabuf->attach = dma_buf_dynamic_attach(dmabuf, device->dma_device, ops, umem_dmabuf)
            dma_resv_lock(umem_dmabuf->attach->dmabuf->resv, NULL)
            err = dma_buf_pin(umem_dmabuf->attach)
            err = ib_umem_dmabuf_map_pages(umem_dmabuf)
            dma_resv_unlock(umem_dmabuf->attach->dmabuf->resv)
        iwmr = irdma_alloc_iwmr(&umem_dmabuf->umem, pd, virt, IRDMA_MEMREG_TYPE_MEM)
        err = irdma_reg_user_mr_type_mem(iwmr, access, true)
	.rereg_user_mr = irdma_rereg_user_mr,
	.req_notify_cq = irdma_req_notify_cq,
        bool promo_event = false -> RDMA/irdma：如果此 CQ 没有 CE，则不要将 CQ 事件上报两次以上。如果允许应用程序在硬件未生成此 CQ 的新 CE 时将 CQ 事件上报两次以上，则完成事件 (CE) 会丢失。检查 CQ 是否已装备，如果没有，则为任何事件装备 CQ，否则仅在请求最后一个装备事件时才提升为任何事件装备 CQ
        cq_notify = notify_flags == IB_CQ_SOLICITED ? IRDMA_CQ_COMPL_SOLICITED : IRDMA_CQ_COMPL_EVENT
        Only promote to arm the CQ for any event if the last arm event was solicited
        if (!atomic_cmpxchg(&iwcq->armed, 0, 1) || promo_event) -> atomic_cmpxchg 的作用是比较并交换。它将目标内存中的值与预期值进行比较：  如果目标值等于预期值，则将其替换为一个新的值。 如果不相等，则保持目标值不变，并返回当前值。 这是一种乐观锁机制，可以用来实现无锁数据结构和操作, 如果当前状态没有ARM, 则原子设置为ARMED, 或promo_event, 则向HW请求事件上报
            iwcq->last_notify = cq_notify
            irdma_uk_cq_request_notification(ukcq, cq_notify)
                get_64bit_val(cq->shadow_area, 32, &temp_val)
                arm_seq_num = (u8)FIELD_GET(IRDMA_CQ_DBSA_ARM_SEQ_NUM, temp_val)
                arm_seq_num++;
                if (cq_notify == IRDMA_CQ_COMPL_EVENT)
                    arm_next = 1;
                set_64bit_val(cq->shadow_area, 32, temp_val)
                writel(cq->cq_id, cq->cqe_alloc_db)
        if ((notify_flags & IB_CQ_REPORT_MISSED_EVENTS) && (!irdma_cq_empty(iwcq) || !list_empty(&iwcq->cmpl_generated))) -> 如果收到错过CQ的完成事件, 且CQ队列有完成条目, 且为了刷新CQ生成的完成通道不为空, 则返回1 -> https://github.com/torvalds/linux/commit/81091d7696ae71627ff80bbf2c6b0986d2c1cce3, RDMA/irdma：添加 SW 机制以在 QP 处于错误状态后生成错误 HW 刷新的完成，这不可靠。这可能导致应用程序挂起等待未完成 WR 的完成。在 QP 修改为错误后，实现 SW 机制以生成任何未完成 WR 的完成。这是通过在 QP 修改为错误并执行 HW 刷新后启动延迟工作器来实现的。工作器将生成完成，并在轮询 CQ 时返回给应用程序。此机制仅适用于内核应用程序
	.resize_cq = irdma_resize_cq,
	INIT_RDMA_OBJ_SIZE(ib_pd, irdma_pd, ibpd),
	INIT_RDMA_OBJ_SIZE(ib_ucontext, irdma_ucontext, ibucontext),
	INIT_RDMA_OBJ_SIZE(ib_ah, irdma_ah, ibah),
	INIT_RDMA_OBJ_SIZE(ib_cq, irdma_cq, ibcq),
	INIT_RDMA_OBJ_SIZE(ib_mw, irdma_mr, ibmw),
	INIT_RDMA_OBJ_SIZE(ib_qp, irdma_qp, ibqp),
};



rdma 5.4.18



nvme_rdma_queue_rq
    nvme_rdma_map_data
        nvme_rdma_map_sg_fr
            ib_map_mr_sg() - 映射 dma 映射 SG 列表的最大前缀并将其设置为内存区域。@mr：内存区域@sg：dma 映射散射列表@sg_nents：sg 中的条目数@sg_offset：sg 中的偏移量（以字节为单位）@page_size：页面向量所需的页面大小* 约束：* - 第一个 sg 元素可以有一个偏移量。- 每个 sg 元素必须与 page_size 对齐或与前一个元素几乎相邻。如果 sg 元素具有不连续的偏移量，则映射前缀将不包括它。- 最后一个 sg 元素的长度可以小于 page_size。- 如果 sg_nents 总字节长度超过了 mr max_num_sge * page_size，那么将只映射 max_num_sg 条目。 - 如果 MR 分配了 IB_MR_TYPE_SG_GAPS 类型，则这些约束均不成立，并且 page_size 参数将被忽略。 * 返回映射到内存区域的 sg 元素的数量。 * 成功完成后，内存区域即可注册。/
                mr->device->ops.map_mr_sg(mr, sg, sg_nents, sg_offset)
            to510




struct bundle_priv
IB/uverbs：为 uverbs_attr_bundle 提供实现私有内存，这已经作为匿名“ctx”结构存在，但这并不是真正有用的形式。 将此结构提升到bundle_priv并重新设计内部内容以使用它。 将一堆处理内部状态移入 priv 并减少函数参数的过度使用



强制实现的函数:
static void ib_device_check_mandatory(struct ib_device *device)
{
#define IB_MANDATORY_FUNC(x) { offsetof(struct ib_device_ops, x), #x }
	static const struct {
		size_t offset;
		char  *name;
	} mandatory_table[] = {
		IB_MANDATORY_FUNC(query_device),
		IB_MANDATORY_FUNC(query_port),
		IB_MANDATORY_FUNC(alloc_pd),
		IB_MANDATORY_FUNC(dealloc_pd),
		IB_MANDATORY_FUNC(create_qp),
		IB_MANDATORY_FUNC(modify_qp),
		IB_MANDATORY_FUNC(destroy_qp),
		IB_MANDATORY_FUNC(post_send),
		IB_MANDATORY_FUNC(post_recv),
		IB_MANDATORY_FUNC(create_cq),
		IB_MANDATORY_FUNC(destroy_cq),
		IB_MANDATORY_FUNC(poll_cq),
		IB_MANDATORY_FUNC(req_notify_cq),
		IB_MANDATORY_FUNC(get_dma_mr),
		IB_MANDATORY_FUNC(reg_user_mr),
		IB_MANDATORY_FUNC(dereg_mr),
		IB_MANDATORY_FUNC(get_port_immutable)
	};




为该端口需要由内核支持的各种功能定义位
/* Define bits for the various functionality this port needs to be supported by
 * the core.
 */
/* Management                           0x00000FFF */
#define RDMA_CORE_CAP_IB_MAD            0x00000001
#define RDMA_CORE_CAP_IB_SMI            0x00000002
#define RDMA_CORE_CAP_IB_CM             0x00000004
#define RDMA_CORE_CAP_IW_CM             0x00000008
#define RDMA_CORE_CAP_IB_SA             0x00000010
#define RDMA_CORE_CAP_OPA_MAD           0x00000020

/* Address format                       0x000FF000 */
#define RDMA_CORE_CAP_AF_IB             0x00001000
#define RDMA_CORE_CAP_ETH_AH            0x00002000
#define RDMA_CORE_CAP_OPA_AH            0x00004000
#define RDMA_CORE_CAP_IB_GRH_REQUIRED   0x00008000

/* Protocol                             0xFFF00000 */
#define RDMA_CORE_CAP_PROT_IB           0x00100000
#define RDMA_CORE_CAP_PROT_ROCE         0x00200000
#define RDMA_CORE_CAP_PROT_IWARP        0x00400000
#define RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP 0x00800000
#define RDMA_CORE_CAP_PROT_RAW_PACKET   0x01000000
#define RDMA_CORE_CAP_PROT_USNIC        0x02000000


iw40


static int irdma_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)
    rdma_entry = rdma_user_mmap_entry_get(&ucontext->ibucontext, vma)
    switch (entry->mmap_flag)
    case IRDMA_MMAP_IO_NC:
        ret = rdma_user_mmap_io(context, vma, pfn, PAGE_SIZE, pgprot_noncached(vma->vm_page_prot), rdma_entry) -> Map IO memory into a process
            lockdep_assert_held(&ufile->device->disassociate_srcu)
            io_remap_pfn_range(vma, vma->vm_start, pfn, size, prot) -> 将I/O内存映射到用户空间 -> RDMA/core：将核心内容从 ib_uverbs 移至 ib_core 将驱动程序调用的与 umap 相关的功能移至将在 ib_core 中链接的新文件。这是稍后使 ib_uverbs 成为可选的第一步。vm_ops 现在在 ib_uverbs_mmap 而不是 priv_init 中初始化，以避免必须移动所有 rdma_umap 函数
            rdma_umap_priv_init(priv, vma, entry)
				list_add(&priv->list, &ufile->umaps)





mmap
SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,
    ksys_mmap_pgoff
        vm_mmap_pgoff
            do_mmap
                addr = get_unmapped_area(file, addr, len, pgoff, flags)
                switch (flags & MAP_TYPE)
                mmap_region
                    count_vma_pages_range
                    call_mmap
                        file->f_op->mmap(file, vma) -> or filemap_fault -> or xtrdma_mmap by cmd_fd fs
                    khugepaged_enter_vma

install module:
make modules_install

install kernel
make install

will install three file to /boot
initramfs-5.16.9.img
System.map-5.16.9
vmlinuz-5.16.9

Update grub config
centos
$ sudo grub2-mkconfig -o /boot/grub2/grub.cfg
$ sudo grubby --set-default /boot/vmlinuz-5.16.9
grubby --info=ALL | more
grubby --default-index
grubby --default-kernel

ubuntu:
$ sudo update-initramfs -c -k 5.16.9
$ sudo update-grub


mkinitramfs 
qemu-system-x86_64 \
  -hda ${disk_img} \
  -enable-kvm \
  -append "root=/dev/sda3" \
  -kernel /kernel/src/path/arch/x86/boot/bzImage \
  -initrd ${initrd-file-path} \
  -cpu host \
  -m 8G \
  -smp 8




server: 服务端创建事件通道,创建通信标识ID, 启动RDMA监听
rdma_create_event_channel <- vrb_eq_open <- fi_eq_open
rdma_create_id <- vrb_create_ep <- vrb_open_ep <- fi_endpoint, librdmacm/cma.c -> rdma_create_event_channel HG创建端点 na_ofi_basic_ep_open
rdma_listen -> fi_listen -> .listen = vrb_pep_listen -> vrb_pep_listen -> rdma_listen, na_ofi_basic_ep_open -> fi_enable -> rxm_ep_ctrl -> rxm_start_listen -> fi_listen

if (ofi_epoll_add(_eq->epollfd, _eq->channel->fd, OFI_EPOLL_IN, NULL)) -> 将rdma事件通道的fd关联到eq的epollfd

client:客户端创建事件通道,创建通信标识ID, 解析服务端地址, 发送数据时, 获取连接, 解析路由
rdma_create_event_channel
rdma_create_id
rdma_resolve_addr -> RDMA_CM_EVENT_ADDRESS_RESOLVED ->  rxm_open_conn -> fi_endpoint (vrb_open_ep) -> rdma_resolve_addr HG -> HG_Trigger -> hg_op_id->callback(&hg_cb_info) 查询地址设置的回调 lookup_callback ->  HG_Forward -> NA_Msg_send_unexpected -> fi_senddata -> rxm_get_conn -> fi_endpoint -> vrb_open_ep -> vrb_create_ep -> rdma_resolve_addr
rdma_resolve_route -> RDMA_CM_EVENT_ROUTE_RESOLVED -> rxm_send_connect -> fi_connect -> rdma_resolve_route 也是HG发送的时候建立连接
------------------ 
分配RDMA结构(服务端和客户端对等, 均要执行), 查询网卡, 分配保护域, 创建完成通道,完成队列, 通知完成队列准备好接收完成事件, 创建队列对, 注册内存
ibv_query_device <- fi_getinfo -> vrb_getinfo -> ibv_query_device
ibv_alloc_pd <- fi_domain -> rxm_domain_open -> ibv_alloc_pd
ibv_create_comp_channel -> na_ofi_eq_open -> fi_cq_open -> vrb_cq_open -> ibv_create_comp_channel
ibv_create_cq -> na_ofi_eq_open -> fi_cq_open -> vrb_cq_open -> ibv_create_cq
ibv_req_notify_cq -> na_ofi_poll_try_wait -> fi_trywait -> vrb_trywait -> vrb_cq_trywait
rdma_create_qp -> na_ofi_context_create -> fi_enable -> rdma_create_qp
ibv_reg_mr -> NA_Mem_register -> na_ofi_mem_register -> fi_mr_regv -> ibv_reg_mr
------------------
轮训完成队列
ibv_poll_cq -> na_ofi_msg_send_unexpected -> fi_senddata -> fi_send -> vrb_flush_cq -> ibv_poll_cq

接收端提前往接收队列放置工作请求WR
ibv_post_recv -> rxm_open_conn -> ibv_post_recv | na_ofi_tag_recv, na_ofi_msg_multi_recv -> fi_trecv -> ibv_post_recv

客户端与服务端建立连接
rdma_connect -> server -> RDMA_CM_EVENT_CONNECT_REQUEST, -> fi_senddata -> rxm_get_conn -> rdma_connect

server:
case RDMA_CM_EVENT_CONNECT_REQUEST
------------------ 
分配RDMA结构
ibv_query_device
ibv_alloc_pd
ibv_create_comp_channel
ibv_create_cq
ibv_req_notify_cq
rdma_create_qp
ibv_reg_mr
------------------
ibv_post_recv
rdma_accept
RDMA_CM_EVENT_ESTABLISHED
ibv_post_send

clinet: 客户端发送非预期消息
RDMA_CM_EVENT_ESTABLISHED
ibv_post_send -> na_ofi_msg_send_unexpected -> ibv_post_send

销毁资源
server:
rdma_disconnect

ibv_dereg_mr
ibv_destroy_cq
ibv_destroy_comp_channel
rdma_destroy_qp

rdma_destroy_id
rdma_destroy_event_channel

client:
rdma_disconnect

ibv_dereg_mr
ibv_destroy_cq
ibv_destroy_comp_channel
rdma_destroy_qp

rdma_destroy_id
rdma_destroy_event_channel



以下是部分接口详解:
创建事件通道:
rdma_create_event_channel - 打开用于报告通信事件的通道。 描述：异步事件通过事件通道上报给用户。 每个事件通道映射到一个文件描述符。 注意：所有创建的事件通道必须通过调用 rdma_destroy_event_channel 销毁。 用户应调用 rdma_get_cm_event 来检索事件通道上的事件。 另请参见：rdma_get_cm_event、rdma_destroy_event_channel, 流程: 查询获取所有IB设备，存放在cma_dev_array全局数组中；检测是否支持AF_IB协议, 打开CM的fd, 返回事件
struct rdma_event_channel *rdma_create_event_channel(void)
  ucma_init()
  channel->fd = open_cdev(dev_name, dev_cdev) -> 打开fd /dev/infiniband/rdma_cm
  返回通道



分配通信标识
int rdma_create_id(struct rdma_event_channel *channel, struct rdma_cm_id **id, void *context, enum rdma_port_space ps)
cmd = UCMA_CMD_CREATE_ID
ret = write(id_priv->id.channel->fd, &cmd, sizeof cmd) -> 通知内核
ucma_insert_id(id_priv)
  idm_set -> librdmacm：定义通过 RDMA 接口 (rsockets) 的流式传输，引入了一组新的 API，支持 RDMA 设备上的字节流接口。 新接口与套接字匹配，只是所有函数调用都以“r”为前缀。 定义了以下函数： rsocket rbind、rlisten、raccept、rconnect rshutdown、rclose rrecv、rrecvfrom、rrecvmsg、rread、rreadv rsend、rsendto、rsendmsg、rwrite、rwritev rpoll、rselect rgetpeername、rgetsockname rsetsockopt、rgetsockopt、rfcntl 函数采用相同的方法 参数与用于套接字的参数相同。 目前支持以下功能和标志： PF_INET、PF_INET6、SOCK_STREAM、IPPROTO_TCP MSG_DONTWAIT、MSG_PEEK SO_REUSEADDR、TCP_NODELAY、SO_ERROR、SO_SNDBUF、SO_RCVBUF O_NONBLOCK rpoll 调用支持轮询 rsockets 和普通 fd, 
  index_map(二级指针): 索引映射 - 将结构与索引关联起来。 同步必须由调用者提供。 调用者必须通过将索引映射设置为 0 来初始化它
  提供一组索引操作接口, 设置,插入(idx_insert),增长(idx_grow),替换,移除,清理等
  rsocket是附在rdma_cm库中的一个子模块，提供了完全类似于socket接口的rdma调用
  对于rdma编程，目前主流实现是利用rdma_cm来建立连接，然后利用verbs来传输数据。  rdma_cm和ibverbs分别会创建一个fd，这两个fd的分工不同。rdma_cm fd主要用于通知建连相关的事件，verbs fd则主要通知有新的cqe发生。当直接对rdma_cm fd进行poll/epoll监听时，此时只能监听到POLLIN事件，这意味着有rdma_cm事件发生。当直接对verbs fd进行poll/epoll监听时，同样只能监听到POLLIN事件，这意味着有新的cqe  作者：异客z 链接：https://www.jianshu.com/p/4d71f1c8e77c



监听客户端的连接请求, 给内核发送监听命令, 查询地址/路由
int rdma_listen(struct rdma_cm_id *id, int backlog)
cmd = UCMA_CMD_LISTEN
write(id->channel->fd, &cmd, sizeof cmd)



解析地址
int rdma_resolve_addr(struct rdma_cm_id *id, struct sockaddr *src_addr, struct sockaddr *dst_addr, int timeout_ms)



ibv_query_device
  mlx5_query_device_ex
  IB_USER_VERBS_EX_CMD_QUERY_DEVICE


发起连接请求
int rdma_connect(struct rdma_cm_id *id, struct rdma_conn_param *conn_param)
ucma_valid_param
CMA_INIT_CMD(&cmd, sizeof cmd, CONNECT)
cmd = UCMA_CMD_CONNECT -> kernel -> static ssize_t (*ucma_cmd_table[]) -> ucma_connect
ucma_copy_conn_param_to_kern
  dst->retry_count = 7;   // 无限次重试
    dst->rnr_retry_count = 7; // 无限次重试
ucma_copy_ece_param_to_kern_req
ret = write(id->channel->fd, &cmd, sizeof cmd)






often use verbs api:
ibv_advise_mr
ibv_alloc_dm
ibv_open_device
ibv_get_device_list
ibv_get_device_guid
ibv_query_device_ex
ibv_get_device_name
ibv_req_notify_cq
ibv_query_gid
ibv_memcpy_to_dm
ibv_get_cq_event
ibv_start_poll
ibv_end_poll
ibv_next_poll
ibv_wc_read_completion_ts
ibv_ack_cq_events
ibv_free_device_list
ibv_cq_ex_to_cq
ibv_create_qp
ibv_modify_qp

ibv_modify_qp
.modify_qp = irdma_umodify_qp,
    ibv_cmd_modify_qp_ex
        copy_modify_qp_fields
            ...
            cmd->retry_cnt = attr->retry_cnt
            ...
        execute_cmd_write_ex IB_USER_VERBS_EX_CMD_MODIFY_QP -> to kernel
    irdma_mmap
    or ibv_cmd_modify_qp



ibv_create_qp
.create_qp = irdma_ucreate_qp,
...
info.abi_ver = iwvctx->abi_ver -> 设置参数
...
irdma_uk_calc_depth_shift_sq -> 提供商/irdma：允许准确报告 QP 最大发送/接收 WR ，目前，在创建 QP 期间从用户空间发送的属性 cap.max_send_wr 和 cap.max_recv_wr 是提供商计算的 SQ/RQ 深度，而不是从应用程序传递的原始值。 这会禁止在内核中计算该 QP 的 max_send_wr 和 max_recv_wr 的准确值，该值与用户创建 QP 中返回的值相匹配。 此外，这些功能还需要在查询 QP 中从驱动程序报告。 通过扩展 ABI 添加支持，以允许从用户空间传递原始 cap.max_send_wr 和 cap.max_recv_wr，同时保持对旧方案的兼容性。 添加新的助手来协助完成此操作：irdma_uk_calc_depth_shift_sq、irdma_uk_calc_depth_shift_rq
    irdma_get_wqe_shift
    irdma_get_sqdepth
iwuqp = memalign(1024, sizeof(*iwuqp)) -> 在GNU系统中，malloc或realloc返回的内存块地址都是8的倍数（如果是64位系统，则为16的倍数）。如果你需要更大的粒度，请使用memalign或valloc
irdma_uk_calc_depth_shift_rq
irdma_vmapped_qp(iwuqp, pd, attr, &info, iwvctx->legacy_mode)
    irdma_alloc_hw_buf
    reg_mr_cmd.reg_type = IRDMA_MEMREG_TYPE_QP
    ibv_cmd_reg_mr
        execute_cmd_write(pd->context, IB_USER_VERBS_CMD_REG_MR -> to kernel
    ibv_cmd_create_qp
irdma_uk_qp_init




ibv_open_device -> LATEST_SYMVER_FUNC(ibv_open_device -> verbs_open_device
    verbs_get_device
    cmd_fd = open_cdev -> verbs：启用 verbs_open_device() 以在非 sysfs 设备上工作，从 mlx5 开始，启用 verbs_open_device() 通过 VFIO 在非 sysfs 设备上工作。 verbs_sysfs_dev 上的任何其他 API 都应该彻底失败
    verbs_device->ops->alloc_context -> mlx5_alloc_context -> verbs：始终分配 verbs_context，现在所有内容都在一棵树中，我们可以修改旧版 init_context 路径，通过在所有提供程序的包装结构中将 ibv_context 交换为 verbs_context 来始终分配 verbs_context。 为了保持提供者差异最小，这个补丁同时做了几件事： - 引入 verbs_init_and_alloc_context() 宏。 这会为每个驱动程序分配、清零并初始化 verbs_context。 值得注意的是，这个新宏在失败时根据需要正确设置 errno。 - 从所有驱动程序、calloc、malloc、memset、cmd_fd 和设备分配中删除样板文件 - 与 verbs_init 方案一起必然出现 verbs_uninit 方案，该方案将 uninit 调用降低到提供者而不是公共代码中。 这使我们能够在 init 错误路径上正确地 uninit。 总之，这遵循我们在内核中看到的相当成功的模式，用于对子系统进行驱动程序初始化。 此外，这会将 ibv_cmd_get_context 更改为接受 verbs_context，因为大多数调用者现在都提供该内容，这使得差异较小。 这使得整个流程更加一致，并且可以让我们消除 init_context 流程
        mlx5_init_context
            verbs_init_and_alloc_context -> _verbs_init_and_alloc_context
                verbs_init_context
                    ibverbs_device_hold
                    verbs_set_ops(context_ex, &verbs_dummy_ops) -> rdma verbs操作 -> 在上下文中设置 -> 如果更改，则必须更改 PRIVATE IBVERBS_PRIVATE_ 符号。 这是驱动程序可以支持的每个操作的联合。 如果向此结构添加新元素，则 verbs_dummy_ops 也必须更新。 保持排序
                        SET_OP -> 设置一系列操作
                        ...
                    use_ioctl_write = has_ioctl_write(context)
            mlx5_open_debug_file
            mlx5_set_debug_mask
            single_threaded_app
            get_uar_info
                get_total_uuars
                get_num_low_lat_uuars
        mlx5_cmd_get_context
            ibv_cmd_get_context
                ...
                execute_write_bufs(context, IB_USER_VERBS_CMD_GET_CONTEXT
        mlx5_set_context
            adjust_uar_info
            cl_qmap_init
            mlx5_mmap
            mlx5_read_env
            verbs_set_ops(v_ctx, &mlx5_ctx_common_ops)
            mlx5_query_device_ctx
                get_hca_general_caps
                    mlx5dv_devx_general_cmd MLX5_CMD_OP_QUERY_HCA_CAP
                ibv_cmd_query_device_any
                    execute_cmd_write_ex IB_USER_VERBS_EX_CMD_QUERY_DEVICE
                    execute_cmd_write(context, IB_USER_VERBS_CMD_QUERY_DEVICE -> 转到内核态
            mlx5_set_singleton_nc_uar
    set_lib_ops
    ibv_cmd_alloc_async_fd





rdma verbs ops
const struct verbs_context_ops verbs_dummy_ops = {
    advise_mr,
    alloc_dm,
    alloc_mw,
    alloc_null_mr,
    alloc_parent_domain,
    alloc_pd,
    alloc_td,
    async_event,
    attach_counters_point_flow,
    attach_mcast,
    bind_mw,
    close_xrcd,
    cq_event,
    create_ah,
    create_counters,
    create_cq,
    create_cq_ex,
    create_flow,
    create_flow_action_esp,
    create_qp,
    create_qp_ex,
    create_rwq_ind_table,
    create_srq,
    create_srq_ex,
    create_wq,
    dealloc_mw,
    dealloc_pd,
    dealloc_td,
    dereg_mr,
    destroy_ah,
    destroy_counters,
    destroy_cq,
    destroy_flow,
    destroy_flow_action,
    destroy_qp,
    destroy_rwq_ind_table,
    destroy_srq,
    destroy_wq,
    detach_mcast,
    free_context,
    free_dm,
    get_srq_num,
    import_dm,
    import_mr,
    import_pd,
    modify_cq,
    modify_flow_action_esp,
    modify_qp,
    modify_qp_rate_limit,
    modify_srq,
    modify_wq,
    open_qp,
    open_xrcd,
    poll_cq,
    post_recv,
    post_send,
    post_srq_ops,
    post_srq_recv,
    query_device_ex,
    query_ece,
    query_port,
    query_qp,
    query_qp_data_in_order,
    query_rt_values,
    query_srq,
    read_counters,
    reg_dm_mr,
    reg_dmabuf_mr,
    reg_mr,
    req_notify_cq,
    rereg_mr,
    resize_cq,
    set_ece,
    unimport_dm,
    unimport_mr,
    unimport_pd,
};

设置rdma verbs操作:
static const struct verbs_context_ops mlx5_ctx_common_ops = {
    .query_port    = mlx5_query_port,
    .alloc_pd      = mlx5_alloc_pd,
    .async_event   = mlx5_async_event,
    .dealloc_pd    = mlx5_free_pd,
    .reg_mr	       = mlx5_reg_mr,
    .reg_dmabuf_mr = mlx5_reg_dmabuf_mr,
    .rereg_mr      = mlx5_rereg_mr,
    .dereg_mr      = mlx5_dereg_mr,
    .alloc_mw      = mlx5_alloc_mw,
    .dealloc_mw    = mlx5_dealloc_mw,
    .bind_mw       = mlx5_bind_mw,
    .create_cq     = mlx5_create_cq,
    .poll_cq       = mlx5_poll_cq,
    .req_notify_cq = mlx5_arm_cq,
    .cq_event      = mlx5_cq_event,
    .resize_cq     = mlx5_resize_cq,
    .destroy_cq    = mlx5_destroy_cq,
    .create_srq    = mlx5_create_srq,
    .modify_srq    = mlx5_modify_srq,
    .query_srq     = mlx5_query_srq,
    .destroy_srq   = mlx5_destroy_srq,
    .post_srq_recv = mlx5_post_srq_recv,
    .create_qp     = mlx5_create_qp,
    .query_qp      = mlx5_query_qp,
    .modify_qp     = mlx5_modify_qp,
    .destroy_qp    = mlx5_destroy_qp,
    .post_send     = mlx5_post_send,
    .post_recv     = mlx5_post_recv,
    .create_ah     = mlx5_create_ah,
    .destroy_ah    = mlx5_destroy_ah,
    .attach_mcast  = mlx5_attach_mcast,
    .detach_mcast  = mlx5_detach_mcast,

    .advise_mr = mlx5_advise_mr,
    .alloc_dm = mlx5_alloc_dm,
    .alloc_parent_domain = mlx5_alloc_parent_domain,
    .alloc_td = mlx5_alloc_td,
    .attach_counters_point_flow = mlx5_attach_counters_point_flow,
    .close_xrcd = mlx5_close_xrcd,
    .create_counters = mlx5_create_counters,
    .create_cq_ex = mlx5_create_cq_ex,
    .create_flow = mlx5_create_flow,
    .create_flow_action_esp = mlx5_create_flow_action_esp,
    .create_qp_ex = mlx5_create_qp_ex,
    .create_rwq_ind_table = mlx5_create_rwq_ind_table,
    .create_srq_ex = mlx5_create_srq_ex,
    .create_wq = mlx5_create_wq,
    .dealloc_td = mlx5_dealloc_td,
    .destroy_counters = mlx5_destroy_counters,
    .destroy_flow = mlx5_destroy_flow,
    .destroy_flow_action = mlx5_destroy_flow_action,
    .destroy_rwq_ind_table = mlx5_destroy_rwq_ind_table,
    .destroy_wq = mlx5_destroy_wq,
    .free_dm = mlx5_free_dm,
    .get_srq_num = mlx5_get_srq_num,
    .import_dm = mlx5_import_dm,
    .import_mr = mlx5_import_mr,
    .import_pd = mlx5_import_pd,
    .modify_cq = mlx5_modify_cq,
    .modify_flow_action_esp = mlx5_modify_flow_action_esp,
    .modify_qp_rate_limit = mlx5_modify_qp_rate_limit,
    .modify_wq = mlx5_modify_wq,
    .open_qp = mlx5_open_qp,
    .open_xrcd = mlx5_open_xrcd,
    .post_srq_ops = mlx5_post_srq_ops,
    .query_device_ex = mlx5_query_device_ex,
    .query_ece = mlx5_query_ece,
    .query_rt_values = mlx5_query_rt_values,
    .read_counters = mlx5_read_counters,
    .reg_dm_mr = mlx5_reg_dm_mr,
    .alloc_null_mr = mlx5_alloc_null_mr,
    .free_context = mlx5_free_context,
    .set_ece = mlx5_set_ece,
    .unimport_dm = mlx5_unimport_dm,
    .unimport_mr = mlx5_unimport_mr,
    .unimport_pd = mlx5_unimport_pd,
    .query_qp_data_in_order = mlx5_query_qp_data_in_order,
};



代码路径: rdma-core, libibverbs/device.c
ibv_get_device_list
    ibverbs_get_device_list -> verbs: 刷新缓存的 ibv_device 列表 问题 ======== 目前，libibverbs 仅在第一次调用 ibv_get_device_list 时构建缓存的 ibv_device 列表，因此无论硬件是否发生变化，该列表都不会更新。 系统。 解决方案======== 修改 ibv_get_device_list() 的实现，以便连续的调用将以与今天相同的方式重新扫描 sysfs，以便每次创建一个新的 ibv_device 列表。 为此，将缓存的设备列表更改为真正的链表而不是动态数组。 我们如何识别新设备​============================= 根据 /sys/class/infiniband_verbs/ 的时间戳创建来识别同一设备 uverbs%d/ibdev。 我们使用 stat 系统调用获取文件状态，并使用 st_mtime 字段来实现此目的。 当我们重新扫描 sysfs 设备时，我们会检查每个 sysfs 设备是否已经在上次扫描中，如果没有，则分配新的 ibv_device 并将其添加到缓存设备列表中。 本系列的下一个补丁处理设备不再使用的情况。 注意：此补丁根据上面 verbs_device 结构体注释中的要求更改了 IBVERBS_PRIVATE 符号
        find_sysfs_devs_nl -> verbs：使用 netlink 来发现 uverbs 设备而不是 sysfs，netlink 查询为我们提供了 ibdev idx，它对于设备来说大多是唯一的，并且在设备重命名时充当稳定的 id。 如果在 verbs 用户操作期间重命名设备，这会使 verbs 更加健壮。 此外，netlink 仅返回在进程的网络命名空间中实际可见的设备，从而简化了发现过程
            rdmanl_socket_alloc
                nl_socket_alloc
                nl_socket_disable_auto_ack
                nl_socket_disable_msg_peek
                nl_connect(nl, NETLINK_RDMA) -> rdma：允许按需加载 NETLINK_RDMA，提供模块别名，以便如果用户空间打开 RDMA 的 netlink 套接字，则会自动加载内核支持
            rdmanl_get_devices find_sysfs_devs_nl_cb
                nl_send_simple RDMA_NL_GET_TYPE(RDMA_NL_NLDEV, RDMA_NLDEV_CMD_GET) NLM_F_DUMP -> nldev_get_doit
                nl_socket_modify_err_cb
                nl_socket_modify_cb
                nl_recvmsgs_default
            find_uverbs_nl find_uverbs_sysfs try_access_device
                rdmanl_get_chardev(nl, sysfs_dev->ibdev_idx, "uverbs", find_uverbs_nl_cb
                    nlmsg_alloc_simple RDMA_NLDEV_CMD_GET_CHARDEV
                    ...
                    check_snprintf(path, sizeof(path), "%s/device/infiniband_verbs",
                    setup_sysfs_uverbs
                        abi_version
                    ...
                    stat(devpath, &cdev_stat)
            nl_socket_free
        find_sysfs_devs
            %s/class/infiniband_verbs
            ibv_read_sysfs_file_at(uv_dirfd, "ibdev", 
        check_abi_version
            "class/infiniband_verbs/abi_version"
        try_all_drivers
            try_drivers
                match_driver_id -> VERBS_MATCH_SENTINEL -> 动词：提供通用代码以将提供程序与内核设备进行匹配 根据表检查 PCI 设备基本上在每个驱动程序中都是重复的。 遵循内核的模式，并将匹配表附加到 verbs_device_ops 驱动程序入口点，该入口点描述提供程序可以处理的所有内核设备，并使核心代码与该表匹配。 驱动程序获取一个指向与分配函数中匹配的表条目的指针。 此实现基于模式别名，而不是读取 PCI 特定供应商和设备文件。 modalias 让我们支持 ACPI 和 OF 提供程序，并提供了一个简单的路径，使提供程序根据其支持的 modalias 字符串（如内核）进行需求加载
                try_driver
                    match_device
                    alloc_device -> mlx5_device_alloc
                    dev->transport_type = IBV_TRANSPORT_IB -> 传输类型
                    ...
        load_drivers
            dlhandle = dlopen(so_name, RTLD_NOW)
    ibverbs_device_hold




query_device_ex -> mlx5_query_device_ex
    



.alloc_context = irdma_ualloc_context
    verbs_init_and_alloc_context
    verbs_set_ops(&iwvctx->ibv_ctx, &irdma_uctx_ops)
    irdma_mmap
        mmap
        ibv_dontfork_range
            ibv_madvise_range(base, size, MADV_DONTFORK)
    irdma_ualloc_pd
        ibv_cmd_alloc_pd




static const struct verbs_context_ops irdma_uctx_ops = {
    .alloc_mw = irdma_ualloc_mw,
    .alloc_pd = irdma_ualloc_pd,
    .attach_mcast = irdma_uattach_mcast,
    .bind_mw = irdma_ubind_mw,
    .cq_event = irdma_cq_event,
    .create_ah = irdma_ucreate_ah,
    .create_cq = irdma_ucreate_cq,
    .create_cq_ex = irdma_ucreate_cq_ex,
    .create_qp = irdma_ucreate_qp,
    .dealloc_mw = irdma_udealloc_mw,
    .dealloc_pd = irdma_ufree_pd,
    .dereg_mr = irdma_udereg_mr,
    .destroy_ah = irdma_udestroy_ah,
    .destroy_cq = irdma_udestroy_cq,
    .destroy_qp = irdma_udestroy_qp,
    .detach_mcast = irdma_udetach_mcast,
    .modify_qp = irdma_umodify_qp,
    .poll_cq = irdma_upoll_cq,
    .post_recv = irdma_upost_recv,
    .post_send = irdma_upost_send,
    .query_device_ex = irdma_uquery_device_ex,
    .query_port = irdma_uquery_port,
    .query_qp = irdma_uquery_qp,
    .reg_dmabuf_mr = irdma_ureg_mr_dmabuf,
    .reg_mr = irdma_ureg_mr,
    .rereg_mr = irdma_urereg_mr,
    .req_notify_cq = irdma_uarm_cq,
    .resize_cq = irdma_uresize_cq,
    .free_context = irdma_ufree_context,
};


LATEST_SYMVER_FUNC(ibv_alloc_pd
.alloc_pd      = mlx5_alloc_pd
    ibv_cmd_alloc_pd
         execute_cmd_write(context, IB_USER_VERBS_CMD_ALLOC_PD -> 转到内核处理
    pthread_mutex_init(&pd->opaque_mr_mutex, NULL -> mlx5：引入 mlx5dv_wr_memcpy builder ，引入 mlx5dv_wr_memcpy 用于构建 DMA memcpy 请求。 DMA memcpy 是从 BlueField-2 开始提供的多种内存到内存卸载 (MMO) 之一。 它利用 DPU 上的 GGA 模块执行从 src 到 dest 的 DMA memcpy，从而提高性能。 src 和 dest 可以是主机和 SoC 的任意组合。 请注意，在 Host 到 SoC 或 SoC 到 Host memcpy 的情况下，需要特殊的跨 gvmi MKey




poll_cq
    mlx5_stall_cycles_poll_cq
    or mlx5_stall_poll_cq
    mlx5_poll_one
        mlx5_get_next_cqe -> 添加惰性CQ轮询，目前，当用户想要轮询CQ是否完成时，他别无选择，只能获得整个工作完成（WC）。 这有几个含义 - 例如： * 扩展 WC 是有限的，因为添加新字段会使 WC 更大并且可能占用更多缓存行。 * 每个字段都被复制到 WC - 甚至是用户不关心的字段。 此补丁添加了对以惰性方式处理 CQE 的一些支持。 新的惰性模式将在下游补丁中调用。 我们只解析必需的字段，以便找出 CQE，例如类型、状态、wr_id 等。为了与遗留模式共享代码而不影响性能，对遗留代码进行了重构，并使用了“always_inline”机制，以便 分支条件将在编译时被删除
            next_cqe_sw
                ...
                return cq->active_buf->buf + n * cq->cqe_sz
            VALGRIND_MAKE_MEM_DEFINED -> memory check
            dump_cqe
        mlx5_parse_cqe -> 多分支函数
            case MLX5_CQE_REQ
            ...
    update_cons_index
    mlx5_get_cycles


++cq->mcq.cons_index;
get_sw_cqe


test:
man: https://man7.org/linux/man-pages/man1/ibv_rc_pingpong.1.html
libibverbs/examples/rc_pingpong.c -> main
...
ibv_get_device_list
pp_init_ctx
    ctx->buf = memalign(page_size, size)
    ibv_open_device
    ibv_create_comp_channel
    ctx->pd = ibv_alloc_pd(ctx->context)
    if (use_odp || use_ts || use_dm)
        ibv_query_device_ex(ctx->context, NULL, &attrx) -> 查询设备属性/支持的功能
        ctx->dm = ibv_alloc_dm(ctx->context, &dm_attr)
        access_flags |= IBV_ACCESS_ZERO_BASED -> 使用从 MR 开始的字节偏移量来访问该 MR，而不是指针地址
    ctx->mr = ibv_reg_mr
    or ibv_reg_dm_mr
    if (prefetch_mr)
        ibv_advise_mr
    ibv_create_cq_ex
    or ibv_create_cq
    ibv_create_qp_ex
    or ibv_create_qp
    ibv_qp_to_qp_ex
    ibv_query_qp
    IBV_QPS_INIT
    ibv_modify_qp
pp_post_recv
    for (i = 0; i < n; ++i)
        ibv_post_recv
ibv_req_notify_cq
pp_get_port_info -> 由于 IBoE 需要使用 GRH，因此更新 ibv_*_pinpong 示例以接受 GID。 GID 作为本地端口表的索引给出，并通过套接字连接在客户端和服务器之间交换
    ibv_query_port
ibv_query_gid
if (servername)
    pp_client_exch_dest
        getaddrinfo(servername, service, &hints, &res)
        socket
        connect
        gid_to_wire_gid
        (write(sockfd, msg
        read(sockfd, msg
        wire_gid_to_gid(gid, &rem_dest->gid)
or pp_server_exch_dest
    accept
    wire_gid_to_gid
    pp_connect_ctx
        IBV_QPS_RTR
        ibv_modify_qp
        IBV_QPS_RTS
        ...
pp_connect_ctx
客户端
    ibv_memcpy_to_dm
    pp_post_send
        struct ibv_send_wr wr
        .opcode     = IBV_WR_SEND,
        ibv_wr_start
        ibv_wr_send
        ibv_wr_set_sge
        ibv_wr_complete
        or ibv_post_send
ibv_get_cq_event
if (use_ts) -> RoCE 时间戳允许您在将数据包发送到线路或从线路接收数据包时对其进行标记。 时间戳以原始硬件周期给出，但可以轻松转换为硬件参考的基于纳秒的时间。 此外，它使您能够查询硬件的硬件时间，从而标记其他应用程序的事件并比较时间
    ibv_start_poll
parse_single_wc
or ibv_poll_cq
...



rdma_create_id
    rdma_create_id2
        ucma_init
        ucma_alloc_id
        write
        ucma_insert_id



librdmacm/cma.c

ibv_query_port -> __lib_query_port
    get_ops(context)->query_port


ibv_get_device_list
    ibverbs_init -> verbs：修改 init 的排序方式，将检查 uverbs ABI 移至从内核加载设备列表之后。 当通过 netlink 加载时，我们可以假设 ABI 是 6，而无需转到 sysfs。 这允许我们使用内核依赖项来初始化库，并且错误（例如缺少内核支持）是从 ibverbs_get_device_list() 而不是 ibverbs_init() 返回的。 如果内核支持 netlink，ibverbs 在启动期间不再读取 /sys/ 路径
        check_env("RDMAV_FORK_SAFE") || check_env("IBV_FORK_SAFE")
        ibv_fork_init -> libibverbs/memory.c
            getenv("RDMAV_HUGEPAGES_SAFE") -> 允许在多次调用 ibv_fork_init 中使用大页，设置环境变量 RDMAV_HUGEPAGES_SAFE 告诉库检查内核用于内存区域的基础页大小。 如果应用程序直接或通过 libhugetlbfs 等库间接使用大页，则这是必需的。 该变量的检查是在第一次调用 ibv_fork_init 时执行的。 这会导致具有多个底层库的复杂应用程序出现不可预测的行为。 提议的更改将允许支持大页面，而不依赖于 ibv_fork_init 调用顺序
            if (mm_root)
            get_page_size
                "/proc/%d/smaps", pid
                smaps_page_size
                    KernelPageSize
            madvise(tmp_aligned, size, MADV_DONTFORK) -> 在 ibv_fork_init() 和 madvise 跟踪中处理大页，当在 libibverbs 中启用 fork 支持时，将为注册为内存区域的每个内存页调用 madvise() 。 传递给 madvise() 的内存范围必须是页对齐的，并且大小必须是页大小的倍数。 libibverbs 使用 sysconf(_SC_PAGESIZE) 找出系统页面大小，并根据此页面大小对传递给 reg_mr() 的所有范围进行舍入。 当 libhugetlbfs 中的内存传递给 reg_mr() 时，这不起作用，因为该内存范围的页面大小可能不同（例如 16MB）。 因此 libibverbs 必须使用巨大的页面大小来计算 madvise 的页面对齐范围。 由于在预加载 libhugetlbfs 时向应用程序“在后台”提供大页面，因此应用程序不知道何时注册大页面或普通页面。 要解决此问题，请检测 libibverbs 中大页面的使用，并根据大页面大小调整传递给 madvise 的内存范围。 通过观察 madvise() 失败来确定给定内存范围的页面大小已被证明是不可靠的。 因此，我们引入 RDMAV_HUGEPAGES_SAFE 环境变量，让用户决定是否应在每次 reg_mr() 调用时检查页面大小。 这要求用户了解正在运行的应用程序是否使用大页面。 我没有添加额外的 API 调用来启用此功能，因为应用程序可以使用 setenv() + ibv_fork_init() 来启用检查代码中的大页面
            mm_root->color  = IBV_BLACK -> 初始化红黑树(会影响性能)
            ...
        verbs_allow_disassociate_destroy -> verbs：引入ENV来控制销毁命令时的EIO，引入环境变量（即RDMAV_ALLOW_DISASSOC_DESTROY）来控制销毁命令返回的代码。 一旦设置完毕，任何将从内核获取 EIO 的销毁命令都将被视为成功。 在这种情况下，该对象的底层内核资源必须已通过分离机制销毁，并且用户空间驱动程序和应用程序也可以安全地清理其资源。 这是为了防止用户空间区域的内存泄漏
        ibv_get_sysfs_path
        check_memlock_limit
            rlim.rlim_cur <= 32768
        verbs_set_log_level -> verbs：添加通用日志记录 API，调试打印机制在调试应用程序故障时非常有用。 此补丁添加了一个通用 API，可供所有提供商使用并替换特定于提供商的对应项。 调试消息通过名为 VERBS_LOG_LEVEL 的环境变量进行控制，其中值指示应启用哪些打印： enum { VERBS_LOG_LEVEL_NONE, VERBS_LOG_ERR, VERBS_LOG_WARN, VERBS_LOG_INFO, VERBS_LOG_DEBUG, }; 例如，要启用警告级别或更高级别的打印，VERBS_LOG_LEVEL 应设置为 2。输出应写入 VERBS_LOG_FILE 环境变量中提供的文件。 当在调试模式下编译库并且未提供文件时，输出应写入 stderr。 对于数据路径流，附加 if 语句的开销很重要，可以使用 verbs_*_datapath() 宏，该宏将在编译库以供发布时编译出来, 参考: https://github.com/ssbandjl/rdma-core/commit/5c3514eb87ca86e817a8b610ada3200bbcdde6f4, 编译开关: Enabling debug prints, 可作为日志实现的一个参考
        verbs_set_log_file
    ibverbs_get_device_list -> verbs: 刷新缓存的 ibv_device 列表，问题 ======== 目前，libibverbs 仅在第一次调用 ibv_get_device_list 时构建缓存的 ibv_device 列表，因此无论是否有硬件更改，该列表都不会更新 在系统中。 解决方案======== 修改 ibv_get_device_list() 的实现，以便连续的调用将以与今天相同的方式重新扫描 sysfs，以便每次创建一个新的 ibv_device 列表。 为此，将缓存的设备列表更改为真正的链表而不是动态数组。 我们如何识别新设备​============================= 根据 /sys/class/infiniband_verbs/ 的时间戳创建来识别同一设备 uverbs%d/ibdev。 我们使用 stat 系统调用获取文件状态，并使用 st_mtime 字段来实现此目的。 当我们重新扫描 sysfs 设备时，我们会检查每个 sysfs 设备是否已经在上次扫描中，如果没有，则分配新的 ibv_device 并将其添加到缓存设备列表中。 本系列的下一个补丁处理设备不再使用的情况。 注意：此补丁根据上面 verbs_device 结构体注释中的要求更改了 IBVERBS_PRIVATE 符号 -> verbs: 整理 ibverbs_get_device_list ，现在我们有了 ccan 列表，这里的逻辑可以大大简化。 消除令人困惑的used和have_driver值，而只是在我们运行进程时从sysfs列表中删除项目。 这直接保证了发现的 sysfs 项仅处理一次，并使 sysfs 指针的生命周期更加清晰
        find_sysfs_devs_nl(&sysfs_list) -> verbs：使用 netlink 来发现 uverbs 设备而不是 sysfs，netlink 查询为我们提供了 ibdev idx，它对于设备来说大多是唯一的，并且在设备重命名时充当稳定的 id。 如果在 verbs 用户操作期间重命名设备，这会使 verbs 更加健壮。 此外，netlink 仅返回在进程的网络命名空间中实际可见的设备，从而简化了发现过程
            rdmanl_socket_alloc
            rdmanl_get_devices(nl, find_sysfs_devs_nl_cb, tmp_sysfs_dev_list)
            list_for_each_safe (tmp_sysfs_dev_list
                find_uverbs_nl(nl, dev) && find_uverbs_sysfs(dev)
                try_access_device(dev)
                    stat(devpath, &cdev_stat)
            nl_socket_free(nl)
        find_sysfs_devs(&sysfs_list)
        check_abi_version
        list_for_each_safe(device_list
            same_sysfs_dev
            ibverbs_device_put
        try_all_drivers
        load_drivers()
        try_all_drivers
        return num_devices
    ibverbs_device_hold(l[i]) -> verbs: 避免 ibv_device 内存泄漏，现在，每次调用 ibv_get_device_list 时都会刷新 ibv_device 列表，因此我们需要从之前的扫描中释放不再绑定的设备，否则可能会导致 ibv_device 结构的内存泄漏。 仅当用户不再使用 ibv_device 的内存时，我们才能释放它。 我们如何识别设备是否仍在使用​, 我们将引用计数添加到动词设备结构中。 在以下情况下，该引用计数会增加： 设置为 1 以使该设备位于列表中，直到应将其删除为止。 b. 用户调用 ibv_get_device_list。 C。 用户调用 ibv_open_device。 在以下情况下，引用计数会减少： 用户调用 ibv_free_device_list。 b. 用户调用 ibv_close_device。 C。 设备不再存在于 sysfs 中。 当引用计数减少到零时，设备将被释放。 为了释放 ibv_device 结构，我们将 uninit_device 回调函数添加到 verbs_device_ops
        verbs_get_device
            container_of(dev, struct verbs_device, device)
        atomic_fetch_add(&verbs_device->refcount, 1) -> 加引用
    *num = num_devices




module_init(mlx5_ib_init);
    alloc_ordered_workqueue
    mlx5_ib_qp_event_init
    mlx5_ib_odp_init
    mlx5r_rep_init
    auxiliary_driver_register(&mlx5r_mp_driver)
    auxiliary_driver_register(&mlx5r_driver)
static struct auxiliary_driver mlx5r_driver = {
    .name = "rdma",
    .probe = mlx5r_probe,
    .remove = mlx5r_remove,
    .id_table = mlx5r_id_table,
};
mlx5r_probe
    mlx5_port_type_cap_to_rdma_ll -> {net, IB}/mlx5：管理多端口 RoCE 的端口关联，调用 mlx5_ib_add 时确定要添加的 mlx5 核心设备是否能够进行双端口 RoCE 操作。 如果是，请使用 num_vhca_ports 和affiliate_nic_vport_criteria 功能确定它是主设备还是从设备。 如果该设备是从属设备，请尝试找到与其关联的主设备。 可以关联的设备将共享系统映像 GUID。 如果没有找到，请将其放入非关联端口列表中。 如果找到主设备，则通过在 NIC vport 上下文中配置端口从属关系将端口绑定到它。 同样，当调用 mlx5_ib_remove 时确定端口类型。 如果它是从端口，则将其与主设备取消关联，否则只需将其从非关联端口列表中删除即可。 即使第二个端口不可用于关联，IB 设备也会注册为多端口设备。 当第二个端口稍后附属时，必须刷新 GID 缓存才能获取缓存中第二个端口的默认 GID。 导出roce_rescan_device以提供在绑定新端口后刷新缓存的机制。 在多端口配置中，所有 IB 对象（QP、MR、PD 等）相关命令应流经主站 mlx5_core_dev，其他命令必须发送到从端口 mlx5_core_mdev，提供一个接口来获取非 IB 对象命令的正确 mdev
    ib_alloc_device
    if (ll == IB_LINK_LAYER_ETHERNET && !mlx5_get_roce_state(mdev))
        profile = &raw_eth_profile;
    else
        profile = &pf_profile;
    __mlx5_ib_add(dev, profile)
        for (i = 0; i < MLX5_IB_STAGE_MAX; i++)
            err = profile->stage[i].init(dev) -> run every stage's init func -> raw_eth_profile or pf_profile
    auxiliary_set_drvdata



enum mlx5_ib_stages -> IB/mlx5：创建配置文件基础设施来添加和删除阶段，今天我们有一个函数，在添加 IB 接口时使用，将此函数分解为多个函数。   创建阶段和执行每个阶段的通用机制。 这是为 RDMA/IB 表示器做准备，它们可能不需要所有阶段，或者在某些阶段会以不同的方式执行操作。   此补丁不会改变任何功能

const struct mlx5_ib_profile raw_eth_profile = {
	STAGE_CREATE(MLX5_IB_STAGE_INIT,
		     mlx5_ib_stage_init_init,
		     mlx5_ib_stage_init_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_FS,
		     mlx5_ib_fs_init,
		     mlx5_ib_fs_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_CAPS,
		     mlx5_ib_stage_caps_init,
		     mlx5_ib_stage_caps_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_NON_DEFAULT_CB,
		     mlx5_ib_stage_raw_eth_non_default_cb,
		     NULL),
	STAGE_CREATE(MLX5_IB_STAGE_ROCE,
		     mlx5_ib_roce_init,
		     mlx5_ib_roce_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_QP,
		     mlx5_init_qp_table,
		     mlx5_cleanup_qp_table),
	STAGE_CREATE(MLX5_IB_STAGE_SRQ,
		     mlx5_init_srq_table,
		     mlx5_cleanup_srq_table),
	STAGE_CREATE(MLX5_IB_STAGE_DEVICE_RESOURCES,
		     mlx5_ib_dev_res_init,
		     mlx5_ib_dev_res_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_DEVICE_NOTIFIER,
		     mlx5_ib_stage_dev_notifier_init,
		     mlx5_ib_stage_dev_notifier_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_COUNTERS,
		     mlx5_ib_counters_init,
		     mlx5_ib_counters_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_CONG_DEBUGFS,
		     mlx5_ib_stage_cong_debugfs_init,
		     mlx5_ib_stage_cong_debugfs_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_UAR,
		     mlx5_ib_stage_uar_init,
		     mlx5_ib_stage_uar_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_BFREG,
		     mlx5_ib_stage_bfrag_init,
		     mlx5_ib_stage_bfrag_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_PRE_IB_REG_UMR,
		     NULL,
		     mlx5_ib_stage_pre_ib_reg_umr_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_WHITELIST_UID,
		     mlx5_ib_devx_init,
		     mlx5_ib_devx_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_IB_REG,
		     mlx5_ib_stage_ib_reg_init,
		     mlx5_ib_stage_ib_reg_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_POST_IB_REG_UMR,
		     mlx5_ib_stage_post_ib_reg_umr_init,
		     NULL),
	STAGE_CREATE(MLX5_IB_STAGE_DELAY_DROP,
		     mlx5_ib_stage_delay_drop_init,
		     mlx5_ib_stage_delay_drop_cleanup),
	STAGE_CREATE(MLX5_IB_STAGE_RESTRACK,
		     mlx5_ib_restrack_init,
		     NULL),
};


...
static const struct mlx5_ib_profile pf_profile
    mlx5_ib_counters_init
        ib_set_device_ops(&dev->ib_dev, &counters_ops)
        ib_set_device_ops(&dev->ib_dev, &hw_switchdev_stats_ops)
        or
        ib_set_device_ops(&dev->ib_dev, &hw_stats_ops);
        mlx5_ib_alloc_counters
            mlx5_ib_fill_counters
                basic_q_cnts
            mlx5_cmd_exec_inout


mlx5 counter:
static const struct counter_desc pport_802_3_stats_desc[] = {
    { "tx_packets_phy", PPORT_802_3_OFF(a_frames_transmitted_ok) },
    { "rx_packets_phy", PPORT_802_3_OFF(a_frames_received_ok) },
    { "rx_crc_errors_phy", PPORT_802_3_OFF(a_frame_check_sequence_errors) },
    { "tx_bytes_phy", PPORT_802_3_OFF(a_octets_transmitted_ok) },
    { "rx_bytes_phy", PPORT_802_3_OFF(a_octets_received_ok) },
    { "tx_multicast_phy", PPORT_802_3_OFF(a_multicast_frames_xmitted_ok) },
    { "tx_broadcast_phy", PPORT_802_3_OFF(a_broadcast_frames_xmitted_ok) },
    { "rx_multicast_phy", PPORT_802_3_OFF(a_multicast_frames_received_ok) },
    { "rx_broadcast_phy", PPORT_802_3_OFF(a_broadcast_frames_received_ok) },
    { "rx_in_range_len_errors_phy", PPORT_802_3_OFF(a_in_range_length_errors) },
    { "rx_out_of_range_len_phy", PPORT_802_3_OFF(a_out_of_range_length_field) },
    { "rx_oversize_pkts_phy", PPORT_802_3_OFF(a_frame_too_long_errors) },
    { "rx_symbol_err_phy", PPORT_802_3_OFF(a_symbol_error_during_carrier) },
    { "tx_mac_control_phy", PPORT_802_3_OFF(a_mac_control_frames_transmitted) },
    { "rx_mac_control_phy", PPORT_802_3_OFF(a_mac_control_frames_received) },
    { "rx_unsupported_op_phy", PPORT_802_3_OFF(a_unsupported_opcodes_received) },
    { "rx_pause_ctrl_phy", PPORT_802_3_OFF(a_pause_mac_ctrl_frames_received) },
    { "tx_pause_ctrl_phy", PPORT_802_3_OFF(a_pause_mac_ctrl_frames_transmitted) },
};

rx_write_requests
static const struct mlx5_ib_counter basic_q_cnts[] = {
    INIT_Q_COUNTER(rx_write_requests),
    INIT_Q_COUNTER(rx_read_requests),
    INIT_Q_COUNTER(rx_atomic_requests),
    INIT_Q_COUNTER(rx_dct_connect),
    INIT_Q_COUNTER(out_of_buffer),
};


MLX5E_DECLARE_STATS_GRP_OP_UPDATE_STATS



struct mlx5_ifc_query_q_counter_out_bits {
    u8         status[0x8];
    u8         reserved_at_8[0x18];

    u8         syndrome[0x20];

    u8         reserved_at_40[0x40];

    u8         rx_write_requests[0x20];


static const struct ib_device_ops hw_stats_ops = {
    .alloc_hw_port_stats = mlx5_ib_alloc_hw_port_stats,
    .get_hw_stats = mlx5_ib_get_hw_stats,
    .counter_bind_qp = mlx5_ib_counter_bind_qp,
    .counter_unbind_qp = mlx5_ib_counter_unbind_qp,
    .counter_dealloc = mlx5_ib_counter_dealloc,
    .counter_alloc_stats = mlx5_ib_counter_alloc_stats,
    .counter_update_stats = mlx5_ib_counter_update_stats,
    .modify_hw_stat = IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS) ?
              mlx5_ib_modify_stat : NULL,
};


rdma_counter_init
    alloc_hw_port_stats


nlmsg_put RDMA_NLDEV_CMD_STAT_SET
.doit = nldev_stat_set_doit
    nldev_stat_set_counter_dynamic_doit
        rdma_counter_modify -> RDMA/nldev：允许通过 RDMA netlink 进行可选计数器状态配置，提供允许用户通过 RDMA netlink 启用/禁用可选计数器的选项。 将其限制为仅具有管理员权限的用户。 示例： 1. 启用可选计数器 cc_rx_ce_pkts 和 cc_rx_cnp_pkts（并禁用所有其他计数器）： $ sudo rdma statistic set link rocep8s0f0/1 option-counters \ cc_rx_ce_pkts,cc_rx_cnp_pkts 2. 删除所有可选计数器： $ sudo rdma statistic unset link rocep8s0 f0/1 可选 -计数器




ib_modify_qp
    _ib_modify_qp
        rdma_counter_bind_qp_auto -> RDMA：支持超过 255 个 rdma 端口，当前代码在处理 RDMA 设备的端口时使用许多不同的类型：u8、unsigned int 和 u32。 切换到 u32 来清理逻辑。 这使我们（至少）能够使核心视图保持一致并使用相同的类型。 不幸的是，并非所有地方都可以转换。 许多 uverbs 函数期望端口为 u8，因此保留这些位置以免破坏 UAPI。 硬件/规范定义的值也不得更改。 通过切换到 u32，我们现在可以支持具有超过 255 个端口的设备。 U32_MAX 被保留以使控制逻辑更容易处理。 由于具有 U32_MAX 端口的设备可能不会很快发生这种情况，这似乎不是问题。 当创建具有超过 255 个端口的设备时，uverbs 将报告 RDMA 设备具有 255 个端口，因为这是当前支持的最大值。 verbs 接口尚未更改，因为 IBTA 规范在太多地方将端口大小限制为 u8，并且所有依赖 verbs 的应用程序将无法应对此更改。 在此阶段，我们正在扩展仅使用供应商通道的接口。一旦解除限制，switchdev 模式下的 mlx5 将能够拥有设备创建的数千个 SF。 由于报告超过 255 个端口的 RDMA 设备的唯一实例将是代表设备，并且它将自身暴露为仅原始以太网设备 CM/MAD/IPoIB 和其他 ULP 不受此更改的影响，并且它们的 sysfs/接口 暴露给用户空间的内容可以保持不变。 虽然在这里清理了一些对齐问题并删除了不需要的健全性检查（主要在 rdmavt 中）
            rdma_restrack_is_tracked
            rdma_is_port_valid
            rdma_get_counter_auto_mode -> qp 在初始化期间根据自动模式自动与计数器绑定（例如，qp 类型，...）
            __rdma_counter_bind_qp -> RDMA/计数器：结合分配和绑定逻辑，RDMA 计数器随后立即分配并绑定到 QP。 只有经过这两个步骤之后，它们才真正可用。 通过组合逻辑，我们确保一旦计数器返回给调用者，它将完成所有设置
                counter_bind_qp -> mlx5_ib_counter_bind_qp -> RDMA/mlx5：与 main.c 分开的计数器，mlx5_ib 中支持多种计数器类型：硬件计数器、拥塞计数器、Q 计数器和流量计数器。 几乎所有支持代码都放在 main.c 中，这使得几乎不可能再维护代码了。 让我们为计数器创建单独的代码命名空间，以方便将来的泛化工作
                    mlx5_cmd_exec_inout(dev->mdev, alloc_q_counter, in, out)
                    mlx5_ib_qp_set_counter -> IB/mlx5：支持设置qp计数器，支持将qp与计数器绑定。 如果 counter 为 null，则将 qp 绑定到默认计数器。 不同的QP状态有不同的操作： - RESET：设置计数器字段，使其在RST2INIT更改期间生效； - RTS：发出RTS2RTS更改以更新QP计数器； - 其他：设置计数器字段并标记 counter_pending 标志，当 QP 转移到 RTS 状态并设置该标志时，然后发出 RTS2RTS 修改来更新计数器
                        MLX5_CAP_GEN(dev->mdev, rts2rts_qp_counters_set_id)
                        if (mqp->state == IB_QPS_RTS)
                            __mlx5_ib_qp_set_counter
                                mlx5_ib_get_counters_id
                                __mlx5_ib_qp_set_raw_qp_counter
                                    mlx5_core_modify_rq
                                        MLX5_CMD_OP_MODIFY_RQ
                                        mlx5_cmd_exec_in(dev, modify_rq, in);
                                            ...
                                MLX5_SET(rts2rts_qp_in, in, opcode, MLX5_CMD_OP_RTS2RTS_QP)
                                return mlx5_cmd_exec_in(dev->mdev, rts2rts_qp, in)
            else -> alloc_and_bind




fill_res_counter_entry



rdma_counter_query_stats
    counter_update_stats


counter_history_stat_update -> RDMA/计数器：在释放之前查询计数器，在释放之前查询动态分配的计数器，以更新其硬件计数器并将所有计数器记录到历史数据中。 否则这些硬件计数器的所有值都将丢失



.modify_qp = mlx5_ib_modify_qp




uverbs
module_init(ib_uverbs_init) -> static int __init ib_uverbs_init(void) -> [PATCH] IB uverbs：核心实现，添加 InfiniBand 用户空间动词实现的核心，包括创建字符设备节点、从用户空间分派请求以及将事件通知传递回用户空间 -> commit: https://github.com/ssbandjl/linux/commit/bc38a6abdd5a50e007d0fcd9b9b6280132b79e62
drivers/infiniband/core/uverbs.h
drivers/infiniband/core/uverbs_cmd.c
drivers/infiniband/core/uverbs_main.c
    register_chrdev_region(IB_UVERBS_BASE_DEV, infiniband_verbs
    alloc_chrdev_region(&dynamic_uverbs_dev, 0,
    class_register(&uverbs_class)
    class_create_file(&uverbs_class, &class_attr_abi_version.attr);
    ib_register_client(&uverbs_client) -> ib_register_client - 注册 IB 客户端，@client:Client 来注册 IB 驱动程序的上层用户可以使用 ib_register_client() 来注册 IB 设备添加和删除的回调。 当添加 IB 设备时，将调用每个已注册客户端的 add 方法（按照客户端注册的顺序），而当删除设备时，将调用每个客户端的 remove 方法（按照客户端注册的相反顺序）。 此外，当调用 ib_register_client() 时，客户端将收到所有已注册设备的添加回调
        init_completion(&client->uses_zero)
        assign_client_id(client)
        xa_for_each_marked
            add_client_context(device, client)
                client->add(device) -> ib_uverbs_add_one


drivers/infiniband/core/device.c -> ib_register_device - 向 IB 核心注册 IB 设备 @device：要注册的设备 @name：唯一的字符串设备名称。 这可能包括“%”，这将导致将唯一索引添加到传递的设备名称中。 @dma_device：指向支持 DMA 的设备的指针。 如果%NULL，则将使用IB 设备。 在这种情况下，调用者应该为 DMA 完全设置 ibdev。 这通常意味着使用 dma_virt_ops。 低级驱动程序使用 ib_register_device() 将其设备注册到 IB 内核。 所有注册的客户端都将收到添加的每个设备的回调。 @device 必须使用 ib_alloc_device() 进行分配。 如果驱动程序使用 ops.dealloc_driver 并异步调用任何 ib_unregister_device() ，则一旦该函数返回，设备指针可能会被释放
int ib_register_device(struct ib_device *device, const char *name, struct device *dma_device) -> roce和IB注册的flow: https://blog.csdn.net/tiantao2012/article/details/77746141
    assign_name(device, name) -> DMA/devices：使用 xarray 来存储 client_data，现在我们为每个客户端都有了一个小 ID，我们可以使用 xarray 而不是线性搜索链表来获取客户端数据。 这将提供更快且可扩展的客户端数据查找，并使我们能够修改锁定方案。 由于xarray可以使用标记来存储'going_down'，因此完全消除了struct ib_client_data并将client_data值直接存储在xarray中。 然而，这确实需要一个特殊的迭代器，因为我们仍然必须迭代任何 NULL client_data 值。 还消除了 client_data_lock 以支持内部 xarray 锁定
        if (strchr(name, '%')) -> if name have %
            alloc_name(device, name)
        dev_set_name
            err = kobject_set_name_vargs(&dev->kobj, fmt, vargs) -> Set the name of a kobject
        __ib_device_get_by_name <- static DEFINE_XARRAY_FLAGS(devices, XA_FLAGS_ALLOC);
            xa_for_each (&devices, index, device)
        strscpy -> 将字符串或尽可能多的字符串复制到目标缓冲区中。 该例程返回复制的字符数（不包括尾随的 NUL）或 -E2BIG（如果目标缓冲区不够大）。 如果字符串缓冲区重叠，则行为未定义。 目标缓冲区始终以 NUL 终止，除非它的大小为零。，优先于 strlcpy，因为 API 不需要从 src 字符串读取超出指定“count”字节的内存，并且返回值更容易进行错误检查 比 strlcpy 的。 此外，与当前的 strlcpy 实现不同，该实现对于从其下面更改的字符串具有鲁棒性。 优于 strncpy，因为它始终返回有效字符串，并且不会不必要地强制目标缓冲区的尾部清零。 如果需要归零，使用 strscpy 进行溢出测试可能会更干净，然后只需 memset 目标缓冲区的尾部
        xa_alloc_cyclic(&devices, &device->index, device, xa_limit_31b, -> 在 XArray 中找到存储此条目的位置 -> 在 @xa 中查找 @limit.min 和 @limit.max 之间的空条目，将索引存储到 @id 指针中，然后将条目存储在该索引处。 并发查找不会看到未初始化的@id。 对空条目的搜索将从 @next 开始，并在必要时回绕。 只能在使用 xa_init_flags() 中设置的标志 XA_FLAGS_ALLOC 初始化的 xarray 上进行操作。 上下文：任何上下文。 获取并释放 xa_lock。 如果 @gfp 标志允许，可以睡觉。 返回：如果分配成功且没有包装，则返回 0。 如果包装后分配成功，则为 1；如果无法分配内存，则为 -ENOMEM；如果 @limit 中没有可用条目，则为 -EBUSY。 */
    setup_device(device) -> ib_register_device() 执行多个分配和初始化步骤。 将其拆分为更小、更易读的函数，以便于审查和维护 -> setup_device() 分配内存并设置需要调用设备操作的数据，这是在 ib_alloc_device 期间未完成这些操作的唯一原因。 它由 ib_dealloc_device() 撤消
        ib_device_check_mandatory -> must option(强制检查提供的verbs API 顺序及是否支持)
            #define IB_MANDATORY_FUNC(x) { offsetof(struct ib_device_ops, x), #x }
            mandatory_table
            IB_MANDATORY_FUNC(query_device),
            ...
        setup_port_data -> RDMA/device：将 ib_device per_port 数据合并到一个位置，没有理由对每个端口数据进行 3 次分配。 将它们组合在一起并使所有每端口数据的生命周期与 struct ib_device 匹配。 后续补丁将需要更多特定于端口的数据，现在有一个好地方可以放置它
            alloc_port_data -> likely alloc before
            rdma_for_each_port
                get_port_immutable -> .get_port_immutable = irdma_roce_port_immutable -> 获取端口信息并固化
                    immutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP -> #define RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP (RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP | RDMA_CORE_CAP_IB_MAD | RDMA_CORE_CAP_IB_CM | RDMA_CORE_CAP_AF_IB | RDMA_CORE_CAP_ETH_AH)
                    ib_query_port -> 校验端口有效性, 是否iwarp -> __ib_query_port
                        return device->ops.query_port(device, port_num, port_attr) -> or xtrdma_qeury_port
                            static int irdma_query_port(struct ib_device *ibdev, u32 port,
                                props->max_mtu = IB_MTU_4096 -> 5
                                props->active_mtu = ib_mtu_int_to_enum(netdev->mtu)
                                props->lid = 1; -> 本地标识符
                                props->lmc = 0; -> LID 掩码控制(lid mask control) 由子网管理器分配的每个端口值。 LMC的值指定本地标识符中的路径比特数
                                props->sm_lid = 0;
                                props->sm_sl = 0;
                                props->state = IB_PORT_ACTIVE;
                                ib_get_eth_speed(ibdev, port, &props->active_speed,
                                    rdma_port_get_link_layer -> IB_LINK_LAYER_ETHERNET
                                    ib_device_get_netdev -> RDMA/device：添加 ib_device_set_netdev() 作为 get_netdev 的替代方案，关联的 netdev 实际上不应该非常动态，因此对于大多数驱动程序来说，没有理由进行这样的回调。 提供一个 API 来通知核心代码有关网络开发从属关系，并使用核心维护的数据结构。 这使得核心代码能够更加了解 ndev 关系，从而允许一些基于此的新 API。 这也使用了某种意义上的锁定，许多驱动程序都有令人困惑的 RCU 锁定，或者缺少不正确的锁定
                                        pdata = &ib_dev->port_data[port]
                                        ib_dev->ops.get_netdev(ib_dev, port) -> mlx5_ib_get_netdev -> IB/mlx5：支持 IB 设备的回调以获取其 netdev，仅适用于 Eth 端口：在 mlx5_ib_device 中维护网络设备指针，如果网络设备和 IB 设备具有相同的 PCI 父设备，则在 NETDEV_REGISTER 和 NETDEV_UNREGISTER 事件时更新它。 实现 get_netdev 回调以返回该网络设备
                                            mdev = mlx5_ib_get_native_port_mdev(ibdev, port_num, NULL)
                                                mlx5_ib_port_link_layer
                                                mlx5_core_mp_enabled
                                            ndev = mlx5_lag_get_roce_netdev(mdev) -> net/mlx5：获取 RoCE netdev，当 LAG 处于活动状态时，IB 驱动程序使用它来确定 IB 绑定设备的 netdev。 如果模式不是主动备份，则返回 PF0 的 netdev；如果模式是主动备份，则返回主动从机的 PF netdev
                                                ldev = mlx5_lag_dev(dev) -> net/mlx5：更改lag的所有权模型，Lag用于将同一HCA的两个PCI功能组合成单个逻辑单元。 这是核心功能，因此应由核心驱动程序管理。 目前情况并非如此。 当我们将滞后软件结构存储在较低设备内时，其生命周期（创建/销毁）由 mlx5e 部分决定。 更改所有权模型，使延迟与较低级别驱动程序的生命周期相关，而不是与 mlx5e 部分相关
                                                ldev && __mlx5_lag_is_roce(ldev)
                                            mlx5_ib_put_native_port_mdev(ibdev, port_num)
                                                mlx5_core_mp_enabled
                                                mpi = ibdev->port[port_num - 1].mp.mpi
                                        rcu_dereference_protected -> rcu_dereference_protected() - 当更新被阻止时获取 RCU 指针 @p：在解除引用之前要读取的指针 @c：发生解除引用的条件 返回指定 RCU 保护指针的值，但省略 READ_ONCE() 。 这在更新端锁阻止指针值更改的情况下很有用。 请注意，此原语不会阻止编译器重复此引用或将其与其他引用组合，因此不应在没有适当锁保护的情况下使用它。 该功能仅供更新端使用。 仅受 rcu_read_lock() 保护时使用此函数将导致罕见但非常难看的失败
                                            __rcu_dereference_protected
                                        如果我们开始通过防止传播取消注册的 netdev 来加快取消注册的速度
                                    __ethtool_get_link_ksettings -> net: ethtool: 添加新的 ETHTOOL_xLINKSETTINGS API，此补丁定义了新的 ETHTOOL_GLINKSETTINGS/SLINKSETTINGS API，由新的 get_link_ksettings/set_link_ksettings 回调处理。 此 API 提供对大多数旧版 ethtool_cmd 字段的支持，添加对更大链接模式掩码（最多 4064 位，可变长度）的支持，并删除 ethtool_cmd 已弃用的字段（transceiver/maxrxpkt/maxtxpkt）。 此 API 弃用了旧版 ETHTOOL_GSET/SSET API，并提供以下向后兼容性属性： - 带有旧版驱动程序的旧版 ethtool：没有变化，仍然使用 get_settings/set_settings 回调。 - 具有新 get/set_link_ksettings 驱动程序的旧版 ethtool：使用新的驱动程序回调，数据在内部转换为旧版 ethtool_cmd。 ETHTOOL_GSET 将仅返回每个链接模式掩码的第一个 32b。 如果用户尝试将 ethtool_cmd 弃用字段设置为非 0（收发器/maxrxpkt/maxtxpkt），ETHTOOL_SSET 将失败。 如果驱动程序设置较高位，则会记录内核警告。 - 未来的 ethtool 与遗留驱动程序：没有变化，仍然使用 get_settings/set_settings 回调，内部转换为新的数据结构。 不推荐使用的字段（transceiver/maxrxpkt/maxtxpkt）将被忽略，并在用户空间中被视为 0。 请注意，“未来”的 ethtool 工具将不允许更改这些已弃用的字段。 - 未来的 ethtool 具有新的驱动程序：直接调用新的回调。 “未来”ethtool 的含义是： - 查询：首先尝试 ETHTOOL_GLINKSETTINGS，如果失败则恢复到 ETHTOOL_GSET - 设置：首先查询并记住 ETHTOOL_GLINKSETTINGS 或 ETHTOOL_GSET 中哪一个成功了 + 如果 ETHTOOL_GLINKSETTINGS 成功，则使用 ETHTOOL_SLINKSETTINGS 更改配置。 失败是最终的（不要尝试 ETHTOOL_SSET）。 + 否则 ETHTOOL_GSET 成功，使用 ETHTOOL_SSET 更改配置。 失败是最终的（不要尝试 ETHTOOL_SLINKSETTINGS）。 通过新 API 的交互用户/内核首先需要进行一次小的 ETHTOOL_GLINKSETTINGS 握手，以就链接模式位图的长度达成一致。 如果内核不同意用户的要求，它将返回用户期望的位图长度作为负长度（并且 cmd 字段为 0）。 当内核和用户同意时，内核返回所有字段中的有效信息（即链接模式长度> 0且cmd为ETHTOOL_GLINKSETTINGS）。 跨越用户/内核边界的数据结构与 32/64 位无关。 在内部转换为合法的内核位图。 当第一个“link_settings”驱动程序开始出现时，内部 __ethtool_get_settings 内核帮助程序将逐渐被 __ethtool_get_link_ksettings 取代。 所以这个补丁并没有改变它，在需要改变之前它就会被删除
                                        dev->ethtool_ops->get_link_ksettings(dev, link_ksettings)
                                            cmd->base.duplex = DUPLEX_FULL;
                                    ib_get_width_and_speed(netdev_speed, lksettings.lanes, speed, width) -> 获取/计算RDMA网卡位宽和速度 -> RDMA/core：从netdev获取IB宽度和速度，以前无法查询网卡的通道数(lanes )，因此相同的netdev_speed会得到固定的位宽和速度。 随着网卡规格越来越多样化，这种固定模式已经不再适用，因此需要一种方法来根据通道数获取正确的宽度和速度。 该补丁从 net_device 检索 netdev 通道和速度，并将其转换为 IB 位宽和速度
                                        switch (netdev_speed / lanes)
                                        case SPEED_2500:
                                            *speed = IB_SPEED_SDR;
                                        case SPEED_5000:
                                            *speed = IB_SPEED_DDR;
                                        case SPEED_10000:
                                            *speed = IB_SPEED_FDR10;
                                        case SPEED_14000:
                                            *speed = IB_SPEED_FDR;
                                        case SPEED_25000:
                                            *speed = IB_SPEED_EDR;
                                        case SPEED_50000:
                                            *speed = IB_SPEED_HDR;
                                        case SPEED_100000: -> 100G
                                            *speed = IB_SPEED_NDR;
                                        default:
                                            *speed = IB_SPEED_SDR;
                                props->gid_tbl_len = 32;
                                props->pkey_tbl_len = IRDMA_PKEY_TBL_SZ -> 1
                                props->max_msg_sz = iwdev->rf->sc_dev.hw_attrs.max_hw_outbound_msg_size
                                props->port_cap_flags |= IB_PORT_CM_SUP | IB_PORT_REINIT_SUP -> 指示该端口支持节点的重新初始化, 参考Linux内核网络实现原理
                                props->max_msg_sz = iwdev->rf->sc_dev.hw_attrs.max_hw_outbound_msg_size <- dev->hw_attrs.max_hw_outbound_msg_size = IRDMA_MAX_OUTBOUND_MSG_SIZE -> 2GB
                            ib_get_cached_subnet_prefix -> IB/core：通过缓存读取ib_query_port中的subnet_prefix。 ib_query_port() 调用 device->ops.query_port() 来获取端口属性。 查询方法是特定于设备驱动程序的。 相同的函数调用device->ops.query_gid()来获取GID并提取subnet_prefix (gid_prefix)。 GID 和subnet_prefix 存储在缓存中。 但如果设备是 Infiniband 设备，则不会从缓存中读取它们。 以下更改利用了缓存的subnet_prefix。 RDBMS 测试表明，此更改使性能有了显着提高
                                *sn_pfx = device->port_data[port_num].cache.subnet_prefix;
                    immutable->max_mad_size = IB_MGMT_MAD_SIZE -> 256
                    immutable->pkey_tbl_len = attr.pkey_tbl_len -> 设置设备支持的pkey和gid数量
                    immutable->gid_tbl_len = attr.gid_tbl_len
                verify_immutable -> 校验固化的端口数据
                    rdma_cap_ib_mad -> rdma_cap_ib_mad - 检查设备的端口是否支持 Infiniband 管理数据报。 @device：要检查的设备 @port_num：要检查的端口号 管理数据报 (MAD) 是 InfiniBand 规范的必需部分，并且受所有 InfiniBand 设备支持。 OPA 接口还支持稍微扩展的版本。 返回：如果端口支持发送/接收MAD数据包，则返回true
                        device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_IB_MAD
                    rdma_max_mad_size -> 返回此 RDMA 端口所需的最大 MAD 大小。 @device：设备 @port_num：端口号 该 MAD 大小包括 MAD 标头和 MAD 负载。 不包含其他标头。 返回端口所需的最大 MAD 大小。 如果端口不支持 MAD，则返回 0
                         device->port_data[port_num].immutable.max_mad_size
        device->ops.query_device(device, &device->attrs, &uhw) -> irdma_query_device
    ib_cache_setup_one(device) -> 设置IB(GID)缓存 -> IB/核心：添加 RoCE GID 表管理，RoCE GID 基于与 RDMA (RoCE) 设备端口相关的以太网网络设备上配置的 IP 地址。 目前，每个支持 RoCE（ocrdma、mlx4）的低级驱动程序都管理自己的 RoCE 端口 GID 表。 由于本质上没有任何特定于供应商的内容，因此我们对其进行概括，并增强 RDMA 核心 GID 缓存来完成这项工作。 为了填充 GID 表，我们监听事件： (a) netdev up/down/change_addr 事件 - 如果 netdev 构建在我们的 RoCE 设备上，我们需要添加/删除其 IP。 这涉及添加与此 ndev 相关的所有 GID、添加默认 GID 等。 (b) inet 事件 - 将新 GID（根据 IP 地址）添加到表中。 为了对端口 RoCE GID 表进行编程，提供商必须实现 add_gid 和 del_gid 回调。 RoCE GID 管理要求我们在 GID 旁边声明关联的 net_device。 为了管理 GID 表，此信息是必需的。 例如，当删除 net_device 时，其关联的 GID 也需要删除。 RoCE 要求根据相关网络设备的 IPv6 本地链路为每个端口生成默认 GID。 与基于常规 IPv6 链路本地的 GID（因为我们为每个 IP 地址生成 GID）相反，当网络设备关闭时，默认 GID 也可用（为了支持环回）。 锁定的完成方式如下：该补丁修改了 GID 表代码，适用于实现 add_gid/del_gid 回调的新 RoCE 驱动程序以及未实现 add_gid/del_gid 回调的当前 RoCE 和 IB 驱动程序。 更新表的流程不同，因此锁定要求也不同。 更新 RoCE GID 表时，通过 mutex_lock(&table->lock) 实现针对多个写入者的保护。 由于写入表需要我们在表中查找一个条目（可能是空闲条目）然后修改它，因此该互斥锁保护 find_gid 和 write_gid 确保操作的原子性。 GID 缓存中的每个条目均受 rwlock 保护。 在 RoCE 中，写入（通常来自 netdev 通知程序的结果）涉及调用供应商的 add_gid 和 del_gid 回调，这些回调可能会休眠。 因此，为每个条目添加无效标志。 RoCE 的更新是通过工作队列完成的，因此允许休眠。 在IB中，更新是在write_lock_irq(&device->cache.lock)中完成的，因此write_gid不允许休眠并且add_gid/del_gid不会被调用。 当将网络设备传入/传出 GID 缓存时，该设备始终被传递为保持 (dev_hold)。 该代码使用单个工作项来更新所有 RDMA 设备，遵循 netdev 或 inet 通知程序。 该补丁将缓存从客户端（这是不正确的，因为缓存是 IB 基础设施的一部分）转变为在设备注册/删除时显式初始化/释放
        gid_table_setup_one
            _gid_table_setup_one
                struct ib_gid_table *table
                rdma_for_each_port (ib_dev, rdma_port) -> 每个Port一个GID表
                    table = alloc_gid_table(ib_dev->port_data[rdma_port].immutable.gid_tbl_len)
                        struct ib_gid_table *table = kzalloc(sizeof(*table),
                        table->data_vec = kcalloc(sz, sizeof(*table->data_vec),
                        table->sz = sz;
                    gid_table_reserve_default(ib_dev, rdma_port, table)
                        roce_gid_type_mask = roce_gid_type_mask_support(ib_dev, port)
                        num_default_gids = hweight_long(roce_gid_type_mask) -> 计算1的个数, 得到默认GID个数
                        table->default_gid_indices |= BIT(i) -> 保留GID
                    ib_dev->port_data[rdma_port].cache.gid = table -> init gid table
            rdma_roce_rescan_device -> 重新扫描系统中的所有网络设备，并根据需要将其 gid 添加到相关 RoCE 设备 -> {net, IB}/mlx5：管理多端口 RoCE 的端口关联，调用 mlx5_ib_add 时确定要添加的 mlx5 核心设备是否能够进行双端口 RoCE 操作。 如果是，请使用 num_vhca_ports 和affiliate_nic_vport_criteria 功能确定它是主设备还是从设备。 如果该设备是从属设备，请尝试找到与其关联的主设备。 可以关联的设备将共享系统映像 GUID。 如果没有找到，请将其放入非关联端口列表中。 如果找到主设备，则通过在 NIC vport 上下文中配置端口从属关系将端口绑定到它。 同样，当调用 mlx5_ib_remove 时确定端口类型。 如果它是从端口，则将其与主设备取消关联，否则只需将其从非关联端口列表中删除即可。 即使第二个端口不可用于关联，IB 设备也会注册为多端口设备。 当第二个端口稍后附属时，必须刷新 GID 缓存才能获取缓存中第二个端口的默认 GID。 导出roce_rescan_device以提供在绑定新端口后刷新缓存的机制。 在多端口配置中，所有 IB 对象（QP、MR、PD 等）相关命令应流经主站 mlx5_core_dev，其他命令必须发送到从端口 mlx5_core_mdev，提供一个接口来获取非 IB 对象命令的正确 mdev
                ib_enum_roce_netdev pass_all_filter enum_all_gids_of_dev_cb
        rdma_for_each_port
            ib_cache_update(device, p, true, true, true) -> IB/核心：仅在相应事件上更新 PKEY 和 GID 缓存，HCA 中的 PKEY 和 GID 表都可以保存数百个条目。 阅读它们是昂贵的。 部分原因是用于检索它们的 API 一次仅返回一个条目。 此外，在某些实现上，例如 CX-3，VF 在这方面是半虚拟化的，并且必须依赖 PF 驱动程序来执行读取。 这再次需要 VF 到 PF 的通信。 IB Core 的缓存会根据所有事件进行刷新。 因此，根据收到的事件分别为 IB_EVENT_PKEY_CHANGE 和 IB_EVENT_GID_CHANGE 来过滤 PKEY 和 GID 缓存的刷新
                rdma_is_port_valid
                ib_query_port(device, port, tprops)
                rdma_protocol_roce
                config_non_roce_gid_cache(device, port,	tprops) -> IB/核心：重构RoCE的GID修改代码，代码被重构为RoCE准备单独的函数，可以执行与引用计数相关的更复杂的操作，同时仍然保持代码的可读性。 这包括 (a) 简化为不执行 IB 链路层的网络设备检查和修改。 (b) 不要添加具有 NULL 网络设备的 RoCE GID 条目； 相反，返回一个错误。 (c) 如果 GID 添加在提供者级别 add_gid() 失败，则不要在缓存中添加该条目并保持该条目标记为 INVALID。 (d) 简化并重用 ib_cache_gid_add()/del() 例程，以便它们甚至可以用于修改默认 GID。 这避免了修改默认 GID 时的一些代码重复。 (e) find_gid() 例程引用数据条目标志来将 GID 限定为有效或无效 GID，而不是依赖于 GID 内容的属性和零。 (f) gid_table_reserve_default() 在设置 GID 表时一开始就设置 GID 默认属性。 无需在 write_gid()、add_gid()、del_gid() 等低级函数中使用 default_gid 标志，因为它们在 GID 表更新期间永远不需要更新 GID 条目的 DEFAULT 属性。 作为此重构的结果，保留的 GID 0:0:0:0:0:0:0:0 不再可搜索，如下所述。 根据 IB 规范版本 1.3 第 4.1.1 节第 (6) 点，单播 GID 条目 0:0:0:0:0:0:0:0 是保留 GID，其片段如下。 “单播 GID 地址 0:0:0:0:0:0:0:0 是保留的 - 称为保留 GID。它不得分配给任何终端端口。它不得用作目标地址或 全局路由标头（GRH）。” GID 表缓存现在仅存储有效的 GID 条目。 在此补丁之前，可以使用 ib_find_cached_gid_by_port() 和其他类似的查找例程在 GID 表中搜索保留 GID 0:0:0:0:0:0:0:0。 零 GID 不再可搜索，因为它不应出现在 GRH 或路径记录条目中，如 IB 规范版本 1.3 第 4.1.1 节第 (6) 点、第 12.7.10 节和第 12.7.20 节中所述。 ib_cache_update() 被简化为检查链路层一次，对所有链路层使用统一的锁定方案，删除临时 gid 表分配/释放逻辑。 此外，(a) 扩展 ib_gid_attr 以存储端口和索引，以便 GID 查询例程可以从属性结构中获取端口和索引信息。 (b) 扩展 ib_gid_attr 来存储设备，以便在将来的代码中，当完成 GID 引用计数时，设备用于返回到 GID 表条目
                    device->ops.query_gid(device, port, i, &gid_attr.gid)
                    rdma_protocol_iwarp(device, port)
                    add_modify_gid(table, &gid_attr)
                ib_query_pkey -> Get P_Key table entry
                    device->ops.query_pkey(device, port_num, index, pkey) -> irdma_query_pkey -> #define IRDMA_DEFAULT_PKEY		0xFFFF
                ib_security_cache_change -> IB/核心：在 QP 上强制执行 PKey 安全性，添加新的 LSM 挂钩以分配和释放安全上下文，并检查访问 PKey 的权限。 创建和销毁 QP 时分配和释放安全上下文。 此上下文用于控制对 PKey 的访问。 当请求修改 QP 来更改端口、PKey 索引或备用路径时，请检查 QP 是否具有对该端口子网前缀上的 PKey 表索引中的 PKey 的权限。 如果 QP 是共享的，请确保 QP 的所有句柄也具有访问权限。 存储 QP 正在使用的端口和 PKey 索引。 重置到初始化转换后，用户可以独立修改端口、PKey 索引和备用路径。 因此，端口和 PKey 设置更改可以是先前设置和新设置的合并。 为了在 PKey 表或子网前缀更改时维持访问控制，请保留每个端口上使用每个 PKey 索引的所有 QP 的列表。 如果发生更改，则使用该设备和端口的所有 QP 都必须强制执行新缓存设置的访问权限。 这些更改将事务添加到 QP 修改过程中。 如果修改失败，则必须保持与旧端口和 PKey 索引的关联；如果修改成功，则必须将其删除。 必须在修改之前建立与新端口和 PKey 索引的关联，如果修改失败则将其删除。 1. 当 QP 被修改为特定端口时，PKey 索引或备用路径将该 QP 插入到适当的列表中。 2. 检查访问新设置的权限。 3. 如果步骤 2 授予访问权限，则尝试修改 QP。 4a. 如果步骤 2 和 3 成功，则删除任何先前的关联。 4b. 如果以太失败，请删除新的设置关联。 如果 PKey 表或子网前缀发生更改，则遍历 QP 列表并检查它们是否具有权限。 如果没有，则将 QP 发送到错误状态并引发致命错误事件。 如果它是共享 QP，请确保共享 real_qp 的所有 QP 也具有权限。 如果拥有安全结构的 QP 被拒绝访问，则安全结构将被标记为此类，并且 QP 将被添加到 error_list 中。 一旦将 QP 移至错误完成，安全结构标记就会被清除。 正确维护列表会将 QP 销毁转变为事务。 设备的硬件驱动程序释放 ib_qp 结构，因此当销毁正在进行时，ib_qp_security 结构中的 ib_qp 指针未定义。 当销毁过程开始时，ib_qp_security 结构被标记为正在销毁。 这可以防止对 QP 指针采取任何操作。 QP 成功销毁后，它仍然可以列在 error_list 上，等待该流处理它，然后再清理结构。 如果销毁失败，则 QP 端口和 PKey 设置将重新插入到适当的列表中，销毁标志将被清除，并强制执行访问控制，以防在销毁流程期间发生任何缓存更改。 为了保持安全更改隔离，使用新文件来保存与安全相关的功能
                    list_for_each_entry (pkey, &device->port_data[port_num].pkey_list,
                        check_pkey_qps(pkey, device, port_num, subnet_prefix)
                            ib_get_cached_pkey
                            enforce_qp_pkey_security
                                security_ib_pkey_access -> Check if access to an IB pkey is allowed
                                    return call_int_hook(ib_pkey_access, 0, sec, subnet_prefix, pkey) -> selinux_ib_pkey_access
                                        sel_ib_pkey_sid(subnet_prefix, pkey_val, &sid)
                                        avc_has_perm(sec->sid, sid,
                            qp_to_error(pp->sec)
                                .qp_state = IB_QPS_ERR
                                .event = IB_EVENT_QP_FATAL
                                ib_modify_qp(sec->qp,
                                sec->qp->event_handler(&event,
                            list_del(&pp->to_error_list)
                            complete(&pp->sec->error_complete)
    device->groups[0] = &ib_dev_attr_group; -> RDMA/core：通过普通组机制创建设备 hw_counters，而不是调用 device_add_groups() 将组添加到通过 device_add() 管理的现有组数组中。 这需要在 device_add() 之前设置 hw_counters，以便它从已经分割的端口 sysfs 流中分割出来 -> 第一组用于设备属性，第二组用于驱动程序提供的属性（可选）。 第三组是 hw_stats 它是一个以 NULL 结尾的数组
    device->groups[1] = device->ops.device_group;
    ib_setup_device_attrs
        data = alloc_hw_stats_device(ibdev)
        ibdev->ops.get_hw_stats
        sysfs_attr_init(&attr->attr.attr)
        attr->attr.show = hw_stat_device_show
        attr->show = show_hw_stats
        attr->show = show_stats_lifespan;
        attr->attr.store = hw_stat_device_store;
        attr->store = set_stats_lifespan;
    ib_device_register_rdmacg
    rdma_counter_init
    dev_set_uevent_suppress
    ib_setup_port_attrs
    enable_device_and_get
        add_client_context
            client->add(device) -> .add    = ib_uverbs_add_one,
        add_compat_devs(device) -> RDMA/core：在net命名空间中实现compat device/sysfs树，实现ib_core的兼容层sysfs条目，以便非init_net net命名空间也可以发现rdma设备。 每个非 init_net 网络命名空间都在其中创建了 ib_core_device。 这样的 ib_core_device sysfs 树类似于 init_net 命名空间中找到的 rdma 设备。 这允许通过 sysfs 条目在多个非 init_net 网络命名空间中发现 rdma 设备，并且对 rdma-core 用户空间很有帮助
            add_one_compat_dev
    dev_set_uevent_suppress
    kobject_uevent(&device->dev.kobj, KOBJ_ADD)
    ib_device_put


static struct ib_client uverbs_client = {
    .name   = "uverbs",
    .no_kverbs_req = true,
    .add    = ib_uverbs_add_one,
    .remove = ib_uverbs_remove_one,
    .get_nl_info = ib_uverbs_get_nl_info,
};

ib_uverbs_add_one  -> RDMA：允许 ib_client 在调用 add() 时失败，添加客户端时不允许失败，但所有客户端在其添加例程中都有各种失败路径。 这会产生一种非常边缘的情况：添加客户端后，在添加过程中失败并且未设置 client_data。 然后，核心代码仍然会使用 NULL client_data 调用其他以 client_data 为中心的操作，例如 remove()、rename()、get_nl_info() 和 get_net_dev_by_params() - 这是令人困惑和意外的。 如果 add() 回调失败，则不要再为设备调用任何客户端操作，甚至删除。 删除操作回调中现在对 NULL client_data 的所有冗余检查。 更新所有 add() 回调以正确返回错误代码。 EOPNOTSUPP 用于 ULP 不支持 ib_device 的情况 - 例如，因为它仅适用于 IB
参考: https://www.cnblogs.com/vlhn/p/8301427.html
    device_initialize(&uverbs_dev->dev)
    init_completion(&uverbs_dev->comp)
    uverbs_dev->xrcd_tree = RB_ROOT
    INIT_LIST_HEAD(&uverbs_dev->uverbs_file_list) <- list_add_tail(&file->list, &dev->uverbs_file_list) <- ib_uverbs_open
    ib_uverbs_create_uapi
        uverbs_alloc_api
            uapi_merge_def(uapi, ibdev, uverbs_core_api, false)
                uapi_merge_obj_tree
                    uapi_merge_method
                case UAPI_DEF_WRITE:
		            rc = uapi_create_write(uapi, ibdev, def, cur_obj_key, &cur_method_key);
                        method_elm->handler = def->func_write -> 注册内核态uverbs接口
    dev_set_name uverbs/xxx
    cdev_init device->ops.mmap ? &uverbs_mmap_fops : &uverbs_fops);
    cdev_device_add
    ib_set_client_data
        xa_store(&device->client_data, client->client_id, data,


定义内核态供用户态使用的verbs内核接口
drivers/infiniband/core/uverbs_uapi.c
static const struct uapi_definition uverbs_core_api[] = {
	UAPI_DEF_CHAIN(uverbs_def_obj_counters),
	UAPI_DEF_CHAIN(uverbs_def_obj_cq),
	UAPI_DEF_CHAIN(uverbs_def_obj_device),
	UAPI_DEF_CHAIN(uverbs_def_obj_dm),
	UAPI_DEF_CHAIN(uverbs_def_obj_flow_action),
	UAPI_DEF_CHAIN(uverbs_def_obj_intf),
	UAPI_DEF_CHAIN(uverbs_def_obj_mr),
	UAPI_DEF_CHAIN(uverbs_def_write_intf),
	{},
};

rdma user/kernel api/abi:
const struct uapi_definition uverbs_def_write_intf[] = {
    ...
    DECLARE_UVERBS_OBJECT(
    UVERBS_OBJECT_PD,
    DECLARE_UVERBS_WRITE(
        IB_USER_VERBS_CMD_ALLOC_PD,
        ib_uverbs_alloc_pd,
        UAPI_DEF_WRITE_UDATA_IO(struct ib_uverbs_alloc_pd,
                    struct ib_uverbs_alloc_pd_resp),
        UAPI_DEF_METHOD_NEEDS_FN(alloc_pd)),
    DECLARE_UVERBS_WRITE(
        IB_USER_VERBS_CMD_DEALLOC_PD,
        ib_uverbs_dealloc_pd,
        UAPI_DEF_WRITE_I(struct ib_uverbs_dealloc_pd),
        UAPI_DEF_METHOD_NEEDS_FN(dealloc_pd))),
    ...
}
{
    .kind = UAPI_DEF_OBJECT_START, 
    .object_start = { .object_id = UVERBS_OBJECT_PD }, 
},
{ 
    .kind = UAPI_DEF_WRITE, 
    .scope = UAPI_SCOPE_OBJECT, 
    .write = { 
        .is_ex = 0, 
        .command_num = IB_USER_VERBS_CMD_ALLOC_PD 
        }, 
        .func_write = ib_uverbs_alloc_pd,   <- method_elm->handler = def->func_write
        .write.has_resp = 1 + (sizeof(struct { int:(-!!(offsetof(struct ib_uverbs_alloc_pd, response) != 0)); })) + (sizeof(struct { int:(-!!(sizeof(((struct ib_uverbs_alloc_pd *)0)->response) != sizeof(u64))); })), 
        .write.req_size = sizeof(struct ib_uverbs_alloc_pd), .write.resp_size = sizeof(struct ib_uverbs_alloc_pd_resp), 
        .write.has_udata = 1 + (sizeof(struct { int:(-!!(offsetof(struct ib_uverbs_alloc_pd, driver_data) != sizeof(struct ib_uverbs_alloc_pd))); })) + (sizeof(struct { int:(-!!(offsetof(struct ib_uverbs_alloc_pd_resp, driver_data) != sizeof(struct ib_uverbs_alloc_pd_resp))); })), 
},
{ 
    .kind = UAPI_DEF_IS_SUPPORTED_DEV_FN, 
    .scope = UAPI_SCOPE_METHOD, 
    .needs_fn_offset = offsetof(struct ib_device_ops, alloc_pd) + (sizeof(struct { int:(-!!(sizeof(((struct ib_device_ops *)0)->alloc_pd) != sizeof(void *))); })), }, 
{ 
    .kind = UAPI_DEF_WRITE, 
    .scope = UAPI_SCOPE_OBJECT, 
    .write = { 
        .is_ex = 0, 
        .command_num = IB_USER_VERBS_CMD_DEALLOC_PD 
    }, 
    .func_write = ib_uverbs_dealloc_pd, 
    .write.req_size = sizeof(struct ib_uverbs_dealloc_pd), 
},
{ 
    .kind = UAPI_DEF_IS_SUPPORTED_DEV_FN, 
    .scope = UAPI_SCOPE_METHOD, 
    .needs_fn_offset = offsetof(struct ib_device_ops, dealloc_pd) + (sizeof(struct { int:(-!!(sizeof(((struct ib_device_ops *)0)->dealloc_pd) != sizeof(void *))); })), 
}


    

qemu guest vm:
ibv_alloc_pd
root@u20:~/project/rdma/rdma-core/build/bin# ./ibv_rc_pingpong -d rxe_ens3 -g 0
Thread 3 hit Breakpoint 2, ib_uverbs_alloc_pd (attrs=0xffffc900045f3c88) at drivers/infiniband/core/uverbs_cmd.c:406
406     {
(gdb) bt
#0  ib_uverbs_alloc_pd (attrs=0xffffc900045f3c88) at drivers/infiniband/core/uverbs_cmd.c:406
#1  0xffffffffa065c825 in ib_uverbs_handler_UVERBS_METHOD_INVOKE_WRITE (attrs=0xffffc900045f3c88) at drivers/infiniband/core/uverbs_std_types_device.c:41
#2  0xffffffffa0659a95 in ib_uverbs_run_method (num_attrs=<optimized out>, pbundle=<optimized out>) at drivers/infiniband/core/uverbs_ioctl.c:471
#3  ib_uverbs_cmd_verbs (ufile=<optimized out>, hdr=<optimized out>, user_attrs=<optimized out>) at drivers/infiniband/core/uverbs_ioctl.c:612
#4  0xffffffffa0659cc8 in ib_uverbs_ioctl (filp=<optimized out>, cmd=<optimized out>, arg=140726932025184) at drivers/infiniband/core/uverbs_ioctl.c:644
#5  0xffffffff81371f87 in vfs_ioctl (arg=<optimized out>, cmd=<optimized out>, filp=<optimized out>) at fs/ioctl.c:47
#6  file_ioctl (arg=<optimized out>, cmd=<optimized out>, filp=<optimized out>) at fs/ioctl.c:510
#7  do_vfs_ioctl (filp=0xffff8880aa0dff00, fd=<optimized out>, cmd=<optimized out>, arg=140726932025184) at fs/ioctl.c:697
#8  0xffffffff81372257 in ksys_ioctl (fd=3, cmd=3222805249, arg=140726932025184) at fs/ioctl.c:714
#9  0xffffffff8137229a in __do_sys_ioctl (arg=<optimized out>, cmd=<optimized out>, fd=<optimized out>) at fs/ioctl.c:721
#10 __se_sys_ioctl (arg=<optimized out>, cmd=<optimized out>, fd=<optimized out>) at fs/ioctl.c:719
#11 __x64_sys_ioctl (regs=<optimized out>) at fs/ioctl.c:719
#12 0xffffffff81005497 in do_syscall_64 (nr=<optimized out>, regs=0xffffc900045f3f58) at arch/x86/entry/common.c:290
#13 0xffffffff81e0008c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:175
#14 0x0000000000000004 in fixed_percpu_data ()
#15 0x00005596cd698260 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#16 0x0000000000000004 in fixed_percpu_data ()
#17 0x00007ffd8acb30ec in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#18 0x00007ffd8acb2f78 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#19 0x00007ffd8acb2f40 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#20 0x0000000000000246 in ib_umem_odp_unmap_dma_pages (umem_odp=0x0 <fixed_percpu_data>, virt=<optimized out>, bound=<optimized out>)

(gdb) info threads
  Id   Target Id                    Frame 
  1    Thread 1.1 (CPU#0 [running]) timerqueue_add (head=0xffff88813ba1dee0, node=0xffff88813ba1e3a0) at lib/timerqueue.c:37
  2    Thread 1.2 (CPU#1 [running]) vring_map_single (vq=0xffff88813a868a80, cpu_addr=0xffff888138762e00, size=432, direction=<optimized out>) at drivers/virtio/virtio_ring.c:342
* 3    Thread 1.3 (CPU#2 [running]) timerqueue_add (head=0xffff88813ba9dee0, node=0xffff88813ba9e3a0) at lib/timerqueue.c:47
  4    Thread 1.4 (CPU#3 [running]) 0xffffffff811e5125 in seccomp_run_filters (match=<optimized out>, sd=<optimized out>) at kernel/seccomp.c:272
  5    Thread 1.5 (CPU#4 [halted ]) 0xffffffff81c46cce in native_safe_halt () at ./arch/x86/include/asm/irqflags.h:60
  6    Thread 1.6 (CPU#5 [halted ]) 0xffffffff81c46cce in native_safe_halt () at ./arch/x86/include/asm/irqflags.h:60
  7    Thread 1.7 (CPU#6 [halted ]) 0xffffffff81c46cce in native_safe_halt () at ./arch/x86/include/asm/irqflags.h:60
  8    Thread 1.8 (CPU#7 [running]) 0x00007f99178f6142 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306




ib_uverbs_ex_create_flow


static const struct file_operations uverbs_fops = {
	.owner	 = THIS_MODULE,
	.write	 = ib_uverbs_write,
	.open	 = ib_uverbs_open,
	.release = ib_uverbs_close,
	.llseek	 = no_llseek,
	.unlocked_ioctl = ib_uverbs_ioctl,
	.compat_ioctl = ib_uverbs_ioctl,
};

...
ib_uverbs_add_one
    cdev_init(&uverbs_dev->cdev,device->ops.mmap ? &uverbs_mmap_fops : &uverbs_fops);

ioctl:
...
ENTRY(entry_SYSCALL_64)
    movq	%rsp, %rsi
    call	do_syscall_64 /* returns with IRQs disabled */ <- entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:175 -> __visible void do_syscall_64
        enter_from_user_mode
        ti = current_thread_info()
        regs->ax = sys_call_table[nr](regs) -> SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)-> ksys_ioctl(fd, cmd, arg)
            security_file_ioctl
            error = do_vfs_ioctl(f.file, fd, cmd, arg)
                switch (cmd)
                default:
                    error = file_ioctl(filp, cmd, arg)
                        switch (cmd)
                        vfs_ioctl(filp, cmd, arg) -> error = filp->f_op->unlocked_ioctl(filp, cmd, arg) <- .unlocked_ioctl = ib_uverbs_ioctl
                            copy_from_user
                            srcu_read_lock
                            ib_uverbs_cmd_verbs
                                radix_tree_iter_lookup uapi_key_ioctl_method(hdr->method_id)
                                ib_uverbs_run_method -> ret = handler(&pbundle->bundle) -> static int UVERBS_HANDLER(UVERBS_METHOD_INVOKE_WRITE)
                                    return method_elm->handler(attrs)
                                        ib_uverbs_alloc_pd

                            srcu_read_unlock
        syscall_return_slowpath(regs)



#define DECLARE_UVERBS_NAMED_METHOD(_method_id, ...)                           \
	static const struct uverbs_attr_def *const UVERBS_METHOD_ATTRS(        \
		_method_id)[] = { __VA_ARGS__ };                               \
	static const struct uverbs_method_def UVERBS_METHOD(_method_id) = {    \
		.id = _method_id,                                              \
		.handler = UVERBS_HANDLER(_method_id),                         \
		.num_attrs = ARRAY_SIZE(UVERBS_METHOD_ATTRS(_method_id)),      \
		.attrs = &UVERBS_METHOD_ATTRS(_method_id),                     \
	}    


...
ib_uverbs_alloc_pd
    uverbs_request
    uobj_alloc(UVERBS_OBJECT_PD, attrs, &ib_dev) -> handle SIGTRAP nostop noprint ignore
    rdma_zalloc_drv_obj(ib_dev, ib_pd)
    pd->res.type = RDMA_RESTRACK_PD
    ret = ib_dev->ops.alloc_pd(pd, &attrs->driver_udata) -> rxe_alloc_pd
        rxe_add_to_pool
            might_sleep_if -> _cond_resched
                should_resched -> unlikely(raw_cpu_read_4(__preempt_count) == preempt_offset)
                preempt_schedule_common
                do
                    preempt_latency_start
                    __schedule(true)
                    preempt_latency_stop(1)
                    preempt_enable_no_resched_notrace()
                rcu_all_qs
            kref_get
            ib_device_try_get
            elem->pool = pool
            kref_init
    uobj->object = pd
    rdma_restrack_uadd(&pd->res)
    uverbs_response
    uobj_alloc_commit

...
.create_qp = rxe_create_qp,
rxe_create_qp
    rxe_qp_from_init
        rxe_qp_init_req
            rxe_init_task(rxe, &qp->comp.task, qp, rxe_completer, "comp")
            timer_setup(&qp->rnr_nak_timer, rnr_nak_timer, 0)
            timer_setup(&qp->retrans_timer, retransmit_timer, 0) <- mod_timer(&qp->retrans_timer
        rxe_qp_init_resp
            rxe_init_task(rxe, &qp->resp.task, qp, rxe_responder, "resp");
                check_resource
                    if (pkt->mask & RXE_RWR_MASK)
                        get_srq_wqe
                            wqe = queue_head(q)
                            memcpy(&qp->resp.srq_wqe, wqe, sizeof(qp->resp.srq_wqe))
                            qp->resp.wqe = &qp->resp.srq_wqe.wqe
                            advance_consumer(q)
                            srq->ibsrq.event_handler(&ev, srq->ibsrq.srq_context)
                        qp->resp.wqe = queue_head(qp->rq.queue);
rxe_completer
    case COMPST_GET_WQE
        state = get_wqe(qp, pkt, &wqe)



bool timerqueue_add
    rb_link_node(&node->node, parent, p)


在 virtqueue_add 函数中，能看到free_head指向整个内存块空闲链表的起始位置，用head变量记住这个起始位置。接下来，i也指向这个起始位置，然后是一个for循环，将数据放到内存块里面，放的过程中next不断指向下一个空闲位置，这样空闲的内存块被不断的占用。等所有的写入都结束了，i就会指向这次存放的内存块的下一个空闲位置，然后free_head就指向i，因为前面的都填满了。, 至此，从head到i之间的内存块，就是这次写入的全部数据。于是，在vring的avail变量中，在ring[]数组中分配新的一项，在avail的位置，avail的计算是avail_idx_shadow & (vq->vring.num - 1)，其中avail_idx_shadow是上一次的avail的位置，这里如果超过了ring[]数组的下标，则重新跳到起始位置，就说明是一个环，这次分配的新的avail的位置就存放新写入的从head到i之间的内存块。然后是avail_idx_shadow++，这说明这一块内存可以被接收方读取了
static inline int virtqueue_add_split
    head = vq->free_head
    i = head
    for (n = 0; n < out_sgs; n++)
        for (sg = sgs[n]; sg; sg = sg_next(sg))
            i = virtqueue_add_desc_split(_vq, desc, i, addr, sg->length, VRING_DESC_F_NEXT, indirect)
    static dma_addr_t vring_map_single


(gdb) thread 2
[Switching to thread 2 (Thread 1.2)]
#0  vring_map_single (vq=0xffff88813a868a80, cpu_addr=0xffff888138762e00, size=432, direction=<optimized out>) at drivers/virtio/virtio_ring.c:342
342     static dma_addr_t vring_map_single(const struct vring_virtqueue *vq,
(gdb) bt
#0  vring_map_single (vq=0xffff88813a868a80, cpu_addr=0xffff888138762e00, size=432, direction=<optimized out>) at drivers/virtio/virtio_ring.c:342
#1  0xffffffff8174396e in virtqueue_add_split (gfp=<optimized out>, ctx=<optimized out>, data=<optimized out>, in_sgs=<optimized out>, out_sgs=<optimized out>, total_sg=<optimized out>, sgs=<optimized out>, _vq=<optimized out>) at drivers/virtio/virtio_ring.c:512
#2  virtqueue_add (gfp=<optimized out>, ctx=<optimized out>, data=<optimized out>, in_sgs=<optimized out>, out_sgs=<optimized out>, total_sg=<optimized out>, sgs=<optimized out>, _vq=<optimized out>) at drivers/virtio/virtio_ring.c:1706
#3  virtqueue_add_sgs (_vq=0xffff88813a868a80, sgs=<optimized out>, out_sgs=<optimized out>, in_sgs=947269120, data=<optimized out>, gfp=<optimized out>) at drivers/virtio/virtio_ring.c:1740
#4  0xffffffffa0016d7b in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#5  0x0000000000000010 in fixed_percpu_data ()
#6  0x0000000100000000 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#7  0xffff88813a868a80 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#8  0xffffc90001143a18 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#9  0x0000000000000286 in consumer_addr (q=<optimized out>) at drivers/infiniband/sw/rxe/rxe_resp.c:923
#10 queue_head (q=<optimized out>) at drivers/infiniband/sw/rxe/rxe_queue.h:171
#11 check_resource (pkt=<optimized out>, qp=<optimized out>) at drivers/infiniband/sw/rxe/rxe_resp.c:395
#12 rxe_responder (arg=0x2 <fixed_percpu_data+2>) at drivers/infiniband/sw/rxe/rxe_resp.c:1264
#13 0xffffffff815a2426 in __blk_mq_issue_directly (last=<optimized out>, cookie=<optimized out>, rq=<optimized out>, hctx=<optimized out>) at block/blk-mq.c:1809
#14 __blk_mq_try_issue_directly (hctx=0xffff888035689000, rq=0xffff8880363363c0, cookie=0xffffc90001143a74, bypass_insert=true, last=<optimized out>) at block/blk-mq.c:1861
#15 0xffffffff815a30eb in blk_mq_request_issue_directly (rq=0xffff8880363363c0, last=<optimized out>) at block/blk-mq.c:1897
#16 0xffffffff815a31c6 in blk_mq_try_issue_list_directly (hctx=0xffff888035689000, list=0xffffc90001143b50) at ./include/linux/list.h:268
#17 0xffffffff815a880e in blk_mq_sched_insert_requests (hctx=0xffff888035689000, ctx=0xffffe8ffffc47bc0, list=<optimized out>, run_queue_async=false) at block/blk-mq-sched.c:437
#18 0xffffffff815a2ff2 in blk_mq_flush_plug_list (plug=<optimized out>, from_schedule=<optimized out>) at block/blk-mq.c:1772
#19 0xffffffff815977d3 in blk_flush_plug_list (plug=0xffffc90001143c70, from_schedule=<optimized out>) at block/blk-core.c:1769
#20 0xffffffff81597826 in blk_finish_plug (plug=<optimized out>) at block/blk-core.c:1786
#21 blk_finish_plug (plug=<optimized out>) at block/blk-core.c:1782
#22 0xffffffff8143cd58 in ext4_writepages (mapping=<optimized out>, wbc=<optimized out>) at fs/ext4/inode.c:2939
#23 0xffffffff812973e3 in do_writepages (mapping=0xffff8881153170d8, wbc=0xffffc90001143dc0) at mm/page-writeback.c:2344
#24 0xffffffff8128d0c5 in __filemap_fdatawrite_range (mapping=0xffff8881153170d8, start=<optimized out>, end=<optimized out>, sync_mode=<optimized out>) at mm/filemap.c:421
#25 0xffffffff8128e574 in file_write_and_wait_range (file=0xffff888138ae7d00, lstart=0, lend=9223372036854775807) at mm/filemap.c:782
#26 0xffffffff8142a023 in ext4_sync_file (file=0xffff888138ae7d00, start=<optimized out>, end=9223372036854775807, datasync=0) at fs/ext4/fsync.c:128
#27 0xffffffff81398449 in vfs_fsync_range (file=0xffff888138ae7d00, start=0, end=<optimized out>, datasync=<optimized out>) at fs/sync.c:197
#28 0xffffffff813984dd in vfs_fsync (datasync=<optimized out>, file=<optimized out>) at fs/sync.c:221
#29 do_fsync (fd=<optimized out>, datasync=0) at fs/sync.c:221
#30 0xffffffff81398524 in __do_sys_fsync (fd=<optimized out>) at fs/sync.c:229
#31 __se_sys_fsync (fd=<optimized out>) at fs/sync.c:227
#32 __x64_sys_fsync (regs=<optimized out>) at fs/sync.c:227
#33 0xffffffff81005497 in do_syscall_64 (nr=<optimized out>, regs=0xffffc90001143f58) at arch/x86/entry/common.c:290
#34 0xffffffff81e0008c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:175
#35 0x0000000000000000 in ?? ()





(gdb) bt
#0  ib_uverbs_alloc_pd (attrs=0xffffc90004243c88) at drivers/infiniband/core/uverbs_cmd.c:406
#1  0xffffffffa065c825 in ib_uverbs_handler_UVERBS_METHOD_INVOKE_WRITE (attrs=0xffffc90004243c88) at drivers/infiniband/core/uverbs_std_types_device.c:41
#2  0xffffffffa0659a95 in ib_uverbs_run_method (num_attrs=<optimized out>, pbundle=<optimized out>) at drivers/infiniband/core/uverbs_ioctl.c:471
#3  ib_uverbs_cmd_verbs (ufile=<optimized out>, hdr=<optimized out>, user_attrs=<optimized out>) at drivers/infiniband/core/uverbs_ioctl.c:612
#4  0xffffffffa0659cc8 in ib_uverbs_ioctl (filp=<optimized out>, cmd=<optimized out>, arg=140737488346176) at drivers/infiniband/core/uverbs_ioctl.c:644
#5  0xffffffff81371f87 in vfs_ioctl (arg=<optimized out>, cmd=<optimized out>, filp=<optimized out>) at fs/ioctl.c:47
#6  file_ioctl (arg=<optimized out>, cmd=<optimized out>, filp=<optimized out>) at fs/ioctl.c:510
#7  do_vfs_ioctl (filp=0xffff8880b6aa8600, fd=<optimized out>, cmd=<optimized out>, arg=140737488346176) at fs/ioctl.c:697
#8  0xffffffff81372257 in ksys_ioctl (fd=3, cmd=3222805249, arg=140737488346176) at fs/ioctl.c:714
#9  0xffffffff8137229a in __do_sys_ioctl (arg=<optimized out>, cmd=<optimized out>, fd=<optimized out>) at fs/ioctl.c:721
#10 __se_sys_ioctl (arg=<optimized out>, cmd=<optimized out>, fd=<optimized out>) at fs/ioctl.c:719
#11 __x64_sys_ioctl (regs=<optimized out>) at fs/ioctl.c:719
#12 0xffffffff81005497 in do_syscall_64 (nr=<optimized out>, regs=0xffffc90004243f58) at arch/x86/entry/common.c:290
#13 0xffffffff81e0008c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:175
#14 0x0000000000000004 in fixed_percpu_data ()
#15 0x0000555555567260 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#16 0x0000000000000004 in fixed_percpu_data ()
#17 0x00007fffffffddcc in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#18 0x00007fffffffdc58 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#19 0x00007fffffffdc20 in ?? () at drivers/infiniband/core/uverbs_cmd.c:3306
#20 0x0000000000000246 in consumer_addr (q=<optimized out>) at drivers/infiniband/sw/rxe/rxe_resp.c:974
#21 queue_head (q=<optimized out>) at drivers/infiniband/sw/rxe/rxe_queue.h:171
#22 get_srq_wqe (qp=<optimized out>) at drivers/infiniband/sw/rxe/rxe_resp.c:328
#23 check_resource (pkt=<optimized out>, qp=<optimized out>) at drivers/infiniband/sw/rxe/rxe_resp.c:393
#24 rxe_responder (arg=0x0 <fixed_percpu_data>) at drivers/infiniband/sw/rxe/rxe_resp.c:1264








rxe:
commit: https://github.com/ssbandjl/linux/commit/8700e3e7c4857d28ebaa824509934556da0b3e76

Soft RoCE 驱动程序 Soft RoCE (RXE) - 软件 RoCE 驱动程序 ib_rxe 实现 RDMA 传输并作为内核动词提供程序注册到 RDMA 核心设备。 它还实现了数据包IO层。 另一方面，ib_rxe 作为 udp 封装协议（在这种情况下为 RDMA）注册到 Linux netdev 堆栈，用于通过任何以太网设备发送和接收数据包。 这会在 UDP/以太网网络层上产生 RDMA 传输，形成 RoCEv2 兼容设备。 Soft RoCE 驱动程序的配置过程需要绑定到任何现有的以太网网络设备。 这是通过 /sys 接口完成的。 用户空间 Soft RoCE 库 (librxe) 为用户应用程序提供了与 Soft RoCE 设备一起运行的能力。 在用户空间中使用 rxe 动词需要包含 librxe 作为 libibverbs 的设备特定插件。 librxe是单独打包的

Architecture:

     +-----------------------------------------------------------+
     |                          Application                      |
     +-----------------------------------------------------------+
                            +-----------------------------------+
                            |             libibverbs            |
User                        +-----------------------------------+
                            +----------------+ +----------------+
                            | librxe         | | HW RoCE lib    |
                            +----------------+ +----------------+
+---------------------------------------------------------------+
     +--------------+                           +------------+
     | Sockets      |                           | RDMA ULP   |
     +--------------+                           +------------+
     +--------------+                  +---------------------+
     | TCP/IP       |                  | ib_core             |
     +--------------+                  +---------------------+
                             +------------+ +----------------+
Kernel                       | ib_rxe     | | HW RoCE driver |
                             +------------+ +----------------+
     +------------------------------------+
     | NIC driver                         |
     +------------------------------------+





late_initcall(rxe_module_init); -> rxe_module_init -> rdma_rxe：确保rdma_rxe init在正确的时间发生，当CONFIG_RDMA_RXE=y且CONFIG_IPV6=y时出现问题。 这会导致 rdma_rxe 初始化在 IPv6 服务准备就绪之前发生。 该补丁将 rdma_rxe 的初始化延迟到 IPv6 服务准备就绪之后。 此修复基于 Logan Gunthorpe 在更旧的代码库上提出的修复
    rxe_alloc_wq
        rxe_wq = alloc_workqueue("rxe_wq", WQ_UNBOUND, WQ_MAX_ACTIVE)
    rxe_net_init
        rxe_net_ipv4_init
            rxe_setup_udp_tunnel
                udp_sock_create
                    udp_sock_create4
                        sock_create_kern
                        kernel_bind
                    or udp_sock_create6
                tnl_cfg.encap_rcv = rxe_udp_encap_recv
                    rxe_get_dev_from_net
                    if (skb_linearize(skb))
                    udph = udp_hdr(skb)
                    pkt->paylen = be16_to_cpu(udph->len) - sizeof(*udph)
                    skb_pull(skb, sizeof(struct udphdr))
                    rxe_rcv(skb)
                        rxe_chk_dgid
                        pkt->opcode = bth_opcode(pkt)
                        pkt->psn = bth_psn(pkt)
                        pkt->mask |= rxe_opcode[pkt->opcode].mask -> struct rxe_opcode_info rxe_opcode[RXE_NUM_OPCODE]
                        hdr_check
                        rxe_icrc_check
                            rxe_icrc_hdr
                            rxe_crc32
                                SHASH_DESC_ON_STACK
                                crypto_shash_update
                                shash_desc_ctx
                                barrier_data
                            icrc = ~icrc
                        rxe_counter_inc
                            atomic64_inc(&rxe->stats_counters[index])
                        rxe_rcv_pkt(pkt, skb)
                            rxe_resp_queue_pkt
                            or rxe_comp_queue_pkt
                                skb_queue_tail(&qp->resp_pkts, skb)
                                rxe_sched_task(&qp->comp.task)
                                or rxe_run_task(&qp->comp.task)
                setup_udp_tunnel_sock
        rxe_net_ipv6_init
        register_netdevice_notifier(&rxe_net_notifier)
    rdma_link_register(&rxe_link_ops) -> RDMA/core：添加 RDMA_NLDEV_CMD_NEWLINK/DELLINK 支持，添加对新 LINK 消息的支持以允许添加和删除 rdma 接口。 这最初将用于软 rdma 驱动程序，该驱动程序由管理员指定要使用的 netdev 设备动态实例化设备实例。 rdma_rxe 模块将是这些消息的第一个用户。 该设计是根据 RTNL_NEWLINK/DELLINK 建模的：如果 rdma 驱动程序提供链接添加/删除功能，则它们会向 rdma 内核注册。 每个驱动程序都注册一个唯一的“类型”字符串，用于调度来自用户空间的消息。 为“type”字符串定义了新的 RDMA_NLDEV_ATTR。 用户模式将在 NEWLINK 消息中传递 3 个属性：RDMA_NLDEV_ATTR_DEV_NAME 表示要创建的所需 rdma 设备名称，RDMA_NLDEV_ATTR_LINK_TYPE 表示要添加的链接“类型”，RDMA_NLDEV_ATTR_NDEV_NAME 表示用于此链接的 net_device 接口。 DELLINK 消息将包含要删除的设备的 RDMA_NLDEV_ATTR_DEV_INDEX
        down_write(&link_ops_rwsem)
        list_add(&ops->list, &link_ops)
        up_write(&link_ops_rwsem)
    pr_info("loaded\n")


static struct rdma_link_ops rxe_link_ops = {
	.type = "rxe",
	.newlink = rxe_newlink,
};
rxe_newlink -> 添加对 RDMA_NLDEV_CMD_NEWLINK/DELLINK 消息的支持，允许动态添加新的 RXE 链接。 暂时弃用旧的模块选项
    is_vlan_dev -> RDMA/rxe：防止在 vlan 接口之上创建 rxe，在 vlan 接口之上创建 rxe 设备将创建一个无功能的设备，该设备具有空的 gids 表，并且不能用于 rdma cm 通信。 这是由 enum_all_gids_of_dev_cb()/is_eth_port_of_netdev() 中的逻辑引起的，该逻辑仅考虑连接到已配置网络设备的“上层设备”的网络，导致 vlan 接口的 gid 集为空，并尝试通过此 rdma 连接 由于无法解析 gid，设备在 cm_init_av_for_response 中失败。 显然，实现此行为是为了适应为每个端口创建 RoCE 设备的 HW-RoCE 设备，因此 RXE 的行为必须与 HW-RoCE 设备相同，并且仅为每个真实设备创建 rxe 设备。 为了通过 vlan 接口进行通信，用户必须使用 vlan 地址的 gid 索引，而不是通过 vlan 创建 rxe
    rxe_get_dev_from_net
    rxe_net_add
        ib_alloc_device
        rxe_add
            rxe_init -> RDMA/rxe：用xarray替换红黑树，当前rxe驱动程序使用红黑树向rxe对象池添加索引。 Linux xarrays 提供了一种更好的方法来实现索引的相同功能。 此补丁将池对象的红黑树替换为 xarray。 由于 xarray 已经有一个自旋锁，请使用它来代替池 rwlock。 确保 xarray(index) 和 kref(ref count) 中的所有更改均以原子方式发生
                rxe_init_device_param
                    rxe->attr.vendor_id			= RXE_VENDOR_ID
                    addrconf_addr_eui48((unsigned char *)&rxe->attr.sys_image_guid -> RDMA/rxe：将 sys_image_guid 设置为与 HW IB 设备对齐，RXE 驱动程序不设置 sys_image_guid，并且用户空间应用程序看到零。 这会导致 pyverbs 测试失败并出现以下回溯，因为 IBTA 规范要求具有有效的 sys_image_guid。 回溯（最近一次调用最后一次）：文件“./tests/test_device.py”，第 51 行，在 test_query_device self.verify_device_attr(attr) 文件“./tests/test_device.py”，第 74 行，在 verify_device_attr 中断言 attr.sys_image_guid != 0 为了修复它，请将 sys_image_guid 设置为等于 node_guid
                rxe_init_ports -> RDMA/rxe：删除 pkey 表，RoCE 规范要求 RoCE 设备仅支持默认 pkey。 然而，rxe 驱动程序维护一个 64 个实体的 pkey 表，并且仅使用第一个条目。 删除 pkey 表并使用默认 pkey 硬连接长度为 1 的表进行硬编码。 将 pkey_table 的所有检查替换为与 default_pkey 的比较
                    rxe_init_port_param
                        port->attr.state		= IB_PORT_DOWN
                        ...
                rxe_init_pools
                    rxe_pool_init(rxe, &rxe->uc_pool, RXE_TYPE_UC)
                        pool->rxe		= rxe
                        pool->elem_size		= ALIGN(info->size, RXE_POOL_ALIGN)
                        xa_init_flags(&pool->xa, XA_FLAGS_ALLOC)
                rxe->mcg_tree = RB_ROOT
            rxe_set_mtu
                eth_mtu_int_to_enum
                mtu = mtu ? min_t(enum ib_mtu, mtu, IB_MTU_4096) : IB_MTU_256
            rxe_register_device(rxe, ibdev_name)
                dev->node_type = RDMA_NODE_IB_CA
                ib_set_device_ops(dev, &rxe_dev_ops)
                ib_device_set_netdev(&rxe->ib_dev, rxe->ndev, 1) ->  将 ib_dev 与底层 net_device 相关联，@ib_dev：要修改的设备 @ndev：要关联的 net_device，可能为 NULL @port：net_device 连接到的 IB 端口 驱动程序应使用它将 ib_device 链接到 netdev，以便 netdev 显示在 ib_enum_roce_netdev 等接口中。 任何端口只能有一个 netdev 附属。 调用者必须确保给定的 ndev 未未注册或正在取消注册，并且当 ndev 发送 NETDEV_UNREGISTER 事件时，ib_device 未注册或使用 NULL 调用 ib_device_set_netdev()
                    alloc_port_data -> 驱动程序希望在 ib_register_driver 之前调用它，因此我们必须尽早设置端口数据 -> RDMA/device：添加 ib_device_set_netdev() 作为 get_netdev 的替代方案，关联的 netdev 实际上不应该非常动态，因此对于大多数驱动程序来说，没有理由进行这样的回调。 提供一个 API 来通知核心代码有关网络开发从属关系，并使用核心维护的数据结构。 这使得核心代码能够更加了解 ndev 关系，从而允许一些基于此的新 API。 这也使用了某种意义上的锁定，许多驱动程序都有令人困惑的 RCU 锁定，或者缺少不正确的锁定
                        pdata_rcu = kzalloc -> device->port_data 直接按端口号进行索引，以使对该数据的访问尽可能高效。 因此 port_data 被声明为一个从 1 开始的数组，开头可能有空槽 -> struct_size -> 使用尾随数组计算结构的大小。 ：指向结构体的指针
                        device->port_data = pdata_rcu->pdata
                        rdma_for_each_port (device, port)
                            struct ib_port_data *pdata = &device->port_data[port]
                            INIT_LIST_HEAD(&pdata->pkey_list)
                            INIT_HLIST_NODE(&pdata->ndev_hash_link)
                    rdma_is_port_valid
                    pdata = &ib_dev->port_data[port]
                    old_ndev = rcu_dereference_protected(pdata->netdev, lockdep_is_held(&pdata->netdev_lock)) -> cache
                    netdev_tracker_free(ndev, &pdata->netdev_tracker)
                    netdev_hold(ndev, &pdata->netdev_tracker, GFP_ATOMIC)
                    add_ndev_hash
                        might_sleep
                        if (hash_hashed(&pdata->ndev_hash_link))
                            hash_del_rcu(&pdata->ndev_hash_link) -> delete old item from hash table
                        hash_add_rcu(ndev_hash, &pdata->ndev_hash_link, -> RDMA/device：添加 ib_device_get_by_netdev()，多个驱动程序需要从给定的 netdev 查找 ib_device。 rxe 在无法入睡的情况下需要快速完成此操作，因此选择使用 RCU 安全哈希表来实现转换。 哈希表可以具有多对一的映射。 这是为了支持未来多个 IB 驱动程序（即 iWarp 和 RoCE）连接到相同网络设备的情况。 driver_ids 需要不同才能支持这一点。 在此过程中，这通过推迟其 kfree 使 struct ib_device 和 ib_port_data RCU 变得安全 <- static struct hlist_head ndev_hash[1 << (5)]
                        hash_add_rcu(ndev_hash, &pdata->ndev_hash_link,(uintptr_t)pdata->netdev); -> add an object to a rcu enabled hashtable
                rxe_icrc_init
                    tfm = crypto_alloc_shash("crc32", 0, 0)
                ib_register_device -> 注册IB设备
                

rxe设备操作表
static const struct ib_device_ops rxe_dev_ops = {
	.owner = THIS_MODULE,
	.driver_id = RDMA_DRIVER_RXE,
	.uverbs_abi_ver = RXE_UVERBS_ABI_VERSION,

	.alloc_hw_port_stats = rxe_ib_alloc_hw_port_stats,
	.alloc_mr = rxe_alloc_mr,
	.alloc_mw = rxe_alloc_mw,
	.alloc_pd = rxe_alloc_pd,
	.alloc_ucontext = rxe_alloc_ucontext,
	.attach_mcast = rxe_attach_mcast,
	.create_ah = rxe_create_ah,
	.create_cq = rxe_create_cq,
	.create_qp = rxe_create_qp,
	.create_srq = rxe_create_srq,
	.create_user_ah = rxe_create_ah,
	.dealloc_driver = rxe_dealloc,
	.dealloc_mw = rxe_dealloc_mw,
	.dealloc_pd = rxe_dealloc_pd,
	.dealloc_ucontext = rxe_dealloc_ucontext,
	.dereg_mr = rxe_dereg_mr,
	.destroy_ah = rxe_destroy_ah,
	.destroy_cq = rxe_destroy_cq,
	.destroy_qp = rxe_destroy_qp,
	.destroy_srq = rxe_destroy_srq,
	.detach_mcast = rxe_detach_mcast,
	.device_group = &rxe_attr_group,
	.enable_driver = rxe_enable_driver,
	.get_dma_mr = rxe_get_dma_mr,
	.get_hw_stats = rxe_ib_get_hw_stats,
	.get_link_layer = rxe_get_link_layer,
	.get_port_immutable = rxe_port_immutable,
	.map_mr_sg = rxe_map_mr_sg,
	.mmap = rxe_mmap,
	.modify_ah = rxe_modify_ah,
	.modify_device = rxe_modify_device,
	.modify_port = rxe_modify_port,
	.modify_qp = rxe_modify_qp,
	.modify_srq = rxe_modify_srq,
	.peek_cq = rxe_peek_cq,
	.poll_cq = rxe_poll_cq,
	.post_recv = rxe_post_recv,
	.post_send = rxe_post_send,
	.post_srq_recv = rxe_post_srq_recv,
	.query_ah = rxe_query_ah,
	.query_device = rxe_query_device,
	.query_pkey = rxe_query_pkey,
	.query_port = rxe_query_port,
	.query_qp = rxe_query_qp,
	.query_srq = rxe_query_srq,
	.reg_user_mr = rxe_reg_user_mr,
	.req_notify_cq = rxe_req_notify_cq,
	.rereg_user_mr = rxe_rereg_user_mr,
	.resize_cq = rxe_resize_cq,

	INIT_RDMA_OBJ_SIZE(ib_ah, rxe_ah, ibah),
	INIT_RDMA_OBJ_SIZE(ib_cq, rxe_cq, ibcq),
	INIT_RDMA_OBJ_SIZE(ib_pd, rxe_pd, ibpd),
	INIT_RDMA_OBJ_SIZE(ib_qp, rxe_qp, ibqp),
	INIT_RDMA_OBJ_SIZE(ib_srq, rxe_srq, ibsrq),
	INIT_RDMA_OBJ_SIZE(ib_ucontext, rxe_ucontext, ibuc),
	INIT_RDMA_OBJ_SIZE(ib_mw, rxe_mw, ibmw),
};



mlx5_ib_stage_caps_init
    ib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_ops)
static const struct ib_device_ops mlx5_ib_dev_ops = {
	.owner = THIS_MODULE,
	.driver_id = RDMA_DRIVER_MLX5,
	.uverbs_abi_ver	= MLX5_IB_UVERBS_ABI_VERSION,

	.add_gid = mlx5_ib_add_gid,
	.alloc_mr = mlx5_ib_alloc_mr,
	.alloc_mr_integrity = mlx5_ib_alloc_mr_integrity,
        __mlx5_ib_alloc_mr(pd, IB_MR_TYPE_INTEGRITY, max_num_sg, max_num_meta_sg)
            mlx5_alloc_integrity_descs
                mlx5_ib_alloc_pi_mr
	.alloc_pd = mlx5_ib_alloc_pd,
	.alloc_ucontext = mlx5_ib_alloc_ucontext,
	.attach_mcast = mlx5_ib_mcg_attach,
	.check_mr_status = mlx5_ib_check_mr_status,
	.create_ah = mlx5_ib_create_ah,
	.create_cq = mlx5_ib_create_cq,
	.create_qp = mlx5_ib_create_qp,
	.create_srq = mlx5_ib_create_srq,
	.create_user_ah = mlx5_ib_create_ah,
	.dealloc_pd = mlx5_ib_dealloc_pd,
	.dealloc_ucontext = mlx5_ib_dealloc_ucontext,
	.del_gid = mlx5_ib_del_gid,
	.dereg_mr = mlx5_ib_dereg_mr,
	.destroy_ah = mlx5_ib_destroy_ah,
	.destroy_cq = mlx5_ib_destroy_cq,
	.destroy_qp = mlx5_ib_destroy_qp,
        if (mqp->type == IB_QPT_GSI)
            return mlx5_ib_destroy_gsi(mqp)
        if (mqp->type == MLX5_IB_QPT_DCT)
            return mlx5_ib_destroy_dct(mqp)
        destroy_qp_common(dev, mqp, udata)
            if (qp->state != IB_QPS_RESET)
            get_cqs(qp->type, qp->ibqp.send_cq, qp->ibqp.recv_cq, &send_cq,	&recv_cq)
            mlx5_ib_lock_cqs(send_cq, recv_cq)
            list_del(&qp->qps_list)
            list_del(&qp->cq_send_list)
            list_del(&qp->cq_recv_list)
			if (!udata) -> kernel
				__mlx5_ib_cq_clean(recv_cq, base->mqp.qpn, qp->ibqp.srq ? to_msrq(qp->ibqp.srq) : NULL)
					for (prod_index = cq->mcq.cons_index; get_sw_cqe(cq, prod_index); prod_index++)
						if (prod_index == cq->mcq.cons_index + cq->ibcq.cqe)
							break
					while ((int) --prod_index - (int) cq->mcq.cons_index >= 0)
						cqe = get_cqe(cq, prod_index & cq->ibcq.cqe)
						else if (nfreed)
							dest = get_cqe(cq, (prod_index + nfreed) & cq->ibcq.cqe)
							memcpy(dest, cqe, cq->mcq.cqe_sz)
					if (nfreed)
						cq->mcq.cons_index += nfreed
						wmb()
						mlx5_cq_set_ci(&cq->mcq)
							*cq->set_ci_db = cpu_to_be32(cq->cons_index & 0xffffff)
				if (send_cq != recv_cq)
					__mlx5_ib_cq_clean(send_cq, base->mqp.qpn, NULL)
			if (qp->type == IB_QPT_RAW_PACKET || qp->flags & IB_QP_CREATE_SOURCE_QPN)
				destroy_raw_packet_qp(dev, qp)
			else
				err = mlx5_core_destroy_qp(dev, &base->mqp)
					destroy_resource_common(dev, qp)
						radix_tree_delete(&table->tree, qp->qpn | (qp->common.res << MLX5_USER_INDEX_LEN))
						mlx5_core_put_rsc((struct mlx5_core_rsc_common *)qp)
							if (refcount_dec_and_test(&common->refcount)) -> If the reference count is zero after decrementing, it returns true
								complete(&common->free)
						wait_for_completion(&qp->common.free)
					MLX5_SET(destroy_qp_in, in, opcode, MLX5_CMD_OP_DESTROY_QP)
			destroy_qp(dev, qp, base, udata)
				if (udata)
					mlx5_ib_db_unmap_user(context, &qp->db);
					ib_umem_release(base->ubuffer.umem);
				mlx5_db_free(dev->mdev, &qp->db)
				mlx5_frag_buf_free(dev->mdev, &qp->buf)
	.destroy_srq = mlx5_ib_destroy_srq,
	.detach_mcast = mlx5_ib_mcg_detach,
	.disassociate_ucontext = mlx5_ib_disassociate_ucontext,
	.drain_rq = mlx5_ib_drain_rq,
	.drain_sq = mlx5_ib_drain_sq,
	.device_group = &mlx5_attr_group,
	.enable_driver = mlx5_ib_enable_driver,
	.get_dev_fw_str = get_dev_fw_str,
	.get_dma_mr = mlx5_ib_get_dma_mr,
	.get_link_layer = mlx5_ib_port_link_layer,
	.map_mr_sg = mlx5_ib_map_mr_sg,
        ib_dma_sync_single_for_cpu
        if (mr->access_mode == MLX5_MKC_ACCESS_MODE_KLMS)
            mlx5_ib_sg_to_klms(mr, sg, sg_nents, sg_offset, NULL, 0, NULL)
        else
            n = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, mlx5_set_page)
                mlx5_set_page
                    descs[mr->mmkey.ndescs++] = cpu_to_be64(addr | MLX5_EN_RD | MLX5_EN_WR)
        ib_dma_sync_single_for_device(ibmr->device, mr->desc_map, mr->desc_size * mr->max_descs, DMA_TO_DEVICE)
	.map_mr_sg_pi = mlx5_ib_map_mr_sg_pi,
        mlx5_ib_map_pa_mr_sg_pi
            mr->meta_length = sg_dma_len(meta_sg) - sg_offset
            mr->pi_iova = sg_dma_address(meta_sg) + sg_offset
        mlx5_ib_map_mtt_mr_sg_pi
        mlx5_ib_map_klm_mr_sg_pi
	.mmap = mlx5_ib_mmap,
	.mmap_free = mlx5_ib_mmap_free,
	.modify_cq = mlx5_ib_modify_cq,
	.modify_device = mlx5_ib_modify_device,
	.modify_port = mlx5_ib_modify_port,
	.modify_qp = mlx5_ib_modify_qp,
	.modify_srq = mlx5_ib_modify_srq,
	.poll_cq = mlx5_ib_poll_cq,
	.post_recv = mlx5_ib_post_recv_nodrain,
	.post_send = mlx5_ib_post_send_nodrain,
	.post_srq_recv = mlx5_ib_post_srq_recv,
	.process_mad = mlx5_ib_process_mad,
	.query_ah = mlx5_ib_query_ah,
	.query_device = mlx5_ib_query_device,
	.query_gid = mlx5_ib_query_gid,
	.query_pkey = mlx5_ib_query_pkey,
	.query_qp = mlx5_ib_query_qp,
	.query_srq = mlx5_ib_query_srq,
	.query_ucontext = mlx5_ib_query_ucontext,
	.reg_user_mr = mlx5_ib_reg_user_mr,
	.reg_user_mr_dmabuf = mlx5_ib_reg_user_mr_dmabuf,
        if (!mlx5r_umr_can_load_pas(dev, length))
        umem_dmabuf = ib_umem_dmabuf_get(&dev->ib_dev, offset, length, fd, access_flags, &mlx5_ib_dmabuf_attach_ops)
        mr = alloc_cacheable_mr(pd, &umem_dmabuf->umem, virt_addr, access_flags)
        atomic_add(ib_umem_num_pages(mr->umem), &dev->mdev->priv.reg_pages)
        err = mlx5r_store_odp_mkey(dev, &mr->mmkey)
        err = mlx5_ib_init_dmabuf_mr(mr)
            ret = pagefault_dmabuf_mr(mr, mr->umem->length, NULL, MLX5_PF_FLAGS_ENABLE)
                err = ib_umem_dmabuf_map_pages(umem_dmabuf)
                return ib_umem_num_pages(mr->umem)
	.req_notify_cq = mlx5_ib_arm_cq,
		if (cq->notify_flags != IB_CQ_NEXT_COMP)
			cq->notify_flags = flags & IB_CQ_SOLICITED_MASK
		if ((flags & IB_CQ_REPORT_MISSED_EVENTS) && !list_empty(&cq->wc_list))
			ret = 1;
		mlx5_cq_arm(&cq->mcq, (flags & IB_CQ_SOLICITED_MASK) == IB_CQ_SOLICITED ? MLX5_CQ_DB_REQ_NOT_SOL : MLX5_CQ_DB_REQ_NOT, uar_page, to_mcq(ibcq)->mcq.cons_index)
			sn = cq->arm_sn & 3
			ci = cons_index & 0xffffff
			*cq->arm_db = cpu_to_be32(sn << 28 | cmd | ci)
			wmb()
			doorbell[0] = cpu_to_be32(sn << 28 | cmd | ci);
			doorbell[1] = cpu_to_be32(cq->cqn);
			mlx5_write64(doorbell, uar_page + MLX5_CQ_DOORBELL)
	.rereg_user_mr = mlx5_ib_rereg_user_mr,
	.resize_cq = mlx5_ib_resize_cq,

	INIT_RDMA_OBJ_SIZE(ib_ah, mlx5_ib_ah, ibah),
	INIT_RDMA_OBJ_SIZE(ib_counters, mlx5_ib_mcounters, ibcntrs),
	INIT_RDMA_OBJ_SIZE(ib_cq, mlx5_ib_cq, ibcq),
	INIT_RDMA_OBJ_SIZE(ib_pd, mlx5_ib_pd, ibpd),
	INIT_RDMA_OBJ_SIZE(ib_qp, mlx5_ib_qp, ibqp),
	INIT_RDMA_OBJ_SIZE(ib_srq, mlx5_ib_srq, ibsrq),
	INIT_RDMA_OBJ_SIZE(ib_ucontext, mlx5_ib_ucontext, ibucontext),
};


check_resource
    get_srq_wqe
        queue_advance_consumer -> RDMA/rxe：向内核队列添加内存屏障，早期补丁添加内存屏障以保护用户空间到内核空间的通信。 先前显示用户空间队列偶尔会出现内存同步错误，这些错误通过添加 smp_load_acquire、smp_store_release 屏障来消除。 该补丁将其扩展到内核空间线程之间使用队列的情况。 此补丁还扩展了队列类型以包括内核 ULP 队列，这些队列在内核动词调用（如 poll_cq 和 post_send/recv）中访问队列的另一端
            switch (type)
            case QUEUE_TYPE_FROM_CLIENT
            ...



[RDMA_NLDEV_CMD_NEWLINK] = {
    .doit = nldev_newlink,
    .flags = RDMA_NL_ADMIN_PERM,
},
static int nldev_newlink
    nlmsg_parse_deprecated
    ops = link_ops_get(type)
    ops->newlink(ibdev_name, ndev)


ib端口状态及位宽等:
enum ib_port_state {
	IB_PORT_NOP		= 0,
	IB_PORT_DOWN		= 1,
	IB_PORT_INIT		= 2,
	IB_PORT_ARMED		= 3,
	IB_PORT_ACTIVE		= 4,
	IB_PORT_ACTIVE_DEFER	= 5
};

enum ib_port_phys_state {
	IB_PORT_PHYS_STATE_SLEEP = 1,
	IB_PORT_PHYS_STATE_POLLING = 2,
	IB_PORT_PHYS_STATE_DISABLED = 3,
	IB_PORT_PHYS_STATE_PORT_CONFIGURATION_TRAINING = 4,
	IB_PORT_PHYS_STATE_LINK_UP = 5,
	IB_PORT_PHYS_STATE_LINK_ERROR_RECOVERY = 6,
	IB_PORT_PHYS_STATE_PHY_TEST = 7,
};




rxe ib_alloc_mr -> static struct ib_mr *rxe_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,
rxe_add_to_pool(&rxe->mr_pool, mr) -> RDMA/rxe：删除rxe_alloc()，目前rxe驱动程序中除MR之外的所有对象类型都在rdma-core中分配。 通过将 kzalloc() 调用移到池代码之外，可以消除 rxe_alloc() 子例程，并且可以删除作为特殊情况的 MR 代码检查。 此补丁将 kzalloc() 和 kfree_rcu() 调用移至 mr 注册和销毁动词中。 它从 rxe_pool.c 中删除了该代码，包括不再使用的 rxe_alloc() 子例程 -> RDMA/rxe：停止查找部分构建的对象，目前 rdma_rxe 驱动程序存在安全漏洞，因为提供了部分初始化索引的对象，允许外部参与者通过发送引用其索引的数据包来访问它们（例如 qpn、rkey、 等）导致不可预测的结果。 此补丁添加了一个新的 API rxe_finalize(obj)，它允许使用 rxe_pool_get_index() 从 AH、QP、MR 和 MW 的索引中查找池对象。 仅在对象完全初始化后，它们才会添加到 create 动词中。 它还添加了等待销毁/释放动词完成的操作，以确保在返回 rdma_core 之前已删除所有引用，方法是实现新的 rxe_pool API rxe_cleanup()，该 API 将删除对对象的引用，然后等待删除所有其他引用。 当最后一个引用被删除时，该对象由 kref 完成。 之后，它会清理对象，如果是本地分配的，则会释放内存。 在地址句柄对象的特殊情况下，如果 destroy_ah 调用不可休眠，则延迟将单独实现。 与延迟清理代码相结合以键入特定的清理例程，这允许引用对象的所有挂起活动在返回 rdma_core 之前完成
    init_completion(&elem->complete)
    xa_alloc_cyclic
rxe_get(pd)
rxe_mr_init_fast(max_num_sg, mr)
    rxe_mr_init
        u32 key = mr->elem.index << 8 | rxe_get_next_key(-1) -> get_random_bytes
        mr->state = RXE_MR_STATE_INVALID
    rxe_mr_alloc
        XA_STATE(xas, &mr->page_list, 0
        xa_init(&mr->page_list)
        do
            xas_lock(&xas)
            xas_store(&xas, XA_ZERO_ENTRY)
            xas_next(&xas)
        while (xas_nomem(&xas, GFP_KERNEL))
    mr->state = RXE_MR_STATE_FREE
    mr->ibmr.type = IB_MR_TYPE_MEM_REG
rxe_finalize(mr)
    xa_store(&elem->pool->xa, elem->index, elem, GFP_KERNEL)



memory type:
IB/core：添加任意 sg_list 支持，能够注册具有间隙的 SG 列表的设备现在可以使用新的设备功能 IB_DEVICE_SG_GAPS_REG 将其在核心中公开给 ULP（在设备属性中的新字段 device_cap_flags_ex 中，因为我们用完了位） )，以及一个新的 mr_type IB_MR_TYPE_SG_GAPS_REG，它分配一个能够处理带间隙的 SG 列表的内存区域。
/**
 * enum ib_mr_type - memory region type
 * @IB_MR_TYPE_MEM_REG:       memory region that is used for
 *                            normal registration
 * @IB_MR_TYPE_SG_GAPS:       memory region that is capable to
 *                            register any arbitrary sg lists (without
 *                            the normal mr constraints - see
 *                            ib_map_mr_sg)
 * @IB_MR_TYPE_DM:            memory region that is used for device
 *                            memory registration
 * @IB_MR_TYPE_USER:          memory region that is used for the user-space
 *                            application
 * @IB_MR_TYPE_DMA:           memory region that is used for DMA operations
 *                            without address translations (VA=PA)
 * @IB_MR_TYPE_INTEGRITY:     memory region that is used for
 *                            data integrity operations
 */
enum ib_mr_type {
	IB_MR_TYPE_MEM_REG,
	IB_MR_TYPE_SG_GAPS,
	IB_MR_TYPE_DM,
	IB_MR_TYPE_USER,
	IB_MR_TYPE_DMA,
	IB_MR_TYPE_INTEGRITY,
};



.reg_user_mr = rxe_reg_user_mr,
mr = kzalloc(sizeof(*mr), GFP_KERNEL)
rxe_add_to_pool(&rxe->mr_pool, mr)
rxe_mr_init_user(rxe, start, length, iova, access, mr)
    rxe_mr_init
    xa_init
    umem = ib_umem_get(&rxe->ib_dev, start, length, access)
    rxe_mr_fill_pages_from_sgt(mr, &umem->sgt_append.sgt)
        bool persistent = !!(mr->access & IB_ACCESS_FLUSH_PERSISTENT)
        XA_STATE(xas, &mr->page_list, 0)
        __sg_page_iter_start -> lib/scatterlist：添加简单页面迭代器，添加一个迭代器以从特定页面偏移量开始一次遍历分散列表一页。 与映射迭代器相反，它很小，即使在简单的循环中也能表现良好，例如将分散列表上的所有页面收集到数组中或根据页面的 DMA 地址设置 iommu 表。
        __sg_page_iter_next
        sg_page_iter_page -> lib/scatterlist: sg_page_iter: 支持不带支持页面的 sg 列表，i915 驱动程序使用 sg 列表作为内存，而不支持“struct page”页面，与其他 IO 内存区域类似，仅设置这些内存区域的 DMA 地址。 它这样做是为了能够以统一的方式对带有和不带有后备页的 sg 列表的 HW MMU 表进行编程。 如果没有有效的页面指针，我们无法调用 nth_page 来获取 __sg_page_iter_next 中的当前页面，因此添加一个相关用户可以单独调用的帮助程序。 还添加一个助手来获取当前页面的 DMA 地址（来自 Daniel 的想法）。 转换 i915 中的所有位置，以使用新的 API
        is_pmem_page
            unsigned long paddr = page_to_phys(pg) -> physical
            region_intersects -> Region_intersects() - 确定区域与已知资源的交集， @start：区域起始地址 @size：区域大小 @flags：资源标志（在 iomem_resource 中） @desc：资源描述符（在 iomem_resource 中）或 IORES_DESC_NONE 检查是否指定 区域部分重叠或完全遮蔽由 @flags 和 @desc 标识的资源（IORES_DESC_NONE 可选）。 如果该区域不与 @flags/@desc 重叠，则返回 REGION_DISJOINT；如果该区域与 @flags/@desc 和其他资源重叠，则返回 REGION_MIXED；如果该区域与 @flags/@desc 重叠并且没有其他定义的资源，则返回 REGION_INTERSECTS。 请注意，当指定区域与 RAM 和未定义的内存孔重叠时，也会返回 REGION_INTERSECTS。 内存重新映射函数使用region_intersect()来确保用户不会重新映射RAM，并且比逐页遍历资源表的速度要快得多
                __region_intersects  -> I/O 资源描述符，walk_iomem_res_desc() 和region_intersects() 使用描述符来搜索iomem 表中的特定资源范围。 当资源范围支持搜索接口时分配新的描述符。 否则，resource.desc 必须设置为 IORES_DESC_NONE (0)
    mr->ibmr.type = IB_MR_TYPE_USER
rxe_finalize(mr)


I/O Resource Descriptors
enum {
	IORES_DESC_NONE				= 0,
	IORES_DESC_CRASH_KERNEL			= 1,
	IORES_DESC_ACPI_TABLES			= 2,
	IORES_DESC_ACPI_NV_STORAGE		= 3,
	IORES_DESC_PERSISTENT_MEMORY		= 4,
	IORES_DESC_PERSISTENT_MEMORY_LEGACY	= 5,
	IORES_DESC_DEVICE_PRIVATE_MEMORY	= 6,
	IORES_DESC_RESERVED			= 7,
	IORES_DESC_SOFT_RESERVED		= 8,
	IORES_DESC_CXL				= 9,
};

auto load modules, like: ib_code.ko
[NETLINK]：为内核 netlink 套接字添加正确的模块引用计数。 - 删除将 netlink 编译为模块的虚假代码 - 为实现 netlink 协议的模块添加模块引用计数支持 - 添加对实现 netlink 协议的自动加载模块的支持，只要有人打开该协议的套接字
MODULE_ALIAS_NET_PF_PROTO(PF_NETLINK, NETLINK_RDMA);
NETLINK_RDMA -> https://github.com/ssbandjl/linux/commit/4fdb3bb723db469717c6d38fda667d8b0fa86ebd
#define MODULE_ALIAS_NET_PF_PROTO(pf, proto) \
	MODULE_ALIAS("net-pf-" __stringify(pf) "-proto-" __stringify(proto))


drivers/infiniband/core/device.c
IB协议栈初始化, 包含mad, gid等
fs_initcall(ib_core_init); -> RDMA/core：将 sysfs 条目视图限制为 init_net，这是一个准备补丁，用于在网络命名空间中提供 rdma 设备的隔离。 第一步，使 rdma 设备仅在 init net 命名空间中可见。 后续补丁将使用 compat ib_core_device 设备/sysfs 树在多个网络命名空间中启用 rdma 设备可见性。 由于IB子系统依赖于net stack，因此需要在netdev之后初始化，并且由于它支持设备，因此需要在设备子系统之前初始化； 因此，将 initcall 顺序更改为 fs_initcall，以便在内核映像中编译 ib_core 时遵循正确的 init 顺序
static int __init ib_core_init(void)
    ib_wq = alloc_workqueue("infiniband", 0, 0)
    ib_comp_unbound_wq -> IB/core：向新的 CQ API 添加未绑定的 WQ 类型，下面引用的上游内核提交将新的 CQ API 中的工作队列修改为绑定到特定的 CPU（而不是未绑定）。 这导致新 CQ API 的所有用户都使用相同的绑定 WQ。 具体来说，当绑定到 WQ 的 CPU 忙于处理（更高优先级）中断时，MAD 处理会严重延迟。 这导致 MAD“心跳”响应处理延迟，从而导致端口被错误地分类为“关闭”。 要解决此问题，请向新的 CQ API 添加新的“未绑定”WQ 类型，以便用户可以选择绑定 WQ 或未绑定 WQ。 对于 MAD，选择新的“未绑定”WQ
    class_register(&ib_class) -> int class_register(const struct class *cls)
        pr_debug("device class '%s': registering\n", cls->name)
        error = sysfs_create_groups(&cp->subsys.kobj, cls->class_groups)
    rdma_nl_init -> IB/core：在 netlink 消息处理期间避免死锁，当未加载 rdmacm 模块时，以及当接收 netlink 消息以获取 char 设备信息时，由于使用以下调用序列对 rdma_nl_mutex 进行递归锁定，因此会导致死锁。 [..] rdma_nl_rcv() mutex_lock() [..] rdma_nl_rcv_msg() ib_get_client_nl_info() request_module() iw_cm_init() rdma_nl_register() mutex_lock(); <- 死锁，再次获取互斥体 由于上述调用序列，观察到以下调用跟踪和死锁。 内核：__mutex_lock+0x35e/0x860 内核：？ __mutex_lock+0x129/0x860 内核：？ rdma_nl_register+0x1a/0x90 [ib_core] 内核： rdma_nl_register+0x1a/0x90 [ib_core] 内核：？ 0xffffffffc029b000 内核：iw_cm_init+0x34/0x1000 [iw_cm] 内核：do_one_initcall+0x67/0x2d4 内核：？ kmem_cache_alloc_trace+0x1ec/0x2a0 内核：do_init_module+0x5a/0x223 内核：load_module+0x1998/0x1e10 内核：？ __symbol_put+0x60/0x60 内核：__do_sys_finit_module+0x94/0xe0 内核：do_syscall_64+0x5a/0x270 内核：entry_SYSCALL_64_after_hwframe+0x49/0xbe 进程堆栈跟踪：[<0>] __request_module+0x1c9/0x460 [<0>] ib_get_client_nl _信息+0x5e/0xb0 [ib_core] [<0>] nldev_get_chardev+0x1ac/0x320 [ib_core] [<0>] rdma_nl_rcv_msg+0xeb/0x1d0 [ib_core] [<0>] rdma_nl_rcv+0xcd/0x120 [ib_core] [<0>] netlink_unicast+0x179 /0x220 [<0>] netlink_sendmsg+0x2f6/0x3f0 [<0>] sock_sendmsg+0x30/0x40 [<0>] ___sys_sendmsg+0x27a/0x290 [<0>] __sys_sendmsg+0x58/0xa0 [<0>] do_syscall_64+0x5a /0x270 [<0>]entry_SYSCALL_64_after_hwframe+0x49/0xbe 为了克服此死锁并允许多个 netlink 消息并行处理，实施了以下方案。 1. 将保护cb_table 的锁拆分为per-index 锁，并使其成为rwlock。 此锁用于确保取消注册返回后不会运行任何回调。 由于模块一旦已经运行回调就不会被注册，这避免了死锁。 2. 注册时使用smp_store_release()更新cb_table，这样就不需要锁了。 这避免了认为所有 rwsem 都是相同锁类的 lockdep 问题
        init_rwsem(&rdma_nl_types[idx].sem)
    addr_init -> IB/core：将IB地址解析模块集成到核心中，IB地址解析被声明为一个模块（ib_addr.ko），该模块在IB核心模块（ib_core.ko）之前加载自身。 这会导致由IB core初始化的IB netlink无法被ib_addr.ko使用的情况。 为了解决这个问题，我们将 ib_addr.ko 转换为 IB 核心模块的一部分
        alloc_ordered_workqueue("ib_addr", 0)
        register_netevent_notifier(&nb) -> netevent_callback
            NETEVENT_NEIGH_UPDATE
            set_timeout(req, jiffies)
    ib_mad_init
        INIT_LIST_HEAD(&ib_mad_port_list) <- ib_mad_port_open
            list_add_tail(&port_priv->port_list, &ib_mad_port_list)
        ib_register_client(&mad_client)
    ib_sa_init -> 中间核心层:Mid-layer Core , 核心服务包括管理接口（MAD）、连接管理器（CM）接口和子网管理员（SA）接口。 该堆栈包括用于用户模式和内核应用程序的组件。 核心服务在内核中运行，并为动词、CM 和管理向用户模式公开接口 -> 第 5 章 配置 INFINIBAND 子网管理器, 所有 InfiniBand 网络都必须运行子网管理器才能正常工作。即使两台机器没有使用交换机直接进行连接， 也是如此。 有可能有一个以上的子网管理器。在那种情况下，当主子网管理器出现故障时，另外一个作为从网管理器 的系统会接管。 大多数 InfiniBand 交换机都包含一个嵌入式子网管理器。然而，如果您需要一个更新的子网管理器，或者 您需要更多控制，请使用 Red Hat Enterprise Linux 提供的 OpenSM 子网管理器
        get_random_bytes(&tid, sizeof tid)
        ib_register_client(&sa_client)
        mcast_init -> IB/sa：跟踪多播加入/离开请求，IB SA 以每个端口为基础跟踪多播加入/离开请求，并且不执行任何引用计数：如果同一端口的两个用户加入同一组，并且其中一个用户离开该组 ，那么 SA 将从组中删除该端口，即使有一个用户想要保留成员身份。 因此，为了支持来自同一端口的同一组播组的多个用户，我们需要在本地执行引用计数。 为此，向 ib_sa 添加一个多播子模块以执行多播加入/离开操作的引用计数。 修改ib_ipoib（多播的唯一内核用户）以使用新接口
            ib_sa_register_client(&sa_client)
            ib_register_client(&mcast_client) -> mcast_add_one -> InfiniBand 子网管理 (SA) 服务是由子网管理器 (SM) 提供的预定义通用服务代理 (GSA)。 在 InfiniBand 结构上，设备应通过联系 SA 查询正确的路由来解析到其他主机的路由
                rdma_cap_ib_mcast -> rdma_cap_ib_mcast - 检查设备端口是否具有 Infiniband、组播功能。 @device：要检查的设备 @port_num：要检查的端口号 * InfiniBand 多播注册比普通 IPv4 或 IPv6 多播注册更复杂。 当每个主机通道适配器希望加入多播组时，必须向子网管理器注册。 无论它订阅该组有多少队列对，它都应该只执行一次。 只有在附加到该组的所有队列对都已分离后，它才应该离开该组。 * 返回：如果端口必须承担向 SM 注册/取消注册以及跟踪附加到多播组的队列对总数的额外管理开销，则返回 true
                ib_set_client_data(device, &mcast_client, dev)
                INIT_IB_EVENT_HANDLER(&dev->event_handler, device, mcast_event_handler)
                    switch (event->event)
                    mcast_groups_event
                        rb_first
                        rb_next
                        rb_entry
                ib_register_event_handler(&dev->event_handler)
        alloc_ordered_workqueue("ib_nl_sa_wq", WQ_MEM_RECLAIM)
        INIT_DELAYED_WORK(&ib_nl_timed_work, ib_nl_request_timeout) -> ib_nl_request_timeout
            queue_delayed_work(ib_nl_wq, &ib_nl_timed_work, delay)
            ib_sa_disable_local_svc
            send_handler -> callback
    register_blocking_lsm_notifier(&ibdev_lsm_nb) -> LSM：切换到阻止策略更新通知程序，原子策略更新程序不是很有用，因为它们通常无法自行执行策略更新。 由于似乎对原子性没有严格的要求，因此切换到阻塞变体。 执行此操作时，相应地重命名函数
        blocking_notifier_chain_register
    register_pernet_device(&rdma_dev_net_ops)
    nldev_init()
    rdma_nl_register(RDMA_NL_LS, ibnl_ls_cb_table) -> rdma_nl_register(RDMA_NL_NLDEV, nldev_cb_table)
        down_write(&rdma_nl_types[index].sem)
        rdma_nl_types[index].cb_table = NULL
    roce_gid_mgmt_init -> IB/核心：添加 RoCE GID 表管理，RoCE GID 基于与 RDMA (RoCE) 设备端口相关的以太网网络设备上配置的 IP 地址。 目前，每个支持 RoCE（ocrdma、mlx4）的低级驱动程序都管理自己的 RoCE 端口 GID 表。 由于本质上没有任何特定于供应商的内容，因此我们对其进行概括，并增强 RDMA 核心 GID 缓存来完成这项工作。 为了填充 GID 表，我们监听事件： (a) netdev up/down/change_addr 事件 - 如果 netdev 构建在我们的 RoCE 设备上，我们需要添加/删除其 IP。 这涉及添加与此 ndev 相关的所有 GID、添加默认 GID 等。 (b) inet 事件 - 将新 GID（根据 IP 地址）添加到表中。 为了对端口 RoCE GID 表进行编程，提供商必须实现 add_gid 和 del_gid 回调。 RoCE GID 管理要求我们在 GID 旁边声明关联的 net_device。 为了管理 GID 表，此信息是必需的。 例如，当删除 net_device 时，其关联的 GID 也需要删除。 RoCE 要求根据相关网络设备的 IPv6 本地链路为每个端口生成默认 GID。 与基于常规 IPv6 链路本地的 GID（因为我们为每个 IP 地址生成 GID）相反，当网络设备关闭时，默认 GID 也可用（为了支持环回）。 锁定的完成方式如下：该补丁修改了 GID 表代码，适用于实现 add_gid/del_gid 回调的新 RoCE 驱动程序以及未实现 add_gid/del_gid 回调的当前 RoCE 和 IB 驱动程序。 更新表的流程不同，因此锁定要求也不同。 更新 RoCE GID 表时，通过 mutex_lock(&table->lock) 实现针对多个写入者的保护。 由于写入表需要我们在表中查找一个条目（可能是空闲条目）然后修改它，因此该互斥锁保护 find_gid 和 write_gid 确保操作的原子性。 GID 缓存中的每个条目均受 rwlock 保护。 在 RoCE 中，写入（通常来自 netdev 通知程序的结果）涉及调用供应商的 add_gid 和 del_gid 回调，这些回调可能会休眠。 因此，为每个条目添加无效标志。 RoCE 的更新是通过工作队列完成的，因此允许休眠。 在IB中，更新是在write_lock_irq(&device->cache.lock)中完成的，因此write_gid不允许休眠并且add_gid/del_gid不会被调用。 当将网络设备传入/传出 GID 缓存时，该设备始终被传递为保持 (dev_hold)。 该代码使用单个工作项来更新所有 RDMA 设备，遵循 netdev 或 inet 通知程序。 该补丁将缓存从客户端（这是不正确的，因为缓存是 IB 基础设施的一部分）转变为在设备注册/删除时显式初始化/释放, commit: https://github.com/ssbandjl/linux/commit/03db3a2d81e6e84f3ed3cb9e087cae17d762642b, drivers/infiniband/core/cache.c
        gid_cache_wq = alloc_ordered_workqueue("gid-cache-wq", 0) -> 串行
        register_inetaddr_notifier(&nb_inetaddr) -> 注册网络地址事件 -> blocking_notifier_chain_register -> 将通知程序添加到阻塞通知程序链 @nh：指向阻塞通知程序链头部的指针 @n：通知程序链中的新条目 将通知程序添加到阻塞通知程序链。 必须在进程上下文中调用。 成功时返回 0，错误时返回 %-EEXIST
            notifier_chain_register(&nh->head, n, unique_priority)
                trace_notifier_register((void *)n->notifier_call)
        register_inet6addr_notifier(&nb_inet6addr)
        register_netdevice_notifier(&nb_netdevice) -> 我们依靠 netdevice 通知程序来枚举系统中所有现有的设备。 最后注册到此通知程序以确保我们不会错过任何 IP 添加/删除回调


IPv4网络事件
static struct notifier_block nb_inetaddr = {
	.notifier_call = inetaddr_event
};
static struct notifier_block nb_inet6addr = {
	.notifier_call = inet6addr_event
};

IPv6网络事件
inetaddr_event
    addr_event(struct notifier_block *this, unsigned long event, struct sockaddr *sa, struct net_device *ndev)
        case NETDEV_UP:
            gid_op = GID_ADD
        case NETDEV_DOWN:
            gid_op = GID_DEL
        INIT_WORK(&work->work, update_gid_event_work_handler)
        rdma_ip2gid(sa, &work->gid)
        queue_work(gid_cache_wq, &work->work) -> 端口UP/Down时触发事件并更新GID
update_gid_event_work_handler
    ib_enum_all_roce_netdevs(is_eth_port_of_netdev_filter, work->gid_attr.ndev, callback_for_addr_gid_device_scan, work) -> callback_for_addr_gid_device_scan
        update_gid(parsed->gid_op, device, port, &parsed->gid, &parsed->gid_attr)





notifier_call_chain - 通知已注册的通知程序有关事件的信息。 @nl：指向阻塞通知器链头的指针 @val：未修改地传递给通知器函数的值 @v：未修改地传递给通知器函数的指针 @nr_to_call：要调用的通知器函数的数量。 不在乎这个参数的值为-1。 @nr_calls：记录发送的通知数量。 不关心该字段的值为 NULL。 返回：notifier_call_chain 返回最后一个调用的通知函数的返回值。 */
    ret = nb->notifier_call(nb, val, v)


static struct class ib_class = {
	.name    = "infiniband",
	.dev_release = ib_device_release,
	.dev_uevent = ib_device_uevent,
	.ns_type = &net_ns_type_operations,
	.namespace = net_namespace,
};



static struct notifier_block nb = {
	.notifier_call = netevent_callback
};


static struct ib_client mad_client = {
	.name   = "mad",
	.add = ib_mad_init_device,
	.remove = ib_mad_remove_device
};


ib_mad_init_device -> RDMA：调用 add() 时允许 ib_client 失败，添加客户端时不允许失败，但所有客户端在其添加例程中都有各种失败路径。这会产生非常边缘的情况，即添加客户端时失败，并且未设置 client_data。核心代码仍将使用 NULL client_data 调用其他以 client_data 为中心的操作，如 remove()、rename()、get_nl_info() 和 get_net_dev_by_params() - 这令人困惑且出乎意料。如果 add() 回调失败，则不要再为设备调用任何客户端操作，甚至删除。删除操作回调中所有现在多余的 NULL client_data 检查。更新所有 add() 回调以适当返回错误代码。EOPNOTSUPP 用于 ULP 不支持 ib_device 的情况 - 例如因为它仅适用于 IB
    rdma_start_port
    for (i = start; i <= rdma_end_port(device); i++)
        rdma_cap_ib_mad
        ib_mad_port_open
            char name[sizeof "ib_mad123"]
            init_mad_qp(port_priv, &port_priv->qp_info[0]);
                init_mad_queue(qp_info, &qp_info->send_queue)
                init_mad_queue(qp_info, &qp_info->recv_queue)
                INIT_LIST_HEAD(&qp_info->overflow_list)
            init_mad_qp(port_priv, &port_priv->qp_info[1]);
            rdma_cap_ib_smi
            port_priv->pd = ib_alloc_pd(device, 0)
            port_priv->cq = ib_alloc_cq
            create_mad_qp(&port_priv->qp_info[0], IB_QPT_SMI)
                qp_info->qp = ib_create_qp
            create_mad_qp(&port_priv->qp_info[1], IB_QPT_GSI)
            list_add_tail(&port_priv->port_list, &ib_mad_port_list)
            ib_mad_port_start
                ib_find_pkey
                ib_modify_qp IB_QPS_RTR
                ib_modify_qp IB_QPS_RTS
                ib_req_notify_cq
                for (i = 0; i < IB_MAD_QPS_CORE; i++)
                    ib_mad_post_receive_mads
                        sg_list.addr = ib_dma_map_single(qp_info->port_priv->device,
                        mad_priv->header.mad_list.cqe.done = ib_mad_recv_done
                        list_add_tail(&mad_priv->header.mad_list.list, &recv_queue->list)
                        ib_post_recv
        ib_agent_port_open
            port_priv->agent[0] = ib_register_mad_agent(device, port_num, IB_QPT_SMI, NULL, 0, &agent_send_handler, NULL, NULL, 0)
                get_spl_qp_index
                dev_dbg_ratelimited
                ib_is_mad_class_rmpp
                ib_get_mad_port
                INIT_DELAYED_WORK(&mad_agent_priv->timed_work, timeout_sends)
                INIT_WORK(&mad_agent_priv->local_work, local_completions)
                ib_mad_agent_security_setup
                xa_alloc_cyclic ib_mad_client_next
                convert_mgmt_class
                add_nonoui_reg_req
                add_oui_reg_req
            port_priv->agent[1] = ib_register_mad_agent
            list_add_tail(&port_priv->port_list, &ib_agent_port_list)



static struct ib_client sa_client = {
	.name   = "sa",
	.add    = ib_sa_add_one,
	.remove = ib_sa_remove_one
};

ib_sa_add_one -> RDMA：允许 ib_client 在调用 add() 时失败，添加客户端时不允许失败，但所有客户端在其添加例程中都有各种失败路径。 这会产生一种非常边缘的情况：添加客户端后，在添加过程中失败并且未设置 client_data。 然后，核心代码仍然会使用 NULL client_data 调用其他以 client_data 为中心的操作，例如 remove()、rename()、get_nl_info() 和 get_net_dev_by_params() - 这是令人困惑和意外的。 如果 add() 回调失败，则不要再为设备调用任何客户端操作，甚至删除。 删除操作回调中现在对 NULL client_data 的所有冗余检查。 更新所有 add() 回调以正确返回错误代码。 EOPNOTSUPP 用于 ULP 不支持 ib_device 的情况 - 例如，因为它仅适用于 IB
    ib_register_mad_agent(device, i + s, IB_QPT_GSI, send_handler recv_handler
        get_spl_qp_index
        INIT_DELAYED_WORK(&mad_agent_priv->timed_work, timeout_sends)
        INIT_WORK(&mad_agent_priv->local_work, local_completions);
        ib_mad_agent_security_setup
            security_ib_alloc_security
            security_ib_endport_manage_subnet
                security_ib_endport_sid
                avc_has_perm
            list_add(&agent->mad_agent_sec_list, &mad_agent_list)
    INIT_WORK(&sa_dev->port[i].update_task, update_sm_ah);
    INIT_DELAYED_WORK(&sa_dev->port[i].ib_cpi_work, update_ib_cpi
    ib_set_client_data
    INIT_IB_EVENT_HANDLER(&sa_dev->event_handler, device, ib_sa_event)
    ib_register_event_handler(&sa_dev->event_handler)
    update_sm_ah



send_handler
    query->callback




static struct notifier_block ibdev_lsm_nb = {
	.notifier_call = ib_security_change,
};



static struct pernet_operations rdma_dev_net_ops = {
	.init = rdma_dev_init_net,
	.exit = rdma_dev_exit_net,
	.id = &rdma_dev_net_id,
	.size = sizeof(struct rdma_dev_net),
};
rdma_dev_init_net
    write_pnet
    rdma_nl_net_init
        rdma_init_coredev
        ib_setup_port_attrs
            rdma_for_each_port
                ib_query_port
                setup_port
                     alloc_port_table_group("gids", &p->groups[0], p->attrs_list,
                     alloc_port_table_group("pkeys",
                     setup_hw_port_stats
                     sysfs_create_groups
                     list_add_tail(&p->kobj.entry, &coredev->port_list)
                setup_gid_attrs -> RDMA/core：简化端口 sysfs 的创建方式，使用与 gid_attrs 现在用于管理端口 sysfs 相同的技术。 将所有内容捆绑到三个分配中，并使用单个 sysfs_create_groups() 一次性构建所有内容。 所有内存始终在 kobj 释放函数中释放，消除了大部分错误展开。 gid_attr 技术和 hw_counters 非常相似，将两者合并在一起，并将 hw_counters 的 sysfs_create_group() 调用与单个 sysfs 组设置相结合
                    alloc_port_table_group("ndevs", show_port_gid_attr_ndev
                    alloc_port_table_group("types", show_port_gid_attr_gid_type
    net_eq
    add_one_compat_dev -> RDMA/core：在net命名空间中实现compat device/sysfs树，实现ib_core的兼容层sysfs条目，以便非init_net net命名空间也可以发现rdma设备。 每个非 init_net 网络命名空间都在其中创建了 ib_core_device。 这样的 ib_core_device sysfs 树类似于 init_net 命名空间中找到的 rdma 设备。 这允许通过 sysfs 条目在多个非 init_net 网络命名空间中发现 rdma 设备，并且对 rdma-core 用户空间很有帮助
        cdev = xa_load(&device->compat_devs, rnet->id)
         xa_reserve(&device->compat_devs, rnet->id, GFP_KERNEL)
        rdma_init_coredev
            device_initialize
        cdev->dev.release = compatdev_release
        ret = dev_set_name(&cdev->dev, "%s", dev_name(&device->dev))
        ret = device_add(&cdev->dev) -> int device_add(struct device *dev) -> 将设备添加到设备层次结构中。 @dev：设备。 这是 device_register() 的第 2 部分，尽管可以单独调用 _iff_ device_initialize() 已被单独调用。 这通过 kobject_add() 将 @dev 添加到 kobject 层次结构中，将其添加到设备的全局列表和同级列表中，然后将其添加到驱动程序模型的其他相关子系统中。 对于任何设备结构，请勿多次调用此例程或 device_register()。 驱动程序模型核心不适用于未注册然后又恢复正常的设备。 （除此之外，很难保证对 @dev 的先前化身的所有引用都已被删除。）而是分配并注册一个全新的结构设备。 注意：_决不_在调用此函数后直接释放@dev，即使它返回错误！ 始终使用 put_device() 来放弃您的引用。 经验法则是：如果 device_add() 成功，当您想要删除它时，应该调用 device_del()。 如果 device_add() *未*成功，则 *仅* 使用 put_device() 删除引用计数
            dev = get_device(dev)
            device_private_init(dev)
            get_device_parent
            kobject_add(&dev->kobj, dev->kobj.parent, NULL)
            device_platform_notify(dev)
            device_create_file(dev, &dev_attr_uevent) -> sysfs_create_file
                kobject_get_ownership
                sysfs_add_file_mode_ns
                    kn = __kernfs_create_file(parent, attr->name, mode & 0777, uid, gid, PAGE_SIZE, ops, (void *)attr, ns, key);
                        kernfs_new_node -> kernfs：允许使用任意 uid/gid 创建 kernfs 对象，此更改允许使用任意 uid/gid 创建 kernfs 文件和目录，而不是通过使用 uid/gid 参数扩展 kernfs_create_dir_ns() 和 kernfs_create_file_ns() 来始终使用 GLOBAL_ROOT_UID/GID。 “简单”的 kernfs_create_file() 和 kernfs_create_dir() 被单独保留，并且始终创建属于全局根的对象。 创建符号链接时，所有权 (uid/gid) 取自目标 kernfs 对象
                            __kernfs_new_node(kernfs_root(parent), parent,
                                name = kstrdup_const(name, GFP_KERNEL)
                                kn = kmem_cache_zalloc(kernfs_node_cache, GFP_KERNEL) -> kmem_cache_zalloc( )函数与kmem_cache_alloc( )函数功能类似，都是用来从一个给定的缓存分配一个对象。但kmem_cache_zalloc( )除了分配内存对象之外，还把内存对象所代表的内存空间初始化为0
                                idr_preload(GFP_KERNEL);
                                idr_alloc_cyclic(&root->ino_idr, kn, 1, 0, GFP_ATOMIC)
                                idr_preload_end()
                                RB_CLEAR_NODE(&kn->rb)
                                __kernfs_setattr -> xattrs->rb_root = RB_ROOT
                                security_kernfs_init_security -> Init LSM context for a kernfs node
                                    call_int_hook(kernfs_init_security, 0, kn_dir, kn) -> LSM_HOOK_INIT(kernfs_init_security, selinux_kernfs_init_security)
                        kernfs_add_one(kn)
            device_add_class_symlinks
            device_add_attrs
            bus_add_device
            dpm_sysfs_add
            device_pm_add
            bus_notify(dev, BUS_NOTIFY_ADD_DEVICE)
            bus_probe_device(dev)
            class_to_subsys
        ib_setup_port_attrs
        xa_store(&device->compat_devs, rnet->id, -> store device




nldev_get_doit
    nlmsg_parse_deprecated
    nla_get_u32
    ib_device_get_by_index -> RDMA/core：扩展 ib_device_get_by_index 以获取网络命名空间，扩展 ib_device_get_by_index() API 以检查设备对网络命名空间的访问，以提供 netlink 命令。 还对 dumpit 命令强制执行 net ns 检查，这些命令迭代所有注册的 rdma 设备并且不调用 ib_device_get_by_index()
        device = xa_load(&devices, index) -> 三个 rwsem 锁（devices、clients、client_data）中的每一个都保护同名的 xarray。 具体来说，它允许调用者断言 MARK 在锁定下将/不会改变，并且对于设备和客户端而言，xarray 中的值仍然是有效的指针。 MARK 的更改与对象状态相关联，因此持有锁并测试 MARK 也断言所包含的对象处于某种状态。 这用于构建两阶段注册/取消注册流程，其中对象可以继续位于 xarray 中，即使它们仍在注册/取消注册过程中。 xarray本身提供了额外的锁定，以及可重启的迭代，这也是值得依赖的。 锁不应该嵌套，但 client_data 除外，它允许嵌套在其他两个锁的读取端下方。 devices_rwsem 还保护设备名称列表，设备名称的任何更改或分配也必须保留写入端以保证名称唯一。 */ devices 包含已分配名称的设备。 设备可能未注册。 关心注册状态的用户需要在设备上调用 ib_device_try_get() 以确保其已注册，并在所需的时间内保持注册状态 -> this devices from register
        rdma_dev_access_netns
            net_eq(read_pnet(&dev->coredev.rdma_net), net)
        ib_device_try_get -> RDMA/device：公开 ib_device_try_get(()，事实证明，未来的补丁现在非常广泛地需要此功能，而不仅仅是 netlink，因此提供两个全局函数来管理注册锁引用计数。这也将锁变为 1 的点移至以内 ib_register_device() 使得公共 API 的语义非常健全和清晰，在仅分配但尚未注册的设备上调用 ib_device_try_get() 将失败。 -> ib_device_try_get：持有注册锁，设备：要锁定的设备处于活动注册锁下的设备无法取消注册。 只有在完全注册的设备上才能获得注册锁，否则该函数返回 false。 仅当需要设备仍处于注册状态时才需要注册锁。 仅要求设备指针有效的用途应使用 get_device(&ibdev->dev) 来保存内存
            refcount_inc_not_zero(&dev->refcount)
    nlmsg_new
    rdma_nl_unicast(sock_net(skb->sk), msg, NETLINK_CB(skb).portid)


fs_initcall(ib_core_init);
register_pernet_device(&rdma_dev_net_ops)
static struct pernet_operations rdma_dev_net_ops = {
    .init = rdma_dev_init_net,
    .exit = rdma_dev_exit_net,
    .id = &rdma_dev_net_id,
    .size = sizeof(struct rdma_dev_net),
};
rdma_dev_init_net
    ...
    rdma_nl_net_init
        read_pnet
        .input	= rdma_nl_rcv
        netlink_kernel_create(net, NETLINK_RDMA, &cfg)


static const struct rdma_nl_cbs nldev_cb_table[RDMA_NLDEV_NUM_OPS] = {
    [RDMA_NLDEV_CMD_GET] = {
        .doit = nldev_get_doit,
        .dump = nldev_get_dumpit,
    },
如果用户提供特定索引，我们可以使用 .doit 回调加速查询，并在之后保存完整转储和过滤
nldev_get_dumpit _nldev_get_dumpit
    rdma_dev_access_netns
    _nldev_get_dumpit
        nlmsg_end




qemu_vm:
/root/project/rdma/iproute2/rdma
gdb --args ./rdma link add rxe_ens3 type rxe netdev ens3
main
filename = basename(argv[0]);
err = rd_init(&rd, filename);
    rd_prepare_msg
err = rd_batch(&rd, batch_file, force)
rd_cmd(&rd, argc, argv)
    rd_exec_cmd
        rd_argv_match
        c->func(rd) -> int cmd_link
            rd_exec_cmd -> link_add
link_add -> link_add_type -> link_add_netdev
    rd_prepare_msg(rd, RDMA_NLDEV_CMD_NEWLINK, &seq, -> to kernel 通过Netlink转到内核态
    mnl_attr_put_strz(rd->nlh, RDMA_NLDEV_ATTR_DEV_NAME, rd->link_name);
    ...


kernel: RDMA_NLDEV_CMD_NEWLINK
static const struct rdma_nl_cbs nldev_cb_table[RDMA_NLDEV_NUM_OPS] = {
	[RDMA_NLDEV_CMD_GET] = {
		.doit = nldev_get_doit,
		.dump = nldev_get_dumpit,
	},
	[RDMA_NLDEV_CMD_GET_CHARDEV] = {
		.doit = nldev_get_chardev,
	},
	[RDMA_NLDEV_CMD_SET] = {
		.doit = nldev_set_doit,
		.flags = RDMA_NL_ADMIN_PERM,
	},
	[RDMA_NLDEV_CMD_NEWLINK] = {
		.doit = nldev_newlink,
		.flags = RDMA_NL_ADMIN_PERM,
	},



#0  nldev_newlink (skb=0xffff88813772c800, nlh=0xffff888036b87000, extack=0xffffc90003aabc18) at drivers/infiniband/core/nldev.c:1670
#1  0xffffffffc0787dc0 in rdma_nl_rcv_msg (extack=0xffffc90003aabc18, nlh=0xffff888036b87000, skb=0xffff88813772c800) at drivers/infiniband/core/netlink.c:195 -> err = cb_table[op].doit(skb, nlh, extack)
#2  rdma_nl_rcv_skb (cb=<optimized out>, skb=0xffff88813772c800) at drivers/infiniband/core/netlink.c:239
#3  rdma_nl_rcv (skb=0xffff88813772c800) at drivers/infiniband/core/netlink.c:259
#4  0xffffffff81aeb4a5 in netlink_unicast_kernel (ssk=0xffff888117765800, skb=0xffff88813772c800, sk=0xffff88811b1cb000) at net/netlink/af_netlink.c:1319
#5  netlink_unicast (ssk=ssk@entry=0xffff888117765800, skb=skb@entry=0xffff88813772c800, portid=portid@entry=0, nonblock=<optimized out>) at net/netlink/af_netlink.c:1345
#6  0xffffffff81aeb796 in netlink_sendmsg (sock=<optimized out>, msg=0xffffc90003aabd88, len=<optimized out>) at net/netlink/af_netlink.c:1935
#7  0xffffffff81a3db52 in sock_sendmsg_nosec (msg=0xffff888036b87000, sock=0xffff88813772c800) at ./include/linux/uio.h:255
#8  sock_sendmsg (sock=0xffff88813772c800, sock@entry=0xffff88800473ce00, msg=0xffff888036b87000, msg@entry=0xffffc90003aabd88) at net/socket.c:724
#9  0xffffffff81a3f5c3 in __sys_sendto (fd=<optimized out>, buff=<optimized out>, len=<optimized out>, flags=0, addr=0x7ffff7db39e0, addr_len=12) at net/socket.c:2036
#10 0xffffffff81a3f669 in __do_sys_sendto (addr_len=<optimized out>, addr=<optimized out>, flags=<optimized out>, len=<optimized out>, buff=<optimized out>, fd=<optimized out>) at net/socket.c:2048
#11 __se_sys_sendto (addr_len=<optimized out>, addr=<optimized out>, flags=<optimized out>, len=<optimized out>, buff=<optimized out>, fd=<optimized out>) at net/socket.c:2044
#12 __x64_sys_sendto (regs=<optimized out>) at net/socket.c:2044
#13 0xffffffff81cff979 in do_syscall_x64 (nr=<optimized out>, regs=0xffffc90003aabf58) at arch/x86/entry/common.c:50
#14 do_syscall_64 (regs=0xffffc90003aabf58, nr=<optimized out>) at arch/x86/entry/common.c:80
#15 0xffffffff81e0007c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:113
#16 0x0000000000000001 in fixed_percpu_data ()
#17 0x00007fffffffe000 in ?? () at drivers/infiniband/core/device.c:2857
#18 0x0000000000000000 in ?? ()



rdma_dev_init_net
    rdma_nl_net_init
        .input	= rdma_nl_rcv
        nls = netlink_kernel_create(net, NETLINK_RDMA, &cfg)


nldev_newlink
    nlmsg_parse_deprecated
    dev_get_by_name
    link_ops_get
    CONFIG_MODULES
    request_module("rdma-link-%s", type) <- #define MODULE_ALIAS_RDMA_LINK(type) MODULE_ALIAS("rdma-link-" type) -> MODULE_ALIAS_RDMA_LINK("rxe");
    ops->newlink(ibdev_name, ndev)

static struct rdma_link_ops rxe_link_ops = {
	.type = "rxe",
	.newlink = rxe_newlink,
};
rxe_newlink
    is_vlan_dev -> 判断是否VLAN设备
        return dev->priv_flags & IFF_802_1Q_VLAN
    rxe_get_dev_from_net
        ib_device_get_by_netdev(ndev, RDMA_DRIVER_RXE)
            hash_for_each_possible_rcu (ndev_hash, cur, ndev_hash_link,
            ib_device_try_get(cur->ib_dev))
            res = cur->ib_dev
    rxe_net_add
        ib_alloc_device(rxe_dev, ib_dev)
        rxe_add(rxe, ndev->mtu, ibdev_name)
            rxe_init
            rxe_set_mtu
            rxe_register_device

rxe_post_send
    rxe_run_task
    rxe_post_send_kernel
        post_one_send
            validate_send_wr
                wr_opcode_mask
            queue_full -> RDMA/rxe：向内核队列添加内存屏障，早期补丁添加内存屏障以保护用户空间到内核空间的通信。 先前显示用户空间队列偶尔会出现内存同步错误，这些错误通过添加 smp_load_acquire、smp_store_release 屏障来消除。 该补丁将其扩展到内核空间线程之间使用队列的情况。 此补丁还扩展了队列类型以包括内核 ULP 队列，这些队列在内核动词调用（如 poll_cq 和 post_send/recv）中访问队列的另一端
                u32 prod = queue_get_producer(q, type)
                u32 cons = queue_get_consumer(q, type)
                return ((prod + 1 - cons) & q->index_mask) == 0
            queue_producer_addr
                return q->buf->data + (prod << q->log2_elem_size)
            init_send_wqe -> RDMA/rxe：使用 qp->state_lock 保护 QP 状态，目前 rxe 驱动程序很少努力对 qp 状态（包括 qp->attr.qp_state、qp->attr.sq_draining 和 qp->valid）进行原子更改 不同的客户端线程和IO线程之间。 特别是，RDMA 应用程序的通用模板是调用 ib_modify_qp() 将 qp 移至 ERR 状态，然后等到所有数据包和工作队列都已耗尽，然后再调用 ib_destroy_qp()。 这些状态更改均不受锁保护，以确保更改以原子方式执行并且包含内存屏障。 据观察，这会导致 qp 清理方面的错误行为。 该补丁延续了本系列之前补丁的工作，并添加了围绕 qp 状态更改和查找的锁定代码
                init_send_wr
                copy_inline_data_to_wqe(wqe, ibwr)
                    memcpy(p, ib_virt_dma_to_page(sge->addr), sge->length) -> RDMA：添加 ib_virt_dma_to_page()，通过添加一个从“虚拟”dma_addr 返回到 kva 和另一个返回到结构页的函数，可以更清楚地了解发生的情况。 这用于 ib_uses_virt_dma() 样式驱动程序（siw、rxe、hfi、qib）。 当使用由各种 ib_map 函数编码的 dma_addr 值时，调用它们而不是裸转换和 virt_to_page() 。 这也解决了 Linus Walleij 一直在追求的 virt_to_page() 转换问题
                        return virt_to_page(ib_virt_dma_to_ptr(dma_addr))
                            return (void *)(uintptr_t)dma_addr;
                or memcpy(wqe->dma.sge, ibwr->sg_list,
                wqe->state		= wqe_state_posted
        rxe_sched_task(&qp->req.task) -> rxe_requester
            ...
            if (unlikely(qp->req.need_retry && !qp->req.wait_for_rnr_timer)) -> RDMA/rxe：修复 rnr 重试行为，当前，当重传计时器或 rnr 计时器触发相同标志 (qp->req.need_retry) 时，完成者微线程被设置，以便如果任一计时器触发，它将尝试在 发送队列。 这具有在第一次重传定时器事件时响应 RNR NAK 的效果，这可能不允许请求的 rnr 超时。 此补丁添加了一个新标志 (qp->req.wait_for_rnr_timer)，如果设置该标志，则会阻止重试流程，直到 rnr nak 计时器触发。 此补丁修复了 rnr 重试错误，可以通过多次运行 pyverbs test_rdmacm_async_traffic_external_qp 来观察到这些错误。 应用此补丁后，它们不会发生
                req_retry(qp)
            req_next_wqe
            rxe_wqe_is_fenced(qp, wqe)
                wqe->wr.send_flags & IB_SEND_FENCE
            rxe_do_local_ops or -> RDMA/rxe：将本地操作移至子例程，通过将本地操作移至子例程来简化 rxe_requester()。 添加非法发送 WR 操作码的错误返回。 将 next_index 移至 rxe_run_task 之前，这修复了一个小错误，即工作完成被延迟到下一个 wqe 之后，这不是预期的行为。 让错误返回它们自己的 WC 状态。 以前，所有错误都被报告为保护错误，这是不正确的。 将错误返回从 rxe_do_local_ops() 更改为 err: ，这会导致立即完成。 如果没有这个，最后一个 WR 上的错误可能会丢失。 将 fill_packet() 更改为 finish_packet() 更准确
                switch (opcode)
                case IB_WR_LOCAL_INV
                    rxe_invalidate_mw
                    or rxe_invalidate_mr
                case IB_WR_REG_MR
                    rxe_reg_fast_mr
                case IB_WR_BIND_MW
                    ret = rxe_bind_mw(qp, wqe)
            psn_compare
            next_opcode
                next_opcode_rc
            check_init_depth
            if (payload > mtu)
                queue_next_index
                wqe->status = IB_WC_SUCCESS
                rxe_sched_task(&qp->comp.task)
            save_state(wqe, qp, &rollback_wqe, &rollback_psn) -> RDMA/rxe：修复 rxe_requester 中不完整的状态保存，如果发送数据包被 rxe_requester() 中的 IP 层丢弃，则对 rxe_xmit_packet() 的调用可能会失败，并出现 err == -EAGAIN。 为了恢复，wqe 的状态将恢复到数据包发送之前的状态，以便可以重新发送。 然而，保存和恢复状态的例程错过了 wqe（用于处理 sge 表的 dma 结构）中变量状态的重要部分。 并且，在构建修改 dma 结构的数据包之前不会保存状态。 在快速节点上的许多 QP 向慢速节点发送大消息的重压力测试下，会观察到丢弃的数据包，并且由于 dma 结构未恢复，重新发送的数据包已损坏。 此补丁修复了此行为并允许测试用例成功
                rollback_wqe->state = wqe->state;
                *rollback_psn = qp->req.psn
            rxe_get_av(&pkt, &ah) -> RDMA/rxe：修复 rxe_av.c 中的引用错误，下面引用的提交可以引用永远不会被删除的 AH。 这仅发生在 UD 请求路径中。 该补丁可以选择将该 AH 传递回调用者，以便它可以在访问 AV 时保存引用，然后删除它。 执行此操作的代码已添加到 rxe_req.c 中。 AV 还传递给 rxe_net.c 中的 rxe_prepare 作为优化
                return &pkt->qp->pri_av
                rxe_pool_get_index
                    elem = xa_load(xa, index)
                rxe_ah_pd
            init_req_packet
                int			pad = (-payload) & 0x3
                paylen = rxe_opcode[opcode].length + payload + pad + RXE_ICRC_SIZE
                rxe_init_packet
                    rdma_get_gid_attr -> RDMA：支持超过 255 个 rdma 端口，当前代码在处理 RDMA 设备的端口时使用许多不同的类型：u8、unsigned int 和 u32。 切换到 u32 来清理逻辑。 这使我们（至少）能够使核心视图保持一致并使用相同的类型。 不幸的是，并非所有地方都可以转换。 许多 uverbs 函数期望端口为 u8，因此保留这些位置以免破坏 UAPI。 硬件/规范定义的值也不得更改。 通过切换到 u32，我们现在可以支持具有超过 255 个端口的设备。 U32_MAX 被保留以使控制逻辑更容易处理。 由于具有 U32_MAX 端口的设备可能不会很快发生这种情况，这似乎不是问题。 当创建具有超过 255 个端口的设备时，uverbs 将报告 RDMA 设备具有 255 个端口，因为这是当前支持的最大值。 verbs 接口尚未更改，因为 IBTA 规范在太多地方将端口大小限制为 u8，并且所有依赖 verbs 的应用程序将无法应对此更改。 在此阶段，我们正在扩展仅使用供应商通道的接口。一旦解除限制，switchdev 模式下的 mlx5 将能够拥有设备创建的数千个 SF。 由于报告超过 255 个端口的 RDMA 设备的唯一实例将是代表设备，并且它将自身暴露为仅原始以太网设备 CM/MAD/IPoIB 和其他 ULP 不受此更改的影响，并且它们的 sysfs/接口 暴露给用户空间的内容可以保持不变。 虽然在这里清理了一些对齐问题并删除了不需要的健全性检查（主要在 rdmavt 中
                        rdma_gid_table
                        get_gid_entry
                    rdma_read_gid_attr_ndev_rcu
                    skb = alloc_skb(paylen + hdr_len + LL_RESERVED_SPACE(ndev), -> rdma_rxe：使 rxe 在 802.1q VLAN 设备上工作，此补丁修复了 802.1q VLAN 设备上的 RDMA/rxe。 如果没有它，我会观察到以下行为：a) 通过 rxe_net_add() 将 VLAN 设备添加到 RXE 会创建一个无法正常工作的 RDMA 设备。 这是由 enum_all_gids_of_dev_cb() / is_eth_port_of_netdev() 中的逻辑引起的，该逻辑仅考虑连接到已配置网络设备的“上层设备”的网络，导致本身就是“上层设备”的 VLAN 接口的 gid 集为空 。 稍后尝试通过此 rdma 设备进行连接会在 cma_acuire_dev() 中失败，因为无法解析 gids。 b) 添加 VLAN 设备的主设备似乎最初可以正常工作，通过 VLAN 设备的目标地址已成功解析。 但连接超时，因为以太网数据包中没有插入 802.1q VLAN 标头，因此永远不会收到以太网数据包。 发生这种情况是因为 RXE 层通过主设备而不是 VLAN 设备发送数据包。 该问题可以通过更改 a) 或 b) 来解决。 我的想法是a）中的逻辑是故意创建的，因此我决定在b）上工作。 事实证明，有关当前 gid 的 VLAN 接口的信息在 AV 信息中可用。 我的补丁将 RXE 代码转换为使用此 netdev 而不是 rxe->ndev。 通过此更改，RXE over vlan 可在我的测试系统上运行
                    skb_reserve(skb, hdr_len + LL_RESERVED_SPACE(ndev)) -> RDMA/rxe：根据GID的netdev考虑skb预留空间，始终根据GID属性的netdevice考虑skb预留空间，无论vlan还是非vlan netdevice
                    pkt->hdr	= skb_put(skb, paylen)
                bth_init(pkt, pkt->opcode, solicited, 0, pad, IB_DEFAULT_PKEY_FULL
                    bth->opcode = opcode;
                reth_set_rkey -> RDMA/rxe：在请求方实现RC RDMA FLUSH服务，在请求方实现FLUSH请求操作
                reth_set_va
                reth_set_len -> IB/rxe：修复了 rdma 读取重试问题，当对剩余部分数据重试读取请求时，响应可能会首先从读取响应或仅读取响应重新启动。 所以支持这些案例。 不要将 comp psn 推进到当前 wqe 的 last_psn 之外，因为这可能会跳过整个读取 wqe，并导致 req_retry() 逻辑设置不正确的 req psn。 示例序列如下： 写入 PSN 40——这是当前的 WQE。 读取请求 PSN 41 写入 PSN 42 接收 ACK PSN 42——这将完成 PSN 40 的当前 WQE，并将 comp psn 设置为 42，这是一个问题，因为 PSN 41 处的读取请求已被跳过。 因此，当 req_retry() 尝试重新传输读取请求时，它将 req psn 设置为 42，这是不正确的。 重试读请求时，根据 dma resid 而不是 wqe first_psn 计算完成的 psn 数。 如果多次重试读取请求，则 wqe first_psn 可能已移动。 将reth长度设置为dma resid以处理剩余部分数据的读取重试
                feth_init
                immdt_set_imm
                ieth_set_rkey
                deth_set_qkey
                deth_set_sqp
            finish_packet
                rxe_prepare
                    ether_addr_equal(skb->dev->dev_addr, av->dmac) -> 比较两个以太网地址，如果相等则返回 true
                bth_pad(pkt) -> rxe：正确计算未对齐有效负载的 iCRC，如果发送或接收的 RoCE PDU 包含填充字节，则 iCRC 计算错误，导致 RXE 发出具有不正确 iCRC 的 PDU，以及由于错误检测到入口 PDU 而被丢弃 PDU 中的 iCRC 错误。 解决方法是在 iCRC 计算中包含填充字节（如果有）。 注意：自从软 RoCE 驱动程序首次放入主流内核以来，此错误已导致与实际硬件 RoCE 设备的在线兼容性损坏。 修复它会导致与原始软 RoCE 设备不兼容，但对于与真实硬件设备兼容是必要的, commit: https://github.com/ssbandjl/linux/commit/2030abddec6884aaf5892f5724c48fc340e6826f
                     rxe_crc32(rxe, crc, pad, bth_pad(pkt))
            update_wqe_state(qp, wqe, &pkt) -> IB/rxe：修复请求者和完成者之间的竞争条件，rxe_requester() 使用 rxe_xmit_packet() 发送 pkt，然后调用 rxe_update() 来更新 wqe 和 qp 的 psn 值。 但有时，在请求者有时间更新 wqe 之前就收到了响应，在这种情况下，完成者会对错误的 wqe 值采取行动。 此修复在实际发送请求之前更新 wqe 和 qp，并在 xmit 失败时回滚
            update_wqe_psn(qp, wqe, &pkt, payload)
                int num_pkt = (wqe->dma.resid + payload + qp->mtu - 1) / qp->mtu
                qp->req.psn = (wqe->first_psn + num_pkt) & BTH_PSN_MASK
            rxe_xmit_packet(qp, &pkt, skb)
                rxe_icrc_generate -> compute pkt icrc
                    rxe_icrc_hdr
                    rxe_crc32
                rxe_loopback
                    skb_pull(skb, sizeof(struct iphdr)) -> RDMA/rxe：纠正环回路径上的 skb，rxe_net.c 在 IP 层发送数据包，其中 skb->data 指向 IP 标头，但从 UDP 隧道接收数据包，其中 skb->data 指向 UDP 标头。 在环回路径上，没有正确考虑这一点。 此补丁通过使用 sbk_pull() 从接收到的数据包的 skb 中剥离 IP 标头来纠正此问题 -> 从数据区头部移除数据
                    or skb_pull(skb, sizeof(struct ipv6hdr))
                    skb_pull(skb, sizeof(struct udphdr))
                    rxe_rcv(skb) -> RDMA/rxe：修复 rxe_send 和 rxe_loopback，修复 rxe_net.c 中的 rxe_send() 和 rxe_loopback() 以具有相同的调用序列。 这个补丁使它们静态并具有相同的参数列表和返回值
                        
                or rxe_send
                rxe_sched_task(&qp->comp.task)
                rxe_counter_inc(rxe, RXE_CNT_SENT_PKTS)
            update_state(qp, &pkt)



struct rxe_wr_opcode_info rxe_wr_opcode_info[] = {
	[IB_WR_RDMA_WRITE]				= {
		.name	= "IB_WR_RDMA_WRITE",
		.mask	= {
			[IB_QPT_RC]	= WR_INLINE_MASK | WR_WRITE_MASK,
			[IB_QPT_UC]	= WR_INLINE_MASK | WR_WRITE_MASK,
		},
	},



qp type:
enum ib_qp_type {
	/*
	 * IB_QPT_SMI and IB_QPT_GSI have to be the first two entries
	 * here (and in that order) since the MAD layer uses them as
	 * indices into a 2-entry table.
	 */
	IB_QPT_SMI,
	IB_QPT_GSI,

	IB_QPT_RC = IB_UVERBS_QPT_RC,
	IB_QPT_UC = IB_UVERBS_QPT_UC,
	IB_QPT_UD = IB_UVERBS_QPT_UD,
	IB_QPT_RAW_IPV6,
	IB_QPT_RAW_ETHERTYPE,
	IB_QPT_RAW_PACKET = IB_UVERBS_QPT_RAW_PACKET,
	IB_QPT_XRC_INI = IB_UVERBS_QPT_XRC_INI,
	IB_QPT_XRC_TGT = IB_UVERBS_QPT_XRC_TGT,
	IB_QPT_MAX,
	IB_QPT_DRIVER = IB_UVERBS_QPT_DRIVER,
	/* Reserve a range for qp types internal to the low level driver.
	 * These qp types will not be visible at the IB core layer, so the
	 * IB_QPT_MAX usages should not be affected in the core layer
	 */
	IB_QPT_RESERVED1 = 0x1000,
	IB_QPT_RESERVED2,
	IB_QPT_RESERVED3,
	IB_QPT_RESERVED4,
	IB_QPT_RESERVED5,
	IB_QPT_RESERVED6,
	IB_QPT_RESERVED7,
	IB_QPT_RESERVED8,
	IB_QPT_RESERVED9,
	IB_QPT_RESERVED10,
};


wqe state:
enum wqe_state {
	wqe_state_posted,
	wqe_state_processing,
	wqe_state_pending,
	wqe_state_done,
	wqe_state_error,
};



rxe opcode:
struct rxe_opcode_info rxe_opcode[RXE_NUM_OPCODE] = {
	[IB_OPCODE_RC_SEND_FIRST]			= {
		.name	= "IB_OPCODE_RC_SEND_FIRST",
		.mask	= RXE_PAYLOAD_MASK | RXE_REQ_MASK | RXE_RWR_MASK |
			  RXE_SEND_MASK | RXE_START_MASK,
		.length = RXE_BTH_BYTES,
		.offset = {
			[RXE_BTH]	= 0,
			[RXE_PAYLOAD]	= RXE_BTH_BYTES,
		}
	},
    ...


bth, Base Transport Header -> IBA 标头类型和方法，其中一些仅供参考和完整性，因为 rxe 目前不支持 RD 传输，其中大部分可以移至 IB 核心中。 ib_pack.h 包含其中的一部分，但不完整 用于向标头插入值或从标头中提取值的标头特定例程 名为 __hhh_(set_)fff() 的例程采用指向 hhh 标头的指针并获取（设置） fff 字段。 名为 hhh_(set_)fff 的例程采用数据包信息结构，并根据数据包中的操作码查找标头和字段。 还完成了从 cpu 顺序到网络字节顺序的转换
struct rxe_bth {
	u8			opcode;
	u8			flags;
	__be16			pkey;
	__be32			qpn;
	__be32			apsn;
};


wqe state:
enum wqe_state {
	wqe_state_posted,
	wqe_state_processing,
	wqe_state_pending,
	wqe_state_done,
	wqe_state_error,
};


link script:
arch/x86/kernel/vmlinux.lds.S
do_initcall_level




modprobe rdma_rxe
ls -alh /lib/modules/
drivers/infiniband/sw/rxe/Makefile
obj-$(CONFIG_RDMA_RXE) += rdma_rxe.o


drivers/infiniband/core/Makefile



rdma_nl_rcv_msg
    unsigned int index = RDMA_NL_GET_CLIENT(type)
    get_cb_table




MODULE_AUTHOR("Roland Dreier");
MODULE_DESCRIPTION("InfiniBand userspace verbs access");
MODULE_LICENSE("Dual BSD/GPL");


static const struct class uverbs_class = {
	.name = "infiniband_verbs",
	.devnode = uverbs_devnode,
        kasprintf(GFP_KERNEL, "infiniband/%s", dev_name(dev))
};



static int ib_uverbs_open(struct inode *inode, struct file *filp) -> ib_uverbs_device 结构进行了正确的引用计数，并且其他所有内容都纯粹是正在创建的文件的本地内容，因此与其他打开调用的竞争不是问题； 没有可与之竞争的 ioctl 方法； open 方法将立即运行 -ENXIO，或者将完成所有必需的初始化
    get_device(&dev->dev)
    rdma_dev_access_netns(ib_dev, current->nsproxy->net_ns)
    list_add_tail(&file->list, &dev->uverbs_file_list)
    setup_ufile_idr_uobject(file)
        xa_init_flags(&ufile->idr, XA_FLAGS_ALLOC)
    stream_open -> Stream_open 由需要类流文件描述符的子系统使用。 此类文件描述符不可查找，并且没有位置概念（file.f_pos 始终为 0，传递给 .read()/.write() 的 ppos 始终为 NULL）。 与其他常规文件的文件描述符相反，.read() 和 .write() 可以同时运行。 Stream_open 永远不会失败，并被标记为返回 int，以便它可以直接用作 file_operations.open -> fs：stream_open - 类似流文件的打开器，以便读取和写入……可以同时运行而不会出现死锁 Commit 9c225f2（“vfs：按照 POSIX 的原子 f_pos 访问”）为 file.f_pos 访问添加了锁定，特别是并发读取和写入 write 不可能 - 现在这两个函数在整个运行过程中都采用 f_pos 锁定，因此如果例如 读取被阻塞等待数据，写入将死锁等待读取完成。 这导致了类似流的文件的回归，以前的读取和写入可以同时运行，但在该补丁之后就不能再这样做了。 参见例如 提交 581d21a（“xenbus：修复写入 /proc/xen/xenbus 时的死锁”），它修复了 /proc/xen/xenbus 特定情况下的这种回归。 2014 年添加 f_pos 锁的补丁是为了保证读/写/lseek 的 POSIX 线程安全，并将锁定添加到所有常规文件的文件描述符。 在 2014 年，线程安全问题并不是什么新鲜事，因为它已经在 2006 年早些时候讨论过。然而，即使 Linus 补丁的 2006 版本添加了 f_pos 锁定，“仅适用于用 FMODE_LSEEK 标记为可查找的文件（从而避免了类似流的情况） 管道和套接字等对象）”，2014 年版本 - 实际上将其作为 9c225f2 放入树中 - 无论文件是否可查找，都会这样做。 请参阅 https://lore.kernel.org/lkml/53022DB1.4070805@gmail.com/ https://lwn.net/Articles/180387 https://lwn.net/Articles/180396 了解历史背景。 这样做的原因可能是有许多文件被标记为不可查找，但是例如 他们的读取实现实际上取决于了解当前位置以正确处理读取。 一些示例： kernel/power/user.c snapshot_read fs/debugfs/file.c u32_array_read fs/fuse/control.c fusion_conn_waiting_read + ... drivers/hwmon/asus_atk0110.c atk_debugfs_ggrp_read arch/s390/hypfs/inode.c hypfs_read_iter 。 .. 尽管如此，许多 nonseekable_open 用户使用纯流语义实现读写 - 他们根本不依赖于传递的 ppos。 对于那些读取可能等待内部某些内容的情况，它会创建类似于 xenbus 的情况 - 在读取完成之前写入可能永远不会进行，并且读取正在等待一些可能是外部的事件，时间可能不受限制 - > 僵局。 除了xenbus之外，我还通过语义补丁在内核中发现了14个这样的地方（见下文）： drivers/xen/evtchn.c:667:8-24: ERROR: evtchn_fops: .read() can deadlock .write () drivers/isdn/capi/capi.c:963:8-24: 错误: capi_fops: .read() 可能会死锁 .write() drivers/input/evdev.c:527:1-17: 错误: evdev_fops: .read() 可能死锁 .write() drivers/char/pcmcia/cm4000_cs.c:1685:7-23: 错误：cm4000_fops: .read() 可能死锁 .write() net/rfkill/core.c:1146: 8-24：错误：rfkill_fops：.read（）可以死锁.write（）驱动程序/s390/char/fs3270.c：488：1-17：错误：fs3270_fops：.read（）可以死锁.write（）驱动程序/ usb/misc/ldusb.c:310:1-17: 错误: ld_usb_fops: .read() 可能死锁 .write() drivers/hid/uhid.c:635:1-17: 错误: uhid_fops: .read() 可以死锁 .write() net/batman-adv/icmp_socket.c:80:1-17: 错误：batadv_fops: .read() 可以死锁 .write() drivers/media/rc/lirc_dev.c:198:1- 17：错误：lirc_fops：.read（）可以死锁.write（）drivers/leds/uleds.c：77：1-17：错误：uleds_fops：.read（）可以死锁.write（）drivers/input/misc/ uinput.c：400：1-17：错误：uinput_fops：.read（）可以死锁.write（）drivers / infiniband / core / user_mad.c：985：7-23：错误：umad_fops：.read（）可以死锁 .write() drivers/gnss/core.c:45:1-17: ERROR: gnss_fops: .read() can deadlock .write() 除了上述情况之外，由 f_pos 锁定引起的另一个回归是现在 FUSE 文件系统 使用 FOPEN_NONSEEKABLE 标志实现打开，不能再实现双向流式文件 - 原因与上述相同，例如 读可能会死锁内核中 file.f_pos 上的写锁定。 FUSE 的 FOPEN_NONSEEKABLE 于 2008 年在 a7c1b99 中添加（“fuse：实现不可查找打开”）以支持 OSSPD。 OSSPD 在用户空间中使用 FOPEN_NONSEEKABLE 标志实现 /dev/dsp，相应的读写例程根本不依赖于当前位置，并且读写都可能是阻塞操作：请参阅 https://github.com/libfuse/osspd https ://lwn.net/Articles/308445 https://github.com/libfuse/osspd/blob/14a9cff0/osspd.c#L1406 https://github.com/libfuse/osspd/blob/14a9cff0/osspd.c #L1438-L1477 https://github.com/libfuse/osspd/blob/14a9cff0/osspd.c#L1479-L1510 相应的 libfuse 示例/测试也将 FOPEN_NONSEEKABLE 描述为“有点类似管道的文件...”，读取处理程序不 使用偏移量。 然而，该测试仅实现读而不写，无法执行死锁场景：https://github.com/libfuse/libfuse/blob/fuse-3.4.2-3-ga1bff7d/example/poll.c#L124-L131 https： //github.com/libfuse/libfuse/blob/fuse-3.4.2-3-ga1bff7d/example/poll.c#L146-L163 https://github.com/libfuse/libfuse/blob/fuse-3.4.2-3-ga1bff7d/example/poll.c#L209-L216 我实际上在实现我的 FUSE 文件系统时遇到了读与写死锁，其中 / head/watch 文件，其中 open 在文件系统与其用户之间创建单独的双向套接字流，并且稍后同时执行读取和写入。 从语义上讲，将流分成两个单独的只读和只写通道并不容易：https://lab.nexedi.com/kirr/wendelin.core/blob/f13aa600/wcfs/wcfs.go#L88 -169 让我们修复这个回归。 计划是： 1. 我们无法更改 nonseekable_open 以包含 &~FMODE_ATOMIC_POS - 这样做会破坏许多在读/写处理程序中实际使用 ppos 的内核内 nonseekable_open 用户。 2. 将stream_open()添加到内核以打开类似流的不可查找文件描述符。 对此类文件描述符的读写永远不会使用或更改 ppos。 并且通过类似流的文件的该属性，读取和写入将在不获取 f_pos 锁定的情况下运行 - 即读取和写入可以同时运行。 3. 使用语义补丁搜索并将所有内核中的 nonseekable_open 用户转换为stream_open，这些用户的读写实际上不依赖于 ppos，并且 file_operations 中没有其他方法假定@offset 访问。 4. 将 FOPEN_STREAM 添加到 fs/fuse/ 并通过 steam_open 打开内核文件描述符（如果该位存在于文件系统打开回复中）。 人们很想将 fs/fuse/ open 处理程序更改为仅在 FOPEN_NONSEEKABLE 标志上使用 stream_open 而不是 nonseekable_open，但是通过 Debian codesearch 进行 grep 显示了 FOPEN_NONSEEKABLE 的用户，特别是 GVFS，它实际上在其读写处理程序中使用了偏移量 https:// codesearch.debian.net/search?q=-%3Enonseekable+%3D https://gitlab.gnome.org/GNOME/gvfs/blob/1.40.0-6-gcbc54396/client/gvfsfusedaemon.c#L1080 https:// gitlab.gnome.org/GNOME/gvfs/blob/1.40.0-6-gcbc54396/client/gvfsfusedaemon.c#L1247-1346 https://gitlab.gnome.org/GNOME/gvfs/blob/1.40.0-6 -gcbc54396/client/gvfsfusedaemon.c#L1399-1481 因此，如果我们进行这样的更改，它将破坏真实用户。 5. 从 v3.14+（9c225f2 首次出现的内核）开始，向稳定内核添加stream_open 和 FOPEN_STREAM 处理。 这将允许修补 OSSPD 和其他提供类似流文件的 FUSE 文件系统以返回 FOPEN_STREAM | FOPEN_NONSEEKABLE 在其打开处理程序中，这样可以避免所有内核版本上的死锁。 这应该可行，因为 fs/fuse/ 忽略从文件系统返回的未知打开标志，因此将 FOPEN_STREAM 传递给不知道该标志的内核不会造成伤害。 反过来，不知道 FOPEN_STREAM 的内核将是 < v3.14，其中仅 FOPEN_NONSEEKABLE 就足以实现没有读与写死锁的流。 此补丁添加了stream_open，将/proc/xen/xenbus转换为它，并添加语义补丁以自动定位内核中的位置，这些位置要么由于读与写死锁而需要转换，要么因为读和写而可以安全地转换 write 不使用 ppos，并且 file_operations 中没有其他时髦的方法。 关于语义补丁，我已经手动验证了每个生成的更改 - 转换是正确的 - 以及剩下的每个 nonseekable_open 实例 - 在那里转换不正确，或者由于当前的stream_open.cocci限制而没有转换。 该脚本也不会转换应该有效转换但当前具有 .llseek = noop_llseek 或 generic_file_llseek 的文件，原因未知，尽管文件是使用 nonseekable_open 打开的（例如 drivers/input/mousedev.c）-> commit: https://github.com/ssbandjl/linux/commit/10dce8af34226d90fa56746a934f8da5dcdba3df
    



    


root@u20:~/project/linux/linux-5.15# modinfo rdma_rxe
filename:       /lib/modules/5.15.0/kernel/drivers/infiniband/sw/rxe/rdma_rxe.ko
alias:          rdma-link-rxe
license:        Dual BSD/GPL
description:    Soft RDMA transport
author:         Bob Pearson, Frank Zago, John Groves, Kamal Heib
srcversion:     2F508E948313DB64B26EC87
depends:        ib_core,ip6_udp_tunnel,udp_tunnel,ib_uverbs

# depends define in drivers/infiniband/sw/rxe/Kconfig

MODULE_AUTHOR("Bob Pearson, Frank Zago, John Groves, Kamal Heib");
MODULE_DESCRIPTION("Soft RDMA transport");
MODULE_LICENSE("Dual BSD/GPL");


drivers/infiniband/sw/rxe/Kconfig, 
config RDMA_RXE
	tristate "Software RDMA over Ethernet (RoCE) driver"
	depends on INET && PCI && INFINIBAND
	depends on INFINIBAND_VIRT_DMA


MODULE_ALIAS
MODULE_ALIAS_RDMA_NETLINK(RDMA_NL_LS, 4); -> MODULE_ALIAS("rdma-netlink-subsys-" __stringify(_val))


MODULE_ALIAS_RDMA_CLIENT("uverbs");



get_cb_table
    if (sock_net(skb->sk) != &init_net && type != RDMA_NL_NLDEV)
    request_module("rdma-netlink-subsys-%u", type);



root@u20:/sys/class/infiniband# modinfo ib_core
filename:       /lib/modules/5.15.0/kernel/drivers/infiniband/core/ib_core.ko
alias:          rdma-netlink-subsys-4
license:        Dual BSD/GPL
description:    core kernel InfiniBand API
author:         Roland Dreier
alias:          net-pf-16-proto-20
alias:          rdma-netlink-subsys-5


root@u20:/sys/class/infiniband# modinfo ib_uverbs
filename:       /lib/modules/5.15.0/kernel/drivers/infiniband/core/ib_uverbs.ko
alias:          rdma-client-uverbs
license:        Dual BSD/GPL
description:    InfiniBand userspace verbs access
author:         Roland Dreier
srcversion:     6E9922C32F5205C45103434
depends:        ib_core



rdma netlink type:
enum {
	RDMA_NL_IWCM = 2,
	RDMA_NL_RSVD,
	RDMA_NL_LS,	/* RDMA Local Services */
	RDMA_NL_NLDEV,	/* RDMA device interface */
	RDMA_NL_NUM_CLIENTS
};



异步事件流程:
mlx5_ib_handle_event()
  ib_dispatch_event()
    ib_cache_event()
       queue_work() -> slow cache update

    [..]
    ipoib_event()
     queue_work()
       [..]
       work handler
         ipoib_ib_dev_flush_light()
           __ipoib_ib_dev_flush()
              ipoib_dev_addr_changed_valid()
                rdma_query_gid() <- Returns old GID, cache not updated





IB事件类型
enum ib_event_type {
	IB_EVENT_CQ_ERR,
	IB_EVENT_QP_FATAL,
	IB_EVENT_QP_REQ_ERR,
	IB_EVENT_QP_ACCESS_ERR,
	IB_EVENT_COMM_EST,
	IB_EVENT_SQ_DRAINED,
	IB_EVENT_PATH_MIG,
	IB_EVENT_PATH_MIG_ERR,
	IB_EVENT_DEVICE_FATAL,
	IB_EVENT_PORT_ACTIVE,
	IB_EVENT_PORT_ERR,
	IB_EVENT_LID_CHANGE,
	IB_EVENT_PKEY_CHANGE,
	IB_EVENT_SM_CHANGE,
	IB_EVENT_SRQ_ERR,
	IB_EVENT_SRQ_LIMIT_REACHED,
	IB_EVENT_QP_LAST_WQE_REACHED,
	IB_EVENT_CLIENT_REREGISTER,
	IB_EVENT_GID_CHANGE,  -> GID改变, 存储GID, 
	IB_EVENT_WQ_FATAL,
};


rdma_resolve_route
    cma_resolve_ib_route
        cma_init_resolve_route_work
        cma_query_ib_route
            ib_sa_path_rec_get cma_query_handler
                query->sa_query.callback = callback ? ib_sa_path_rec_callback : NULL



ibv_open_device
...
IB_USER_VERBS_CMD_QUERY_DEVICE
ib_uverbs_query_device
    ib_uverbs_get_ucontext
    uverbs_request
    copy_query_dev_fields
        struct ib_device *ib_dev = ucontext->device;

        resp->fw_ver		= attr->fw_ver;
        resp->node_guid		= ib_dev->node_guid;
        resp->sys_image_guid	= attr->sys_image_guid;
        resp->max_mr_size	= attr->max_mr_size;
        resp->page_size_cap	= attr->page_size_cap;
        resp->vendor_id		= attr->vendor_id;
        resp->vendor_part_id	= attr->vendor_part_id;
        resp->hw_ver		= attr->hw_ver;
        resp->max_qp		= attr->max_qp;
        resp->max_qp_wr		= attr->max_qp_wr;
        resp->device_cap_flags  = lower_32_bits(attr->device_cap_flags);
        resp->max_sge		= min(attr->max_send_sge, attr->max_recv_sge);
        resp->max_sge_rd	= attr->max_sge_rd;
        resp->max_cq		= attr->max_cq;
        resp->max_cqe		= attr->max_cqe;
        resp->max_mr		= attr->max_mr;
        resp->max_pd		= attr->max_pd;
        resp->max_qp_rd_atom	= attr->max_qp_rd_atom;
        resp->max_ee_rd_atom	= attr->max_ee_rd_atom;
        resp->max_res_rd_atom	= attr->max_res_rd_atom;
        resp->max_qp_init_rd_atom	= attr->max_qp_init_rd_atom;
        resp->max_ee_init_rd_atom	= attr->max_ee_init_rd_atom;
        resp->atomic_cap		= attr->atomic_cap;
        resp->max_ee			= attr->max_ee;
        resp->max_rdd			= attr->max_rdd;
        resp->max_mw			= attr->max_mw;
        resp->max_raw_ipv6_qp		= attr->max_raw_ipv6_qp;
        resp->max_raw_ethy_qp		= attr->max_raw_ethy_qp;
        resp->max_mcast_grp		= attr->max_mcast_grp;
        resp->max_mcast_qp_attach	= attr->max_mcast_qp_attach;
        resp->max_total_mcast_qp_attach	= attr->max_total_mcast_qp_attach;
        resp->max_ah			= attr->max_ah;
        resp->max_srq			= attr->max_srq;
        resp->max_srq_wr		= attr->max_srq_wr;
        resp->max_srq_sge		= attr->max_srq_sge;
        resp->max_pkeys			= attr->max_pkeys;
        resp->local_ca_ack_delay	= attr->local_ca_ack_delay;
        resp->phys_port_cnt = min_t(u32, ib_dev->phys_port_cnt, U8_MAX);
    uverbs_response



ibv_cmd_get_context
IB_USER_VERBS_CMD_GET_CONTEXT -> ib_uverbs_get_context
ib_alloc_ucontext(attrs)
    struct ib_ucontext *ucontext;
    ucontext = rdma_zalloc_drv_obj(ib_dev, ib_ucontext)
        kzalloc_node(size, gfp, dev->ops.get_numa_node(dev))
        or kzalloc(size, gfp)
    xa_init_flags(&ucontext->mmap_xa, XA_FLAGS_ALLOC)
    rdma_restrack_new(&ucontext->res, RDMA_RESTRACK_CTX) -> RDMA/restrack：计算对动词对象的引用，重构重新跟踪代码以确保重新跟踪条目内的 kref 正确地是其嵌入的对象。 这一细微的变化对于 MR 和 QP 的未来转换是必要的，这些转换在发布和 kfree 之前会重新计数。 从ib_core角度来看，理想的流程如下： * 使用rdma_zalloc_*分配ib_*结构。 * 将 ib_core 已知的所有内容设置为新创建的对象。 * 使用 retrack 帮助初始化 kref * 调用驱动程序特定的分配函数。 * 插入retrack DB .... * 使用retrack_put 返回并释放retrack。 很大程度上，这意味着应该在分配包含结构时调用 rdma_restrack_new()
        kref_init(&res->kref)
        init_completion(&res->comp)
    rdma_restrack_set_name(&ucontext->res, NULL)
        rdma_restrack_attach_task
            put_task_struct -> static inline void put_task_struct
            get_task_struct
    attrs->context = ucontext
ib_init_ucontext(attrs)
    ib_rdmacg_try_charge RDMACG_RESOURCE_HCA_HANDLE -> rdmacg_try_charge - 分层尝试对 rdma 资源进行充电 @rdmacg：指向将拥有此资源的 rdma cgroup 的指针 @device：指向 rdmacg 设备的指针 @index：cgroup（资源池）中要充电的资源的索引 * 此函数遵循以下中的充电资源 分层方式。 如果收费会导致新值超出层级限制，则会失败。 如果充电成功则返回 0，否则返回 -EAGAIN、-ENOMEM 或 -EINVAL。 当充电成功时，返回指向该资源的 rdmacg 的指针。 * Charger需要根据两个标准来计算资源。 (a) 每个 cgroup 和 (b) 每个设备资源使用情况。 每个 cgroup 资源使用情况可确保 cgroup 的任务不会超出配置的限制。 每设备提供多设备使用的精细配置。 它在层次结构中为其遇到的第一个资源的每个父级分配资源池。 稍后资源池将可用。 因此充电/放电会更快
        get_current_rdmacg
        parent_rdmacg
        get_cg_rpool_locked
    alloc_ucontext -> .alloc_ucontext = irdma_alloc_ucontext,
        struct irdma_uk_attrs *uk_attrs = &iwdev->rf->sc_dev.hw_attrs.uk_attrs;
        if (udata->outlen == IRDMA_ALLOC_UCTX_MIN_RESP_LEN) 
            uresp.max_qps = iwdev->rf->max_qp;
            uresp.max_pds = iwdev->rf->sc_dev.hw_attrs.max_hw_pds;
            uresp.wq_size = iwdev->rf->sc_dev.hw_attrs.max_qp_wr * 2
        else
            u64 bar_off = (uintptr_t)iwdev->rf->sc_dev.hw_regs[IRDMA_DB_ADDR_OFFSET]
            ucontext->db_mmap_entry = irdma_user_mmap_entry_insert(ucontext, bar_off, IRDMA_MMAP_IO_NC, &uresp.db_mmap_key)
                rdma_user_mmap_entry_insert(&ucontext->ibucontext, &entry->rdma_entry, PAGE_SIZE)
                    rdma_user_mmap_entry_insert_range(ucontext, entry, length, 0, U32_MAX)
                        npages = (u32)DIV_ROUND_UP(length, PAGE_SIZE)
                        while true
                            xas_find_marked(&xas, max_pgoff, XA_FREE_MARK)
                         __xa_insert(&ucontext->mmap_xa, i, entry, GFP_KERNEL)
                *mmap_offset = rdma_user_mmap_get_offset(&entry->rdma_entry)
                    return (u64)entry->start_pgoff << PAGE_SHIFT
            uresp.max_hw_sq_chunk = uk_attrs->max_hw_sq_chunk;
            uresp.max_hw_inline = uk_attrs->max_hw_inline
            uresp.max_hw_cq_size = uk_attrs->max_hw_cq_size;
            uresp.min_hw_cq_size = uk_attrs->min_hw_cq_size;
            ib_copy_to_udata(udata, &uresp,
        INIT_LIST_HEAD(&ucontext->cq_reg_mem_list)
        ...
    rdma_restrack_add
        xa_insert(&rt->xa, res->id, res, GFP_KERNEL)
ib_uverbs_init_async_event_file
    ib_uverbs_init_event_queue(&async_file->ev_queue)
        INIT_LIST_HEAD(&ev_queue->event_list)
        init_waitqueue_head(&ev_queue->poll_wait)
        IO信号: https://zhuanlan.zhihu.com/p/571496225
        等待队列: https://cslqm.github.io/2020/01/08/wait_queue_head_t/
        等待队列wait.h详解: https://www.zhihu.com/people/muelsyse-rhinelab/posts
    INIT_IB_EVENT_HANDLER(&async_file->event_handler, ib_dev, ib_uverbs_event_handler);
        ib_uverbs_async_handler(container_of(handler, struct ib_uverbs_async_event_file,event_handler), event->element.port_num, event->event, NULL, NULL)
            list_add_tail(&entry->list, &async_file->ev_queue.event_list)
            list_add_tail(&entry->obj_list, obj_list)
            wake_up_interruptible(&async_file->ev_queue.poll_wait)
			kill_fasync(&async_file->ev_queue.async_queue, SIGIO, POLL_IN)
    ib_register_event_handler(&async_file->event_handler) -> RDMA/core：将 ib_uverbs_async_event_file 制作为 uobject，这使得异步事件与完成事件对齐，因为两者都是 FD 类型的完整 uobject，并使用相同的 uobject 生命周期。 一堆重复的代码被合并，两个 FD 之间的总体流程现在非常相似 -> 异步IB事件定义在IB规范的第11章(11.5 EVENT HANDLING)
        list_add_tail(&event_handler->list, &event_handler->device->event_handler_list) -> 其他线程通过遍历事件控制器链表触发handler回调
rdma_alloc_commit_uobject(uobj, attrs)
    uobj->uapi_object->type_class->alloc_commit(uobj)


[补丁] IB uverbs：核心 API 扩展，一系列补丁中的第一个，添加了对用户空间直接访问 InfiniBand 硬件的支持 - 所谓的“用户空间动词”。 我相信这些补丁已经准备好合并，但最终审查会很有用。 这些补丁应该包含我四月份发布早期版本时讨论中的所有反馈（有关线程的开头，请参阅 http://lkml.org/lkml/2005/4/4/267）。 特别是，由用户空间使用的固定内存在 current->mm->vm_locked 中进行说明，并且根据 RLIMIT_MEMLOCK 检查固定内存的请求。 此补丁：修改 ib_verbs.h 头文件，进行 InfiniBand 用户空间动词支持所需的更改。 我们添加了一些结构来跟踪用户空间上下文，并扩展驱动程序 API，以便低级驱动程序知道何时创建将从用户空间使用的资源
commit: https://github.com/ssbandjl/linux/commit/e2773c062e41f710d8ef1e8a790c7e558aff663d
struct ib_ucontext {
	struct ib_device       *device;
	struct ib_uverbs_file  *ufile;

	struct ib_rdmacg_object	cg_obj;
	/*
	 * Implementation details of the RDMA core, don't use in drivers:
	 */
	struct rdma_restrack_entry res;
	struct xarray mmap_xa;
};



IB_USER_VERBS_CMD_ALLOC_PD -> ib_uverbs_alloc_pd
pd = rdma_zalloc_drv_obj(ib_dev, ib_pd)
rdma_restrack_new(&pd->res, RDMA_RESTRACK_PD)
ret = ib_dev->ops.alloc_pd(pd, &attrs->driver_udata) -> irdma_alloc_pd
    irdma_alloc_rsrc
    irdma_sc_pd_init -> set pd_id, abi_ver, dev to pd
    


struct ib_umem {
	struct ib_device       *ibdev;
	struct mm_struct       *owning_mm;
	u64 iova;
	size_t			length;
	unsigned long		address;
	u32 writable : 1;
	u32 is_odp : 1;
	u32 is_dmabuf : 1;
	struct sg_append_table sgt_append;
};



sysfs:
Documentation/translations/zh_CN/filesystems/sysfs.txt



struct device_attribute - Interface for exporting device attributes



ib port atrribute
struct ib_port_attr {
	u64			subnet_prefix;
	enum ib_port_state	state; -> 端口状态
	enum ib_mtu		max_mtu;
	enum ib_mtu		active_mtu;
	u32                     phys_mtu;
	int			gid_tbl_len;
	unsigned int		ip_gids:1;
	/* This is the value from PortInfo CapabilityMask, defined by IBA, IB规范定义的端口信息/能力掩码 */
	u32			port_cap_flags;
	u32			max_msg_sz;
	u32			bad_pkey_cntr;
	u32			qkey_viol_cntr;
	u16			pkey_tbl_len;
	u32			sm_lid;
	u32			lid;
	u8			lmc;
	u8			max_vl_num;
	u8			sm_sl;
	u8			subnet_timeout;
	u8			init_type_reply;
	u8			active_width;
	u16			active_speed;
	u8          phys_state; -> 物理状态
	u16			port_cap_flags2;
};

IB端口属性
struct ibv_port_attr {
    enum ibv_port_state     state;          /* Logical port state */
    enum ibv_mtu            max_mtu;        /* Max MTU supported by port */
    enum ibv_mtu            active_mtu;     /* Actual MTU */
    int                     gid_tbl_len;    /* Length of source GID table */ -> 源GID表长度
    uint32_t                port_cap_flags; /* Port capabilities */
    uint32_t                max_msg_sz;     /* Maximum message size */
    uint32_t                bad_pkey_cntr;  /* Bad P_Key counter */
    uint32_t                qkey_viol_cntr; /* Q_Key violation counter */ -> Q_key违反次数
    uint16_t                pkey_tbl_len;   /* Length of partition table */ -> 分区键表长度
    uint16_t                lid;            /* Base port LID */ -> 基本端口的本地标识
    uint16_t                sm_lid;         /* SM LID */ -> 子网标识
    uint8_t                 lmc;            /* LMC of LID */ -> LID 掩码控制(lid mask control) 由子网管理器分配的每个端口值。 LMC的值指定本地标识符中的路径比特数
    uint8_t                 max_vl_num;     /* Maximum number of VLs */
    uint8_t                 sm_sl;          /* SM service level */
    uint8_t                 subnet_timeout; /* Subnet propagation delay */ -> 子网传播时延
    uint8_t                 init_type_reply;/* Type of initialization performed by SM */
    uint8_t                 active_width;   /* Currently active link width */ -> 当前位宽
    uint8_t                 active_speed;   /* Currently active link speed */ -> 当前带宽速度
    uint8_t                 phys_state;     /* Physical port state */
    uint8_t                 link_layer;     /* link layer protocol of the port */ -> 链路层类型
};




irdma_query_device -> get device attributes
    addrconf_addr_eui48((u8 *)&props->sys_image_guid, iwdev->netdev->dev_addr)
    props->fw_ver = (u64)irdma_fw_major_ver(&rf->sc_dev) << 32 |
    props->max_mr_size = hw_attrs->max_mr_size
    props->hw_ver = rf->pcidev->revision;
	props->page_size_cap = hw_attrs->page_size_cap;
	props->max_mr_size = hw_attrs->max_mr_size;
	props->max_qp = rf->max_qp - rf->used_qps;
	props->max_qp_wr = hw_attrs->max_qp_wr;
	props->max_send_sge = hw_attrs->uk_attrs.max_hw_wq_frags;
	props->max_recv_sge = hw_attrs->uk_attrs.max_hw_wq_frags;
	props->max_cq = rf->max_cq - rf->used_cqs;
	props->max_cqe = rf->max_cqe - 1;
	props->max_mr = rf->max_mr - rf->used_mrs;
	props->max_mw = props->max_mr;
	props->max_pd = rf->max_pd - rf->used_pds;
	props->max_sge_rd = hw_attrs->uk_attrs.max_hw_read_sges;
	props->max_qp_rd_atom = hw_attrs->max_hw_ird;
	props->max_qp_init_rd_atom = hw_attrs->max_hw_ord;
    ...



irdma_get_hw_stats


sysfs_attr_init
sysfs_create_files
device_create_file
sysfs_create_group
kobject_create_and_add -> create a directory
struct kobject *kobj_ref;

/*Creating a directory in /sys/kernel/ */
kobj_ref = kobject_create_and_add("etx_sysfs",kernel_kobj); //sys/kernel/etx_sysfs

/*Freeing Kobj*/
kobject_put(kobj_ref);

struct kobj_attribute etx_attr = __ATTR(etx_value, 0660, sysfs_show, sysfs_store);



(gdb) bt
#0  0xffffffff81181821 in sysfs_create_file (attr=<optimized out>, kobj=<optimized out>) at ./include/linux/fortify-string.h:191
#1  module_add_modinfo_attrs (mod=0xffffffffc080c300) at kernel/module.c:1769
#2  mod_sysfs_setup (mod=mod@entry=0xffffffffc080c300, info=info@entry=0xffffc90002f5fdf0, kparam=<optimized out>, num_params=<optimized out>) at kernel/module.c:1866
#3  0xffffffff81184e50 in load_module (info=info@entry=0xffffc90002f5fdf0, uargs=uargs@entry=0x55d886f62358 "", flags=flags@entry=0) at kernel/module.c:4080
#4  0xffffffff8118571f in __do_sys_finit_module (fd=0, uargs=0x55d886f62358 "", flags=0) at kernel/module.c:4187
#5  0xffffffff8118579a in __se_sys_finit_module (flags=<optimized out>, uargs=<optimized out>, fd=<optimized out>) at kernel/module.c:4164
#6  __x64_sys_finit_module (regs=<optimized out>) at kernel/module.c:4164
#7  0xffffffff81cff979 in do_syscall_x64 (nr=<optimized out>, regs=0xffffc90002f5ff58) at arch/x86/entry/common.c:50
#8  do_syscall_64 (regs=0xffffc90002f5ff58, nr=<optimized out>) at arch/x86/entry/common.c:80
#9  0xffffffff81e0007c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:113



(gdb) bt
#0  sysfs_create_file (attr=0xffffffff83111e00 <dev_attr_uevent>, kobj=0xffff88811b6ee008) at ./include/linux/sysfs.h:607
#1  device_create_file (dev=dev@entry=0xffff88811b6ee008, attr=attr@entry=0xffffffff83111e00 <dev_attr_uevent>) at drivers/base/core.c:2764
#2  0xffffffff8181690e in device_add (dev=dev@entry=0xffff88811b6ee008) at drivers/base/core.c:3340
#3  0xffffffff8181700b in device_register (dev=dev@entry=0xffff88811b6ee008) at drivers/base/core.c:3477
#4  0xffffffff810da071 in workqueue_sysfs_register (wq=wq@entry=0xffff888115161a00) at kernel/workqueue.c:5743
#5  0xffffffff810da39c in alloc_workqueue (fmt=<optimized out>, flags=<optimized out>, max_active=<optimized out>) at kernel/workqueue.c:4351
#6  0xffffffffc058205c in ?? ()
#7  0xffffffffc0582000 in ?? ()
#8  0xffffc90002f5fc68 in ?? ()
#9  0xffffffff81003926 in do_one_initcall (fn=0xfffffff4) at init/main.c:1304

ib-comp-unb-wq


#0  device_create_file (dev=dev@entry=0xffff88810bba6510, attr=attr@entry=0xffffffff83111e00 <dev_attr_uevent>) at drivers/base/core.c:2754
#1  0xffffffff8181690e in device_add (dev=dev@entry=0xffff88810bba6510) at drivers/base/core.c:3340
#2  0xffffffffc07d5235 in ib_register_device (name=<optimized out>, dma_device=<optimized out>, device=0xffff88810bba6000) at drivers/infiniband/core/device.c:1408
#3  ib_register_device (device=0xffff88810bba6000, name=<optimized out>, dma_device=<optimized out>) at drivers/infiniband/core/device.c:1365
#4  0xffffffffc066cd70 in rxe_register_device () at drivers/infiniband/core/device.c:2859
#5  0xffffffffc0664628 in rxe_add () at drivers/infiniband/core/device.c:2859
#6  0xffffffffc0672772 in rxe_net_add () at drivers/infiniband/core/device.c:2859
#7  0xffffffffc0664063 in rxe_newlink () at drivers/infiniband/core/device.c:2859
#8  0xffffffffc07eafdc in nldev_newlink (skb=<optimized out>, nlh=<optimized out>, extack=0xffffc90000e07bd8) at drivers/infiniband/core/nldev.c:1701
#9  0xffffffffc07d8dc0 in rdma_nl_rcv_msg (extack=0x0 <fixed_percpu_data>, nlh=0xffff88811351be00, skb=0xffff88808b4a1000) at drivers/infiniband/core/netlink.c:195
#10 rdma_nl_rcv_skb (cb=<optimized out>, skb=0xffff88808b4a1000) at drivers/infiniband/core/netlink.c:239
#11 rdma_nl_rcv (skb=0xffff88808b4a1000) at drivers/infiniband/core/netlink.c:259
#12 0xffffffff81aeb4a5 in netlink_unicast_kernel (ssk=0xffff88812f965000, skb=0xffff88808b4a1000, sk=0xffff888119134000) at net/netlink/af_netlink.c:1319
#13 netlink_unicast (ssk=ssk@entry=0xffff88812f965000, skb=skb@entry=0xffff88808b4a1000, portid=portid@entry=0, nonblock=<optimized out>) at net/netlink/af_netlink.c:1345
#14 0xffffffff81aeb796 in netlink_sendmsg (sock=<optimized out>, msg=0xffffc90000e07d48, len=<optimized out>) at net/netlink/af_netlink.c:1935
#15 0xffffffff81a3db52 in sock_sendmsg_nosec (msg=0xffffffff83111e00 <dev_attr_uevent>, sock=0xffff88810bba6510) at ./include/linux/uio.h:255
#16 sock_sendmsg (sock=0xffff88810bba6510, sock@entry=0xffff888036e36e80, msg=0xffffffff83111e00 <dev_attr_uevent>, msg@entry=0xffffc90000e07d48) at net/socket.c:724
#17 0xffffffff81a3f5c3 in __sys_sendto (fd=<optimized out>, buff=<optimized out>, len=<optimized out>, flags=0, addr=0x7fb4023199e0, addr_len=12) at net/socket.c:2036
#18 0xffffffff81a3f669 in __do_sys_sendto (addr_len=<optimized out>, addr=<optimized out>, flags=<optimized out>, len=<optimized out>, buff=<optimized out>, fd=<optimized out>) at net/socket.c:2048
#19 __se_sys_sendto (addr_len=<optimized out>, addr=<optimized out>, flags=<optimized out>, len=<optimized out>, buff=<optimized out>, fd=<optimized out>) at net/socket.c:2044
#20 __x64_sys_sendto (regs=<optimized out>) at net/socket.c:2044
#21 0xffffffff81cff979 in do_syscall_x64 (nr=<optimized out>, regs=0xffffc90000e07f58) at arch/x86/entry/common.c:50
#22 do_syscall_64 (regs=0xffffc90000e07f58, nr=<optimized out>) at arch/x86/entry/common.c:80
#23 0xffffffff81e0007c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:113




attribute,  Use DEVICE_ATTR_RO() instead of a "raw" __ATTR macro,
static DEVICE_ATTR_RO
static DEVICE_ATTR_RO(node_guid);
static DEVICE_ATTR_RO(node_type);

create, dev_attr_

static struct attribute *ib_dev_attrs[] = {
	&dev_attr_node_type.attr,
	&dev_attr_node_guid.attr,
	&dev_attr_sys_image_guid.attr,
	&dev_attr_fw_ver.attr,
	&dev_attr_node_desc.attr,
	NULL,
};
const struct attribute_group ib_dev_attr_group = {
	.attrs = ib_dev_attrs,
};


classes_init(void)
    kset_create_and_add("class"



vxworks,

iosDrvInstall


gid
drivers/infiniband/core/cache.c
store_gid_entry
add_roce_gid
make_default_gid






static int mlx5_ib_add_gid
    mlx5r_add_gid_macsec_operations(const struct ib_gid_attr *attr)
        mlx5_is_macsec_roce_supported
        get_macsec_device
        rdma_find_gid
        set_roce_addr
            rdma_read_gid_l2_fields
            ipv6_addr_v4mapped
            mlx5_core_roce_gid_set
        mlx5_macsec_add_roce_rule
        mlx5_macsec_save_roce_gid


通过扫描设备的回调枚举设备所有的GIDs(根据IP计算GID,并缓存GID)
static void enum_all_gids_of_dev_cb(struct ib_device *ib_dev, u32 port, struct net_device *rdma_ndev,void *cookie)
    for_each_net(net) -> 两层for循环 -> list_for_each_entry(VAR, &net_namespace_list, list)
        for_each_netdev(net, ndev)
    is_ndev_for_default_gid_filter -> 检查给定的网络设备是否可以被视为默认 GID -> 当不处于绑定模式时，过滤并添加主网络设备的默认 GID，或者当处于绑定模式时，添加绑定主设备的默认 GID -> 当rdma netdevice用于bonding时，bonding master netdevice应考虑默认GID。 因此，在考虑绑定时请忽略从属 rdma 网络设备。 另外，当event(cookie)网络设备是bond master设备时，请确保它是rdma网络设备的上层网络设备
        add_default_gids(ib_dev, port, rdma_ndev, ndev) -> ib_cache_gid_set_default_gid
            mask = GID_ATTR_FIND_MASK_GID_TYPE | -> IB/core：修复了更改 mac 地址时删除默认 GID 的问题，在 [1] 之前，当网络设备的 MAC 地址更改时，默认 GID 应该被删除并添加回来，这会按以下顺序影响节点和/或端口 GUID。 netdevice_event() -> NETDEV_CHANGEADDR default_del_cmd() del_netdev_default_ips() bond_delete_netdev_default_gids() ib_cache_gid_set_default_gid() ib_cache_gid_del() add_cmd() [..] 但是，在非绑定场景中不会调用 ib_cache_gid_del()，因为 event_ndev 和 rdma_ndev 相同。 因此，修复这种情况，当事件 ndev 和 rdma_dev 相同时忽略检查上层设备； 类似于 bond_set_netdev_default_gids()。 此修复 ib_cache_gid_del() 被正确调用； 但是 ib_cache_gid_del() 找不到要删除的默认 GID，因为 find_gid() 被赋予 default_gid = false 并设置了 GID_ATTR_FIND_MASK_DEFAULT。 但后来它被 ib_cache_gid_set_default_gid() 覆盖，作为 add_cmd() 的一部分。 因此，mac 地址更改通常适用于默认 GID。 通过重构系列 [1]，可以检测到这种不正确的行为。 因此，删除默认GID时，请设置default_gid并设置MASK标志。 当删除基于IP的GID时，清除default_gid并设置MASK标志。 [1] https://patchwork.kernel.org/patch/10319151/
            for (gid_type = 0; gid_type < IB_GID_TYPE_SIZE; ++gid_type) -> 遍历3种GID类型
                if (1UL << gid_type & ~gid_type_mask)
            make_default_gid(ndev, &gid)
                gid->global.subnet_prefix = cpu_to_be64(0xfe80000000000000LL)
                addrconf_ifid_eui48(&gid->raw[8], dev) -> 函数ipv6_generate_eui64最终调用addrconf_ifid_eui48生成地址。将设备的MAC地址dev_addr的前三个字节拷贝到IPv6地址的后半段（s6_addr+8）开始处；在接下来的第4和第5个字节处添加0xFF和0xFE值；拷贝MAC地址的后三个字节到接下来的IPv6地址的第6个字节开始处
            __ib_cache_gid_add(ib_dev, port, &gid, &gid_attr, mask, true)
                rdma_is_zero_gid(gid) -> IB/core：减少使用 zgid 的地方，而不是开放编码 memcmp() 来检查给定的 GID 是否为零，而是使用辅助函数来执行此操作，并用 memset 替换 memcpy(z,&zgid) 的实例
                find_gid(table, gid, attr, default_gid, mask, &empty) -> find gid from cache
                add_modify_gid(table, attr)
                    entry = alloc_gid_entry(attr)
                    add_roce_gid(entry)
                        rdma_cap_roce_gid_table
                        attr->device->ops.add_gid(attr, &entry->context) -> mlx5_ib_add_gid
                    store_gid_entry(table, entry)
                        table->data_vec[entry->attr.index] = entry; -> store gid
                dispatch_gid_change_event(ib_dev, port)
                    event.event		= IB_EVENT_GID_CHANGE
                    ib_dispatch_event_clients(&event) -> IB/核心：让 IB 核心分发缓存更新事件 目前，当低级驱动程序通知 Pkey、GID 和端口更改事件时，它们会按照注册的顺序通知给已注册的处理程序。 IB 核心和其他 ULP（例如 IPoIB）对 GID、LID、Pkey 更改事件感兴趣。 由于 ULP 完成的所有 GID 查询均由 IB 核心提供服务，并且 IB 核心将缓存更新推迟到工作队列，因此其他客户端在处理自己的事件时可能会看到过时的缓存数据。 例如，下面的调用树显示了 ipoib 如何在更新 WQ 中的缓存的同时调用 rdma_query_gid()。 mlx5_ib_handle_event() ib_dispatch_event() ib_cache_event()queue_work() -> 缓存更新速度较慢 [..] ipoib_event()queue_work() [..] 工作处理程序 ipoib_ib_dev_flush_light() __ipoib_ib_dev_flush() ipoib_dev_addr_changed_valid() rdma_query_gid() <- 返回旧 GID ，缓存未更新。 将所有事件分派移至工作队列，以便始终在通知任何客户端之前完成缓存更新
                        list_for_each_entry(handler, &event->device->event_handler_list, list)
                            handler->handler(handler, event)
    if (is_eth_port_of_netdev_filter(ib_dev, port,
        real_dev = rdma_vlan_dev_real_dev(cookie) -> IB/core：删除从 void 到 net_device 的指针转换，此补丁避免了从 void 到 net_device 的不必要的类型转换 -> IB/核心：为 IBoE 添加 VLAN 支持，为 IBoE 添加 802.1q VLAN 支持。 VLAN 标记按以下方式编码在从链路本地地址派生的 GID 中： 当 GID 包含 VLAN 时，GID[11] GID[12] 包含 VLAN ID。 数据包的 3 位用户优先级字段与 SL 的 3 位相同。 对于 rdma_cm 应用程序，TOS 字段用于通过右移 5 位来生成 SL 字段，从而有效地占用 TOS 字段的 3 MS 位 -> commit: https://github.com/ssbandjl/linux/commit/af7bd463761c6abd8ca8d831f9cc0ac19f3b7d4b
            is_vlan_dev(dev) ? vlan_dev_real_dev(dev) : NULL
                net_device *ret = vlan_dev_priv(dev)->real_dev -> while (is_vlan_dev(ret)) -> 递归拿到vlan设备对应的真实设备
        rdma_is_upper_dev_rcu -> IB/core：将 rdma_is_upper_dev_rcu 移至头文件，为了验证路由，我们需要一种简单的方法来检查网络设备是否属于我们的 RDMA 设备。 将此辅助函数移至头文件以使检查更容易
            netdev_has_upper_dev_all_rcu -> Check if device is linked to an upper device
                netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,
        is_eth_active_slave_of_bonding_rcu -> IB/核心：添加RoCE表绑定(bond)支持，处理绑定和其他设备需要我们所有网络设备的GID，这些网络设备是RoCE端口相关网络设备的上层设备。 活动备份配置带来了更多挑战，因为默认 GID 只能在活动设备上设置（这是必要的，否则相同的 MAC 可以用于多个从设备，因此多个从设备将具有相同的 GID）。 管理这些配置是通过监听来完成的： (a) NETDEV_CHANGEUPPER 事件 (1) 如果链接了相关的网络设备，则删除所有不活动的从设备默认 GID 并添加上层设备 GID。 (2) 如果相关网络设备未链接，则删除所有上层GID，并添加默认GID。 (b) NETDEV_BONDING_FAILOVER: (1) 从非活动从站删除绑定 GID (2) 删除非活动从站的默认 GID (3) 将绑定 GID 添加到活动从站 -> struct bonding
            netif_is_bond_master(upper)
            bond_option_active_slave_get_rcu
                rcu_dereference_rtnl(bond->curr_active_slave)
                bond_uses_primary(bond)
    if (is_eth_port_of_netdev_filter(ib_dev, port,
        _add_netdev_ips(ib_dev, port, ndev)
            enum_netdev_ipv4_ips(ib_dev, port, ndev) -> 枚举IPv4设备的IP地址
                __in_dev_get_rcu
                list_add_tail(&entry->list, &sin_list)
                update_gid_ip(GID_ADD, ib_dev, port, ndev, (struct sockaddr *)&sin_iter->ip);
                    rdma_ip2gid(addr, &gid) -> IP地址到GID的转换算法 -> IB/core：verbs/cm 结构中的以太网 L2 属性，此补丁添加了对 verbs/cm/cma 结构中的以太网 L2 属性的支持。 在处理 L2 以太网时，我们应该以与使用 IB L2（和 L4 PKEY）属性类似的方式使用 smac、dmac、vlan ID 和优先级。 因此，这些属性被添加到以下结构中： * ib_ah_attr - 添加了 dmac * ib_qp_attr - 添加了 smac 和 vlan_id，（sl 保留 vlan 优先级） * ib_wc - 添加了 smac、vlan_id * ib_sa_path_rec - 添加了 smac、dmac、vlan_id * cm_av - 添加了 smac 和 vlan_id 对于路径记录结构，在将其打包为有线格式时特别注意避免新字段，因此我们不会破坏 IB CM 和 SA 有线协议。 在主动侧，CM 被填充。 其内部结构来自 ULP 提供的路径。 我们添加了 ETH L2 属性并将它们放入 CM 地址句柄（struct cm_av）中。 在被动侧，CM 从与 REQ 消息关联的 WC 中填充其内部结构。 我们添加了从 WC 获取 ETH L2 属性的内容。 当硬件驱动程序在 WC 中提供所需的 ETH L2 属性时，它们会设置 IB_WC_WITH_SMAC 和 IB_WC_WITH_VLAN 标志。 IB 核心代码检查这些标志是否存在，如果没有，则从 ib_init_ah_from_wc() 辅助函数进行地址解析。 ib_modify_qp_is_ok 也被更新以考虑链路层。 有些参数对于以太网链路层是必需的，而对于IB来说则无关。 修改供应商驱动程序以支持新的函数签名
                        case AF_INET: -> ipv4
                            ipv6_addr_set_v4mapped(((struct sockaddr_in *)addr)->sin_addr.s_addr, (struct in6_addr *)gid)
                                ipv6_addr_set(v4mapped, 0, 0, htonl(0x0000FFFF), addr)
                                    __ipv6_addr_set_half(&addr->s6_addr32[0], w1, w2)
                                    __ipv6_addr_set_half(&addr->s6_addr32[2], w3, w4)
                        case AF_INET6:
                            *(struct in6_addr *)&gid->raw =	((struct sockaddr_in6 *)addr)->sin6_addr
                    update_gid(gid_op, ib_dev, port, &gid, &gid_attr)
                        unsigned long gid_type_mask = roce_gid_type_mask_support(ib_dev, port) -> IB/core：将gid_type添加到gid属性中，为了支持多种GID类型，我们需要存储每个GID的gid_type。 这也与 RoCE v2 附件“RoCEv2 端口 GID 表条目应具有表示 L3 地址类型的“GID 类型”属性”保持一致。 当前支持的 GID 是 IB_GID_TYPE_IB，这也是 RoCE v1 GID 类型。 这意味着 gid_type 应添加到 roce_gid_table 元数据中
                        ib_cache_gid_add(ib_dev, port, gid, gid_attr) -> .is_supported = &mlx5_rdma_supported,
                            __ib_cache_gid_add
                        or ib_cache_gid_del(ib_dev, port, gid, gid_attr) -> _ib_cache_gid_del
                            find_gid
                            del_gid(ib_dev, port, table, ix) -> del_gid
                                mlx5r_del_gid_macsec_operations -> del flow
                            dispatch_gid_change_event(ib_dev, port)
            enum_netdev_ipv6_ips(ib_dev, port, ndev)
                in6_dev = in6_dev_get(ndev)
                list_add_tail(&entry->list, &sin6_list)
                rdma_ip2gid((struct sockaddr *)&sin6_iter->sin6, &gid)
                update_gid(GID_ADD, ib_dev, port, &gid, &gid_attr)



ucma_copy_iboe_route struct rdma_route -> RDMA/cma：多路径记录支持 netlink 通道，支持通过 RDMA netlink 通道从用户空间服务接收入站和出站 IB 路径记录（以及 GMP PathRecord）。 这3个PR中的LID可以这样使用： 1. GMP PR：用作标准本地/远程LID； 2、出站PR的DLID：用作出站流量的“dlid”字段； 3.入站PR的DLID：用作响应方出站流量的“dlid”字段。 这样做的目的是支持自适应路由。 使用当前的 IB 路由解决方案，当数据包发出时，每个目标都会被分配一个固定的 DLID，这意味着将使用固定的路由器。 入站/出站路径记录中的 LID 可用于识别允许与另一个子网实体进行通信的路由器组。 通过它们，来自子网间连接的数据包可以通过该组中的任何路由器到达目标。 正如 Jason 所确认的，当发送 netlink 请求时，内核使用 LS_RESOLVE_PATH_USE_ALL 以便服务知道内核支持多个 PR



IB：地址转换以将 IP 映射到 IB 地址 (GID)，添加地址转换服务，使用 IPoIB 将 IP 地址映射到 InfiniBand GID 地址
/**
 * struct rdma_dev_addr - Contains resolved RDMA hardware addresses
 * @src_dev_addr:	Source MAC address.
 * @dst_dev_addr:	Destination MAC address.
 * @broadcast:		Broadcast address of the device.
 * @dev_type:		The interface hardware type of the device.
 * @bound_dev_if:	An optional device interface index.
 * @transport:		The transport type used.
 * @net:		Network namespace containing the bound_dev_if net_dev.
 * @sgid_attr:		GID attribute to use for identified SGID
 */
struct rdma_dev_addr {
	unsigned char src_dev_addr[MAX_ADDR_LEN]; -> 源MAC(SMAC)
	unsigned char dst_dev_addr[MAX_ADDR_LEN]; -> 目的MAC(DMAC)
	unsigned char broadcast[MAX_ADDR_LEN];
	unsigned short dev_type;
	int bound_dev_if;
	enum rdma_transport_type transport;
	struct net *net;
	const struct ib_gid_attr *sgid_attr;
	enum rdma_network_type network;
	int hoplimit;
};



iboe_addr_get_sgid


addr_resolve
    rdma_set_src_addr_rcu
    ...
        rdma_translate_ip  -> Translate a local IP address to an RDMA hardware
            dev = dev_get_by_index(dev_addr->net, dev_addr->bound_dev_if) -> IB/addr：将网络命名空间作为参数传递，为ib_addr模块添加网络命名空间支持。 为此，所有地址解析和匹配都应该使用适当的命名空间而不是 init_net 来完成。 这是通过以下方式实现的： 1. 将显式网络命名空间参数添加到需要命名空间的导出函数。 2. 将命名空间保存在 rdma_addr_client 结构中。 3. 调用网络功能时使用。 为了保留调用模块的行为，&init_net 在其他模块的调用中作为参数传递。 随着在更多级别上添加命名空间支持，此内容已被修改 -> Deprecated for new users, call netdev_get_by_index() instead -> netdev_get_by_index() - 通过 ifindex 查找设备， @net：适用的网络命名空间 @ifindex：设备索引 @tracker：获取引用的跟踪对象 @gfp：跟踪器的分配标志 按索引搜索接口。 如果未找到设备或指向设备的指针，则返回 NULL。 返回的设备已添加了引用，并且指针是安全的，直到用户调用 netdev_put() 表明他们已完成使用它
                dev_get_by_index_rcu(net, ifindex)
                    hlist_for_each_entry_rcu(dev, head, index_hlist)
            rdma_copy_src_l2_addr(dev_addr, dev)
                memcpy(dev_addr->broadcast, dev->broadcast, MAX_ADDR_LEN)
            dev = rdma_find_ndev_for_src_ip_rcu(dev_addr->net, addr)
                switch (src_in->sa_family)
                case AF_INET:
                    __ip_dev_find
                        ifa = inet_lookup_ifaddr_rcu(net, addr)
                            u32 hash = inet_addr_hash(net, addr)
                            net_eq(dev_net(ifa->ifa_dev->dev), net)
                        local = fib_get_table(net, RT_TABLE_LOCAL)
                        fib_table_lookup
                            trace_fib_table_lookup
                case AF_INET6:
                    ipv6_chk_addr -> ipv6_chk_addr_and_flags -> VRF 设备与 ip 规则相结合，提供了在 Linux 网络堆栈中创建虚拟路由和转发域（又名 VRF，具体为 VRF-lite）的能力。 一种用例是多租户问题，其中每个租户都有自己独特的路由表，并且至少需要不同的默认网关。 通过将套接字绑定到 VRF 设备，进程可以“感知 VRF”。 然后，通过套接字的数据包使用与 VRF 设备关联的路由表。 VRF 设备实现的一个重要特征是它仅影响第 3 层及以上层，因此 L2 工具（例如 LLDP）不受影响（即它们不需要在每个 VRF 中运行）。 该设计还允许使用更高优先级的 IP 规则（基于策略的路由，PBR），以优先于根据需要引导特定流量的 VRF 设备规则。 此外，VRF 设备允许 VRF 嵌套在命名空间内。 例如，网络命名空间提供设备层网络接口的分离，命名空间内接口上的 VLAN 提供 L2 分离，然后 VRF 设备提供 L3 分离。 设计 VRF 设备是通过关联的路由表创建的。 然后网络接口被从属于 VRF 设备：-> net/ipv6：更改地址检查以始终采用设备参数，ipv6_chk_addr_and_flags 确定地址是否是本地地址，以及（可选）是否是特定设备上的地址。 例如，由 ip6_route_info_create 调用它来确定给定的网关地址是否是本地地址。 地址检查当前不考虑 L3 域，因此如果下一跳指向第二个 VRF 中的地址，则不允许在一个 VRF 中添加路由。 例如，$ ip route add 2001:db8:1::/64 vrf r2 via 2001:db8:102::23 错误：网关地址无效。 其中 2001:db8:102::23 是 vrf r1 中接口上的地址。 ipv6_chk_addr_and_flags 需要允许调用者始终传入带有单独参数的设备，以免将地址限制为特定设备。 该设备用于确定感兴趣的 L3 域。 为此，添加一个参数以跳过设备检查并更新调用者以始终在可能的情况下传递设备，并使用新参数来表示域中的任何地址。 使用 NULL dev 参数更新 ipv6_chk_addr 的少数用户。 此补丁处理对这些调用者的更改，而无需添加域检查。 ip6_validate_gw 需要处理 2 种情况 - 一种是设备作为下一跳规范的一部分给出，另一种是设备被解析。 至少有 1 种 VRF 情况，将检查推迟到仅在路由查找解决之后，设备会失败并出现不直观的错误“RTNETLINK 答案：没有到主机的路由”，而不是首选的“错误：网关不能是本地地址” ”。 “没有到主机的路由”错误是由于回退到完整查找而导致的。 检查两次以避免此错误
                        inet6_addr_hash
                        l3mdev_master_dev_rcu
                        ipv6_addr_equal


bond type, 
#define BOND_MODE_ROUNDROBIN	0
#define BOND_MODE_ACTIVEBACKUP	1
#define BOND_MODE_XOR		2
#define BOND_MODE_BROADCAST	3
#define BOND_MODE_8023AD        4
#define BOND_MODE_TLB           5
#define BOND_MODE_ALB		6 /* TLB + RLB (receive load balancing) */


IB QP属性
struct ib_qp_attr {
	enum ib_qp_state	qp_state;
	enum ib_qp_state	cur_qp_state;
	enum ib_mtu		path_mtu;
	enum ib_mig_state	path_mig_state;
	u32			qkey;
	u32			rq_psn;
	u32			sq_psn;
	u32			dest_qp_num;
	int			qp_access_flags;
	struct ib_qp_cap	cap;
	struct rdma_ah_attr	ah_attr;
	struct rdma_ah_attr	alt_ah_attr;
	u16			pkey_index;
	u16			alt_pkey_index;
	u8			en_sqd_async_notify;
	u8			sq_draining;
	u8			max_rd_atomic;
	u8			max_dest_rd_atomic;
	u8			min_rnr_timer;
	u32			port_num;
	u8			timeout;
	u8			retry_cnt;
	u8			rnr_retry;
	u32			alt_port_num;
	u8			alt_timeout;
	u32			rate_limit;
	struct net_device	*xmit_slave;
};



位宽和速度:
enum ib_port_width {
	IB_WIDTH_1X	= 1,
	IB_WIDTH_2X	= 16,
	IB_WIDTH_4X	= 2,
	IB_WIDTH_8X	= 4,
	IB_WIDTH_12X	= 8
};

static inline int ib_width_enum_to_int(enum ib_port_width width)
{
	switch (width) {
	case IB_WIDTH_1X:  return  1;
	case IB_WIDTH_2X:  return  2;
	case IB_WIDTH_4X:  return  4;
	case IB_WIDTH_8X:  return  8;
	case IB_WIDTH_12X: return 12;
	default: 	  return -1;
	}
}

enum ib_port_speed {
	IB_SPEED_SDR	= 1,
	IB_SPEED_DDR	= 2,
	IB_SPEED_QDR	= 4,
	IB_SPEED_FDR10	= 8,
	IB_SPEED_FDR	= 16,
	IB_SPEED_EDR	= 32,
	IB_SPEED_HDR	= 64,
	IB_SPEED_NDR	= 128,
	IB_SPEED_XDR	= 256,
};



Intel E810 RDMA硬件属性 -> RDMA/irdma：实现硬件管理队列操作集合，驱动程序将特权命令发布到硬件管理队列（控制 QP 或 CQP）以请求硬件的管理操作。 实现 CQP 的创建/销毁以及支持函数、数据结构和标头以处理不同的 CQP 命令
struct irdma_hw_attrs {
	struct irdma_uk_attrs uk_attrs;
	u64 max_hw_outbound_msg_size;
	u64 max_hw_inbound_msg_size;
	u64 max_mr_size;
	u64 page_size_cap;
	u32 min_hw_qp_id;
	u32 min_hw_aeq_size;
	u32 max_hw_aeq_size;
	u32 min_hw_ceq_size;
	u32 max_hw_ceq_size;
	u32 max_hw_device_pages;
	u32 max_hw_vf_fpm_id;
	u32 first_hw_vf_fpm_id;
	u32 max_hw_ird;
	u32 max_hw_ord;
	u32 max_hw_wqes;
	u32 max_hw_pds;
	u32 max_hw_ena_vf_count;
	u32 max_qp_wr;
	u32 max_pe_ready_count;
	u32 max_done_count;
	u32 max_sleep_count;
	u32 max_cqp_compl_wait_time_ms;
	u16 max_stat_inst;
	u16 max_stat_idx;
};


IB GID类型
enum ib_gid_type {
	IB_GID_TYPE_IB = IB_UVERBS_GID_TYPE_IB,
	IB_GID_TYPE_ROCE = IB_UVERBS_GID_TYPE_ROCE_V1,
	IB_GID_TYPE_ROCE_UDP_ENCAP = IB_UVERBS_GID_TYPE_ROCE_V2,
	IB_GID_TYPE_SIZE
};

roce_v2 udp port:
#define ROCE_V2_UDP_DPORT      4791


struct net_device {
    __cacheline_group_begin(net_device_read_tx)
    缓存线组织可以在 Documentation/networking/net_cachelines/net_device.rst 中找到文档。 添加新字段时请更新文档
    const struct net_device_ops *netdev_ops;
    const struct ethtool_ops *ethtool_ops;
    unsigned char		broadcast[MAX_ADDR_LEN]
    struct devlink_port	*devlink_port;
}

struct net {
    struct netns_ipvs	*ipvs
    struct netns_mpls	mpls
    struct netns_xdp	xdp
}



GID table:
table->data_vec[entry->attr.index] = entry
|   0   |   1   |...|  31   |
| entry | entry |...| entry |




VLAN设备结构体
/**
 *	struct vlan_dev_priv - VLAN private device data
 *	@nr_ingress_mappings: number of ingress priority mappings
 *	@ingress_priority_map: ingress priority mappings
 *	@nr_egress_mappings: number of egress priority mappings
 *	@egress_priority_map: hash of egress priority mappings
 *	@vlan_proto: VLAN encapsulation protocol
 *	@vlan_id: VLAN identifier
 *	@flags: device flags
 *	@real_dev: underlying netdevice
 *	@dev_tracker: refcount tracker for @real_dev reference
 *	@real_dev_addr: address of underlying netdevice
 *	@dent: proc dir entry
 *	@vlan_pcpu_stats: ptr to percpu rx stats
 */
struct vlan_dev_priv {
	unsigned int				nr_ingress_mappings;
	u32					ingress_priority_map[8];
	unsigned int				nr_egress_mappings;
	struct vlan_priority_tci_mapping	*egress_priority_map[16];

	__be16					vlan_proto;
	u16					vlan_id;
	u16					flags;

	struct net_device			*real_dev; -> vlan对应的真实设备
	netdevice_tracker			dev_tracker;

	unsigned char				real_dev_addr[ETH_ALEN];

	struct proc_dir_entry			*dent;
	struct vlan_pcpu_stats __percpu		*vlan_pcpu_stats;
#ifdef CONFIG_NET_POLL_CONTROLLER
	struct netpoll				*netpoll;
#endif
};


读写信号量, 对于无竞争的 rwsem，计数和所有者是任务在获取 rwsem 时需要接触的唯一字段。 因此，它们被放置在彼此旁边，以增加它们共享相同缓存行的机会。 在竞争的 rwsem 中，所有者可能是结构中最常访问的字段，因为持有 osq 锁的乐观等待者将在所有者上旋转。 对于嵌入式 rwsem，包含结构中的其他热字段应远离 rwsem，以减少它们共享相同缓存行而导致缓存行弹跳问题的机会
参考: http://linux.laoqinren.net/kernel/rw-semaphore/, https://blog.csdn.net/jkzzxQQQ/article/details/109522694
struct rw_semaphore {
};


网络事件通知类型
enum netevent_notif_type {
	NETEVENT_NEIGH_UPDATE = 1, /* arg is struct neighbour ptr */
	NETEVENT_REDIRECT,	   /* arg is struct netevent_redirect ptr */
	NETEVENT_DELAY_PROBE_TIME_UPDATE, /* arg is struct neigh_parms ptr */
	NETEVENT_IPV4_MPATH_HASH_UPDATE, /* arg is struct net ptr */
	NETEVENT_IPV6_MPATH_HASH_UPDATE, /* arg is struct net ptr */
	NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE, /* arg is struct net ptr */
};

ip_do_redirect -> __ip_do_redirect
    call_netevent_notifiers(NETEVENT_NEIGH_UPDATE, n)



irdma_net_event
    switch (event) {
	case NETEVENT_NEIGH_UPDATE
        real_dev = rdma_vlan_dev_real_dev(netdev)
        iwdev = to_iwdev(ibdev)
        irdma_add_arp(iwdev->rf, local_ipaddr, ipv4, neigh->ha)
            arpidx = irdma_arp_table(rf, &ip[0], ipv4, NULL, IRDMA_ARP_RESOLVE)
                for (arp_index = 0; (u32)arp_index < rf->arp_table_size; arp_index++)
                case IRDMA_ARP_ADD:
                    irdma_alloc_rsrc(rf, rf->allocated_arps, rf->arp_table_size,
                        rsrc_num = find_next_zero_bit(rsrc_array, max_rsrc, *next)
                        __set_bit(rsrc_num, rsrc_array)
                    memcpy(rf->arp_table[arp_index].ip_addr, ip, sizeof(rf->arp_table[arp_index].ip_addr))
                    ether_addr_copy(rf->arp_table[arp_index].mac_addr, mac_addr) -> Copy an Ethernet address
                        a[0] = b[0];
            irdma_manage_arp_cache(rf, mac, ip, ipv4, IRDMA_ARP_ADD) -> manage hw arp cache
                cqp_request = irdma_alloc_and_get_cqp_request(&rf->cqp, false)
                cqp_info->cqp_cmd = IRDMA_OP_ADD_ARP_CACHE_ENTRY
                irdma_handle_cqp_op(rf, cqp_request) -> irdma_sc_add_arp_cache_entry
                    wqe = irdma_sc_cqp_get_next_send_wqe(cqp, scratch)
        or irdma_manage_arp_cache



irdma_inetaddr_event
    case NETDEV_DOWN: -> 网口down
        irdma_manage_arp_cache(iwdev->rf, real_dev->dev_addr, &local_ipaddr, true, IRDMA_ARP_DELETE)
    case NETDEV_UP: -> 网口up
	case NETDEV_CHANGEADDR:
		irdma_add_arp(iwdev->rf, &local_ipaddr, true, real_dev->dev_addr);
		irdma_if_notify(iwdev, real_dev, &local_ipaddr, true, true) -> process an ifdown on an interface
            u16 vlan_id = rdma_vlan_dev_vlan_id(netdev)
                return is_vlan_dev(dev) ? vlan_dev_vlan_id(dev) : 0xffff
            irdma_qhash_ctrl -> irdma_qhash_ctrl - 启用/禁用列表的 qhash @iwdev: 设备指针 @parent_listen_node: 父侦听节点 @nfo: cm 信息节点 @ipaddr: 指向 IPv4 或 IPv6 地址的指针 @ipv4: 为 true 时指示 IPv4 的标志 @ifup: 指示接口打开的标志 true 启用或禁用子监听列表中与 ipaddr 匹配的节点的 qhash。 如果没有找到匹配的 IP，它将分配并添加一个新的子监听节点到父监听节点。 假定在调用时已持有listen_list_lock
                irdma_manage_qhash(iwdev, nfo, IRDMA_QHASH_TYPE_TCP_SYN, op, NULL,
                    cqp_request = irdma_alloc_and_get_cqp_request(iwcqp, wait)
                    cqp_request->callback_fcn = irdma_send_syn_cqp_callback
						irdma_send_syn(cm_node, 1)
							sqbuf = cm_node->cm_core->form_cm_frame(cm_node, &opts, NULL, NULL, flags)
							irdma_schedule_cm_timer(cm_node, sqbuf, IRDMA_TIMER_TYPE_SEND, 1, 0)
								irdma_puda_send_buf(vsi->ilq, sqbuf)
						irdma_rem_ref_cm_node(cm_node)
                    cqp_info->cqp_cmd = IRDMA_OP_MANAGE_QHASH_TABLE_ENTRY -> irdma_sc_manage_qhash_table_entry
                        set_64bit_val(wqe, 0, ether_addr_to_u64(info->mac_addr))
            irdma_manage_qhash
            irdma_cm_teardown_connections
                irdma_teardown_list_prep
                attr.qp_state = IB_QPS_ERR
                irdma_modify_qp(&cm_node->iwqp->ibqp, &attr, IB_QP_STATE, NULL)
                irdma_cm_disconn
                    INIT_WORK(&work->work, irdma_disconnect_worker)
                irdma_rem_ref_cm_node
		irdma_gid_change_event(&iwdev->ibdev);
            ib_event.event = IB_EVENT_GID_CHANGE
            ib_dispatch_event(&ib_event)


rf, 
struct irdma_pci_f {

}




intel cqp ops, 
static const char *const irdma_cqp_cmd_names[IRDMA_MAX_CQP_OPS] = {
	[IRDMA_OP_CEQ_DESTROY] = "Destroy CEQ Cmd",
	[IRDMA_OP_AEQ_DESTROY] = "Destroy AEQ Cmd",
	[IRDMA_OP_DELETE_ARP_CACHE_ENTRY] = "Delete ARP Cache Cmd",
	[IRDMA_OP_MANAGE_APBVT_ENTRY] = "Manage APBV Table Entry Cmd",
	[IRDMA_OP_CEQ_CREATE] = "CEQ Create Cmd",
	[IRDMA_OP_AEQ_CREATE] = "AEQ Destroy Cmd",
	[IRDMA_OP_MANAGE_QHASH_TABLE_ENTRY] = "Manage Quad Hash Table Entry Cmd",
	[IRDMA_OP_QP_MODIFY] = "Modify QP Cmd",
	[IRDMA_OP_QP_UPLOAD_CONTEXT] = "Upload Context Cmd",
	[IRDMA_OP_CQ_CREATE] = "Create CQ Cmd",
	[IRDMA_OP_CQ_DESTROY] = "Destroy CQ Cmd",
	[IRDMA_OP_QP_CREATE] = "Create QP Cmd",
	[IRDMA_OP_QP_DESTROY] = "Destroy QP Cmd",
	[IRDMA_OP_ALLOC_STAG] = "Allocate STag Cmd",
	[IRDMA_OP_MR_REG_NON_SHARED] = "Register Non-Shared MR Cmd",
	[IRDMA_OP_DEALLOC_STAG] = "Deallocate STag Cmd",
	[IRDMA_OP_MW_ALLOC] = "Allocate Memory Window Cmd",
	[IRDMA_OP_QP_FLUSH_WQES] = "Flush QP Cmd",
	[IRDMA_OP_ADD_ARP_CACHE_ENTRY] = "Add ARP Cache Cmd",
	[IRDMA_OP_MANAGE_PUSH_PAGE] = "Manage Push Page Cmd",
	[IRDMA_OP_UPDATE_PE_SDS] = "Update PE SDs Cmd",
	[IRDMA_OP_MANAGE_HMC_PM_FUNC_TABLE] = "Manage HMC PM Function Table Cmd",
	[IRDMA_OP_SUSPEND] = "Suspend QP Cmd",
	[IRDMA_OP_RESUME] = "Resume QP Cmd",
	[IRDMA_OP_MANAGE_VF_PBLE_BP] = "Manage VF PBLE Backing Pages Cmd",
	[IRDMA_OP_QUERY_FPM_VAL] = "Query FPM Values Cmd",
	[IRDMA_OP_COMMIT_FPM_VAL] = "Commit FPM Values Cmd",
	[IRDMA_OP_AH_CREATE] = "Create Address Handle Cmd",
	[IRDMA_OP_AH_MODIFY] = "Modify Address Handle Cmd",
	[IRDMA_OP_AH_DESTROY] = "Destroy Address Handle Cmd",
	[IRDMA_OP_MC_CREATE] = "Create Multicast Group Cmd",
	[IRDMA_OP_MC_DESTROY] = "Destroy Multicast Group Cmd",
	[IRDMA_OP_MC_MODIFY] = "Modify Multicast Group Cmd",
	[IRDMA_OP_STATS_ALLOCATE] = "Add Statistics Instance Cmd",
	[IRDMA_OP_STATS_FREE] = "Free Statistics Instance Cmd",
	[IRDMA_OP_STATS_GATHER] = "Gather Statistics Cmd",
	[IRDMA_OP_WS_ADD_NODE] = "Add Work Scheduler Node Cmd",
	[IRDMA_OP_WS_MODIFY_NODE] = "Modify Work Scheduler Node Cmd",
	[IRDMA_OP_WS_DELETE_NODE] = "Delete Work Scheduler Node Cmd",
	[IRDMA_OP_SET_UP_MAP] = "Set UP-UP Mapping Cmd",
	[IRDMA_OP_GEN_AE] = "Generate AE Cmd",
	[IRDMA_OP_QUERY_RDMA_FEATURES] = "RDMA Get Features Cmd",
	[IRDMA_OP_ALLOC_LOCAL_MAC_ENTRY] = "Allocate Local MAC Entry Cmd",
	[IRDMA_OP_ADD_LOCAL_MAC_ENTRY] = "Add Local MAC Entry Cmd",
	[IRDMA_OP_DELETE_LOCAL_MAC_ENTRY] = "Delete Local MAC Entry Cmd",
	[IRDMA_OP_CQ_MODIFY] = "CQ Modify Cmd",
};


irdma cm,
drivers/infiniband/hw/irdma/cm.h



net/mlx5：添加对端口类型和速度寄存器中引入的 ext_* 字段的支持，此补丁公开了新的链路模式（包括每通道 50Gbps），以及描述端口类型和速度寄存器 (PTYS) 中的新链路模式的 ext_* 字段 。 访问功能、转换功能（速度 <-> HW 位）和链路最大速度功能已修改
enum mlx5e_ext_link_mode {
	MLX5E_SGMII_100M			= 0,
	MLX5E_1000BASE_X_SGMII			= 1,
	MLX5E_5GBASE_R				= 3,
	MLX5E_10GBASE_XFI_XAUI_1		= 4,
	MLX5E_40GBASE_XLAUI_4_XLPPI_4		= 5,
	MLX5E_25GAUI_1_25GBASE_CR_KR		= 6,
	MLX5E_50GAUI_2_LAUI_2_50GBASE_CR2_KR2	= 7,
	MLX5E_50GAUI_1_LAUI_1_50GBASE_CR_KR	= 8,
	MLX5E_CAUI_4_100GBASE_CR4_KR4		= 9,
	MLX5E_100GAUI_2_100GBASE_CR2_KR2	= 10,
	MLX5E_100GAUI_1_100GBASE_CR_KR		= 11,
	MLX5E_200GAUI_4_200GBASE_CR4_KR4	= 12,
	MLX5E_200GAUI_2_200GBASE_CR2_KR2	= 13,
	MLX5E_400GAUI_8_400GBASE_CR8		= 15,
	MLX5E_400GAUI_4_400GBASE_CR4_KR4	= 16,
	MLX5E_800GAUI_8_800GBASE_CR8_KR8	= 19,
	MLX5E_EXT_LINK_MODES_NUMBER,
};

MLX5_BUILD_PTYS2ETHTOOL_CONFIG(MLX5E_1000BASE_CX_SGMII, legacy, ETHTOOL_LINK_MODE_1000baseKX_Full_BIT);
({ 
    struct ptys2ethtool_config *cfg; 
    const unsigned int modes[] = { ETHTOOL_LINK_MODE_1000baseKX_Full_BIT }; 
    unsigned int i, bit, idx; 
    cfg = &ptys2legacy_ethtool_table[MLX5E_1000BASE_CX_SGMII]; 
    bitmap_zero(cfg->supported, __ETHTOOL_LINK_MODE_MASK_NBITS); 
    bitmap_zero(cfg->advertised, __ETHTOOL_LINK_MODE_MASK_NBITS); 
    for (i = 0 ; i < ARRAY_SIZE(modes) ; ++i) { 
        bit = modes[i] % 64; idx = modes[i] / 64; 
        __set_bit(bit, &cfg->supported[idx]); 
        __set_bit(bit, &cfg->advertised[idx]); 
    }
})


commit: https://lore.kernel.org/netdev/1594383913-3295-7-git-send-email-moshe@mellanox.com/T/
* [PATCH net-next v3 0/7] 添加对 devlink 端口的 devlink-health 支持，在每个端口的基础上实现对 devlink health reports 的支持。 该补丁集修复了一个设计问题，因为一些运行状况报告器报告错误并在设备级别运行恢复，而实际功能是在端口级别。 至于当前实现的 devlink health 报告器，它仅与 mlx5 的 Tx 和 Rx 报告器相关，而 mlx5 只有一个端口，因此对功能没有实际影响，但在更多驱动程序使用 devlink health 报告器之前应该修复此问题。 本系列的第一部分为健康报告器的实现准备通用功能部分。 其次介绍了 devlink-health 和 mlx5e 所需的 API，演示了其用法并实现了 mlx5 驱动程序的功能。 每端口报告器功能是通过以类似于现有设备基础设施的方式将 devlink_health_reporters 列表添加到 devlink_port 结构来实现的。 这是唯一的主要区别，它使得完全重用设备报告器操作成为可能。 该效果将与 iproute2 添加一起看到，并将影响所有 devlink health 命令。 用户可以通过查看 devlink 句柄来区分设备和端口报告器。 端口报告器在地址末尾有一个端口索引，并且可以在 devlink-health 接受它的每个地方将此类地址作为参数提供。 这些可以通过 devlink port show 命令获得。 例如： $ devlink health show pci/0000:00:0a.0:reporter fw statehealthy error 0recover 0auto_dump true pci/0000:00:0a.0/1:reportertxstatehealthyerror0recover 0grace_period 500auto_recover true auto_dump true $ devlink health set pci/0000:00:0a.0/1reporter tx Grace_period 1000 \ auto_recover false auto_dump false $devlink health show pci/0000:00:0a.0/1reporter tx pci/0000:00: 0a.0/1:reporter tx statehealthy error 0recover 0grace_period1000auto_recover flase auto_dump false 注意：用户可以使用相同的 devlink health uAPI 命令现在可以获取端口运行状况报告器或设备运行状况报告器。 例如，恢复命令： 在此补丁集之前： devlink health recovery DEV reports REPORTER_NAME 在此补丁集之后： devlink health recovery { DEV | REPORTER_NAME 。 DEV/PORT_INDEX } 记者 REPORTER_NAME


devlink port
PCI 控制器 大多数情况下，一个 PCI 设备只有一个控制器。 控制器可能由多个物理、虚拟功能和子功能组成。 一项功能由一个或多个端口组成。 此端口由 devlink eswitch 端口表示。 然而，连接到多个 CPU 或多个 PCI 根联合体或 SmartNIC 的 PCI 设备可能有多个控制器。 对于具有多个控制器的设备，每个控制器都通过唯一的控制器编号来区分。 eswitch位于PCI设备上，支持多个控制器的端口。 具有两个控制器的系统的示例视图：

```bash
             ---------------------------------------------------------
             |                                                       |
             |           --------- ---------         ------- ------- |
-----------  |           | vf(s) | | sf(s) |         |vf(s)| |sf(s)| |
| server  |  | -------   ----/---- ---/----- ------- ---/--- ---/--- |
| pci rc  |=== | pf0 |______/________/       | pf1 |___/_______/     |
| connect |  | -------                       -------                 |
-----------  |     | controller_num=1 (no eswitch)                   | 外部控制器1
             ------|--------------------------------------------------
             (internal wire 内部线路)
                   |
             ---------------------------------------------------------
             | devlink eswitch ports and reps                        |
             | ----------------------------------------------------- |
             | |ctrl-0 | ctrl-0 | ctrl-0 | ctrl-0 | ctrl-0 |ctrl-0 | |
             | |pf0    | pf0vfN | pf0sfN | pf1    | pf1vfN |pf1sfN | |
             | ----------------------------------------------------- |
             | |ctrl-1 | ctrl-1 | ctrl-1 | ctrl-1 | ctrl-1 |ctrl-1 | |
             | |pf0    | pf0vfN | pf0sfN | pf1    | pf1vfN |pf1sfN | |
             | ----------------------------------------------------- |
             |                                                       |
             |                                                       |
-----------  |           --------- ---------         ------- ------- |
| smartNIC|  |           | vf(s) | | sf(s) |         |vf(s)| |sf(s)| |
| pci rc  |==| -------   ----/---- ---/----- ------- ---/--- ---/--- |
| connect |  | | pf0 |______/________/       | pf1 |___/_______/     |
-----------  | -------                       -------                 |
             |                                                       |
             |  local controller_num=0 (eswitch)                     | 本地控制器0
             ---------------------------------------------------------
```

在上面的示例中，外部控制器（由控制器编号 = 1 标识）没有 eswitch。 本地控制器（由控制器编号 = 0 标识）具有 eswitch。 本地控制器上的 Devlink 实例具有两个控制器的 eswitch devlink 端口




static const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops = {
	.ieee_getets	= mlx5e_dcbnl_ieee_getets,
	.ieee_setets	= mlx5e_dcbnl_ieee_setets,
	.ieee_getmaxrate = mlx5e_dcbnl_ieee_getmaxrate,
	.ieee_setmaxrate = mlx5e_dcbnl_ieee_setmaxrate,
	.ieee_getpfc	= mlx5e_dcbnl_ieee_getpfc,
	.ieee_setpfc	= mlx5e_dcbnl_ieee_setpfc,
	.ieee_setapp    = mlx5e_dcbnl_ieee_setapp,
	.ieee_delapp    = mlx5e_dcbnl_ieee_delapp,
	.getdcbx	= mlx5e_dcbnl_getdcbx,
	.setdcbx	= mlx5e_dcbnl_setdcbx,
	.dcbnl_getbuffer = mlx5e_dcbnl_getbuffer,
	.dcbnl_setbuffer = mlx5e_dcbnl_setbuffer,

/* CEE interfaces */
	.setall         = mlx5e_dcbnl_setall,
	.getstate       = mlx5e_dcbnl_getstate,
	.getpermhwaddr  = mlx5e_dcbnl_getpermhwaddr,

	.setpgtccfgtx   = mlx5e_dcbnl_setpgtccfgtx,
	.setpgbwgcfgtx  = mlx5e_dcbnl_setpgbwgcfgtx,
	.getpgtccfgtx   = mlx5e_dcbnl_getpgtccfgtx,
	.getpgbwgcfgtx  = mlx5e_dcbnl_getpgbwgcfgtx,

	.setpfccfg      = mlx5e_dcbnl_setpfccfg,
	.getpfccfg      = mlx5e_dcbnl_getpfccfg,
	.getcap         = mlx5e_dcbnl_getcap,
	.getnumtcs      = mlx5e_dcbnl_getnumtcs,
	.getpfcstate    = mlx5e_dcbnl_getpfcstate,
	.setpfcstate    = mlx5e_dcbnl_setpfcstate,
};


const struct ethtool_ops mlx5e_ethtool_ops = {
	.cap_rss_ctx_supported	= true,
	.supported_coalesce_params = ETHTOOL_COALESCE_USECS |
				     ETHTOOL_COALESCE_MAX_FRAMES |
				     ETHTOOL_COALESCE_USE_ADAPTIVE |
				     ETHTOOL_COALESCE_USE_CQE,
	.get_drvinfo       = mlx5e_get_drvinfo,
	.get_link          = ethtool_op_get_link,
	.get_link_ext_state  = mlx5e_get_link_ext_state,
	.get_strings       = mlx5e_get_strings,
	.get_sset_count    = mlx5e_get_sset_count,
	.get_ethtool_stats = mlx5e_get_ethtool_stats,
	.get_ringparam     = mlx5e_get_ringparam,
	.set_ringparam     = mlx5e_set_ringparam,
	.get_channels      = mlx5e_get_channels,
	.set_channels      = mlx5e_set_channels,
	.get_coalesce      = mlx5e_get_coalesce,
	.set_coalesce      = mlx5e_set_coalesce,
	.get_link_ksettings  = mlx5e_get_link_ksettings,
	.set_link_ksettings  = mlx5e_set_link_ksettings,
	.get_rxfh_key_size   = mlx5e_get_rxfh_key_size,
	.get_rxfh_indir_size = mlx5e_get_rxfh_indir_size,
	.get_rxfh          = mlx5e_get_rxfh,
	.set_rxfh          = mlx5e_set_rxfh,
	.get_rxnfc         = mlx5e_get_rxnfc,
	.set_rxnfc         = mlx5e_set_rxnfc,
	.get_tunable       = mlx5e_get_tunable,
	.set_tunable       = mlx5e_set_tunable,
	.get_pause_stats   = mlx5e_get_pause_stats,
	.get_pauseparam    = mlx5e_get_pauseparam,
	.set_pauseparam    = mlx5e_set_pauseparam,
	.get_ts_info       = mlx5e_get_ts_info,
	.set_phys_id       = mlx5e_set_phys_id,
	.get_wol	   = mlx5e_get_wol,
	.set_wol	   = mlx5e_set_wol,
	.get_module_info   = mlx5e_get_module_info,
	.get_module_eeprom = mlx5e_get_module_eeprom,
	.get_module_eeprom_by_page = mlx5e_get_module_eeprom_by_page,
	.flash_device      = mlx5e_flash_device,
	.get_priv_flags    = mlx5e_get_priv_flags,
	.set_priv_flags    = mlx5e_set_priv_flags,
	.self_test         = mlx5e_self_test,
	.get_fec_stats     = mlx5e_get_fec_stats,
	.get_fecparam      = mlx5e_get_fecparam,
	.set_fecparam      = mlx5e_set_fecparam,
	.get_eth_phy_stats = mlx5e_get_eth_phy_stats,
	.get_eth_mac_stats = mlx5e_get_eth_mac_stats,
	.get_eth_ctrl_stats = mlx5e_get_eth_ctrl_stats,
	.get_rmon_stats    = mlx5e_get_rmon_stats,
	.get_link_ext_stats = mlx5e_get_link_ext_stats
};





const struct net_device_ops mlx5e_netdev_ops = {
	.ndo_open                = mlx5e_open,
	.ndo_stop                = mlx5e_close,
	.ndo_start_xmit          = mlx5e_xmit,
	.ndo_setup_tc            = mlx5e_setup_tc,
	.ndo_select_queue        = mlx5e_select_queue,
	.ndo_get_stats64         = mlx5e_get_stats,
	.ndo_set_rx_mode         = mlx5e_set_rx_mode,
	.ndo_set_mac_address     = mlx5e_set_mac,
	.ndo_vlan_rx_add_vid     = mlx5e_vlan_rx_add_vid,
	.ndo_vlan_rx_kill_vid    = mlx5e_vlan_rx_kill_vid,
	.ndo_set_features        = mlx5e_set_features,
	.ndo_fix_features        = mlx5e_fix_features,
	.ndo_change_mtu          = mlx5e_change_nic_mtu,
	.ndo_eth_ioctl            = mlx5e_ioctl,
	.ndo_set_tx_maxrate      = mlx5e_set_tx_maxrate,
	.ndo_features_check      = mlx5e_features_check,
	.ndo_tx_timeout          = mlx5e_tx_timeout,
	.ndo_bpf		 = mlx5e_xdp,
	.ndo_xdp_xmit            = mlx5e_xdp_xmit,
	.ndo_xsk_wakeup          = mlx5e_xsk_wakeup,
#ifdef CONFIG_MLX5_EN_ARFS
	.ndo_rx_flow_steer	 = mlx5e_rx_flow_steer,
#endif
#ifdef CONFIG_MLX5_ESWITCH
	.ndo_bridge_setlink      = mlx5e_bridge_setlink,
	.ndo_bridge_getlink      = mlx5e_bridge_getlink,

	/* SRIOV E-Switch NDOs */
	.ndo_set_vf_mac          = mlx5e_set_vf_mac,
	.ndo_set_vf_vlan         = mlx5e_set_vf_vlan,
	.ndo_set_vf_spoofchk     = mlx5e_set_vf_spoofchk,
	.ndo_set_vf_trust        = mlx5e_set_vf_trust,
	.ndo_set_vf_rate         = mlx5e_set_vf_rate,
	.ndo_get_vf_config       = mlx5e_get_vf_config,
	.ndo_set_vf_link_state   = mlx5e_set_vf_link_state,
	.ndo_get_vf_stats        = mlx5e_get_vf_stats,
	.ndo_has_offload_stats   = mlx5e_has_offload_stats,
	.ndo_get_offload_stats   = mlx5e_get_offload_stats,
#endif
};


const struct xdp_metadata_ops mlx5e_xdp_metadata_ops = {
	.xmo_rx_timestamp		= mlx5e_xdp_rx_timestamp,
	.xmo_rx_hash			= mlx5e_xdp_rx_hash,
	.xmo_rx_vlan_tag		= mlx5e_xdp_rx_vlan_tag,
};


const struct xsk_tx_metadata_ops mlx5e_xsk_tx_metadata_ops = {
	.tmo_fill_timestamp		= mlx5e_xsk_fill_timestamp,
	.tmo_request_checksum		= mlx5e_xsk_request_checksum,
};



static int mlx5e_nic_init(struct mlx5_core_dev *mdev,
    mlx5e_build_nic_params
        mlx5e_params_mqprio_reset -> net/mlx5e：提高 MQPRIO 弹性，* 添加 netdev->tc_to_txq 回滚，以防 mlx5e_update_netdev_queues() 失败。 * 修复了两种模式之间损坏的转换：tc==8 的 MQPRIO DCB 模式和 MQPRIO 通道模式。 * 如果使用不同数量的通道重新连接，请禁用 MQPRIO 通道模式。 * 改进代码共享
        params->rx_cqe_compress_def = slow_pci_heuristic(mdev) -> net/mlx5e：统一慢速 PCI 启发式，将链路/pci 速度查询和逻辑集成到单个函数中。 统一启发式并对所有数据使用单一 PCI 阈值 (16G)
        mlx5e_build_rq_params -> net/mlx5e：更改VF表示器的RQ类型，表示器的RQ大小不足以使它们获得足够高的性能，因此需要放大，同时对其内存使用造成最小影响。 为了实现这一点，增加了表示器 RQ 的大小，并且将其类型更改为跨步 RQ（如果支持）。 为了实现这一目标，进行了以下更改： * 将用于设置标准 netdev 的 RQ 参数的序列提取到函数中 * 用标准序列替换用于设置表示器的 RQ 参数的序列 此更改的影响可以在以下测量中看到 通过 VF 设置 VM，通过 VF 表示器连接到 OVS，连接到外部主机： 在当前更改之前：TCP 吞吐量 [Gb/s] VM 到外部主机 ~ 7.2 当前更改后（使用跨步 RQ 测量） )：VM 到外部主机的 TCP 吞吐量 [Gb/s] ~ 23.5 每个表示器现在为其数据包缓冲区消耗 2 [MB] 内存
            首选跨步 RQ，除非满足以下任一条件： - 跨步 RQ 配置不可能/不支持。 - CQE 压缩处于开启状态，并且不支持 stride_index mini_cqe 布局。 - Legacy RQ 将使用线性 SKB，而 Striding RQ 将使用非线性。 无 XSK 参数：一般检查跨步 RQ 的可用性
            mlx5e_rx_mpwqe_is_linear_skb
            mlx5e_set_rq_type
            mlx5e_init_rq_type_params
        mlx5_core_get_terminate_scatter_list_mkey
            mlx5_cmd_exec_inout MLX5_CMD_OP_QUERY_SPECIAL_CONTEXTS
        mlx5e_choose_lro_timeout
        mlx5_query_min_inline -> net/mlx5e：Tx，软化内联模式 VLAN 依赖性，如果有能力，请在 TX WQE 中对非 VLAN 数据包使用零内联模式。 对于 VLAN，请至少保持 L2 内联模式的强制执行，除非 WQE VLAN 插入卸载上限已打开。 性能：经测试单核包速率为64Bytes。 NIC：ConnectX-5 CPU：Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz pktgen：之前：12.46 Mpps 之后：14.65 Mpps (+17.5%) XDP_TX：MPWQE 流不受影响，因为它已经有这个 优化。 因此我们使用 priv-flag xdp_tx_mpwqe: off 进行测试。 之前：9.90 Mpps 之后：10.20 Mpps (+3%)
            mlx5_query_nic_vport_min_inline
        AF_XDP -> net/mlx5e：添加 XSK 零拷贝支持，此提交添加了对 AF_XDP 零拷贝 RX 和 TX 的支持。 我们在通道内创建专用的 XSK RQ，这意味着两个 RQ 同时运行：一个用于非 XSK 流量，另一个用于 XSK 流量。 常规 RQ 和 XSK RQ 使用单个 ID 命名空间，分为两半：下半部分是常规 RQ，上半部分是 XSK RQ。 当任何零拷贝 AF_XDP 套接字处于活动状态时，不允许更改通道数，因为这会破坏 XSK RQ ID 和通道之间的映射。 XSK 需要不同的页面分配和释放例程。 mlx5e_{alloc,free}_rx_mpwqe 和 mlx5e_{get,put}_rx_frag 等函数足够通用，可用于常规 RQ 和 XSK RQ，并且它们在实际分配函数周围使用 mlx5e_page_{alloc,release} 包装器。 不使用函数指针以避免损失 retpolines 的性能。 只要确定应该使用常规（非 XSK）页面释放函数，就会直接调用它。 只有对 XSK 有意义的统计信息才会暴露给用户空间。 那些不参与 XSK 流程的不予考虑。 请注意，我们不会等待 XSK RQ 上的 WQE（与常规 RQ 不同），因为较新的 xdpsock 示例在设置阶段不提供任何填充环条目。 我们在频道中创建了专用的 XSK SQ。 这种分离有其优点： 1. 当 UMEM 关闭时，XSK SQ 也可以关闭并停止接收完成。 如果现有的 SQ 用于 XSK，它将继续接收已关闭套接字的数据包的完成情况。 如果此时打开一个新的 UMEM，它将开始获得不属于它的完成。 2、单独计算统计。 当用户空间启动 TX 时，驱动程序通过将 NOP 发布到专用 XSK ICO（内部控制操作）SQ 来触发硬件中断，以便在正确的 CPU 内核上触发 NAPI。 该 XSK ICO SQ 受自旋锁保护，因为用户空间应用程序可能会从任何核心踢出 TX。 将指向 UMEM 的指针存储在网络设备私有上下文中，独立于内核。 这样驱动程序就可以区分零拷贝和非零拷贝 UMEM。 内核函数 xdp_get_umem_from_qid 不关心这种差异，但驱动程序只对零拷贝 UMEM 感兴趣，特别是在清理时，它通过查看 UMEM 的存在来确定是否关闭 XSK RQ 和 SQ。 使用state_lock来保护对UMEM指针该区域的访问。 LRO 与 XDP 不兼容，但在 XDP 关闭时可能存在活动的 UMEM。 如果是这种情况，请不要允许 LRO 确保可以随时重新启用 XDP。 XSK 参数的验证通常在 XSK 队列打开时进行。 但是，当接口关闭或未设置 XDP 程序时，仍然可以拥有活动的 AF_XDP 套接字，甚至可以打开新套接字，但 XSK 队列将被关闭。 为了涵盖这些情况，请还在以下流程中执行验证： 1. 注册新的 UMEM，但由于缺少 XDP 程序或接口已关闭，因此不会创建 XSK 队列。 2. 当有 UMEM 注册时，MTU 会发生变化。 进行此早期检查可防止 mlx5e_open_channels 在稍后阶段失败，此时恢复是不可能的，并且应用程序没有机会处理错误，因为它获得了 MTU 更改或 XSK 打开操作的成功返回值。 性能测试在具有以下配置的计算机上进行： - 24 个 Intel Xeon E5-2620 v3 内核 @ 2.40 GHz - Mellanox ConnectX-5 Ex，具有 100 Gbit/s 链路 禁用 retpoline 的结果，单流：txonly：33.3 Mpps（21.5 Mpps，队列和应用程序固定到同一 CPU） rxdrop：12.2 Mpps l2fwd：9.4 Mpps 启用 retpoline 的结果，单流：txonly：21.3 Mpps（14.1 Mpps，队列和应用程序固定到同一 CPU） rxdrop： 9.9 Mpps l2fwd：6.8 Mpps
    mlx5e_vxlan_set_netdev_info
        mlx5_vxlan_allowed(priv->mdev->vxlan)
        mlx5e_vxlan_set_port
        mlx5e_vxlan_unset_port
        mlx5_vxlan_max_udp_ports
    mlx5e_build_rep_params -> net/mlx5e：在上行链路初始化期间分配流量控制存储，IPsec 代码依赖于有效的 priv->fs 指针，NIC 流量中就是这种情况，但在上行链路中不正确。 在修复行中提到的提交之前，该指针在所有流中都有效，因为它是与 priv 结构一起分配的。 此外，清理表示例程调用了未初始化的 priv->fs 指针及其内部结构，这导致了 NULL 引用。 因此，尽早移动 FS 分配
        mlx5e_build_rq_params
        ...
    mlx5e_timestamp_init
    priv->dfs_root = debugfs_create_dir("nic",
    mlx5e_fs_init
        mlx5e_fs_vlan_alloc
        mlx5e_fs_tc_alloc
            mlx5e_tc_table_alloc -> net/mlx5e：仅为特色配置文件分配 VLAN 和 TC，作为 fs API 的一部分引入流量控制 VLAN 和 TC 的分配和取消分配功能。 将 VLAN 和 TC 的分配添加为 nic 配置文件功能，这样 fs_init() 仅当配置文件中包含 VLAN 和 TC 时才会分配它们。 VLAN 和 TC 仅与 nic_profile 相关
        mlx5e_fs_ethtool_alloc
        mlx5e_fs_debugfs_init
            fs->dfs_root = debugfs_create_dir("fs", dfs_root)
    mlx5e_ktls_init
        mlx5e_is_ktls_device
        mlx5e_tls_debugfs_init
            tls->debugfs.dfs = debugfs_create_dir("tls", dfs_root)
    mlx5e_health_create_reporters
        mlx5e_reporter_tx_create
            priv->tx_reporter = reporter
        mlx5e_reporter_rx_create
            priv->rx_reporter = reporter
    if (take_rtnl) ->  net_device的改变会通过rtnl_lock和rtnl_unlock收到Routing Netlink信号量的保护。这也是为什么register_netdev执行时需要请求锁，并且在返回时要释放锁的原因。一旦register_netdevice完成了它自己的工作，它会通过net_set_todo将net_device结构体添加到net_todo_list中。net_todo_list包含一系列有注册（或除名）操作需要被完成的设备列表。这份列表有register_netdev在释放锁时间接予以处理
    mlx5e_set_xdp_feature(netdev)



mlx5e_init_nic_tx
    mlx5e_accel_init_tx -> mlx5e_ktls_init_tx
        dek_pool = mlx5_crypto_dek_pool_create -> net/mlx5：添加新的API用于快速更新加密密钥，添加新的API以支持快速更新DEK。 由于池是为每个关键目的（类型）创建的，因此需要一对池 API 来获取/放置池。 另一对 DEK API 是从池中获取 DEK 对象并使用用户密钥更新它，或者将其释放回池中。 由于后续补丁将支持批量分配和销毁，因此这里使用旧的实现。 为了支持这些 API，首先定义 pool 和 dek 结构。 其中仅存储少量字段。 例如，pool 结构中的 key_ Purpose 和 refcnt，dek 结构中的 DEK 对象 id。 更多字段将在以后的补丁中添加到这些结构体中，例如，pool 结构体的不同批量列表，批量指针 dek 结构体所属，以及池中列表的 list_entry，用于保存等待被调用的键。 当其他线程正在同步时被释放。 除了创建和销毁接口外，还新增了一个获取obj id的接口。 目前这些 API 计划仅由 TLS 使用
            INIT_LIST_HEAD(&pool->avail_list);
            INIT_WORK(&pool->sync_work, mlx5_crypto_dek_sync_work_fn)
            INIT_WORK(&pool->destroy_work, mlx5_crypto_dek_destroy_work_fn)
        priv->tls->tx_pool = mlx5e_tls_tx_pool_init
            INIT_WORK(&pool->create_work, create_work) -> net/mlx5e：kTLS，动态调整TX回收池的大小，让TLS TX回收池的大小更加灵活，通过不断动态地分配和释放硬件资源以响应连接速率和负载的变化。 批量分配和释放池条目 (16)。 使用工作队列在后台释放/分配。 当池大小低于低阈值 (1K) 时分配新的批量。 当池大小大于上限阈值（4K）时，将执行对称操作。 每个空闲池条目都包含：1 个 TIS、1 DEK（硬件资源）以及主机内存中的约 100 字节。 从空池开始，以最大程度地减少启用了设备卸载 TLS 的非 TLS 用户的内存和硬件资源浪费。 收到新请求时，如果池为空，请不要等待整个批量分配完成。 相反，触发单个资源的即时分配以减少延迟。 性能测试： 之前：11,684 CPS 之后：16,556 CPS
                bulk_async = mlx5e_bulk_async_init(pool->mdev, MLX5E_TLS_TX_POOL_BULK)
                    mlx5_cmd_init_async_ctx
                    bulk_async->arr[i].async_ctx = &bulk_async->async_ctx
                mlx5e_tls_priv_tx_init
                    mlx5e_ktls_create_tis
                    mlx5e_ktls_create_tis_cb create_tis_callback
        mlx5e_tls_tx_debugfs_init(tls, tls->debugfs.dfs)
    mlx5e_set_mqprio_rl
    mlx5e_dcbnl_initialize -> net/mlx5e：ConnectX-4 固件对 DCBX 的支持，默认情况下 DBCX 由设置了 dcbx 功能位的固件控制。 在此模式下，固件负责从远程伙伴读取 TLV 数据包或向远程伙伴发送 TLV 数据包。 该补丁设置基础设施以在 HOST/FW DCBX 控制模式之间移动


流量类型
enum mlx5_traffic_types {
	MLX5_TT_IPV4_TCP,
	MLX5_TT_IPV6_TCP,
	MLX5_TT_IPV4_UDP,
	MLX5_TT_IPV6_UDP,
	MLX5_TT_IPV4_IPSEC_AH,
	MLX5_TT_IPV6_IPSEC_AH,
	MLX5_TT_IPV4_IPSEC_ESP,
	MLX5_TT_IPV6_IPSEC_ESP,
	MLX5_TT_IPV4,
	MLX5_TT_IPV6,
	MLX5_TT_ANY,
	MLX5_NUM_TT,
	MLX5_NUM_INDIR_TIRS = MLX5_TT_ANY,
};


流命名空间类型
enum mlx5_flow_namespace_type {
	MLX5_FLOW_NAMESPACE_BYPASS,
	MLX5_FLOW_NAMESPACE_KERNEL_RX_MACSEC,
	MLX5_FLOW_NAMESPACE_LAG,
	MLX5_FLOW_NAMESPACE_OFFLOADS,
	MLX5_FLOW_NAMESPACE_ETHTOOL,
	MLX5_FLOW_NAMESPACE_KERNEL,
	MLX5_FLOW_NAMESPACE_LEFTOVERS,
	MLX5_FLOW_NAMESPACE_ANCHOR,
	MLX5_FLOW_NAMESPACE_FDB_BYPASS,
	MLX5_FLOW_NAMESPACE_FDB,
	MLX5_FLOW_NAMESPACE_ESW_EGRESS,
	MLX5_FLOW_NAMESPACE_ESW_INGRESS,
	MLX5_FLOW_NAMESPACE_SNIFFER_RX,
	MLX5_FLOW_NAMESPACE_SNIFFER_TX,
	MLX5_FLOW_NAMESPACE_EGRESS,
	MLX5_FLOW_NAMESPACE_EGRESS_IPSEC,
	MLX5_FLOW_NAMESPACE_EGRESS_MACSEC,
	MLX5_FLOW_NAMESPACE_RDMA_RX,
	MLX5_FLOW_NAMESPACE_RDMA_RX_KERNEL,
	MLX5_FLOW_NAMESPACE_RDMA_TX,
	MLX5_FLOW_NAMESPACE_PORT_SEL,
	MLX5_FLOW_NAMESPACE_RDMA_RX_COUNTERS,
	MLX5_FLOW_NAMESPACE_RDMA_TX_COUNTERS,
	MLX5_FLOW_NAMESPACE_RDMA_RX_IPSEC,
	MLX5_FLOW_NAMESPACE_RDMA_TX_IPSEC,
	MLX5_FLOW_NAMESPACE_RDMA_RX_MACSEC,
	MLX5_FLOW_NAMESPACE_RDMA_TX_MACSEC,
};


流表命令集
static const struct mlx5_flow_cmds mlx5_flow_cmds = {
	.create_flow_table = mlx5_cmd_create_flow_table,
	.destroy_flow_table = mlx5_cmd_destroy_flow_table,
	.modify_flow_table = mlx5_cmd_modify_flow_table,
	.create_flow_group = mlx5_cmd_create_flow_group,
	.destroy_flow_group = mlx5_cmd_destroy_flow_group,
	.create_fte = mlx5_cmd_create_fte,
	.update_fte = mlx5_cmd_update_fte,
	.delete_fte = mlx5_cmd_delete_fte,
	.update_root_ft = mlx5_cmd_update_root_ft,
	.packet_reformat_alloc = mlx5_cmd_packet_reformat_alloc,
	.packet_reformat_dealloc = mlx5_cmd_packet_reformat_dealloc,
	.modify_header_alloc = mlx5_cmd_modify_header_alloc,
	.modify_header_dealloc = mlx5_cmd_modify_header_dealloc,
	.create_match_definer = mlx5_cmd_create_match_definer,
	.destroy_match_definer = mlx5_cmd_destroy_match_definer,
	.set_peer = mlx5_cmd_stub_set_peer,
	.create_ns = mlx5_cmd_stub_create_ns,
	.destroy_ns = mlx5_cmd_stub_destroy_ns,
	.get_capabilities = mlx5_cmd_get_capabilities,
};


HCA虚拟端口上下文
struct mlx5_hca_vport_context {
	u32			field_select;
	bool			sm_virt_aware;
	bool			has_smi;
	bool			has_raw;
	enum port_state_policy	policy;
	enum phy_port_state	phys_state;
	enum ib_port_state	vport_state;
	u8			port_physical_state;
	u64			sys_image_guid;
	u64			port_guid;
	u64			node_guid;
	u32			cap_mask1;
	u32			cap_mask1_perm;
	u16			cap_mask2;
	u16			cap_mask2_perm;
	u16			lid;
	u8			init_type_reply; /* bitmask: see ib spec 14.2.5.6 InitTypeReply */
	u8			lmc;
	u8			subnet_timeout;
	u16			sm_lid;
	u8			sm_sl;
	u16			qkey_violation_counter;
	u16			pkey_violation_counter;
	bool			grh_required;
};




一个包含指向所有每个进程、命名空间的指针的结构 - fs (mount)、uts、network、sysvipc 等。 pid 命名空间是一个例外 - 它是使用 task_active_pid_ns 访问的。 这里的pid命名空间是孩子们将使用的命名空间。 “count”是持有引用的任务数量。 那么，每个命名空间的计数将是指向它的 nsproxy 数量，而不是任务数量。 nsproxy 由共享所有命名空间的任务共享。 一旦克隆或取消共享单个命名空间，就会复制 nsproxy。 */
struct nsproxy {
	refcount_t count;
	struct uts_namespace *uts_ns;
	struct ipc_namespace *ipc_ns;
	struct mnt_namespace *mnt_ns;
	struct pid_namespace *pid_ns_for_children;
	struct net 	     *net_ns;
	struct time_namespace *time_ns;
	struct time_namespace *time_ns_for_children;
	struct cgroup_namespace *cgroup_ns;
};

添加了新的系统调用 setns()。 它将调用线程附加到现有的命名空间。 其原型为 int setns(int fd, int nstype); 参数是： • fd：引用命名空间的文件描述符。 这些是通过打开 /proc/<pid>/ns/ 目录中的链接获得的。 • nstype：可选参数。 当它是新的 CLONE_NEW* 命名空间标志之一时，指定的文件描述符必须引用与指定的 CLONE_NEW* 标志的类型匹配的命名空间。 当未设置 nstype（其值为 0）时，fd 参数可以引用任何类型的命名空间。 如果 nstype 与指定 fd 关联的命名空间类型不对应，则返回 –EINVAL 值。 您可以在 kernel/nsproxy.c 中找到 setns() 系统调用的实现。 • 添加了以下六个新克隆标志以支持命名空间： • CLONE_NEWNS（用于装载命名空间） • CLONE_NEWUTS（用于 UTS 命名空间） • CLONE_NEWIPC（用于 IPC 命名空间） • CLONE_NEWPID（用于 PID 命名空间） • CLONE_NEWNET（用于网络命名空间） • CLONE_NEWUSER（用于用户命名空间） 传统上，使用clone() 系统调用来创建新进程。 它经过调整以支持这些新标志，以便它将创建附加到新命名空间（或多个命名空间）的新进程。 请注意，在本章后面的一些示例中，您将遇到使用 CLONE_NEWNET 标志来创建新的网络命名空间的情况。 • 六个具有名称空间支持的子系统中的每个子系统都实现了自己的唯一名称空间。 例如，挂载命名空间由名为 mnt_namespace 的结构体表示，网络命名空间由名为 net 的结构体表示，这将在本节后面讨论。 我将在本章后面提到其他命名空间。 • 对于名称空间创建，添加了名为create_new_namespaces() 的方法(kernel/nsproxy.c)。 此方法获取 CLONE_NEW* 标志或 CLONE_NEW* 标志的位图作为第一个参数。 它首先通过调用create_nsproxy()方法创建一个nsproxy对象，然后根据指定的flag关联一个命名空间； 由于标志可以是标志的位掩码，因此 create_new_namespaces() 方法可以关联多个名称空间。 我们来看看create_new_namespaces()方法


net_device 结构代表网络设备。 它可以是物理设备，如以太网设备，也可以是软件设备，如桥接设备或 VLAN 设备。 与 sk_buff 结构一样，我将列出它的重要成员。 net_device 结构体在 include/linux/netdevice.h 中定义


IB速率
IB/core：添加对 XDR 链路速度的支持 添加新的 IBTA 速度 XDR，这是作为 XDR 的一部分添加到 Infiniband 规范的新速率，支持 200Gb 的信号速率。为了将该值报告给 rdma-core，请向 query_port 响应添加新的 u32 字段

enum ib_rate {
	IB_RATE_PORT_CURRENT = 0,
	IB_RATE_2_5_GBPS = 2,
	IB_RATE_5_GBPS   = 5,
	IB_RATE_10_GBPS  = 3,
	IB_RATE_20_GBPS  = 6,
	IB_RATE_30_GBPS  = 4,
	IB_RATE_40_GBPS  = 7,
	IB_RATE_60_GBPS  = 8,
	IB_RATE_80_GBPS  = 9,
	IB_RATE_120_GBPS = 10,
	IB_RATE_14_GBPS  = 11,
	IB_RATE_56_GBPS  = 12,
	IB_RATE_112_GBPS = 13,
	IB_RATE_168_GBPS = 14,
	IB_RATE_25_GBPS  = 15,
	IB_RATE_100_GBPS = 16,
	IB_RATE_200_GBPS = 17,
	IB_RATE_300_GBPS = 18,
	IB_RATE_28_GBPS  = 19,
	IB_RATE_50_GBPS  = 20,
	IB_RATE_400_GBPS = 21,
	IB_RATE_600_GBPS = 22,
	IB_RATE_800_GBPS = 23,
};


保护域（PD），PD是将QP和SRQ与MR以及AH与QP相关联的RDMA资源。 可以将PD视为一种颜色，例如：红色MR可以与红色QP配合使用，红色AH可以与红色QP配合使用。 使用带有红色 QP 的绿色 AH 将导致错误


扩展可靠连接 (XRC)，XRC 是一种 IB 传输扩展，与原始可靠传输相比，它在发送方为可靠连接 QP 提供了更好的可扩展性。 使用 XRC 将减少两个特定核心之间的 QP 数量：当使用 RC QP 时，对于每个核心，在每台机器中，都有一个 QP。 使用XRC时，每台主机中都会有一个XRC QP。 发送消息时，发送方需要指定接收该消息的远端SRQ号


地址句柄（AH），AH是一种RDMA资源，描述从本地端口到目的地的远程端口的路径。 它正用于 UD QP



VFS 所处理的系统调用,表 8.2 列出 VFS 的系统调用，这些系统调用涉及文件系统、常规文件、目录及符号链接。 另外还有少数几个由 VFS 处理的其他系统调用：诸如 ioperm( )、ioctl( )、pipe( )和 mknod( )，涉及设备文件和管道文件，有些内容在下一章进行讨论。由 VFS 处理的最后一组 系统调用，诸如 socket( )、connect( )、bind( )和 protocols( )，属于套接字系统 调用并用于实现网络功能。 表 8.2 VFS 的部分系统调用 系统调用名 功能  mount( )/ umount( ) 安装/卸载文件系统 sysfs( ) 获取文件系统信息 statfs( )/ fstatfs( ) /ustat( ) 获取文件系统统计信息 chroot( ) 更改根目录 chdir( ) /fchdir( ) /getcwd( ) 更改当前目录 mkdir( ) /rmdir( ) 创建/删除目录 getdents( ) /readdir( )/ link( ) unlink( ) /rename( ) 对目录项进行操作 readlink( ) /symlink( ) 对软链接进行操作 chown( ) /fchown( ) /lchown( ) 更改文件所有者 chmod( )/ fchmod( ) /utime( ) 更改文件属性 stat( ) /fstat( ) /lstat( ) access( ) 读取文件状态 open( ) /close( ) /creat( ) /umask( ) 打开/关闭文件 dup( ) /dup2( ) /fcntl( ) 对文件描述符进行操作 select( ) /poll( ) 异步 I/O 通信 truncate( ) /ftruncate( ) 更改文件长度 lseek( ) /_llseek( ) 更改文件指针 read( )/ write( ) /readv( ) /writev( ) sendfile( ) 文件 I/O 操作 pread( )/ pwrite( ) 搜索并访问文件 mmap( ) /munmap( ) 文件内存映射 fdatasync( ) /fsync( ) /sync( )/ msync( ) 同步访问文件数据 flock( ) 处理文件锁



static void mlx5e_nic_enable(struct mlx5e_priv *priv)
    mlx5e_fs_init_l2_addr(priv->fs, netdev)
        ether_addr_copy(fs->l2.broadcast.addr, netdev->broadcast)
    mlx5e_ipsec_init -> net/mlx5e：支持 IPsec 核心的 devlink 重新加载，更改 IPsec 初始化流程以允许将来创建应在 devlink 重新加载操作期间释放和分配的硬件资源。 作为该更改的一部分，将函数签名更新为无效，因为没有调用者真正对其感兴趣
        ipsec = kzalloc(sizeof(*ipsec), GFP_KERNEL) -> net/mlx5e：IPSec、Innova IPSec 卸载基础设施、添加 Innova IPSec ESP 加密卸载配置路径。 检测 Innova IPSec 设备并设置 NETIF_F_HW_ESP 标志。 使用先前补丁中引入的 API 配置安全关联。 添加软件解析器硬件描述符布局 软件解析器 (swp) 是 ConnectX 中的一项硬件功能，它允许主机软件指定 TX 路径中的协议标头偏移量，从而覆盖硬件解析器。 这对于 ASIC 可能无法自行解析的协议很有用。 请注意，由于内联元数据，Innova IPSec 不支持 XDP
        xa_init_flags
        ipsec->wq = alloc_workqueue("mlx5e_ipsec: %s", WQ_UNBOUND, 0,
        mlx5e_ipsec_aso_init -> net/mlx5e：为 IPsec 创建高级转向操作对象，设置 IPsec 与软件堆栈交互所需的 ASO（高级转向操作）对象，以处理各种快速变化的事件：重播窗口、生命周期限制等
        mlx5e_is_uplink_rep -> net/mlx5e：为 switchdev 模式准备 IPsec 数据包卸载，由于仅在 switchdev 模式下创建上行链路表示器，因此为 IPsec 添加本地变量以指示设备处于此模式。 在此模式下，IPsec ROCE 被禁用，并且加密卸载保持原样。 但是，由于在 FDB 中创建了用于数据包卸载的表，因此添加了 ipsec->rx_esw 和 ipsec->tx_esw
        mlx5e_accel_ipsec_fs_init -> net/mlx5：将 devcom 指针存储在 IPsec RoCE 内，将 mlx5e priv devcom 组件存储在 IPsec RoCE 内，以使 IPsec RoCE 代码能够访问其他设备的私有信息。 这包括检索必要的设备信息和 IPsec 数据库，这有助于确定是否配置了 IPsec
    mlx5e_macsec_init -> net/mlx5e：为 MACsec 创建高级转向操作 (ASO) 对象，添加对 ASO 工作队列条目 (WQE) 数据的支持，以允许在查询 ASO 工作队列 (WQ) 时读取数据。 在 ASO WQ 初始化时注册用户模式内存注册 (UMR)，在 ASO WQ 清理时取消注册 UMR。 MACsec 使用 UMR 来确定硬件触发事件的原因，因为不同的场景可能会触发相同的事件。 设置 MACsec ASO 对象以将硬件与软件同步，了解各种 macsec 流状态功能，例如：重播窗口、生命周期限制等
    mlx5e_modify_admin_state(mdev, MLX5_PORT_DOWN)
    mlx5e_set_netdev_mtu_boundaries
    mlx5e_set_dev_port_mtu
    mlx5_lag_add_netdev
    mlx5e_enable_async_events
    mlx5e_enable_blocking_events
    mlx5e_monitor_counter_init
    mlx5e_hv_vhca_stats_create
    mlx5e_dcbnl_init_app
    mlx5e_nic_set_rx_mode
        mlx5_hv_vhca_agent_create
        INIT_DELAYED_WORK(&priv->stats_agent.work, mlx5e_hv_vhca_stats_work) -> net/mlx5e：添加 mlx5e HV VHCA 统计代理，HV VHCA 统计代理负责运行 preiodic rx/tx 数据包/字节统计更新。 目前支持的格式是版本 MLX5_HV_VHCA_STATS_VERSION。 块 ID 1 专用于从 VF 到 PF 的统计数据传输。 报告器从所有打开的通道中获取统计数据，将其填充到缓冲区中并将其发送到mlx5_hv_vhca_write_agent。 由于统计层应包含每个块的一些元数据（序列和偏移量），因此 HV VHCA 层应在通过块 1 实际发送缓冲区之前修改缓冲区
    mlx5e_open
    udp_tunnel_nic_reset_ntf -> udp_tunnel_nic_reset_ntf() - 设备发起的重置通知，@dev：网络接口设备结构 * 由驱动程序调用，通知核心整个 UDP 隧道端口状态已丢失，通常是由于设备重置。 核心将假定设备忘记了所有端口，并根据需要发出 .set_port 和 .sync_table 回调。 * 该函数必须在持有 rtnl 锁的情况下调用，并且会在返回之前发出所有回调
    netif_device_attach



常用常量, 
#define SZ_1				0x00000001
#define SZ_2				0x00000002
#define SZ_4				0x00000004
#define SZ_8				0x00000008
#define SZ_16				0x00000010
#define SZ_32				0x00000020
#define SZ_64				0x00000040
#define SZ_128				0x00000080
#define SZ_256				0x00000100
#define SZ_512				0x00000200

#define SZ_1K				0x00000400
#define SZ_2K				0x00000800
#define SZ_4K				0x00001000
#define SZ_8K				0x00002000
#define SZ_16K				0x00004000
#define SZ_32K				0x00008000
#define SZ_64K				0x00010000
#define SZ_128K				0x00020000
#define SZ_256K				0x00040000
#define SZ_512K				0x00080000

#define SZ_1M				0x00100000
#define SZ_2M				0x00200000
#define SZ_4M				0x00400000
#define SZ_8M				0x00800000
#define SZ_16M				0x01000000
#define SZ_32M				0x02000000
#define SZ_64M				0x04000000
#define SZ_128M				0x08000000
#define SZ_256M				0x10000000
#define SZ_512M				0x20000000

#define SZ_1G				0x40000000
#define SZ_2G				0x80000000

#define SZ_4G				_AC(0x100000000, ULL)




register addr,
static int irdma_probe(
    struct ice_pf *pf = iidc_adev->pf
    irdma_fill_device_info(iwdev, pf, vsi)
        rf->hw.hw_addr = pf->hw.hw_addr; <- info.bar0 = rf->hw.hw_addr;
        rf->msix_count =  pf->num_rdma_msix -> 中断
    irdma_ctrl_init_hw
        irdma_setup_init_state
            static int irdma_initialize_dev
                info.bar0 = rf->hw.hw_addr;
                irdma_sc_dev_init
                    dev->hw->hw_addr = info->bar0
                    irdma_sc_init_hw                    
                        icrdma_init_hw
                            dev->hw_regs[i] = (u32 __iomem *)(hw_addr + icrdma_regs[i])

ring, 环形链表, 从头部取出一个元数后, 将头指针向后移动一位
#define IRDMA_RING_MOVE_HEAD(_ring, _retcode) \
	{ \
		register u32 size; \
		size = (_ring).size;  \
		if (!IRDMA_RING_FULL_ERR(_ring)) { \
			(_ring).head = ((_ring).head + 1) % size; \
			(_retcode) = 0; \
		} else { \
			(_retcode) = -ENOMEM; \
		} \
	}

分配一段设备和软件共享的DMA内存, 作为发送队列的内存, 起始VA作为发送队列的基址
cqp->sq.va = dma_alloc_coherent(dev->hw->device, cqp->sq.size, &cqp->sq.pa, GFP_KERNEL);


puda: privileged UDA



count line:
find . -name "*.c" |wc -l
find . -name "*.c" |xargs cat|wc -l
or wc -l `find ./ -name "*.c"`|tail -n1
代码行数，过滤了空行：
find . -name "*.c" |xargs cat|grep -v ^$|wc -l
xb@bin1:~/project/linux/master/linux/drivers/infiniband/core$ find . -name "*.h" |xargs cat|grep -v ^$|wc -l
2405
xb@bin1:~/project/linux/master/linux/drivers/infiniband/core$ find . -name "*.c" |xargs cat|grep -v ^$|wc -l
47169
xb@bin1:~/project/linux/master/linux/drivers/infiniband/core$ find . -name "*.h" |wc -l
16
xb@bin1:~/project/linux/master/linux/drivers/infiniband/core$ find . -name "*.c" |wc -l
56


Firmware Management Protocol (FMP), fpm




vfio, Documentation/driver-api/vfio.rst
echo 1 | sudo tee /sys/module/vfio_pci/parameters/enable_sriov
drivers/vfio/pci/vfio_pci.c
modprobe vfio-pci enable_sriov=1 -> module_init(vfio_pci_init) -> vfio/pci：将 pci_driver 代码从 vfio_pci_core.c 中拆分出来，将 vfio_pci 驱动程序拆分为两个逻辑部分，即实现“对任何 PCI 设备的通用 VFIO 支持”的“struct pci_driver”（vfio_pci.c）和代码库（ vfio_pci_core.c），帮助在 PCI 设备之上实现 struct vfio_device。 vfio_pci.ko 继续在 sysfs 下提供相同的接口，此更改不会对功能产生影响。 以下补丁将把 vfio_pci 和 vfio_pci_core 变成一个单独的模块。 这是为了允许另一个模块提供 pci_driver 并允许该模块自定义 VFIO 的设置方式、注入其自己的操作以及轻松扩展供应商特定的功能。 此时，vfio_pci_core 仍然包含许多混合到其中的 vfio_pci 功能。 后续补丁将移出更多大型物品，但需要另一个清理系列才能清除所有物品
    vfio_pci_core_set_params
    pci_register_driver(&vfio_pci_driver)
    vfio_pci_fill_ids()
        pci_add_dynid(&vfio_pci_driver, vendor, device, subvendor, subdevice, class, class_mask, 0) -> 将新的 PCI 设备 ID 添加到此驱动程序并重新探测设备


static struct pci_driver vfio_pci_driver = {
	.name			= "vfio-pci",
	.id_table		= vfio_pci_table,
	.probe			= vfio_pci_probe,
	.remove			= vfio_pci_remove,
	.sriov_configure	= vfio_pci_sriov_configure,
	.err_handler		= &vfio_pci_core_err_handlers,
	.driver_managed_dma	= true,
};

vfio_pci_probe -> vfio_add_group_dev 添加自己的ops，同时生成一个dev，qemu通过 /dev/vfio 获取这个dev
    struct vfio_pci_core_device *vdev
    vfio_pci_is_denylisted
        vfio_pci_dev_in_denylist
    vdev = vfio_alloc_device(vfio_pci_core_device, vdev, &pdev->dev, &vfio_pci_ops) -> container_of(_vfio_alloc_device
        vfio_init_device(device, dev, ops) -> vfio：添加统一 vfio_device 生命周期的帮助程序，其想法是让 vfio 核心管理 vfio_device 生命周期，而不是重复跨驱动程序的逻辑。 这也是将struct device添加到vfio_device的准备步骤。 新的助手对和 vfio_device 中的 kref： - vfio_alloc_device() - vfio_put_device() 驱动程序可以注册 @init/@release 回调来管理包装 vfio_device 的任何私有状态。 然而，vfio-ccw 不适合这个模型，因为它的私有结构混合了父级和 mdev 信息，因此生命周期混乱，因此必须在 vfio 设备的生命周期之外分配/释放。 根据之前的讨论，IBM 人员不会在短期内解决这个问题。 不必等待这些修改，而是引入另一个帮助程序 vfio_init_device()，以便 ccw 可以调用它来初始化预分配的 vfio_device。 ccw 技巧的进一步含义是 vfio_device 无法在 vfio 核心中统一释放。 相反，需要*每个*驱动程序来实现@release并释放内部的vfio_device。 那么ccw可以自行选择延迟免费。 另一个技巧是 kvzalloc() 用于满足 gvt 的需要，gvt 使用 vzalloc()，而所有其他都使用 kzalloc()。 因此，驱动程序应该调用帮助程序 vfio_free_device() 来释放 vfio_device，而不是假设 kfree() 或 vfree() 是适用的。 稍后，一旦 ccw 混乱得到修复，我们就可以删除这些技巧并完全处理 vfio 核心中的结构分配/释放。 所有现有用法转换为新模型后，现有 vfio_{un}init_group_dev() 将被弃用
            ida_alloc_max
            init_completion(&device->comp)
            ret = ops->init(device) -> vfio_pci_core_init_dev
                INIT_LIST_HEAD(&vdev->dummy_resources_list)
                INIT_LIST_HEAD(&vdev->ioeventfds_list)
                INIT_LIST_HEAD(&vdev->vma_list)
                INIT_LIST_HEAD(&vdev->sriov_pfs_item)
                xa_init(&vdev->ctx) -> vfio/pci：使用xarray进行中断上下文存储，中断上下文在分配中断时静态分配。 分配后，通过使用向量作为索引直接访问数组的元素来管理上下文。 当中断被禁用时，存储被释放。 启用 MSI-X 后，可以动态分配单个 MSI-X 中断。 需要中断上下文的动态存储来支持这一点。 将中断上下文数组替换为 xarray（类似于内核用作 MSI 描述符存储的数组），它可以支持动态扩展，同时保持使用向量作为索引的自定义。 使用动态存储，不再需要在分配中断时预分配中断上下文。 MSI 和 MSI-X 中断上下文仅在中断启用时使用。 因此，它们的分配可以被延迟，直到中断使能为止。 只有启用的中断才会有关联的中断上下文。 是否已分配中断（Linux irq number存在）成为是否允许中断的标准
            device_initialize(&device->device)
    dev_set_drvdata(&pdev->dev, vdev) -> vfio/pci：让所有 VFIO PCI 驱动程序将 vfio_pci_core_device 存储在 drvdata 中，在 drvdata 中拥有一致的指针将允许下一个补丁使用某些核心代码帮助程序中的 drvdata。 在 vfio_pci_core_register_device() 内使用 WARN_ON 来检测错过此的驱动程序
    vfio_pci_core_register_device(vdev)
        if (pdev->hdr_type != PCI_HEADER_TYPE_NORMAL)
        if (pci_num_vf(pdev)) -> 防止绑定到启用了 VF 的 PF，VF 可能正在被主机或其他用户使用。 如果 VF 已经存在，我们就无法捕获它们，也无法跟踪 VF 用户。 此处禁用 SR-IOV 将开始删除 VF，这将解除驱动程序的绑定，如果该 VF 也被 vfio-pci 使用，则驱动程序很容易发生阻塞。 直接拒绝这些PF，让用户去整理就好了
        vfio_assign_device_set -> vfio/pci：转向设备集基础设施，PCI 希望拥有通常的 open/close_device() 逻辑，但略有不同，即 open/close_device() 必须在由所有 vfio_device 共享的单锁下完成 PCI“重置组”。 复位组以及设备集由 pci_reset_bus() 接触的设备决定，可以是整个总线，也可以只是插槽。 依靠核心代码来完成 reflck 所做的一切并完全删除 reflck -> vfio：为打开/释放vfio_device_ops提供更好的通用支持，当前驱动程序ops有一个打开/释放对，每次打开或关闭设备FD时都会调用一次。 添加一组额外的 open/close_device() 操作，这些操作在设备 FD 第一次打开和最后一次关闭时调用。 分析表明所有驱动程序都需要这种语义。 有些人将其开放编码作为其 reflck 实现的一部分，有些人只是有错误并且完全错过了它。 为了保留 PCI 和 FSL 所依赖的当前语义，引入“设备集”的概念，它是一组在打开时共享相同锁的 vfio_device。 设备集是通过提供“set_id”指针来建立的。 所有提供相同指针的 vfio_device 将连接到相同的单例内存并锁定整个集合。 这有效地取代了名称奇怪的 reflck。 转换后，set_id 将源自： - fsl_mc_device (fsl) 的 struct device - struct pci_slot (pci) - struct pci_bus (pci) - struct vfio_device（所有） 该设计确保上述指针有效 只要 vfio_device 已注册，它们就会形成可靠的唯一密钥以将 vfio_device 分组。 此实现使用 xarray 而不是搜索驱动程序核心结构，这简化了该区域中有些棘手的锁定。 以下补丁转换所有驱动程序
            INIT_LIST_HEAD(&new_dev_set->device_list)
            dev_set = __xa_cmpxchg -> 比较和存储, 调用此函数时，您必须已经持有 xa_lock。 如果需要分配内存，它将释放锁，然后重新获取它
        vfio_pci_vf_init -> vfio_pci_probe() 相当复杂，有可选的 VF 和 VGA 子组件。 将它们移至清晰的 init/uninit 函数中，并在探测/删除中具有线性流程。 这修复了一些小错误： - vfio_pci_remove() 的顺序错误，vga_client_register() 删除通知程序并且位于 kfree(vdev) 之后，但通知程序引用 vdev，因此它可以在比赛中在 free 之后使用。 - vga_client_register() 可能会失败但被忽略 组织事物，因此销毁顺序与创建顺序相反
            if (pdev->is_virtfn) 
                pci_physfn
                list_for_each_entry(cur, &vfio_pci_sriov_pfs,
            vdev->nb.notifier_call = vfio_pci_bus_notifier
                if (action == BUS_NOTIFY_ADD_DEVICE
                    pci_info(vdev->pdev, "Captured SR-IOV VF %s driver_override\n"
            ret = bus_register_notifier(&pci_bus_type, &vdev->nb)
        vfio_pci_vga_init -> Video Graphics Array (VGA) connector
            vfio_pci_is_vga
            aperture_remove_conflicting_pci_devices
            vga_client_register(pdev, vfio_pci_set_decode)
            vga_set_legacy_decoding(pdev, vfio_pci_set_decode(pdev, false))
        vfio_pci_probe_power_state
            pci_read_config_word(pdev, pdev->pm_cap + PCI_PM_CTRL, &pmcsr)
        vfio_pci_set_power_state
        pm_runtime_allow
        vfio_register_group_dev -> vfio：简化中介设备的 iommu 组分配，重用 vfio_noiommu_group_alloc 中的逻辑，通过分解通用函数为中介设备分配假单设备 iommu 组，并用枚举替换 struct vfio_group 中的 noiommu 布尔字段以区分这三个 不同类型的团体
            dev_set_name(&device->device, "vfio%d", device->index)
            vfio_device_set_group(device, type)
                vfio_group_find_or_alloc
                    vfio_noiommu_group_alloc
                    group = vfio_group_find_from_iommu(iommu_group) -> list_for_each_entry(group, &vfio.group_list
                        vfio_group_has_device
                    group = vfio_create_group(iommu_group, VFIO_IOMMU)
                        group = vfio_group_alloc(iommu_group, type)
                            cdev_init(&group->cdev, &vfio_group_fops)
                            INIT_LIST_HEAD(&group->device_list)
                        dev_set_name(&group->dev, "%s%d"
                        cdev_device_add(&group->cdev, &group->dev)
                        list_add(&group->vfio_next, &vfio.group_list)
                or vfio_noiommu_group_alloc
            vfio_device_add(device) -> vfio：为 vfio_device 添加 cdev，这会添加对 vfio_device 的 cdev 支持。 它允许用户直接打开 vfio 设备，而不使用旧容器/组接口，这是支持嵌套翻译等新 iommu 功能的先决条件。以这种方式打开的设备 fd 无法访问 设备，因为 fops open() 不会打开设备，直到成功的 VFIO_DEVICE_BIND_IOMMUFD ioctl（将在以后的补丁中添加）。 通过此补丁，注册到 vfio 核心的设备将同时创建旧组和新设备接口。 - 组接口：/dev/vfio/$groupID - 设备接口：/dev/vfio/devices/vfioX - 普通设备（“X”是跨 vfio 设备的唯一编号） 对于给定设备，用户可以识别匹配的 vfioX 通过搜索设备的 sysfs 路径下的 vfio-dev 文件夹。 以 PCI 设备 (0000:6a:01.0) 为例，/sys/bus/pci/devices/0000\:6a\:01.0/vfio-dev/vfioX 表示 /dev/vfio/devices/ 下匹配的 vfioX，并且 vfio-dev/vfioX/dev 包含匹配的 /dev/vfio/devices/vfioX 的主次编号。 用户可以通过打开/dev/vfio/devices/vfioX 来获取设备fd。 此补丁中的 vfio_device cdev 逻辑： *) __vfio_register_dev() 路径最终会为每个 vfio_device 执行 cdev_device_add()（如果配置了 VFIO_DEVICE_CDEV）。 *) vfio_unregister_group_dev() 路径执行 cdev_device_del(); cdev 接口不支持 noiommu 设备，因此 VFIO 仅为没有 IOMMU 的物理设备创建旧组接口。 noiommu 用户应使用旧组界面
                device_add
            vfio_device_group_register -> list_add(&device->group_next, &device->group->device_list)
            vfio_device_debugfs_init -> vfio/migration：为热迁移驱动添加debugfs，热迁移过程涉及多个设备、软件和操作步骤。 任何一个节点出现错误都可能导致热迁移操作失败。 这个复杂的过程使得当功能出现故障时定位和分析原因变得非常困难。 为了在热迁移失败时快速定位问题原因，我在vfio热迁移驱动中添加了一组debugf, commit, https://github.com/ssbandjl/linux/commit/2202844e4468c7539dba0c0b06577c93735af952
                vdev->debug_root = debugfs_create_dir(dev_name(vdev->dev),
                vfio_dev_migration = debugfs_create_dir("migration",
                debugfs_create_devm_seqfile(dev, "state", vfio_dev_migration,


整个debugfs目录将基于CONFIG_DEBUG_FS宏的定义。 如果不启用该宏，vfio.h中的接口将为空定义，并且不会执行debugfs目录的创建和初始化, debugfs将创建一个公共根目录“vfio”文件，然后为每个实时迁移设备创建一个dev_name()文件。 首先，在此设备目录下创建“迁移”的统一状态采集文件。 然后，创建一个公共实时迁移状态查找文件“state”
    +-------------------------------------------+
    |                                           |
    |                                           |
    |                  QEMU                     |
    |                                           |
    |                                           |
    +---+----------------------------+----------+
        |      ^                     |      ^
        |      |                     |      |
        |      |                     |      |
        v      |                     v      |
     +---------+--+               +---------+--+
     |src vfio_dev|               |dst vfio_dev|
     +--+---------+               +--+---------+
        |      ^                     |      ^
        |      |                     |      |
        v      |                     |      |
   +-----------+----+           +-----------+----+
   |src dev debugfs |           |dst dev debugfs |
   +----------------+           +----------------+



static const struct vfio_device_ops vfio_pci_ops = {
	.name		= "vfio-pci",
	.init		= vfio_pci_core_init_dev,
	.release	= vfio_pci_core_release_dev,
	.open_device	= vfio_pci_open_device,
	.close_device	= vfio_pci_core_close_device,
	.ioctl		= vfio_pci_core_ioctl,
        case VFIO_DEVICE_IOEVENTFD
            vfio_pci_ioctl_ioeventfd
                vfio_pci_ioeventfd
                    vfio_virqfd_enable(ioeventfd, vfio_pci_ioeventfd_handler, vfio_pci_ioeventfd_thread, NULL,&ioeventfd->virqfd, fd)
                        INIT_WORK(&virqfd->shutdown, virqfd_shutdown)
                        INIT_WORK(&virqfd->inject, virqfd_inject)
                            virqfd->thread(virqfd->opaque, virqfd->data)
                        init_poll_funcptr(&virqfd->pt, virqfd_ptable_queue_proc)
                        events = vfs_poll(irqfd.file, &virqfd->pt)
                        handler(opaque, data)
                    list_add(&ioeventfd->next, &vdev->ioeventfds_list)
        case VFIO_DEVICE_SET_IRQS
            vfio_pci_ioctl_set_irqs(vdev, uarg)
                struct vfio_irq_set hdr
                max = vfio_pci_get_irq_count(vdev, hdr.index)
                    if (irq_type == VFIO_PCI_INTX_IRQ_INDEX)
                        pci_read_config_byte(vdev->pdev, PCI_INTERRUPT_PIN, &pin)
                    else if (irq_type == VFIO_PCI_MSI_IRQ_INDEX)
                        pos = vdev->pdev->msi_cap
                        pci_read_config_word(vdev->pdev, pos + PCI_MSI_FLAGS, &flags)
                    else if (irq_type == VFIO_PCI_MSIX_IRQ_INDEX)
                        pos = vdev->pdev->msix_cap
                        pci_read_config_word(vdev->pdev, pos + PCI_MSIX_FLAGS, &flags)
                vfio_set_irqs_validate_and_prepare(&hdr, max, VFIO_PCI_NUM_IRQS, &data_size)
                vfio_pci_set_irqs_ioctl(vdev, hdr.flags, hdr.index, hdr.start, hdr.count, data)
                    case VFIO_IRQ_SET_ACTION_TRIGGER
                        vfio_pci_set_msi_trigger
                            vfio_msi_enable
                                pci_alloc_irq_vectors(pdev, 1, nvec, flag)
                            vfio_msi_set_block(vdev, start, count, fds, msix)
                                vfio_msi_set_vector_signal(vdev, j, fd, msix)
                                    irq = pci_irq_vector(pdev, vector)
                                    trigger = eventfd_ctx_fdget(fd)
                                    request_irq(irq, vfio_msihandler, 0, ctx->name, trigger) -> toggle eventfd，进而引起producer状态变化，最终导致consumer状态变化
                                        eventfd_signal(trigger)
                                            eventfd_signal_mask(ctx, 0)
                                                wake_up_locked_poll(&ctx->wqh, EPOLLIN | mask)
                                    irq_bypass_register_producer(&ctx->producer)
	.device_feature = vfio_pci_core_ioctl_feature,
	.read		= vfio_pci_core_read,
	.write		= vfio_pci_core_write,
	.mmap		= vfio_pci_core_mmap,
        pci_request_selected_regions
        vdev->barmap[index] = pci_iomap(pdev, index, 0)
        vma->vm_private_data = vdev
	.request	= vfio_pci_core_request,
	.match		= vfio_pci_core_match,
	.bind_iommufd	= vfio_iommufd_physical_bind,
        idev = iommufd_device_bind(ictx, vdev->dev, out_device_id)
            iommufd_get_group
            iommu_group_has_isolated_msi
            iommu_device_claim_dma_owner
            iommufd_selftest_is_mock_dev
            iommufd_object_finalize
	.unbind_iommufd	= vfio_iommufd_physical_unbind,
	.attach_ioas	= vfio_iommufd_physical_attach_ioas,
	.detach_ioas	= vfio_iommufd_physical_detach_ioas,
};




vfio_disable_interrupts



pci, Documentation/PCI/pci.rst
How To Write Linux PCI Drivers


intel eth, ice, 
Documentation/networking/device_drivers/ethernet/intel/ice.rst


linux import file, 
include/linux/types.h
_Bool则是C99引入的标准，typedef为bool更符合多数编程者的习惯
ssize_t：用于表示有符号整数的数据类型。ssize_t 通常用作文件读写函数（如 read() 和 write()）的返回类型，表示成功读取或写入的字节数，或者在出现错误时返回负值
typedef uint64_t u64;



struct callback_head， 也被重定义为了rcu_head，是Linux中著名的RCU（Read-Copy Update）机制的结构体。该结构体可以在读取时共享数据且不需要加锁，因此可以在一定程度上提高并发性能。可以理解为一种不需要加锁的读-写锁。但是这里并不是RCU锁的实际使用方法（如list_add_rcu()），它们只是类型定义，用于声明函数指针，这些函数指针可以作为参数传递给与 RCU 相关的函数或其他功能

__attribute__((aligned(n))) 是 GNU 编译器的一个特性，用于指定数据或结构体的对齐方式。所以aligned(sizeof(void *)) 指定了对齐方式为指针大小，确保结构体的对齐方式是指针大小的整数倍，这样可以使得结构体在内存中对齐到指针大小的边界上。以便于提高访问速度，特别是在涉及到缓存行对齐的情况下可以提高性能

vdso文件夹（Virtual Dynamic Shared Object）是 Linux 内核源代码中的一个特殊文件夹



rdma_resolve_route
    cma_resolve_iboe_route
        cma_iboe_set_path_rec_l2_fields -> RDMA/cma：提供设置 RoCE 路径记录 L2 参数的函数，引入帮助函数来设置 RoCE 路径记录 L2 字段。 这包括设置 GID 类型、目标 MAC 地址和 netdev ifindex
        rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr, &route->path_rec->sgid);
	    rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.dst_addr, &route->path_rec->dgid);
        route->path_rec->hop_limit = 1 -> IB/core：将 rdma_network_type 添加到 wc，提供商应告诉 IB core wc 的网络类型。 它用于在 GID 表中搜索正确的 GID。 当使用无法提供此信息的 HCA 时，IB 核心会尝试深入检查数据包并自行提取 GID 类型。 我们根据 IP 堆栈的提示从 RDMA-CM 中的所有匹配条目中选择 sgid_index 和类型，并根据 IP 堆栈的上述提示为 IP 数据包设置 hop_limit
        route->path_rec->reversible = 1 -> IB/核心：为 IBoE 添加 VLAN 支持，为 IBoE 添加 802.1q VLAN 支持。 VLAN 标记按以下方式编码在从链路本地地址派生的 GID 中： 当 GID 包含 VLAN 时，GID[11] GID[12] 包含 VLAN ID。 数据包的 3 位用户优先级字段与 SL 的 3 位相同。 对于 rdma_cm 应用程序，TOS 字段用于通过右移 5 位来生成 SL 字段，从而有效地占用 TOS 字段的 3 MS 位
        route->path_rec->sl = iboe_tos_to_sl(ndev, tos)
        如果设置了 ACK 超时，请使用该值来计算 PacketLifeTime。 根据 IBTA 12.7.34，本地 ACK 超时 = (2 * PacketLifeTime + 本地 CA 的 ACK 延迟)。 假设本地 ACK 延迟可以忽略不计，我们可以使用 PacketLifeTime = 本地 ACK 超时/2 作为 RoCE 网络的合理近似值
        route->path_rec->packet_life_time = id_priv->timeout - 1 -> RDMA/cma：修复不正确的数据包生命周期计算 PacketLifeTime 的近似值是本地 ACK 超时的一半。 两个定时器的编码都是对数的。 如果设置了本地 ACK 超时但为零，则表示计时器已禁用。 在本例中，我们选择 CMA_IBOE_PACKET_LIFETIME 值，因为 50% 的无限没有意义。 在此提交之前，如果本地 ACK 超时为零（未运行），PacketLifeTime 将变为 255。 通过显式测试超时为零来修复
        rdma_protocol_roce_udp_encap(id_priv->id.device, id_priv->id.port_num)) -> RDMA/cma：初始化CM的路由路径记录的流标签，如果流标签不是用户设置的或者不是IPv4，则根据“Kernighan和Ritchie的哈希函数”用cma src/dst初始化它 -> IB/core：添加ROCE_UDP_ENCAP（RoCE V2）类型，添加RoCE v2 GID类型和端口类型。 支持此类型的供应商将自动使用 RoCE v2 GID 填充其 GID 表
            cma_get_roce_udp_flow_label(id_priv)
                fl = hash & IB_GRH_FLOWLABEL_MASK
        cma_init_resolve_route_work(work, id_priv)
            INIT_WORK(&work->work, cma_work_handler)
                cma_cm_event_handler
                    id_priv->id.event_handler(&id_priv->id, event)
        queue_work(cma_wq, &work->work)







rdma/rdma_cma.h



rdma_cm
static struct miscdevice ucma_misc = {
	.minor		= MISC_DYNAMIC_MINOR,
	.name		= "rdma_cm",
	.nodename	= "infiniband/rdma_cm",
	.mode		= 0666,
	.fops		= &ucma_fops,
};

static const struct file_operations ucma_fops = {
	.owner 	 = THIS_MODULE,
	.open 	 = ucma_open,
	.release = ucma_close,
	.write	 = ucma_write,
	.poll    = ucma_poll,
	.llseek	 = no_llseek,
};

rdma_create_event_channel -> ucma_open
    init_waitqueue_head(&file->poll_wait)
    return stream_open(inode, filp) -> 2. 将stream_open()添加到内核以打开类似流的不可查找文件、描述符。 对此类文件描述符的读写永远不会使用或更改 ppos。 并且通过类似流的文件的该属性，读取和写入将在不获取 f_pos 锁定的情况下运行 - 即读取和写入可以同时运行。 3. 使用语义补丁搜索并将所有内核中的 nonseekable_open 用户转换为stream_open，这些用户的读写实际上不依赖于 ppos，并且 file_operations 中没有其他方法假定@offset 访问。 4. 将 FOPEN_STREAM 添加到 fs/fuse/ 并通过 steam_open 打开内核文件描述符（如果该位存在于文件系统打开回复中）。 更改 fs/fuse/ open 处理程序以在 FOPEN_NONSEEKABLE 标志上使用 stream_open 而不是 nonseekable_open 是很诱人的，但是通过 Debian codesearch 进行 grep 会显示 FOPEN_NONSEEKABLE 的用户，特别是 GVFS，它实际上在其读写处理程序中使用偏移量


module_init(ucma_init) -> static int __init ucma_init(void)
    misc_register(&ucma_misc)
    device_create_file(ucma_misc.this_device, &dev_attr_abi_version)
    ucma_ctl_table_hdr = register_net_sysctl(&init_net, "net/rdma_ucm", ucma_ctl_table)
    ret = ib_register_client(&rdma_cma_client)







ucma_event_handler
    if (event->event == RDMA_CM_EVENT_CONNECT_REQUEST)
        ucma_connect_event_handler
    ucma_create_uevent
    wake_up_interruptible(&ctx->file->poll_wait) -> wait_queue_head_t	poll_wait


rdma_listen -> ucma_listen
    atomic_set(&ctx->backlog, cmd.backlog)
    rdma_listen
        if (!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND, RDMA_CM_LISTEN))
            rdma_bind_addr
                rdma_bind_addr_dst cma_dst_addr
                
        cma_check_port
        rdma_cap_ib_cm
        cma_ib_listen
            ib_cm_insert_listen(id_priv->id.device, cma_ib_req_handler, svc_id)
                
        cma_listen_on_all




rdma_get_request -> ucma_get_event
    while (list_empty(&file->event_list))
        if (wait_event_interruptible(file->poll_wait, -> wait event
    uevent = list_first_entry(&file->event_list, struct ucma_event, list)


tee<<EOF>ucma_event_handler.bt
#include <rdma/ib_cm.h>
kprobe:ucma_event_handler
{
	//printf("event:%d\n",(int)(struct ib_cm_event *)arg1->event));
}
EOF
bpftrace ucma_event_handler.bt

root@u20:~/project/rdma/rdma-core/build# bpftrace -e 'kprobe:ucma_event_handler{ printf("bt:%s, %lu\n", kstack, arg1); }'
Attaching 1 probe...
bt:
        ucma_event_handler+1
        addr_handler+269
        process_one_req+75
        process_one_work+555
        worker_thread+77
        kthread+298
        ret_from_fork+34
bt:
        ucma_event_handler+1
        cma_work_handler+104
        process_one_work+555
        worker_thread+77
        kthread+298
        ret_from_fork+34
bt:
        ucma_event_handler+1
        cma_cm_event_handler+39
        cma_ib_req_handler+1133
        cm_process_work+40
        cm_queue_work_unlock+156  <- IB_CM_DREQ_RECEIVED
        cm_req_handler+1943
        cm_work_handler+988
        process_one_work+555
        worker_thread+77
        kthread+298
        ret_from_fork+34


cm, drivers/infiniband/core/cma.c
module_init(cma_init) -> static int __init cma_init(void)
    cma_wq = alloc_ordered_workqueue("rdma_cm", WQ_MEM_RECLAIM)
    register_pernet_subsys(&cma_pernet_operations)
    ib_sa_register_client(&sa_client)
    register_netdevice_notifier(&cma_nb)
    register_netevent_notifier(&cma_netevent_cb)
    ib_register_client(&cma_client)
    cma_configfs_init
        config_group_init(&cma_subsys.su_group) -> IB/cma：为rdma_cm添加configfs，用户希望控制rdma_cm的行为。 例如，未设置所需 RoCE gid 类型的旧应用程序可以在 RoCE V2 网络类型上执行。 为了支持此配置，我们为 rdma_cm 实现了一个 configfs。 为了使用 configfs，需要将其挂载并 mkdir <IB device name> 到 rdma_cm 目录中。 该补丁添加了对单个配置文件 default_roce_mode 的支持。 模式可以是“IB/RoCE v1”或“RoCE v2”
        configfs_register_subsystem(&cma_subsys)


static struct pernet_operations cma_pernet_operations = {
	.init = cma_init_net,
	.exit = cma_exit_net,
	.id = &cma_pernet_id,
	.size = sizeof(struct cma_pernet),
};


cma_listen_handler+1
cma_ib_req_handler+1133
cm_process_work+40
cm_queue_work_unlock+156
cm_req_handler+1943
cm_work_handler+988
process_one_work+555
worker_thread+77
kthread+298
ret_from_fork+34




mount_linux(){
    mount -t 9p -o trans=virtio host0 /root/project/linux/v5.15/jammy
    cd /root/project/linux/v5.15/jammy
}




rdma_create_id
static ssize_t ucma_write
    ret = ucma_cmd_table[hdr.cmd](file, buf + sizeof(hdr), hdr.in, hdr.out) -> ucma_create_id
        ctx = ucma_alloc_ctx(file)
        cm_id = rdma_create_user_id(ucma_event_handler, ctx, cmd.ps, qp_type)
            __rdma_create_id
                id_priv->state = RDMA_CM_IDLE
                id_priv->id.event_handler = event_handler <- cma_listen_handler
                id_priv->gid_type = IB_GID_TYPE_IB
                id_priv->seq_num &= 0x00ffffff
                rdma_restrack_new(&id_priv->res, RDMA_RESTRACK_CM_ID)
                if (parent)
                    rdma_restrack_parent_name
                        rdma_restrack_attach_task
            rdma_restrack_set_name
                rdma_restrack_attach_task
        ucma_set_ctx_cm_id(ctx, cm_id)
        ucma_finish_ctx(ctx)
            list_add_tail(&ctx->list, &ctx->file->ctx_list)
            xa_store(&ctx_table, ctx->id, ctx, GFP_KERNEL)


ucma_bind
    ucma_get_ctx
    rdma_bind_addr
        state = RDMA_CM_ADDR_BOUND,
        cma_check_linklocal
        memcpy(cma_src_addr(id_priv), addr, rdma_addr_size(addr))
        id_priv->afonly = 1
        daddr = cma_dst_addr(id_priv) -> return (struct sockaddr *) &id_priv->id.route.addr.dst_addr
        ret = cma_get_port(id_priv)
            cma_select_ib_ps -> RDMA/cma：更新端口预留以支持AF_IB，AF_IB使用64位服务ID（SID），用户可以通过使用掩码来控制。 rdma_cm 将根据选定的端口空间和端口号为 SID 的未屏蔽部分分配值。 由于 IB 规范将 SID 范围划分为多个区域，因此 SID/掩码组合可能落入 RDMA CM IP 附件定义的现有端口空间范围之一。 将 AF_IB SID 映射到正确的 RDMA 端口空间
            cma_use_port
                bind_list = cma_ps_find(id_priv->id.route.addr.dev_addr.net, ps, snum)
                cma_alloc_port
                or cma_check_port
                cma_bind_port




#0  cma_listen_on_dev (id_priv=0xffff888108b66800, cma_dev=0xffff88810980e480, to_destroy=0xffffc9000382fd08) at drivers/infiniband/core/cma.c:2540
#1  0xffffffffc05427a8 in cma_listen_on_all (id_priv=0xffff888108b66800) at drivers/infiniband/core/cma.c:2592
#2  rdma_listen (id=0xffff888108b66800, backlog=1024) at drivers/infiniband/core/cma.c:3867
#3  0xffffffffc0483d70 in ucma_listen (file=<optimized out>, inbuf=<optimized out>, in_len=<optimized out>, out_len=<optimized out>) at drivers/infiniband/core/ucma.c:1105

cma_listen_on_dev
    __rdma_create_id(net, cma_listen_handler, id_priv,
    dev_id_priv->state = RDMA_CM_ADDR_BOUND
    _cma_attach_to_dev
    rdma_restrack_add
    cma_id_get
    rdma_listen
        if (!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND, RDMA_CM_LISTEN))
        cma_ib_listen
            addr = cma_src_addr(id_priv)
            svc_id = rdma_get_service_id(&id_priv->id, addr)
            id = ib_cm_insert_listen(id_priv->id.device, cma_ib_req_handler, svc_id)
                cm_id_priv = cm_alloc_id_priv
                cm_init_listen
                listen_id_priv = cm_insert_listen(cm_id_priv, cm_handler)
                    cur_cm_id_priv = rb_entry(parent, struct cm_id_private,
                    rb_link_node(&cm_id_priv->service_node, parent, link)
                    rb_insert_color(&cm_id_priv->service_node, &cm.listen_service_table) -> 该函数是红黑树相对于普通二叉排序树最大的差别。这个函数中平衡二叉树的方式主要有3种：a)改变颜色，0表示red，1表示black。对于对齐的节点，低4位总是0，所以默认的情况下是红色的节点。那么对于父节点是black的情况，直接插入即可，无需改变颜色。如果父节点是红色则需要改变颜色。b)左旋转，将一个节点的右孩子变为父节点，节点本身变为左孩子。c)右旋转，将一个节点的做孩子变为父节点，节点本身变为右孩子
                cm_id_priv->id.state = IB_CM_LISTEN
    list_add_tail(&dev_id_priv->listen_list, &id_priv->listen_list)




ucma_resolve_addr
    ucma_get_ctx
    rdma_resolve_addr
        resolve_prepare_src
            cma_bind_addr -> rdma_bind_addr


cma_ib_req_handler
    cma_ib_id_from_event
        cma_save_req_info
        cma_get_net_dev
        



ucma_init_qp_attr
    rdma_init_qp_attr(ctx->cm_id, &qp_attr, &resp.qp_attr_mask)
        cma_ib_init_qp_attr
            ib_addr_get_pkey
            ib_find_cached_pkey
        or ib_cm_init_qp_attr > 每一个QP状态所设置的QP属性不一样
            cm_init_qp_init_attr
                qp_attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE
                qp_attr->pkey_index = cm_id_priv->av.pkey_index
                qp_attr->port_num = cm_id_priv->av.port->port_num
            cm_init_qp_rtr_attr
                qp_attr->ah_attr = cm_id_priv->av.ah_attr
                qp_attr->ah_attr.ib.dlid = cm_id_priv->av.dlid_datapath
                qp_attr->path_mtu = cm_id_priv->path_mtu
                qp_attr->dest_qp_num = be32_to_cpu(cm_id_priv->remote_qpn)
                qp_attr->rq_psn = be32_to_cpu(cm_id_priv->rq_psn)
                qp_attr->max_dest_rd_atomic
                qp_attr->min_rnr_timer = 0
                rdma_ah_get_dlid(&cm_id_priv->alt_av.ah_attr)
                qp_attr->alt_port_num = cm_id_priv->alt_av.port->port_num
                ...
            cm_init_qp_rts_attr
                qp_attr->sq_psn = be32_to_cpu(cm_id_priv->sq_psn)
                qp_attr->retry_cnt = cm_id_priv->retry_count
                qp_attr->rnr_retry = cm_id_priv->rnr_retry_count
                qp_attr->max_rd_atomic = cm_id_priv->initiator_depth
                qp_attr->timeout = cm_id_priv->av.timeout
                qp_attr->path_mig_state = IB_MIG_REARM
                ...
        or iw_cm_init_qp_attr
            iwcm_init_qp_init_attr
            iwcm_init_qp_rts_attr
        qp_attr->timeout = id_priv->timeout
        qp_attr->min_rnr_timer = id_priv->min_rnr_timer


    
static ssize_t ucma_accept
    else ret = rdma_accept_ece(ctx->cm_id, NULL, &ece)
        rdma_accept -> rdma_accept - 调用以接受连接请求或响应。@id：与请求关联的连接标识符。 @conn_param：建立连接所需的信息。 如果接受连接请求，则必须提供此信息。 如果接受连接响应，则该参数必须为 NULL。 通常，此例程仅由侦听器调用以接受连接请求。 如果用户正在执行自己的 QP 转换，则还必须在连接的活动端调用它。 如果出现错误，则会向远程端发送拒绝消息，并将与 id 关联的 qp 的状态修改为错误，以便刷新任何先前发布的接收缓冲区。 该函数供内核 ULP 使用，并且必须从处理程序回调下调用
            lockdep_assert_held(&id_priv->handler_mutex)
            ret = cma_rep_recv(id_priv)
                cma_modify_qp_rtr -> RDMA/cma：使用用户值覆盖默认的 responder_resources，默认情况下，responder_resources 参数设置为连接请求中收到的参数。 被动方在接受连接时可以覆盖该值。 将 QP 转换为 RTR 状态时使用被动方提供的值，而不是连接请求中给出的值。 如果没有此更改，如果被动方支持的responder_resources 少于请求中的资源，RTR 转换可能会失败。 为了代码一致性并防止 QP 破坏，请重构覆盖 initiator_depth 以匹配 responder_resources 的设置方式
                    qp_attr.qp_state = IB_QPS_INIT
                    rdma_init_qp_attr
                    ib_modify_qp
                    qp_attr.qp_state = IB_QPS_RTR
                    ib_modify_qp
                cma_modify_qp_rts
                trace_cm_send_rtu
                ib_send_cm_rtu -> IB：基于IP地址的RDMA连接管理器，基于InfiniBand的基于IP地址连接的内核连接管理代理。 该代理定义了通用 RDMA 连接抽象，以支持想要通过不同 RDMA 设备进行连接的客户端。 该代理还代表客户端处理 RDMA 设备热插拔事件
                    data = cm_copy_private_data(private_data, private_data_len)
                    msg = cm_alloc_msg(cm_id_priv)
                    cm_format_rtu
                    ib_post_send_mad(msg, NULL)
                    cm_id->state = IB_CM_ESTABLISHED
                    cm_set_private_data(cm_id_priv, data, private_data_len)


include/linux/socket.h
/* Supported address families. */
#define AF_UNSPEC	0
#define AF_UNIX		1	/* Unix domain sockets 		*/
...
#define AF_IB		27	/* Native InfiniBand address	*/
#define AF_MPLS		28	/* MPLS */
...
#define AF_MAX		46	/* For now.. */


IB设备
struct ib_device {
    int			      num_comp_vectors; -> IB：添加 CQ comp_vector 支持，向 struct ib_device 添加 num_comp_vectors 成员，并扩展 ib_create_cq() 以传入 comp_vector 参数——这与用户空间 libibverbs API 并行。 更新所有硬件驱动程序，将 num_comp_vectors 设置为 1，并使所有 ULP 将 comp_vector 值传递为 0。 将 num_comp_vectors 的值传递到用户空间，而不是硬编码值 1。我们需要多个 CQ 事件向量支持（通过 MSI-X 或类似的适配器，可以生成多个中断），但不清楚我们需要多少个向量， 或者我们想要如何处理策略问题，例如如何决定使用哪个向量或如何设置中断关联。 该补丁对于实验很有用，因为在更新驱动程序以支持多个向量时不需要进行核心更改，而且我们知道无论如何我们至少希望进行这些更改
}




struct ib_gid_table
    struct ib_gid_table_entry	**data_vec;
    u32				default_gid_indices; -> bit field




static const struct netdev_event_work_cmd add_cmd_upper_ips = {
	.cb	= add_netdev_upper_ips,
	.filter = is_eth_port_of_netdev_filter
};

ndev_event_link
    cmds[2] = add_cmd_upper_ips

static struct notifier_block nb_netdevice = {
	.notifier_call = netdevice_event
};
netdevice_event
case NETDEV_CHANGEUPPER
    netdevice_event_changeupper -> IB/core：删除bonding场景下的下层netdevice默认GID表项，当NETDEV_CHANGEUPPER事件发生时，下层设备尚未建立为主设备的从设备，当上层设备为bond设备时，默认GID表项不被删除。 因此，当 Bond 设备完全配置时，无法添加 Bond 设备的默认 GID 条目，因为默认 GID 条目已被下层网络设备占用。 这是不正确的。 默认 GID 条目实际上应该是 bond netdevice 的，因为在所有 RoCE GID（默认或 IP）中，将使用 bond 设备的 MAC 地址。 网络设备的默认 GID 并没有真正用于任何目的，这是令人困惑的。 因此，作为第一步，实现（a）过滤功能，该功能过滤CHANGEUPPER事件网络设备和关联的上层设备是否是主设备。 (b) 删除下层默认GID的回调函数（事件netdevice）
        netdevice_event
            cmds[2] = add_cmd_upper_ips


netdevice 通知程序链。 添加新类型时请记得更新 netdev_cmd_to_name() 和 rtnetlink_event() 中的 rtnetlink 通知排除列表 -> net：使 NETDEV_XXX 命令 enum { }，此补丁准备删除 NETDEV_UNREGISTER_FINAL。 由于 usnic_ib_netdev_event_to_string() 中使用 cmd 来获取 cmd 名称，因此在从各处删除 NETDEV_UNREGISTER_FINAL 后，我们会在该函数的 event2str[] 中出现漏洞。 相反，让我们让 NETDEV_XXX 命令名称可供所有人使用，并定义 netdev_cmd_to_name() ，这样我们就不必在名称更改后重新排列名称
enum netdev_cmd {
	NETDEV_UP	= 1,	/* For now you can't veto a device up/down */
	NETDEV_DOWN,
	NETDEV_REBOOT,		/* Tell a protocol stack a network interface
				   detected a hardware crash and restarted
				   - we can use this eg to kick tcp sessions
				   once done */
	NETDEV_CHANGE,		/* Notify device state change */
	NETDEV_REGISTER,
	NETDEV_UNREGISTER,
	NETDEV_CHANGEMTU,	/* notify after mtu change happened */
	NETDEV_CHANGEADDR,	/* notify after the address change */
	NETDEV_PRE_CHANGEADDR,	/* notify before the address change */
	NETDEV_GOING_DOWN,
	NETDEV_CHANGENAME,
	NETDEV_FEAT_CHANGE,
	NETDEV_BONDING_FAILOVER,
	NETDEV_PRE_UP,
	NETDEV_PRE_TYPE_CHANGE,
	NETDEV_POST_TYPE_CHANGE,
	NETDEV_POST_INIT,
	NETDEV_PRE_UNINIT,
	NETDEV_RELEASE,
	NETDEV_NOTIFY_PEERS,
	NETDEV_JOIN,
	NETDEV_CHANGEUPPER,
	NETDEV_RESEND_IGMP,
	NETDEV_PRECHANGEMTU,	/* notify before mtu change happened */
	NETDEV_CHANGEINFODATA,
	NETDEV_BONDING_INFO,
	NETDEV_PRECHANGEUPPER,
	NETDEV_CHANGELOWERSTATE,
	NETDEV_UDP_TUNNEL_PUSH_INFO,
	NETDEV_UDP_TUNNEL_DROP_INFO,
	NETDEV_CHANGE_TX_QUEUE_LEN,
	NETDEV_CVLAN_FILTER_PUSH_INFO,
	NETDEV_CVLAN_FILTER_DROP_INFO,
	NETDEV_SVLAN_FILTER_PUSH_INFO,
	NETDEV_SVLAN_FILTER_DROP_INFO,
	NETDEV_OFFLOAD_XSTATS_ENABLE,
	NETDEV_OFFLOAD_XSTATS_DISABLE,
	NETDEV_OFFLOAD_XSTATS_REPORT_USED,
	NETDEV_OFFLOAD_XSTATS_REPORT_DELTA,
	NETDEV_XDP_FEAT_CHANGE,
};



netdevice_event_work_handler -> 以下功能可在所有 IB 设备上运行。 netdevice_event和addr_event通过一个work执行ib_enum_all_roce_netdevs。 ib_enum_all_roce_netdevs 迭代所有 IB 设备



UVERBS_METHOD_ASYNC_EVENT_ALLOC
    ib_uverbs_init_async_event_file



MMIO空间的大小是在ACPI子系统初始化时确定的。具体的函数调用过程是：acpi_init()---->acpi_scan_init()---->acpi_bus_scan()---->acpi_bus_attach()---->acpi_scan_attach_handler()---->acpi_pci_root_add()---->pci_acpi_scan_root()---->pci_scan_child_bus()---->pci_scan_slot()---->pci_scan_single_device()---->pci_scan_device()---->pci_setup_device()---->pci_read_bases()---->__pci_read_base()。在__pci_read_base()中计算MMIO空间的大小。其计算方法可以归结为：以2为底，以BAR寄存器中read only的bit位的个数为幂的对数值就是MMIO空间大小，单位是Byte,
err = pci_request_selected_regions(pdev, bars, e1000_driver_name) -> 为设备内存分配物理地址空间, 保留选定的 PCI I/O 和内存资源, 标记MMIO所需要的内存空间，实际上最终调用的是 __pci_request_region()


hwmon子系统作为Linux内核中的一个子系统，用于监控硬件传感器的状态和提供对硬件传感器的访问接口, hwmon_device_register_with_groups 注册一个硬件监控设备。,这个函数的第一个参数是一个指向父设备的指针。 name 参数是指向 hwmon 设备名称的指针。 注册函数将创建一个指向该名称的 sysfs 名称属性。 drvdata参数是指向本地驱动数据的指针



include/rdma/rdma_cm.h


#define RDMA_IB_IP_PS_MASK   0xFFFFFFFFFFFF0000ULL
#define RDMA_IB_IP_PS_TCP    0x0000000001060000ULL
#define RDMA_IB_IP_PS_UDP    0x0000000001110000ULL
#define RDMA_IB_IP_PS_IB     0x00000000013F0000ULL



change state, 
cma_comp_exch
    if ((ret = (id_priv->state == comp)))
        id_priv->state = exch;


struct sa_path_rec 



struct net_device_ops {}
该结构体定义了网络设备的管理钩子，可以定义以下钩子； 除非另有说明，它们是可选的并且可以用空指针填充。 
int (*ndo_init)(struct net_device *dev); 当网络设备注册时，该函数被调用一次。 网络设备可以将其用于任何后期初始化或语义验证。 它可能会失败并显示错误代码，该错误代码将传播回 register_netdev。 
void (*ndo_uninit)(struct net_device *dev); 当设备未注册或注册失败时调用此函数。 如果 init 失败，则不会调用它。 
int (*ndo_open)(struct net_device *dev); 当网络设备转换到 up 状态时调用此函数。 
int (*ndo_stop)(struct net_device *dev); 当网络设备转换为 down 状态时调用此函数。 
netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb, struct net_device *dev); 当需要传输数据包时调用。 返回 NETDEV_TX_OK。 可以返回 NETDEV_TX_BUSY，但您应该在此之前停止队列； 它适用于过时的设备和奇怪的极端情况，但如果您返回 NETDEV_TX_BUSY，堆栈实际上会做大量无用的工作。 必需的; 不能为 NULL。 
netdev_features_t (*ndo_features_check)(struct sk_buff *skb, struct net_device *dev netdev_features_t features); 由核心传输路径调用以确定设备是否能够对给定数据包执行卸载操作。 这是为了让设备有机会实现任何无法通过功能标志表达的限制。 使用堆栈计算出的一组功能来调用检查，并返回驱动程序认为合适的功能。 
u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb, struct net_device *sb_dev); 当设备支持多个传输队列时调用以决定使用哪个队列。 
void (*ndo_change_rx_flags)(struct net_device *dev, int flags); 调用此函数是为了允许设备接收器在启用多播或混杂时更改配置。 
void (*ndo_set_rx_mode)(struct net_device *dev); 该功能称为设备更改地址列表过滤。 如果驱动程序处理单播地址过滤，则应在其 priv_flags 中设置 IFF_UNICAST_FLT。 
int (*ndo_set_mac_address)(struct net_device *dev, void *addr); 当需要更改媒体访问控制地址时调用此函数。 如果未定义此接口，则无法更改 MAC 地址。 
int (*ndo_validate_addr)(struct net_device *dev); 测试媒体访问控制地址对于设备是否有效。 
int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd); 旧式 ioctl 入口点。 它由 appletalk 和 ieee802154 子系统内部使用，但不再由设备 ioctl 处理程序调用。 
int (*ndo_siocbond)(struct net_device *dev, struct ifreq *ifr, int cmd); 由绑定驱动程序用于其设备特定的 ioctl：SIOCBONDENSLAVE、SIOCBONDRELEASE、SIOCBONDSETHWADDR、SIOCBONDCHANGEACTIVE、SIOCBONDSLAVEINFOQUERY 和 SIOCBONDINFOQUERY * 
int (*ndo_eth_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd); 调用以太网特定的 ioctls：SIOCGMIIPHY、SIOCGMIIREG、SIOCSMIIREG、SIOCSHWTSTAMP 和 SIOCGHWTSTAMP。 
int (*ndo_set_config)(struct net_device *dev, struct ifmap *map); 用于设置网络设备总线接口参数。 由于遗留原因保留此接口； 新设备应使用总线接口（PCI）进行低级管理。 
int (*ndo_change_mtu)(struct net_device *dev, int new_mtu); 当用户想要更改设备的最大传输单元时调用。 
void (*ndo_tx_timeout)(struct net_device *dev, unsigned int txqueue); 当发送器在 dev->watchdog 滴答声中没有取得任何进展时使用回调。 
void (*ndo_get_stats64)(struct net_device *dev, struct rtnl_link_stats64 *storage); struct net_device_stats* (*ndo_get_stats)(struct net_device *dev); 当用户想要获取网络设备使用统计信息时调用。 驱动程序必须执行以下操作之一： 1. 定义@ndo_get_stats64 以填充调用者传递的零初始化的 rtnl_link_stats64 结构。 2. 定义@ndo_get_stats 来更新net_device_stats 结构（通常应该是dev->stats）并返回指向它的指针。 仅当每个字段都以原子方式写入时，才可以异步更改结构。 3. 异步且原子地更新 dev->stats，并且不定义任何操作。 
bool (*ndo_has_offload_stats)(const struct net_device *dev, int attr_id) 如果此设备支持此 attr_id 的卸载统计信息，则返回 true。 
int (*ndo_get_offload_stats)(int attr_id, const struct net_device *dev, void *attr_data) 通过 attr_id 获取卸载操作的统计信息。 将其写入 attr_data po间。 
int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid); 如果设备支持 VLAN 过滤，则在注册 VLAN id 时调用此函数。 int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid); 如果设备支持 VLAN 过滤，则在取消注册 VLAN id 时调用此函数。 void (*ndo_poll_controller)(struct net_device *dev); SR-IOV管理功能。 int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac); int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan, u8 qos, __be16 proto); int (*ndo_set_vf_rate)(struct net_device *dev, int vf, int min_tx_rate, int max_tx_rate); int (*ndo_set_vf_spoofchk)(struct net_device *dev, int vf, bool 设置); int (*ndo_set_vf_trust)(struct net_device *dev, int vf, bool 设置); int (*ndo_get_vf_config)(struct net_device *dev, int vf, struct ifla_vf_info *ivf); int (*ndo_set_vf_link_state)(struct net_device *dev, int vf, int link_state); int (*ndo_set_vf_port)(struct net_device *dev, int vf, struct nlattr *port[]); 启用或禁用 VF 查询其 RSS 重定向表和哈希密钥的功能。 这是必需的，因为在某些设备上，VF 与 PF 共享此信息，并且查询它可能会引入理论上的安全风险。 int (*ndo_set_vf_rss_query_en)(struct net_device *dev, int vf, bool 设置); int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb); int (*ndo_setup_tc)(struct net_device *dev, enum tc_setup_type 类型, void *type_data); 调用以在 @dev 上设置任何“tc”调度程序、分类器或操作。 这总是从堆栈中调用，并保持 rtnl 锁并且 netif tx 队列停止。 这允许网络设备安全地执行队列管理。 以太网光纤通道 (FCoE) 卸载功能。 int (*ndo_fcoe_enable)(struct net_device *dev); 当 FCoE 协议栈想要开始使用 LLD 进行 FCoE 时调用，以便底层设备可以执行任何所需的配置或初始化以支持 FCoE 流量加速。 int (*ndo_fcoe_disable)(struct net_device *dev); 当 FCoE 协议栈想要停止对 FCoE 使用 LLD 时调用，以便底层设备可以执行任何所需的清理以停止支持 FCoE 流量加速。 int (*ndo_fcoe_ddp_setup)(struct net_device *dev, u16 xid, struct scatterlist *sgl, unsigned int sgc); 当 FCoE 启动器想要初始化可能是直接数据放置 (DDP) 候选的 I/O 时调用。 LLD 可以执行必要的设置并返回 1 表示设备已成功设置以在此 I/O 上执行 DDP，否则返回 0。 int (*ndo_fcoe_ddp_done)(struct net_device *dev, u16 xid); 当 FCoE 发起方/目标完成 DDPed I/O（如 FC 交换 ID“xid”所示）时调用，以便底层设备可以清理并重用资源以用于以后的 DDP 请求。 int (*ndo_fcoe_ddp_target)(struct net_device *dev, u16 xid, struct scatterlist *sql, unsigned int sgc); 当 FCoE 目标想要初始化可能是直接数据放置 (DDP) 候选的 I/O 时调用。 LLD 可以执行必要的设置并返回 1 表示设备已成功设置以在此 I/O 上执行 DDP，否则返回 0。 int (*ndo_fcoe_get_hbainfo)(struct net_device *dev, struct netdev_fcoe_hbainfo *hbainfo); 当 FCoE 协议栈需要有关底层设备的信息时调用。 FCoE 协议栈利用此信息根据 FC-GS 结构设备管理信息 (FDMI) 规范向光纤通道管理服务注册属性。 int (*ndo_fcoe_get_wwn)(struct net_device *dev, u64 *wwn, int 类型); 当底层设备想要覆盖 FCoE 协议栈中的默认全球通用名称 (WWN) 生成机制以将其自己的全球通用端口名称 (WWPN) 或全球通用节点名称 (WWNN) 传递给 FCoE 协议栈以供使用时调用。 RFS 加速。 int (*ndo_rx_flow_steer)(struct net_device *dev, const struct sk_buff *skb, u16 rxq_index, u32 flow_id); 设置 RFS 的硬件过滤器。 rxq_index是目标队列索引； flow_id 是稍后传递给 rps_may_expire_flow() 的流 ID。 成功时返回过滤器 ID，或返回负错误代码。 从机管理功能（用于桥接、绑定等）。 int (*ndo_add_slave)(struct net_device *dev, struct net_device *slave_dev); 被要求让另一个网络开发人员成为下属。 int (*ndo_del_slave)(struct net_device *dev, struct net_device *slave_dev); 呼吁释放之前被奴役的 netdev。 struct net_device *(*ndo_get_xmit_slave)(struct net_device *dev, struct sk_buff *skb, bool all_slaves); 获取主设备的xmit从设备。 如果 all_slaves 为 true，则函数假定所有从站都可以传输。 特征/卸载设置功能。 netdev_features_t (*ndo_fix_features)(struct net_device *dev, netdev_features_t 功能); 根据设备特定的约束调整请求的功能标志，并返回结果标志。 不得修改设备状态。 int (*ndo_set_features)(struct net_device *dev, netdev_features_t features); 调用以将设备配置更新为新功能。 传递的功能集可能小于 ndo_fix_features() 返回的功能集。 如果更改了 dev->features 本身，则必须返回 >0 或 -errno。 int (*ndo_fdb_add)(struct ndmsg *ndm、struct nlattr *tb[]、struct net_device *dev、const unsigned char *addr、u16 vid、u16 标志、struct netlink_ext_ack *extack); 将 FDB 条目添加到 dev 的 addr。 int (*ndo_fdb_del)(struct ndmsg *ndm, struct nlattr *tb[], struct net_device *dev, const unsigned char *addr, u16 vid) 从 dev 中删除对应于 addr 的 FDB 条目。 int (*ndo_fdb_del_bulk)(结构 nlmsghdr *nlh, 结构 net_device *dev, 结构 netlink_ext_ack *extack); int (*ndo_fdb_dump)(struct sk_buff *skb, struct netlink_callback *cb, struct net_device *dev, struct net_device *filter_dev, int *idx) 用于添加 FDB 条目以转储请求。 实施者应该向 skb 添加条目并使用条目数更新 idx。 int (*ndo_mdb_add)(struct net_device *dev, struct nlattr *tb[], u16 nlmsg_flags, struct netlink_ext_ack *extack); 向 dev 添加 MDB 条目。 int (*ndo_mdb_del)(struct net_device *dev, struct nlattr *tb[], struct netlink_ext_ack *extack); 从 dev 中删除 MDB 条目。 int (*ndo_mdb_del_bulk)(struct net_device *dev, struct nlattr *tb[], struct netlink_ext_ack *extack); 从 dev 中批量删除 MDB 条目。 int (*ndo_mdb_dump)(struct net_device *dev, struct sk_buff *skb, struct netlink_callback *cb); 从 dev 转储 MDB 条目。 netlink 回调中的第一个参数（标记）由核心 rtnetlink 代码使用。 int (*ndo_bridge_setlink)(struct net_device *dev, struct nlmsghdr *nlh, u16 标志, struct netlink_ext_ack *extack) int (*ndo_bridge_getlink)(struct sk_buff *skb, u32 pid, u32 seq, struct net_device *dev, u32 filter_mask, int nlflags) int (*ndo_bridge_dellink)(struct net_device *dev, struct nlmsghdr *nlh, u16 标志); int (*ndo_change_carrier)(struct net_device *dev, bool new_carrier); 打电话要求更改设备运营商。 不代表真实硬件的软设备（如虚拟设备、团队设备等）可以定义这一点，以允许其用户空间组件管理其虚拟载体状态。 根据物理硬件属性（例如网络电缆）或协议相关机制（例如 USB_CDC_NOTIFY_NETWORK_CONNECTION）确定载体状态的设备不应实现此功能。 int (*ndo_get_phys_port_id)(struct net_device *dev, struct netdev_phys_item_id *ppid); 调用以获取该设备的物理端口 ID。 如果驱动程序未实现此功能，则假定硬件无法在单个物理端口上拥有多个网络设备。 int (*ndo_get_port_parent_id)(struct net_device *dev, struct netdev_phys_item_id *ppid) 调用以获取该设备的物理端口的父 ID。 void* (*ndo_dfwd_add_station)(struct net_device *pdev, struct net_device *dev) 由上层设备调用，以加速切换或其他站功能到硬件中。 “pdev”是用于卸载的较低设备，“dev”是将支持卸载的网络设备。 返回一个指向上层将维护的私有结构的指针。 void (*ndo_dfwd_del_station)(struct net_device *pdev, void *priv) 由上层设备调用，删除“ndo_dfwd_add_station”创建的站。 “pdev”是支持该站的网络设备，而 priv 是添加操作返回的结构。 int (*ndo_set_tx_maxrate)(struct net_device *dev, int queue_index, u32 maxrate); 当用户想要设置特定 TX 队列的最大速率限制时调用。 int (*ndo_get_iflink)(const struct net_device *dev); 调用以获取该设备的 iflink 值。 int (*ndo_fill_metadata_dst)(struct net_device *dev, struct sk_buff *skb); 此函数用于获取给定 skb 的出口隧道信息。 这对于在采样数据包时检索外部隧道标头参数非常有用。 void (*ndo_set_rx_headroom)(struct net_device *dev, int Needed_headroom); 该函数用于指定skb在数据包接收期间分配skb时必须考虑的headroom。 设置适当的 rx headroom 值可以避免向前进行 skb 头复制。 设置负值会将 rx 余量重置为默认值。 int (*ndo_bpf)(结构 net_device *dev, 结构 netdev_bpf *bpf); 该函数用于设置或查询网络设备上与XDP相关的状态并管理BPF卸载。 有关详细信息，请参阅枚举 bpf_netdev_command 的定义。 int (*ndo_xdp_xmit)(struct net_device *dev, int n, struct xdp_frame **xdp, u32 标志); 此函数用于提交@n XDP 数据包以在网络设备上传输。 退货成功传输的帧数，丢弃的帧通过 xdp_return_frame() 释放/返回。 返回负数，意味着调用 ndo 时发生一般错误，意味着没有帧被 xmit'ed，并且核心调用者将释放所有帧。 结构net_device *(*ndo_xdp_get_xmit_slave)(结构net_device *dev, 结构xdp_buff *xdp); 根据xdp_buff获取master设备的xmit Slave。 int (*ndo_xsk_wakeup)(struct net_device *dev, u32 queue_id, u32 flags); 该函数用于唤醒负责在绑定到 AF_XDP 套接字的特定队列 ID 上发送和/或接收数据包的 softirq、ksoftirqd 或 kthread。 标志字段指定是否应使用标志 XDP_WAKEUP_RX 和 XDP_WAKEUP_TX 唤醒仅 RX、仅 Tx 或两者。 int (*ndo_tunnel_ctl)(struct net_device *dev, struct ip_tunnel_parm *p, int cmd); 添加、更改、删除或获取有关 IPv4 隧道的信息。 结构net_device *(*ndo_get_peer_dev)(结构net_device *dev); 如果设备与对等设备配对，则返回对等实例。 调用者必须处于 RCU 读取上下文中。 int (*ndo_fill_forward_path)(struct net_device_path_ctx *ctx, struct net_device_path *path); 获取从硬件目的地址到达真实设备的转发路径 ktime_t (*ndo_get_tstamp)(struct net_device *dev, const struct skb_shared_hwtstamps *hwtstamps, bool Cycles); 根据正常/可调时间或自由运行周期计数器获取硬件时间戳。 如果物理时钟支持自由运行的周期计数器，则需要此函数。 int (*ndo_hwtstamp_get)(struct net_device *dev, struct kernel_hwtstamp_config *kernel_config); 获取 NIC 设备当前配置的硬件时间戳参数。 int (*ndo_hwtstamp_set)(struct net_device *dev, struct kernel_hwtstamp_config *kernel_config, struct netlink_ext_ack *extack); 更改 NIC 设备的硬件时间戳参数

intel, ice, eth
static const struct net_device_ops ice_netdev_ops = {
	.ndo_open = ice_open,
	.ndo_stop = ice_stop,
	.ndo_start_xmit = ice_start_xmit,
	.ndo_select_queue = ice_select_queue,
	.ndo_features_check = ice_features_check,
	.ndo_fix_features = ice_fix_features,
	.ndo_set_rx_mode = ice_set_rx_mode,
	.ndo_set_mac_address = ice_set_mac_address,
	.ndo_validate_addr = eth_validate_addr,
	.ndo_change_mtu = ice_change_mtu,
	.ndo_get_stats64 = ice_get_stats64,
	.ndo_set_tx_maxrate = ice_set_tx_maxrate,
	.ndo_eth_ioctl = ice_eth_ioctl,
	.ndo_set_vf_spoofchk = ice_set_vf_spoofchk,
	.ndo_set_vf_mac = ice_set_vf_mac,
	.ndo_get_vf_config = ice_get_vf_cfg,
	.ndo_set_vf_trust = ice_set_vf_trust,
	.ndo_set_vf_vlan = ice_set_vf_port_vlan,
	.ndo_set_vf_link_state = ice_set_vf_link_state,
	.ndo_get_vf_stats = ice_get_vf_stats,
	.ndo_set_vf_rate = ice_set_vf_bw,
	.ndo_vlan_rx_add_vid = ice_vlan_rx_add_vid,
	.ndo_vlan_rx_kill_vid = ice_vlan_rx_kill_vid,
	.ndo_setup_tc = ice_setup_tc,
	.ndo_set_features = ice_set_features,
	.ndo_bridge_getlink = ice_bridge_getlink,
	.ndo_bridge_setlink = ice_bridge_setlink,
	.ndo_fdb_add = ice_fdb_add,
	.ndo_fdb_del = ice_fdb_del,
#ifdef CONFIG_RFS_ACCEL
	.ndo_rx_flow_steer = ice_rx_flow_steer,
#endif
	.ndo_tx_timeout = ice_tx_timeout,
	.ndo_bpf = ice_xdp,
	.ndo_xdp_xmit = ice_xdp_xmit,
	.ndo_xsk_wakeup = ice_xsk_wakeup,
};





const struct xdp_metadata_ops ice_xdp_md_ops = {
	.xmo_rx_timestamp		= ice_xdp_rx_hw_ts,
	.xmo_rx_hash			= ice_xdp_rx_hash,
	.xmo_rx_vlan_tag		= ice_xdp_rx_vlan_tag,
};


static const struct ethtool_ops ice_ethtool_ops = {
	.cap_rss_ctx_supported  = true,
	.supported_coalesce_params = ETHTOOL_COALESCE_USECS |
				     ETHTOOL_COALESCE_USE_ADAPTIVE |
				     ETHTOOL_COALESCE_RX_USECS_HIGH,
	.cap_rss_sym_xor_supported = true,
	.get_link_ksettings	= ice_get_link_ksettings,
	.set_link_ksettings	= ice_set_link_ksettings,
	.get_drvinfo		= ice_get_drvinfo,
	.get_regs_len		= ice_get_regs_len,
	.get_regs		= ice_get_regs,
	.get_wol		= ice_get_wol,
	.set_wol		= ice_set_wol,
	.get_msglevel		= ice_get_msglevel,
	.set_msglevel		= ice_set_msglevel,
	.self_test		= ice_self_test,
	.get_link		= ethtool_op_get_link,
	.get_eeprom_len		= ice_get_eeprom_len,
	.get_eeprom		= ice_get_eeprom,
	.get_coalesce		= ice_get_coalesce,
	.set_coalesce		= ice_set_coalesce,
	.get_strings		= ice_get_strings,
	.set_phys_id		= ice_set_phys_id,
	.get_ethtool_stats      = ice_get_ethtool_stats,
	.get_priv_flags		= ice_get_priv_flags,
	.set_priv_flags		= ice_set_priv_flags,
	.get_sset_count		= ice_get_sset_count,
	.get_rxnfc		= ice_get_rxnfc,
	.set_rxnfc		= ice_set_rxnfc,
	.get_ringparam		= ice_get_ringparam,
	.set_ringparam		= ice_set_ringparam,
	.nway_reset		= ice_nway_reset,
	.get_pauseparam		= ice_get_pauseparam,
	.set_pauseparam		= ice_set_pauseparam,
	.get_rxfh_key_size	= ice_get_rxfh_key_size,
	.get_rxfh_indir_size	= ice_get_rxfh_indir_size,
	.get_rxfh		= ice_get_rxfh,
	.set_rxfh		= ice_set_rxfh,
	.get_channels		= ice_get_channels,
	.set_channels		= ice_set_channels,
	.get_ts_info		= ice_get_ts_info,
	.get_per_queue_coalesce	= ice_get_per_q_coalesce,
	.set_per_queue_coalesce	= ice_set_per_q_coalesce,
	.get_fecparam		= ice_get_fecparam,
	.set_fecparam		= ice_set_fecparam,
	.get_module_info	= ice_get_module_info,
	.get_module_eeprom	= ice_get_module_eeprom,
};




enum ice_pf_flags {
	ICE_FLAG_FLTR_SYNC,
	ICE_FLAG_RDMA_ENA,
	ICE_FLAG_RSS_ENA,
	ICE_FLAG_SRIOV_ENA,
	ICE_FLAG_SRIOV_CAPABLE,
	ICE_FLAG_DCB_CAPABLE,
	ICE_FLAG_DCB_ENA,
	ICE_FLAG_FD_ENA,
	ICE_FLAG_PTP_SUPPORTED,		/* PTP is supported by NVM */
	ICE_FLAG_PTP,			/* PTP is enabled by software */
	ICE_FLAG_ADV_FEATURES,
	ICE_FLAG_TC_MQPRIO,		/* support for Multi queue TC */
	ICE_FLAG_CLS_FLOWER,
	ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA,
	ICE_FLAG_TOTAL_PORT_SHUTDOWN_ENA,
	ICE_FLAG_NO_MEDIA,
	ICE_FLAG_FW_LLDP_AGENT,
	ICE_FLAG_MOD_POWER_UNSUPPORTED,
	ICE_FLAG_PHY_FW_LOAD_FAILED,
	ICE_FLAG_ETHTOOL_CTXT,		/* set when ethtool holds RTNL lock */
	ICE_FLAG_LEGACY_RX,
	ICE_FLAG_VF_TRUE_PROMISC_ENA,
	ICE_FLAG_MDD_AUTO_RESET_VF,
	ICE_FLAG_VF_VLAN_PRUNING,
	ICE_FLAG_LINK_LENIENT_MODE_ENA,
	ICE_FLAG_PLUG_AUX_DEV,
	ICE_FLAG_UNPLUG_AUX_DEV,
	ICE_FLAG_MTU_CHANGED,
	ICE_FLAG_GNSS,			/* GNSS successfully initialized */
	ICE_FLAG_DPLL,			/* SyncE/PTP dplls initialized */
	ICE_PF_FLAGS_NBITS		/* must be last */
};



radix tree, 
/* Keep unconverted code working */
#define radix_tree_root		xarray
#define radix_tree_node		xa_node


struct xarray - XArray 的锚点。@xa_lock：保护 XArray 内容的锁。 要使用 xarray，请静态定义它或将其嵌入到数据结构中。 它是一个非常小的数据结构，因此单独分配它并在数据结构中保留指向它的指针通常没有意义。 您也可以使用 xa_lock 来保护您自己的数据结构。 */ 如果数组中的所有条目都是 NULL，则 @xa_head 是 NULL 指针。 如果数组中唯一的非 NULL 条目位于索引 0，则 @xa_head 就是该条目。 如果数组中的任何其他条目非 NULL，则 @xa_head 指向 @xa_node
struct xarray {
	spinlock_t	xa_lock;
/* private: The rest of the data structure is not to be used directly. */
	gfp_t		xa_flags;
	void __rcu *	xa_head;
};


struct xa_node {
	unsigned char	shift;		/* Bits remaining in each slot */
	unsigned char	offset;		/* Slot offset in parent */
	unsigned char	count;		/* Total entry count */
	unsigned char	nr_values;	/* Value entry count */
	struct xa_node __rcu *parent;	/* NULL at top of tree */
	struct xarray	*array;		/* The array we belong to */
	union {
		struct list_head private_list;	/* For tree user */
		struct rcu_head	rcu_head;	/* Used when freeing node */
	};
	void __rcu	*slots[XA_CHUNK_SIZE];
	union {
		unsigned long	tags[XA_MAX_MARKS][XA_MARK_LONGS];
		unsigned long	marks[XA_MAX_MARKS][XA_MARK_LONGS];
	};
};



dma,
Documentation/driver-api/dmaengine/client.rst


reject_tg
    nf_send_reset
        niph = nf_reject_iphdr_put(nskb, oldskb, IPPROTO_TCP, ip4_dst_hoplimit(skb_dst(nskb)))

br_nf_dev_queue_xmit
    br_nf_ip_fragment(net, sk, skb, br_nf_push_frag_xmit)
        ip_do_fragment



struct udev_monitor



bpf,
#define MAX_BPF_STACK	512
#define BPF_COMPLEXITY_LIMIT_INSNS      1000000 /* yes. 1M insns */ -> bpf：增加复杂性限制和最大程序大小，大型验证器速度改进允许增加验证器复杂性限制。 现在，无论程序组成及其大小如何，验证程序只需很少的时间即可达到 insn_processed 限制。 在典型的 x86 机器上，非调试内核在 1/10 秒内处理 1M 条指令。 （在这些速度改进之前，特制的程序可能会达到多秒的验证时间）对于相同的 1M insns，带有调试的完整 kasan 内核大约需要 1 秒。 因此，将 BPF_COMPLEXITY_LIMIT_INSNS 限制提高到 1M。 还将每个程序的指令数从 4k 增加到内部 BPF_COMPLEXITY_LIMIT_INSNS 限制。 4k 限制会让用户感到困惑，因为具有数百个 insns 的小程序可能会达到 BPF_COMPLEXITY_LIMIT_INSNS 限制。 有时添加更多 insns 和 bpf_trace_printk 调试语句将使验证程序接受该程序，而删除代码将使验证程序拒绝它。 一些用户空间应用程序开始将 #define MAX_FOO 添加到其程序中并执行： MAX_FOO=100; 再次：使用 MAX_FOO 进行编译； 尝试加载； if (fails_to_load) { 减少 MAX_FOO; 再次转到； } 能够将最大处理量放入单个程序中。 其他用户人为地将其单个程序拆分为一组程序，并使用 tail_calls 的所有 32 次迭代来增加计算限制。 最先进的人使用无限的 tc-bpf 过滤器列表来执行许多 bpf 程序。 本质上，用户设法解决了 4k insn 限制。 此补丁消除了 uapi 对 root 程序的限制。 BPF_COMPLEXITY_LIMIT_INSNS 是内核内部限制，加载程序的成功不再取决于程序大小，而仅取决于验证者的“智能”。 随着每个内核版本的发布，验证者将继续变得更加智能

enable test:
cd tools/testing/selftests/bpf/
make
sudo ./test_verifier

cd tools/bpf/bpftool/
make
sudo make install

进行pointer chasing时，使用bpf_probe_read()/bpf_core_read()会变得痛苦：
C
1
u64 inode = task->mm->exe_file->f_inode->i_ino;
你需要逐步的分配指针临时变量，逐步读取字段，非常麻烦。 幸运的是，CO-RE提供了助手宏：
u64 inode = BPF_CORE_READ(task, mm, exe_file, f_inode, i_ino);
 
// 或者
u64 inode;
BPF_CORE_READ_INTO(&inode, task, mm, exe_file, f_inode, i_ino);
类似的，和 bpf_probe_read_str()对应的CO-RE函数是 bpf_core_read_str()，以及助手宏 BPF_CORE_READ_STR_INTO()。

要检查字段是否在目标内核存在，可以使用 bpf_core_field_exists()宏：
pid_t pid = bpf_core_field_exists(task->pid) ? BPF_CORE_READ(task, pid) : -1;

errno,
Linux驱动开发头文件剖析（二十二）：<linux/errno.h>、<uapi/asm-generic/errno.h>、<uapi/asm-generic/errno-base.h>: https://zhuanlan.zhihu.com/p/694143739

static __attribute__((unused))
void perror(const char *msg)
{
	fprintf(stderr, "%s%serrno=%d\n", (msg && *msg) ? msg : "", (msg && *msg) ? ": " : "", errno);
}

glibc, https://elixir.bootlin.com/glibc/latest/source/string/strerror.c
#include <string.h>
#include <locale/localeinfo.h>

char *
strerror (int errnum)
{
  return __strerror_l (errnum, __libc_tsd_get (locale_t, LOCALE));
}


deep kernel, 
<asm-arch/ptrace.h
struct pt_regs { -> register

}

subsys_system_register
subsys_register
drivers/base/bus.c
int bus_register(const struct bus_type *bus)


pci bus,
include/linux/pci_ids.h
struct pci_bus {
    struct pci_bus	*parent;
}

struct pci_dev


struct pci_driver {
	const char		*name;
	const struct pci_device_id *id_table;	/* Must be non-NULL for probe to be called */
	int  (*probe)(struct pci_dev *dev, const struct pci_device_id *id);	/* New device inserted */
	void (*remove)(struct pci_dev *dev);	/* Device removed (NULL if not a hot-plug capable driver) */
	int  (*suspend)(struct pci_dev *dev, pm_message_t state);	/* Device suspended */
	int  (*resume)(struct pci_dev *dev);	/* Device woken up */
	void (*shutdown)(struct pci_dev *dev);
	int  (*sriov_configure)(struct pci_dev *dev, int num_vfs); /* On PF */
	int  (*sriov_set_msix_vec_count)(struct pci_dev *vf, int msix_vec_count); /* On PF */
	u32  (*sriov_get_vf_total_msix)(struct pci_dev *pf);
	const struct pci_error_handlers *err_handler;
	const struct attribute_group **groups;
	const struct attribute_group **dev_groups;
	struct device_driver	driver;
	struct pci_dynids	dynids;
	bool driver_managed_dma;
};
struct pci_driver - PCI驱动结构体，@name：驱动名称。 @id_table：指向驱动程序感兴趣的设备 ID 表的指针。大多数驱动程序应使用 MODULE_DEVICE_TABLE(pci,...) 导出此表。 @probe：对于与 ID 表匹配且尚未被其他驱动程序“拥有”的所有 PCI 设备，将调用此探测函数（在对现有设备执行 pci_register_driver() 期间或稍后插入新设备时）。 对于 ID 表中的条目与设备匹配的每个设备，此函数都会传递一个“struct pci_dev \*”。 当驱动程序选择获取设备的“所有权”时，探测函数返回零，否则返回错误代码（负数）。 探测函数总是从进程上下文中调用，因此它可以休眠。 @remove：每当删除此驱动程序处理的设备时（无论是在驱动程序注销期间还是在手动将其从热插拔插槽中拔出时），都会调用remove()函数。 删除函数总是从进程上下文中调用，因此它可以休眠。 @suspend：将设备置于低功耗状态。 @resume：将设备从低功耗状态唤醒。 （有关 PCI 电源管理和相关功能的说明，请参阅 Documentation/power/pci.rst。） @shutdown：挂接到reboot_notifier_list (kernel/sys.c)。 旨在停止任何空闲的 DMA 操作。 对于启用 LAN 唤醒 (NIC) 或在重新启动之前更改设备的电源状态非常有用。 例如 驱动程序/net/e100.c。 @sriov_configure：可选的驱动程序回调，允许配置通过 sysfs“sriov_numvfs”文件启用的 VF 数量。 @sriov_set_msix_vec_count：PF 驱动程序回调，用于更改 VF 上的 MSI-X 矢量数量。 通过 sysfs“sriov_vf_msix_count”触发。 这将更改 VF 消息控制寄存器中的 MSI-X 表大小。 @sriov_get_vf_total_msix：PF 驱动程序回调，用于获取可分配给 VF 的 MSI-X 向量总数。 @err_handler：请参阅 Documentation/PCI/pci-error-recovery.rst @groups：Sysfs 属性组。 @dev_groups：附加到设备的属性，一旦绑定到驱动程序，将创建该设备。 @driver：驱动程序模型结构。 @dynids：动态添加的设备 ID 列表。 @driver_management_dma：设备驱动程序不使用内核 DMA API 进行 DMA。 对于大多数设备驱动程序来说，只要所有 DMA 都是通过内核 DMA API 处理的，就无需关心此标志。 对于一些特殊的驱动程序，例如VFIO驱动程序，它们知道如何自己管理DMA并设置此标志，以便IOMMU层允许它们设置和管理自己的I/O地址空间


enum zone_type
    ZONE_DMA
    ZONE_NORMAL
    ZONE_HIGHMEM

typedef struct pglist_data


enum node_states {



sriov,
/*
 * num_vfs > 0; number of VFs to enable
 * num_vfs = 0; disable all VFs
 *
 * Note: SRIOV spec doesn't allow partial VF
 *       disable, so it's all or none.
 */
static ssize_t sriov_numvfs_store(struct device *dev,
				  struct device_attribute *attr,
				  const char *buf, size_t count)
{
	struct pci_dev *pdev = to_pci_dev(dev);
	int ret;
	u16 num_vfs;

	ret = kstrtou16(buf, 0, &num_vfs);
	if (ret < 0)
		return ret;

	if (num_vfs > pci_sriov_get_totalvfs(pdev))
		return -ERANGE;

	device_lock(&pdev->dev);

	if (num_vfs == pdev->sriov->num_VFs)
		goto exit;

	/* is PF driver loaded w/callback */
	if (!pdev->driver || !pdev->driver->sriov_configure) {
		pci_info(pdev, "Driver doesn't support SRIOV configuration via sysfs\n");
		ret = -ENOENT;
		goto exit;
	}

	if (num_vfs == 0) {
		/* disable VFs */
		ret = pdev->driver->sriov_configure(pdev, 0);
		goto exit;
	}

	/* enable VFs */
	if (pdev->sriov->num_VFs) {
		pci_warn(pdev, "%d VFs already enabled. Disable before enabling %d VFs\n",
			 pdev->sriov->num_VFs, num_vfs);
		ret = -EBUSY;
		goto exit;
	}

	ret = pdev->driver->sriov_configure(pdev, num_vfs);
	if (ret < 0)
		goto exit;

	if (ret != num_vfs)
		pci_warn(pdev, "%d VFs requested; only %d enabled\n",
			 num_vfs, ret);

exit:
	device_unlock(&pdev->dev);

	if (ret < 0)
		return ret;

	return count;
}


sriov_enable(struct pci_dev *dev, int nr_virtfn)
	pci_read_config_word(dev, iov->pos + PCI_SRIOV_INITIAL_VF, &initial);
	这里省略数行，主要是用来判断initial，total_VFs，nr_virtfn是否合法
	bus = pci_iov_virtfn_bus(dev, nr_virtfn - 1); //计算出来一个bus num分给VFs使用
			dev->bus->number + ((dev->devfn + dev->sriov->offset + dev->sriov->stride * vf_id) >> 8); 
	pci_enable_resources(dev, bars)；//设置VFs 配置空间的PCI_COMMAND字段
	pcibios_sriov_enable(dev, initial);
	pci_iov_set_numvfs(dev, nr_virtfn);/设置VFs number，读取offset，stride/
	iov->ctrl |= PCI_SRIOV_CTRL_VFE | PCI_SRIOV_CTRL_MSE;
	pci_write_config_word(dev, iov->pos + PCI_SRIOV_CTRL, iov->ctrl);/ VF Enable, VF Memory Space Enable/
	sriov_add_vfs(dev, initial);
		for (i = 0; i < num_vfs; i++) //逐个初始化，add VFs
			rc = pci_iov_add_virtfn(dev, i); //和pcie初始化的流程基本一致，分配填充结构体，初始化设备，add 设备
					virtfn_add_bus(dev->bus, pci_iov_virtfn_bus(dev, id));
					virtfn = pci_alloc_dev(bus);
					virtfn->devfn = pci_iov_virtfn_devfn(dev, id);
					virtfn->vendor = dev->vendor;
					virtfn->device = iov->vf_device;
					virtfn->is_virtfn = 1;
					virtfn->physfn = pci_dev_get(dev);
					if (id == 0)
						pci_read_vf_config_common(virtfn);
					pci_setup_device(virtfn); //这两个函数就是按照capability初始化设备
					中间省掉获取bar空间的代码，从PF上的SR-IVO的bar空间获取
					pci_device_add(virtfn, virtfn->bus);
                        list_add_tail(&dev->bus_list, &bus->devices)
					sprintf(buf, "virtfn%u", id);
					rc = sysfs_create_link(&dev->dev.kobj, &virtfn->dev.kobj, buf);
					rc = sysfs_create_link(&virtfn->dev.kobj, &dev->dev.kobj, "physfn");					
					kobject_uevent(&virtfn->dev.kobj, KOBJ_CHANGE);
					pci_bus_add_device(virtfn)
					kobject_uevent(&dev->dev.kobj, KOBJ_CHANGE);


tools/include/linux/kernel.h
#define container_of(ptr, type, member) ({			\
	const typeof(((type *)0)->member) * __mptr = (ptr);	\
	(type *)((char *)__mptr - offsetof(type, member)); })
#endif



include/linux/list.h
static inline void list_add


linux/container_of.h
#define container_of(ptr, type, member) ({				\
	void *__mptr = (void *)(ptr);					\
	static_assert(__same_type(*(ptr), ((type *)0)->member) ||	\
		      __same_type(*(ptr), void),			\
		      "pointer type mismatch in container_of()");	\
	((type *)(__mptr - offsetof(type, member))); })


#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))
__builtin_types_compatible_p 函数能判断a和b的类型是否相同，相同返回1，否则返回0

<init.h>
#define __init __attribute__ ((__section__ (".init.text"))) __cold
#define __initdata __attribute__ ((__section__ (".init.data")))


vmalloc是内核代码用来请求内存的接口函数，该内存在物理内存中不一定是连续的，但在虚拟内存中总是线性的。,<vmalloc.h> void *vmalloc(unsigned long size); 只需一个参数即可指定所需内存区域的大小 - 与前面讨论的函数相比，大小单位不是页而是字节，这在用户空间编程中很常见。 vmalloc 使用的最著名的例子是在内核的模块实现中。 由于模块可以随时加载，因此无法保证（特别是在系统已启动并运行很长时间的情况下）有足够的连续内存可用于有时大量的模块数据。 如果可以从较小的块中拼凑出足够的内存，则可以通过使用 vmalloc 来避免此问题。 vmalloc 还在内核中的大约 400 个其他位置被调用，特别是在设备和声音驱动程序中。 由于用于 vmalloc 的内存页面在任何情况下都必须主动映射到内核地址空间中，因此显然最好使用 ZONE_HIGHMEM 中的页面来实现此目的。 这允许内核保留更有价值的较低区域，而不会产生任何额外的缺点。 因此，vmalloc（以及第 3.5.8 节中讨论的映射函数）是内核能够将 highmem 页用于其自身目的（而不是用于用户空间应用程序）的少数场合之一。


PF_RANDOMIZE


static inline void flush_tlb_all(void); // 使所有的TLB表项失效

// 使指定用户地址空间的所有TLB表项失效，参数mm是进程的内存描述符
static inline void flush_tlb_mm(struct mm_struct *mm);

// 使指定用户地址空间的某个范围TLB表项失效，
// 参数vma虚拟内存区域，start是起始地址，end是结束地址。
static inline void flush_tlb_range(struct vm_area_struct *vma,
				   unsigned long start, unsigned long end);

// 使指定用户地址空间里面的指定虚拟页的TLB表项失效，
// 参数vma是虚拟内存区域，uaddr是一个虚拟页中的任意虚拟地址
static inline void flush_tlb_page(struct vm_area_struct *vma,
				  unsigned long uaddr);

// 使内核的某个虚拟地址范围的TLB表项失效，参数start是起始地址，end是结束地址
static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end);

// 内核把进程从一个处理器迁移到另一个处理器以后，调用该函数以更新页表缓存或上下文特定信息
void tlb_migrate_finish(struct mm_struct *mm);


// 使所有核的所有TLB表项失效
static inline void flush_tlb_all(void)
{
    // 确保屏障之前的存储指令执行完毕，dsb是数据同步的屏障
	dsb(ishst);
    // 是所有核上匹配VMID，阶段1和异常级别1的所有TLB表项失效
	__tlbi(vmalle1is);
    // 确保之前的TLB失效指令执行完毕，ish表示数据同步屏障指令对所有和起作用
	dsb(ish);
    // 指令同步屏障，冲刷处理器流水线，重新读取屏障指令后面的所有指令
	isb();
}

// 宏展开
static inline void flush_tlb_all(void)
{
     asm volatile("dsb ishst" : : : "memory");
     asm ("tlbi vmalle1is" : :);
     asm volatile("dsb ish" : : : "memory");
     asm volatile("isb" : : : "memory");
}



static u32 asid_bits; // 保存ASID长度
static DEFINE_RAW_SPINLOCK(cpu_asid_lock);

static atomic64_t asid_generation; // 高56位保存全局ASID版本号
static unsigned long *asid_map; // 位图asid_map记录哪些ASID被分配

static DEFINE_PER_CPU(atomic64_t, active_asids); // 保存处理器正在使用的ASID
static DEFINE_PER_CPU(u64, reserved_asids); // 保留的ASID，用来在全局ASID版本号加1时保存处理器正在执行的进程的ASID
static cpumask_t tlb_flush_pending;


__schedule() -> context_switch() -> switch_mm_irqs_off() -> switch_mm() -> check_and_switch_context()
arch/arm64/mm/context.c

巨型页,当运行内存需求量较大的应用程序时，如果使用长度为4KB的页，将会产生较多的TLB未命中和缺页异常，严重影响应用程序的性能。如果使用长度为2MB甚至更大的巨型页，可以大幅减少TLB未命中和缺页异常的数量，大幅提高应用程序的性能。这才是内核引入巨型页（Huge Page）的真正原因


// 全局变量hugetlb_max_hstate是巨型页池的数量
int hugetlb_max_hstate __read_mostly; -> 我们可以将经常需要被读取的数据定义为 __read_mostly类型，这样Linux内核被加载时，该数据将自动被存放到Cache中，以提高整个系统的执行效率。另一方面，如果所在的平台没有Cache，或者虽然有Cache，但并不提供存放数据的接口(也就是并不允许人工放置数据在Cache中)，这样定义为 __read_mostly类型的数据将不能存放在Linux内核中，甚至也不能够被加载到系统内存去执行，将造成Linux 内核启动失败
// 全局变量是默认巨型页池的索引
unsigned int default_hstate_idx;
// 全局数组hstates是巨型页池数组
struct hstate hstates[HUGE_MAX_HSTATE];


struct hstate {
    // 分配永久巨型页并添加到巨型页池中的时候，在允许的内存节点集合中轮流从每个内存节点分配永久巨型页，这个成员用来记录下次从哪个内存节点分配永久巨型页
	int next_nid_to_alloc;
    // 从巨型页池释放空闲巨型页的时候，在允许的内存节点集合中轮流从每个内存节点释放巨型页，这个成员用来记录下次从哪个内存节点释放巨型页
	int next_nid_to_free;
    
	unsigned int order; // 巨型页的长度，页的阶数
	unsigned long mask; // 巨型页页号的掩码，将虚拟地址和掩码按位与，得到巨型页页号
    
	unsigned long max_huge_pages; // 永久巨型页的最大数量
	unsigned long nr_huge_pages; // 巨型页的数量
	unsigned long free_huge_pages; // 空闲巨型页的数量
	unsigned long resv_huge_pages; // 预留巨型页的数量，它们已经承诺分配但还没有分配
	unsigned long surplus_huge_pages; // 临时巨型页的数量
	unsigned long nr_overcommit_huge_pages; // 临时巨型页的最大数量
	struct list_head hugepage_activelist;  // 每个内存节点一个空闲巨型页链表
	struct list_head hugepage_freelists[MAX_NUMNODES];  // 把已分配出去的巨型页链接起来
    
	unsigned int nr_huge_pages_node[MAX_NUMNODES]; // 每个内存节点中巨型页的数量
	unsigned int free_huge_pages_node[MAX_NUMNODES]; // 每个内存节点中空闲巨型页的数量
	unsigned int surplus_huge_pages_node[MAX_NUMNODES]; // 每个内存节点中临时巨型页的数量
#ifdef CONFIG_CGROUP_HUGETLB
	/* cgroup control files */
	struct cftype cgroup_files[5];
#endif
	char name[HSTATE_NAME_LEN]; // 巨型页池的名称，格式是“hugepages-<size>kB”
};

static struct file_system_type hugetlbfs_fs_type = {
     .name          = "hugetlbfs",
     .mount         = hugetlbfs_mount,
     .kill_sb       = kill_litter_super,
};


内核有一个透明巨型页线程（线程名称是khugepaged），定期地扫描允许使用透明巨型页的虚拟内存区域，尝试把普通页合并成巨型页


ovs, net：添加 Open vSwitch 内核组件。 Open vSwitch 是一款针对虚拟化环境的多层以太网交换机。 除了支持传统硬件交换机所期望的各种功能之外，它还支持细粒度的编程扩展和基于流的网络控制。 这种控制在各种应用程序中都很有用，但在多服务器虚拟化部署中尤其重要，多服务器虚拟化部署的特点通常是高度动态的端点以及需要维护多个租户的逻辑抽象。 Open vSwitch 数据路径为数据包转发提供了内核内快速路径。 它由用户空间守护进程 ovs-vswitchd 进行补充，该守护进程能够接受来自各种来源的配置并将其转换为数据包处理规则
commit, https://github.com/ssbandjl/linux/commit/ccb1352e76cff0524e7ccb2074826a092dd13016
Documentation/networking/openvswitch.rst


module_init(dp_init); -> static int __init dp_init(void)
ovs_vport_init


static struct vport_ops ovs_netdev_vport_ops = {
	.type		= OVS_VPORT_TYPE_NETDEV,
	.create		= netdev_create,
	.destroy	= netdev_destroy,
	.send		= dev_queue_xmit,
};

static struct vport *netdev_create
    ovs_vport_alloc(0, &ovs_netdev_vport_ops, parms)
    ovs_netdev_link
        netdev_tracker_alloc(vport->dev, &vport->dev_tracker, GFP_KERNEL)
        netdev_master_upper_dev_link
        netdev_rx_handler_register(vport->dev, netdev_frame_hook, vport)
        dev_disable_lro(vport->dev) -> openvswitch：将 dev 指针移至 vport 本身，这是将所有 OVS vport 表示为常规 struct net_devices 的第一步。 将 net_device 指针移至 vport 结构本身以摆脱 struct vport_netdev -> 禁用网络设备上的大型接收卸载 (LRO)。 必须在 RTNL 下调用。 如果接收到的数据包可能转发到另一个接口，则需要这样做
        dev_set_promiscuity(vport->dev, 1) -> 开启混杂模式, 打标记等
        


struct vport {
	struct net_device *dev;
	netdevice_tracker dev_tracker;
	struct datapath	*dp;
	struct vport_portids __rcu *upcall_portids;
	u16 port_no;

	struct hlist_node hash_node;
	struct hlist_node dp_hash_node;
	const struct vport_ops *ops;
	struct vport_upcall_stats_percpu __percpu *upcall_stats;

	struct list_head detach_list;
	struct rcu_head rcu;
};

enum ovs_vport_type {
	OVS_VPORT_TYPE_UNSPEC,
	OVS_VPORT_TYPE_NETDEV,   /* network device */
	OVS_VPORT_TYPE_INTERNAL, /* network device implemented by datapath */
	OVS_VPORT_TYPE_GRE,      /* GRE tunnel. */
	OVS_VPORT_TYPE_VXLAN,	 /* VXLAN tunnel. */
	OVS_VPORT_TYPE_GENEVE,	 /* Geneve tunnel. */
	__OVS_VPORT_TYPE_MAX
};


这是key值，主要是提取数据包中协议相关信息，这是后期要进行流表匹配的关键结构
struct sw_flow_key {



ovs_vport_send


netdev_frame_hook
    netdev_port_receive(skb)
        ovs_netdev_get_vport
            rcu_dereference_rtnl - 带调试检查的 rcu_dereference，@p：要读取的指针，在取消引用之前执行 rcu_dereference(p)，但检查调用者是否持有 rcu_read_lock() 或 RTNL。 注意：请优先选择 rtnl_dereference() 或 rcu_dereference()
        skb = skb_share_check(skb, GFP_ATOMIC) -> 检查缓冲区是否共享的，如果是就克隆它
        ovs_vport_receive(vport, skb, skb_tunnel_info(skb)) -> 将接收到的数据包传递到数据路径进行处理 -> 路由：将lwtunnel状态移动到dst_entry，目前，lwtunnel状态驻留在每个协议数据中。 如果我们将 ipv6 流量封装在 ipv4 隧道中（反之亦然），就会出现问题。 隧道的xmit功能不知道数据包是通过ipv4还是ipv6路由到它的，但它需要lwtstate数据。 将 lwtstate 数据移动到 dst_entry 使得这种协议间隧道成为可能。 作为奖励，这带来了一个很好的差异统计 -> 获得隧道相关信息，比如隧道ID
            unlikely
                skb_scrub_packet(skb, true)
            ovs_flow_key_extract(tun_info, skb, &key)
                IS_ENABLED(CONFIG_NET_TC_SKB_EXT) -> net: openvswitch：从 tc 链索引设置 OvS recirc_id，卸载的 OvS 数据路径规则一对一转换为 tc 规则，例如以下简化的 OvS 规则：recirc_id(0),in_port(dev1),eth_type(0x0800),ct_state( -trk) actions:ct(),recirc(2) 将被翻译成以下 tc 规则： $ tc filter add dev dev1 ingress \ prio 1 chain 0 proto ip \ Flower tcp ct_state -trk \ action ct pipeline \ action goto chain 2 接收到的数据包将首先通过 tc 传输，如果它们没有被 tc 窃取（如上述规则所示），它们将继续通过 OvS 数据路径。 由于我们已经执行了一些可能修改数据包的操作（在本例中为操作 ct），并更新了操作统计信息，因此我们希望在 OvS 中使用正确的 recirc_id（此处为 recirc_id(2)）继续处理我们中断的位置。 为了支持这一点，为 tc 引入一个新的 skb 扩展，它将用于将 tc 链转换为 ovs recirc_id 以处理这些丢失情况。 最后一个 tc 链索引将由 tc goto 链操作设置并由 OvS 数据路径读取
                if (tun_info)
                    ip_tunnel_info_af
                    ip_tunnel_info_opts_get
                key->phy.priority = skb->priority
                key->ovs_flow_hash = 0 -> openvswitch：从 flow_extract 中消除 memset()。随着新协议的添加，流密钥的大小往往会增加，尽管很少有协议关心所有字段。 为了优化散列和匹配，OVS 使用密钥的可变长度部分。 然而，当从数据包中提取字段时，我们仍然必须将整个密钥清零。 现在 OVS 实现了屏蔽，这就不再需要了。 根据定义，不属于给定协议一部分的任何字段（或结构中的漏洞）将不是掩码的一部分，并在查找期间被清零。 此外，由于屏蔽已经使用可变长度密钥，因此归零操作也会自动受益。 原则上，此时唯一需要做的就是在流程开始时删除 memset() 。 但是，某些字段假定它们初始化为零，现在必须显式完成。 此外，如果出现错误，我们还必须将相应字段清零，以表明不存在有效数据。 这些增加了代码总量，但在无错误情况下执行的代码很少。 当在 10G 链路上使用大数据包进行测试时，删除 memset() 将 ovs_flow_extract() 的配置文件从 0.64% 降低到 0.56%
                res = key_extract_mac_proto(skb) -> openvswitch：添加对L3数据包的处理，支持L3数据包（不带以太网头的数据包）的接收、提取流密钥和发送。 请注意，即使在此补丁之后，仍然不允许将非以太网接口添加到网桥中。 同样，用于向/从用户空间发送和接收 L3 数据包的 netlink 接口尚未到位。 基于 Lorand Jakab 和 Simon Horman 之前的版本
                key_extract(skb, key) -> openvswitch：将原始方向conntrack元组添加到sw_flow_key，将conntrack原始方向五元组的字段添加到struct sw_flow_key。 新字段最初被标记为不存在，并且每当执行 conntrack 操作并查找或生成 conntrack 条目时都会填充新字段。 这意味着这些字段对于所有未被 conntrack 拒绝为不可跟踪的数据包都存在。 sw_flow_key中的原始元组字段是从与当前数据包相关的conntrack条目的原始方向元组填充的，或者如果当前conntrack条目有主控，则从主conntrack条目的原始方向元组填充。 通常，具有指定帮助程序（例如 FTP）的连接的预期连接具有主 conntrack 条目。 新的 conntrack 原始元组字段的主要目的是允许出于策略决策目的对其进行匹配，前提是跟踪的连接回复数据包（以及原始方向数据包）以及任何相关连接的双向数据包的可接受性 基于应用于主连接原始方向 5 元组的 ACL 规则。 当实际数据包标头可能已被 NAT 转换时，这也使得更容易做出策略决策，因为原始方向 5 元组表示任何此类转换之前的数据包标头。 当使用原始方向 5 元组时，返回和/或相关数据包的允许性不需要仅基于 conntrack 条目的存在，从而允许将准入策略与已建立的 conntrack 状态分离。 虽然需要存在 conntrack 条目来允许返回或相关数据包，但策略更改可能会导致最初允许的连接随后被拒绝或丢弃。 如果返回和相关数据包的准入仅基于 conntrack 状态（例如，连接处于已建立状态），则导致连接被拒绝或丢弃的策略更改将需要查找并删除受此类影响的所有 conntrack 条目。 改变。 当使用原始方向 5 元组匹配时，可以允许受影响的 conntrack 条目超时，因为连接的已建立状态不再需要作为数据包准入的基础。 需要注意的是，相关连接的方向性可以与主连接的方向性相同或不同，并且原始方向五元组和conntrack状态位均不携带该信息。 如果需要，主连接的方向性可以存储在主连接的 conntrack 标记或标签中，这些标记或标签会自动由预期的相关连接继承。 conntrack 无法跟踪 ARP 和 ND 数据包的事实允许 ARP/ND 与新的 conntrack 原始元组字段之间的互斥。 因此，IP 地址与 ARP 和 ND 字段联合叠加。 这使得 sw_flow_key 不会因为这个补丁而增长太多，但这也意味着我们必须小心，不要在 ARP 或 ND 数据包中使用新的密钥字段。 ARP 很容易根据以太网类型区分并保持互斥，但 ND 作为 ICMPv6 协议需要多加注意
                no err ->  ovs_ct_fill_key(skb, key, post_ct)
            ovs_dp_process_packet -> openvswitch：仅对出口隧道路径使用tun_key。当前tun_key用于传递入口和出口路径上的隧道信息，这会导致混乱。 以下补丁删除了其在入口路径上的使用，使其成为仅出口参数
                flow = ovs_flow_tbl_lookup_stats(&dp->table, key, skb_get_hash(skb), &n_mask_hit, &n_cache_hit) -> net: openvswitch: 添加流掩码缓存以提高性能，此优化的想法来自于 openvswitch 社区于 2014 年提交的一个补丁。 作者是 Pravin B Shelar。 为了获得高性能，我再次实现了它。 以后的补丁会用到它。 Pravin B Shelar 说： 在每个数据包上，OVS 都需要使用每个 | 查找流表。 掩码，直到找到匹配项。 数据包流密钥是第一个 | 用列表中的 mask 进行掩码，然后掩码键为 | 在流表中查找。 因此mask的数量可以| 影响数据包处理性能 -> mask_cache 将流映射到可能的掩码。 该缓存不是紧密耦合的缓存，这意味着掩码列表的更新可能会导致掩码缓存中的缓存条目不一致。 这是每个 cpu 缓存，分为 MC_HASH_SEGS 段。 如果发生哈希冲突，该条目将在下一个段中进行哈希处理
                    unlikely no cache -> return flow_lookup(tbl, ti, ma, key, n_mask_hit, &cache, &mask_index)
                    skb_hash = jhash_1word(skb_hash, key->recirc_id) -> Simple Jenkins hash
                    for (seg = 0; seg < MC_HASH_SEGS; seg++)
                        flow = flow_lookup(tbl, ti, ma, key, n_mask_hit, n_cache_hit, &e->mask_index)
                    flow = flow_lookup(tbl, ti, ma, key, n_mask_hit, n_cache_hit, &ce->mask_index) -> cache miss -> net: openvswitch: addmaskscachehitcounter，添加一个统计maskscache命中次数的计数器，通过megaflownetlink统计导出 -> 流查找对流表进行完整查找。 它以 *index 中传递的索引掩码开始。 由于使用了 CPU 特定变量，因此必须在禁用 BH 的情况下调用此函数
                        if (likely(*index < ma->max))
                            masked_flow_lookup
                                ovs_flow_mask_key(&masked_key, unmasked, false, mask) -> openvswitch：分配时零流量，当引入对巨型流的支持时，OVS 需要开始安装应用了掩码的流。 由于掩码是一项昂贵的操作，OVS 还进行了优化，仅采用非零掩码覆盖的流键部分。 存储在其余部分中的值应该无关紧要，因为它们被屏蔽了。 虽然这对于匹配的目的来说效果很好（必须始终查看掩码），但序列化到 netlink 可能会出现问题。 由于流和掩码是分开序列化的，因此流的未初始化部分可以用恰好存在的任何值进行编码。 就功能而言，这几乎没有影响，因为这些字段将根据定义被屏蔽。 然而，它会将内核内存泄漏到用户空间，这是一个潜在的安全漏洞。 其他代码路径也有可能查看屏蔽键并获取未初始化的数据，尽管目前这在实践中似乎不是问题。 这消除了正在安装的流的掩码优化。 这始终是这种情况，因为掩码优化实际上针对的是每个数据包的流操作
                                hash = flow_hash(&masked_key, &mask->range)
                                    jhash2(hash_key, hash_u32s, 0)
                                head = find_bucket(ti, hash) -> openvswitch：将巨型流列表移出重新哈希结构。ovs-flow 重新哈希不会触及巨型流列表。 以下补丁将其移动到 dp 结构数据路径。 避免在每个数据包接收时访问巨型流列表头的额外间接操作
                                    hash = jhash_1word(hash, ti->hash_seed);
                                    &ti->buckets[hash & (ti->n_buckets - 1)]
                                hlist_for_each_entry_rcu(flow, head, flow_table.node[ti->node_ver],
                                    flow_cmp_masked_key(flow, &masked_key, &mask->range))
                                        cmp_key(&flow->key, key, range->start, range->end)
                ovs_flow_stats_update(flow, key->tp.flags, skb)
                    spin_lock_init(&new_stats->lock);
                    rcu_assign_pointer(flow->stats[cpu], new_stats)
                    cpumask_set_cpu(cpu, flow->cpu_used_mask);
                ovs_execute_actions(dp, skb, sf_acts, key)
                    err = do_execute_actions(dp, skb, key, acts->actions, acts->actions_len)
                        for (a = attr, rem = len; rem > 0;
                            switch (nla_type(a))
                            case OVS_ACTION_ATTR_OUTPUT: {
                            ...
                    if (level == 1) -> ovs：限制 ovs_execute_actions 中的 ovs 递归以免损坏堆栈，我们发现 openvswitch 的有缺陷的配置可能会覆盖 STACK_END_MAGIC 并导致内核硬崩溃，因为 ovs 内的递归过多。 该问题是由于 openvswitch 的堆栈使用率较高而产生的。 内核的其余部分在当前限制 10 (RECURSION_LIMIT) 下就可以了。 我们使用 ovs_execute_actions 中已有的递归计数器来实现 5 次递归的上限
                u64_stats_update_begin(&stats->syncp)
                u64_stats_update_end(&stats->syncp)





static struct pernet_operations ovs_net_ops = {
	.init = ovs_init_net,
	.exit = ovs_exit_net,
	.id   = &ovs_net_id,
	.size = sizeof(struct ovs_net),
};


struct notifier_block ovs_dp_device_notifier = {
	.notifier_call = dp_device_event
};

dp_device_event
    ovs_netdev_get_vport
    ovs_netdev_detach_dev


ovs action, 
enum ovs_action_attr {
	OVS_ACTION_ATTR_UNSPEC,
	OVS_ACTION_ATTR_OUTPUT,	      /* u32 port number. */
	OVS_ACTION_ATTR_USERSPACE,    /* Nested OVS_USERSPACE_ATTR_*. */
	OVS_ACTION_ATTR_SET,          /* One nested OVS_KEY_ATTR_*. */
	OVS_ACTION_ATTR_PUSH_VLAN,    /* struct ovs_action_push_vlan. */
	OVS_ACTION_ATTR_POP_VLAN,     /* No argument. */
	OVS_ACTION_ATTR_SAMPLE,       /* Nested OVS_SAMPLE_ATTR_*. */
	OVS_ACTION_ATTR_RECIRC,       /* u32 recirc_id. */
	OVS_ACTION_ATTR_HASH,	      /* struct ovs_action_hash. */
	OVS_ACTION_ATTR_PUSH_MPLS,    /* struct ovs_action_push_mpls. */
	OVS_ACTION_ATTR_POP_MPLS,     /* __be16 ethertype. */
	OVS_ACTION_ATTR_SET_MASKED,   /* One nested OVS_KEY_ATTR_* including
				       * data immediately followed by a mask.
				       * The data must be zero for the unmasked
				       * bits. */
	OVS_ACTION_ATTR_CT,           /* Nested OVS_CT_ATTR_* . */
	OVS_ACTION_ATTR_TRUNC,        /* u32 struct ovs_action_trunc. */
	OVS_ACTION_ATTR_PUSH_ETH,     /* struct ovs_action_push_eth. */
	OVS_ACTION_ATTR_POP_ETH,      /* No argument. */
	OVS_ACTION_ATTR_CT_CLEAR,     /* No argument. */
	OVS_ACTION_ATTR_PUSH_NSH,     /* Nested OVS_NSH_KEY_ATTR_*. */
	OVS_ACTION_ATTR_POP_NSH,      /* No argument. */
	OVS_ACTION_ATTR_METER,        /* u32 meter ID. */
	OVS_ACTION_ATTR_CLONE,        /* Nested OVS_CLONE_ATTR_*.  */
	OVS_ACTION_ATTR_CHECK_PKT_LEN, /* Nested OVS_CHECK_PKT_LEN_ATTR_*. */
	OVS_ACTION_ATTR_ADD_MPLS,     /* struct ovs_action_add_mpls. */
	OVS_ACTION_ATTR_DEC_TTL,      /* Nested OVS_DEC_TTL_ATTR_*. */

	__OVS_ACTION_ATTR_MAX,	      /* Nothing past this will be accepted
				       * from userspace. */
};



modprobe vfio, vfio.ko
static struct vfio {
	struct class			*device_class;
	struct ida			device_ida;
} vfio; -> VFIO 是一种安全的用户级驱动程序，可与虚拟机和用户级驱动程序一起使用。 VFIO 利用 IOMMU 组来确保使用中的设备的隔离，从而允许非特权用户访问。 VFIO 旨在取代 KVM 设备分配和 UIO 驱动程序（在目标平台包含功能足够的 IOMMU 的情况下）。 此版本 VFIO 的新增功能是支持通过 IOMMU 核心管理的 IOMMU 组，并对 API 进行了修改，删除了组合并接口。 我们现在回到一个更类似于原始 VFIO 且支持 UIOMMU 的模型，其中从 /dev/vfio/vfio 获取的文件描述符允许访问 IOMMU，但仅在添加组之后，避免了以前此类类型的权限问题 模型。 IOMMU 支持现在也是完全模块化的，因为 IOMMU 在不同平台上具有截然不同的接口要求。 VFIO 用户能够查询和初始化他们选择的 IOMMU 模型。 请参阅后续文档提交以获取进一步的描述和使用示例
commit, https://github.com/ssbandjl/linux/commit/cba3345cc494ad286ca8823f44b2c16cae496679
module_init(vfio_init)
    ida_init(&vfio.device_ida)
    vfio_group_init()
        INIT_LIST_HEAD(&vfio.group_list)
        vfio_container_init
            INIT_LIST_HEAD(&vfio.iommu_drivers_list)
            misc_register(&vfio_dev)
            if (IS_ENABLED(CONFIG_VFIO_NOIOMMU))
                vfio_register_iommu_driver(&vfio_noiommu_ops)
                    list_add(&driver->vfio_next, &vfio.iommu_drivers_list)
        vfio.class = class_create("vfio") -> /dev/vfio/$GROUP
        alloc_chrdev_region(&vfio.group_devt, 0, MINORMASK + 1, "vfio") -> register_chrdev_region 需要开发者指定设备的主设备号，而 alloc_chrdev_region 则是由内核自动分配主设备号
    vfio_virqfd_init() -> create_singlethread_workqueue("vfio-irqfd-cleanup")
    vfio.device_class = class_create("vfio-dev")
    vfio_cdev_init(vfio.device_class)
    alloc_chrdev_region(&device_devt, 0, MINORMASK + 1, "vfio-dev")
    vfio_debugfs_create_root -> vfio_debugfs_root = debugfs_create_dir("vfio", NULL)



static struct miscdevice vfio_dev = {
	.minor = VFIO_MINOR,
	.name = "vfio",
	.fops = &vfio_fops,
	.nodename = "vfio/vfio",
	.mode = S_IRUGO | S_IWUGO,
};

static const struct file_operations vfio_fops = {
	.owner		= THIS_MODULE,
	.open		= vfio_fops_open,
        struct vfio_container *container
        container = kzalloc(sizeof(*container)
        INIT_LIST_HEAD(&container->group_list)
	.release	= vfio_fops_release,
	.unlocked_ioctl	= vfio_fops_unl_ioctl,
        switch (cmd)
        case VFIO_SET_IOMMU
            vfio_ioctl_set_iommu(container, arg)
                list_for_each_entry(driver, &vfio.iommu_drivers_list, vfio_next)
                    driver->ops->ioctl(NULL, VFIO_CHECK_EXTENSION, arg)
                    data = driver->ops->open(arg)
                    __vfio_container_attach_groups
                        driver->ops->attach_group
        default
            if (driver)
                ret = driver->ops->ioctl(data, cmd, arg) -> passthrough
	.compat_ioctl	= compat_ptr_ioctl,
};


static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
	.name			= "vfio-iommu-type1",
	.owner			= THIS_MODULE,
	.open			= vfio_iommu_type1_open,
	.release		= vfio_iommu_type1_release,
	.ioctl			= vfio_iommu_type1_ioctl,
        case VFIO_IOMMU_GET_INFO
            vfio_iommu_type1_get_info
                vfio_iommu_migration_build_caps
                    cap_mig.header.id = VFIO_IOMMU_TYPE1_INFO_CAP_MIGRATION
                vfio_iommu_dma_avail_build_caps
                vfio_iommu_iova_build_caps
                    vfio_iommu_iova_add_cap
                vfio_info_cap_shift
        case VFIO_IOMMU_MAP_DMA
            vfio_iommu_type1_map_dma
                vfio_dma_do_map(iommu, &map)
	.attach_group		= vfio_iommu_type1_attach_group,
        vfio_iommu_find_iommu_group(iommu, iommu_group)
        iommu_group_for_each_dev(iommu_group, &domain->domain,vfio_iommu_domain_alloc)
        iommu_enable_nesting
        iommu_attach_group -> __iommu_attach_group
            dev = iommu_group_first_dev(group)
            __iommu_group_set_domain
                __iommu_attach_device
                    domain->ops->attach_dev(domain, dev)
	.detach_group		= vfio_iommu_type1_detach_group,
	.pin_pages		= vfio_iommu_type1_pin_pages,
	.unpin_pages		= vfio_iommu_type1_unpin_pages,
	.register_device	= vfio_iommu_type1_register_device,
	.unregister_device	= vfio_iommu_type1_unregister_device,
	.dma_rw			= vfio_iommu_type1_dma_rw,
	.group_iommu_domain	= vfio_iommu_type1_group_iommu_domain,
};


static const struct vfio_iommu_driver_ops vfio_noiommu_ops = {
	.name = "vfio-noiommu",
	.owner = THIS_MODULE,
	.open = vfio_noiommu_open,
	.release = vfio_noiommu_release,
	.ioctl = vfio_noiommu_ioctl,
	.attach_group = vfio_noiommu_attach_group,
	.detach_group = vfio_noiommu_detach_group,
};


include/uapi/linux/vfio.h

vfio_pci_sriov_configure(struct pci_dev *pdev, int nr_virtfn) -> vfio_pci_core_sriov_configure(vdev, nr_virtfn)
    if (nr_virtfn)
        list_add_tail(&vdev->sriov_pfs_item, &vfio_pci_sriov_pfs)
        pm_runtime_resume_and_get -> 使用运行时 PM 将未使用的设备移至低功耗状态 -> PF 功率状态应始终高于 VF 功率状态。PF 可以通过运行时电源管理（当没有用户时）或用户写入 PCI_PM_CTRL 寄存器来处于低功耗状态。如果 PF 处于低功耗状态，则在启用 SR-IOV 之前先将功率状态更改为 D0。此外，此函数可以随时调用，并且用户空间 PCI_PM_CTRL 写入可以与此代码路径竞争，因此请使用“memory_lock”进行保护
            __pm_runtime_resume
        vfio_pci_set_power_state(vdev, PCI_D0) -> vfio/pci：在启用 VF 之前将 PF 电源状态更改为 D0，根据 [PCIe v5 9.6.2] 中 PF 设备电源管理状态的规定，“PF 的电源管理状态 (D 状态) 对其关联的 VF 具有全局影响。如果 VF 未实现电源管理功能，则其行为就像处于其关联 PF 的等效电源状态一样。如果 VF 实现了电源管理功能，则如果 PF 处于比 VF 更低的电源状态，则设备行为未定义。软件应避免这种情况，方法是在降低其关联 PF 的电源状态之前将所有 VF 置于较低电源状态。”从 vfio 驱动程序方面，用户可以在 PF 处于 D3hot 状态时启用 SR-IOV。如果 VF 未实现电源管理功能，则 VF 实际上将处于 D3hot 状态，然后 VF BAR 访问将失败。如果 VF 实现了电源管理功能，那么当 PF 处于 D3hot 状态时，VF 将假定其当前电源状态为 D0，在这种情况下，行为未定义。为了支持 PF 电源管理，我们需要在 PF 和其 VF 之间创建电源管理依赖关系。运行时电源管理支持可能有助于实现这一点，其中通过设备链接支持电源管理依赖关系。但是，在我们有这样的支持之前，如果 PF 启用了 VF，我们可以禁止 PF 进入低功耗状态。可能存在一种情况，用户首先启用 VF，然后禁用 VF。如果没有 PF 用户，那么 PF 可以再次进入 D3hot 状态。但是使用此补丁，在禁用 VF 后，PF 仍将处于 D0 状态，因为在 vfio_pci_core_sriov_configure() 中检测这种情况需要访问 struct vfio_device::open_count 及其锁。但是，与运行时 PM 相关的后续补丁将处理这种情况，因为运行时 PM 维护其自己的使用计数。此外，vfio_pci_core_sriov_configure() 可以随时调用（有或没有 vfio pci 设备用户），因此需要使用所需的锁来保护电源状态更改和 SR-IOV 启用
        pci_enable_sriov
            might_sleep
            sriov_enable(dev, nr_virtfn)
    pci_disable_sriov


vfio_df_ioctl_bind_iommufd


static const struct file_operations vfio_group_fops = {
	.owner		= THIS_MODULE,
	.unlocked_ioctl	= vfio_group_fops_unl_ioctl,
	.compat_ioctl	= compat_ptr_ioctl,
	.open		= vfio_group_fops_open,
	.release	= vfio_group_fops_release,
};

vfio_group_fops_unl_ioctl
    switch (cmd)
    case VFIO_GROUP_GET_DEVICE_FD
        vfio_group_ioctl_get_device_fd
            vfio_device_open_file
                vfio_df_group_open
                    vfio_df_open
                        vfio_df_device_first_open
                            device->ops->open_device(device) -> vfio_pci_open_device
                                vfio_pci_core_enable
                                    pci_enable_device
                                    pci_try_reset_function
                                    pci_read_config_word(pdev, PCI_COMMAND, &cmd)
                                    vfio_pci_zdev_open_device -> vfio-pci/zdev：添加打开/关闭设备挂钩，在vfio-pci open_device期间，传递与vfio组关联的KVM（如果存在）。 这是为了将特殊指示符 (GISA) 传递给固件，以允许 zPCI 解释工具仅用于与 vfio-pci 设备关联的特定 KVM。 在 vfio-pci close_device 期间，取消注册通知程序
                                        return zpci_kvm_hook.kvm_register(zdev, vdev->vdev.kvm) -> 启用 zPCI 指令的解释执行 + 适配器中断、s390x KVM vfio-pci 的转发。 这是通过在 VFIO 组与 KVM 来宾关联时触发例程、向固件传输特殊令牌（GISA 名称）以使该特定来宾能够在该 zPCI 设备上解释执行来完成的。 然后，加载/存储解释启用由用户空间控制（基于 SHM 位是否放置在虚拟函数句柄中）。 适配器事件通知解释是通过新的 KVM ioctl 从用户空间控制的。 通过允许解释 zPCI 指令以及向 guest 虚拟机传送中断的固件，我们可以降低 zPCI 的 guest SIE 退出频率。 从 guest 配置的角度来看，您可以按照与以前相同的方式传递 zPCI 设备，并且默认情况下使用解释支持（如果在 kernel+qemu 中可用）。 将跟进更新的 QEMU 系列的链接。 变更日志 v7->v8： - 修复 ioctl 文档 (Thomas) - 从 ioctl 中删除 copy_to_user，来自旧版本 (Thomas) - KVM_S390_ZPCIOP_REG_AEN：未定义标志失败 (Thomas) - kvm_s390_pci_zpci_reg_aen：清理主机标志设置 (Thomas) 并修复意外的错误 位反转 - CONFIG_VFIO_PCI_ZDEV_KVM：添加意外遗漏在帮助文本中的“say Y”（Jason） - 重构“vfio：删除 VFIO_GROUP_NOTIFY_SET_KVM”之上的 vfio-pci-zdev 片段（Jason） - open_device/ close_device 现在将直接调用 kvm 注册例程。 将 open_device 调用移至 vfio_pci_core_enable，以便可以传播错误。 对于奇偶校验，将 close_device 调用移至 vfio_pci_core_disable -> commit, https://lwn.net/Articles/897230/
                                    vfio_config_init -> 对于每个设备，我们分配一个 pci_config_map 来指示占用每个双字的能力，从而指示我们用于读写的 struct perm_bits。 我们还分配一个虚拟化配置空间，用于跟踪我们为用户模拟的位的读取和写入。 从设备填充初始值。 在所有 vfio-pci 设备之间使用共享 struct perm_bits 可以使我们免于为每个设备的 virt 和 write 分配 cfg_size 缓冲区。 我们可以删除 vconfig 并为需要模拟位的每个区域分配单独的缓冲区，但指针数组的大小将相当（至少对于标准配置空间）-> 关键数据结构, struct vfio_pci_core_device, 将pci硬件能力和配置通过vfio的pci设备暴露给上层
                                        map = kmalloc(pdev->cfg_size, GFP_KERNEL_ACCOUNT) -> 配置空间、caps 和 ecaps 都是双字对齐的，因此我们可以使用每个双字一个字节来记录类型。 但能力的长度没有要求，所以能力之间的差距需要字节粒度
                                        vfio_fill_vconfig_bytes -> vfio：添加PCI设备驱动，添加PCI设备对VFIO的支持。 PCI 设备公开用于访问设备的配置空间、I/O 端口空间和 MMIO 区域的区域。 PCI 配置访问在内核中虚拟化，使我们能够通过防止各种访问同时减少跨各种用户空间驱动程序的重复支持来确保系统的完整性。 I/O 端口支持读/写访问，而 MMIO 还支持足够大小区域的 mmap。 使用 eventfds 向用户空间提供对 INTx、MSI 和 MSI-X 中断的支持 -> commit, https://github.com/ssbandjl/linux/commit/89e1f7d4c66d85f42c3d52ea3866eb10cadf6153
                                        vdev->rbar[0] = le32_to_cpu(*(__le32 *)&vconfig[PCI_BASE_ADDRESS_0]);
                                        ...
                                        vfio_cap_init
                                        vfio_ecap_init
                                    vdev->has_dyn_msix = pci_msix_can_alloc_dyn(pdev)
                                vfio_pci_core_finish_enable
                                    vfio_pci_probe_mmaps
                                        for (i = 0; i < PCI_STD_NUM_BARS; i++) -> 6
                                            dummy_res->resource.name = "vfio sub-page reserved"
                                    eeh_dev_open -> eeh_dev_open - 增加 PE 的直通设备计数，@pdev：PCI 设备 增加指定 PE 的直通设备计数。 结果，在 PE 上检测到的 EEH 错误将不会被报告。 PE 所有者将负责检测和恢复 -> Extended Error Handling -> 基于 IBM POWER 的 pSeries 和 iSeries 计算机包括 PCI 总线控制器芯片，这些芯片具有检测和报告各种 PCI 总线错误情况的扩展功能。 这些功能的名称为“EEH”，即“扩展错误处理”。 EEH 硬件功能允许清除 PCI 总线错误并“重新启动”PCI 卡，而无需重新启动操作系统。 这与传统的 PCI 错误处理相反，在传统 PCI 错误处理中，PCI 芯片直接连接到 CPU，错误会导致 CPU 机器检查/检查停止条件，从而完全停止 CPU。 另一种“传统”技术是忽略此类错误，这可能导致用户数据或内核数据的数据损坏、适配器挂起/无响应或系统崩溃/锁定。 因此，EEH 背后的想法是，通过保护操作系统免受 PCI 错误的影响，并赋予操作系统“重新启动”/恢复各个 PCI 设备的能力，操作系统可以变得更加可靠和健壮。 其他供应商基于 PCI-E 规范的未来系统可能包含类似的功能, ref, https://android.googlesource.com/kernel/common/+/bcmdhd-3.10/Documentation/powerpc/eeh-pci-error-recovery.txt
    case VFIO_GROUP_GET_STATUS -> vfio_group_ioctl_get_status
        iommu_group_dma_owner_claimed ->  Query group dma ownership status -> 多个设备可以放置在同一个 IOMMU 组中，因为它们无法相互隔离。 这些设备必须完全受内核控制或用户空间控制，不能混合。 这在 iommu 核心中添加了 dma 所有权管理，并公开了设备驱动程序和设备用户空间分配框架（即 VFIO）的多个接口，以便可以在一开始就检测到用户和内核控制的 dma 之间的任何冲突。 面向设备驱动程序的接口是 int iommu_device_use_default_domain(struct device *dev); void iommu_device_unuse_default_domain(struct device *dev); 通过调用 iommu_device_use_default_domain()，设备驱动程序告诉 iommu 层设备 dma 是通过内核 DMA API 处理的。 iommu 层将管理 IOVA 并使用默认域进行 DMA 地址转换。 面向设备用户空间分配框架的接口是 int iommu_group_claim_dma_owner(struct iommu_group *group, void *owner); 无效 iommu_group_release_dma_owner(struct iommu_group *group); bool iommu_group_dma_owner_claimed(struct iommu_group *group); 如果 DMA 所有者声明接口返回失败，则必须禁止设备用户空间分配
    case VFIO_GROUP_SET_CONTAINER -> vfio_group_ioctl_set_container
        f = fdget(fd)
        container = vfio_container_from_file(f.file)
        vfio_container_attach_group
            driver->ops->attach_group(container->iommu_data, group->iommu_group, group->type) -> vfio_noiommu_attach_group
            list_add(&group->container_next, &container->group_list)
            vfio_container_get(container)
        iommufd = iommufd_ctx_from_file(f.file)
        iommufd_vfio_compat_set_no_iommu
        or iommufd_vfio_compat_ioas_create -> 确保创建了 compat IOAS，@ictx：要操作的上下文 兼容性 IOAS 是 vfio 兼容性 ioctls 操作的 IOAS，因为它们的 ABI 中没有 IOAS ID 输入。 仅附加组应该会导致内部 ioas 的默认创建，如果已经以某种方式分配了现有 ioas，则这不会执行任何操作
            ioas = iommufd_ioas_alloc(ictx) -> iommufd：vfio容器FD ioctl兼容性，iommufd可以通过将/dev/vfio/vfio容器IOCTL映射到io_pagetable操作来直接实现它们。 用户空间应用程序可以针对 iommufd 进行测试并确认兼容性，然后只需进行一些小更改即可打开 /dev/iommu 而不是 /dev/vfio/vfio。 出于测试目的，/dev/vfio/vfio 可以符号链接到 /dev/iommu，然后所有应用程序将使用兼容性路径，无需更改代码。 后续系列允许 iommufd 直接提供 /dev/vfio/vfio，这也允许 rlimit 模式同样工作。 该系列仅提供 iommufd 方面的兼容性。 实际上将其链接到 VFIO_SET_CONTAINER 是一个后续系列，在求职信中有一个链接。 在内部，兼容性 API 使用普通的 IOAS 对象，该对象与 vfio 一样，在连接第一个设备时自动分配。 用户空间还可以使用 IOMMU_VFIO_IOAS ioctl 直接查询或设置此 IOAS 对象。 这允许混合和匹配新的仅 iommufd 功能，同时仍然使用 VFIO 样式映射/取消映射 ioctl。 虽然这足以操作 qemu，但它有一些差异： - 资源限制依赖于内存 cgroup 来限制用户空间可以执行的操作，而不是模块参数 dma_entry_limit。 - VFIO P2P 未实现。 vfio 的 DMABUF 补丁是 iommufd 导入特殊 DMABUF 的解决方案的开始。 这是为了避免进一步传播 follow_pfn() 安全问题。 - 尚未完成对迂腐兼容性细节（例如 errnos 等）的全面审核 - powerpc SPAPR 被遗漏，因为它未连接到 iommu_domain 框架。 似乎对 SPAPR 的兴趣很小，因为它目前在 v6.1-rc1 中不起作用。 他们必须转换到 iommu 子系统框架才能享受 iommfd。 以下内容不会被实现，我们希望将它们从 VFIO type1 中删除： - 软件访问“脏跟踪”。 正如求职信中所讨论的，这将在 VFIO 中完成
                ioas = iommufd_object_alloc(ictx, ioas, IOMMUFD_OBJ_IOAS)
                iopt_init_table(&ioas->iopt) -> iommufd：提供 IOVA 到 PFN 映射的数据结构 这是 IOAS 数据结构的其余部分。 提供一个名为 io_pagetable 的对象，该对象由指向 iopt_pages 的 iopt_areas 以及镜像 IOVA 到 PFN 映射的 iommu_domains 列表组成。 顶部是 iopt_areas 的简单区间树，指示 IOVA 到 iopt_pages 的映射。 xarray 跟踪域列表。 基于附加的域，存在区域的最小对齐（可能小于 PAGE_SIZE）、无法映射的保留 IOVA 的间隔树以及始终可映射的允许 IOVA 的 IOVA。 “访问”的概念指的是类似于 VFIO mdev 的东西，它访问 IOVA 并使用“struct page *”进行基于 CPU 的访问。 外部提供了一个 API，该 API 符合 IOCTL 接口对映射/取消映射和域附加的要求。 API 提供“复制”原语，通过重新使用 iopt_pages，在与现有映射不同的 IOAS 中建立新的 IOVA 映射。 这是提供单钉扎的基本机制。 其设计目的是支持预注册流程，其中用户空间将设置一个没有域的虚拟 IOAS，映射到内存中，然后建立将所有 PFN 固定到 xarray 中的访问权限。 然后，可以使用副本在不同的 IOAS 中创建新的 IOVA 映射，并附加 iommu_domains。 复制后，将从 xarray 中读取 PFN 并将其映射到 iommu_domains 中，从而避免任何 pin_user_pages() 开销
                    iopt->area_itree = RB_ROOT_CACHED
                    iopt->allowed_itree = RB_ROOT_CACHED
                    iopt->reserved_itree = RB_ROOT_CACHED
                    xa_init_flags(&iopt->domains, XA_FLAGS_ACCOUNT)
                    xa_init_flags(&iopt->access_list, XA_FLAGS_ALLOC)
                INIT_LIST_HEAD(&ioas->hwpt_list) -> iommufd：添加一个硬件页表对象，hw_pagetable 对象将内部结构 iommu_domain 暴露给用户空间。 当任何 DMA 设备连接到 IOAS 以通过 iommu 驱动程序控制 io 页表时，需要 iommu_domain。 为了与 VFIO 兼容，当 DMA 设备连接到 IOAS 时，会自动创建 hw_pagetable。 如果兼容的 iommu_domain 已存在，则与其关联的 hw_pagetable 将用于附件。 在最初的系列中，hw_pagetable 对象没有 iommufd uAPI。 下一个补丁提供面向驱动程序的 IO 页表附加 API，允许驱动程序接受 IOAS 或 hw_pagetable ID，并让驱动程序返回从 IOAS 自动选择的 hw_pagetable ID。 期望驱动程序将通过其自己的 FD 提供 uAPI，以将其设备附加到 iommufd。 这允许用户空间了解设备到 iommu_domains 的映射并覆盖自动连接。 未来的硬件特定接口将允许用户空间使用带有 IOMMU 驱动程序特定参数的 iommu_domains 创建 hw_pagetable 对象。 该基础设施将允许将这些域链接到 IOAS 和设备
            iommufd_object_finalize -> 允许并发访问该对象，一旦另一个线程可以看到该对象指针，就可以防止对象销毁。 除了特殊的仅内核对象之外，没有内核方法可以可靠地销毁单个对象。 因此，所有创建对象的 API 都必须使用 iommufd_object_abort() 来处理错误，并且只有在对象创建不会失败时才调用 iommufd_object_finalize()
                old = xa_store(&ictx->objects, obj->id, obj, GFP_KERNEL)
    case VFIO_GROUP_UNSET_CONTAINER -> vfio_group_ioctl_unset_container
        vfio_group_detach_container
            driver->ops->detach_group(container->iommu_data,




vfio_group_fops_open
    struct vfio_group *group
    group->opened_file = filep
    filep->private_data = group


struct vfio_device_bind_iommufd bind = {
        .argsz = sizeof(bind),
        .flags = 0,
};
struct iommu_ioas_alloc alloc_data  = {
        .size = sizeof(alloc_data),
        .flags = 0,
};
struct vfio_device_attach_iommufd_pt attach_data = {
        .argsz = sizeof(attach_data),
        .flags = 0,
};
struct iommu_ioas_map map = {
        .size = sizeof(map),
        .flags = IOMMU_IOAS_MAP_READABLE |
                 IOMMU_IOAS_MAP_WRITEABLE |
                 IOMMU_IOAS_MAP_FIXED_IOVA,
        .__reserved = 0,
};

iommufd = open("/dev/iommu", O_RDWR);

bind.iommufd = iommufd;
ioctl(cdev_fd, VFIO_DEVICE_BIND_IOMMUFD, &bind);

ioctl(iommufd, IOMMU_IOAS_ALLOC, &alloc_data);
attach_data.pt_id = alloc_data.out_ioas_id;
ioctl(cdev_fd, VFIO_DEVICE_ATTACH_IOMMUFD_PT, &attach_data);

/* Allocate some space and setup a DMA mapping */
map.user_va = (int64_t)mmap(0, 1024 * 1024, PROT_READ | PROT_WRITE,
                            MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
map.iova = 0; /* 1MB starting at 0x0 from device view */
map.length = 1024 * 1024;
map.ioas_id = alloc_data.out_ioas_id;;

ioctl(iommufd, IOMMU_IOAS_MAP, &map);

/* Other device operations as stated in "VFIO Usage Example" */



id_table -> 对于与 ID 表匹配且尚未被其他驱动程序“拥有”的所有 PCI 设备，将调用此探测函数（在对现有设备执行 pci_register_driver() 期间或稍后插入新设备时）。 对于 ID 表中的条目与设备匹配的每个设备，此函数都会传递一个“struct pci_dev *”。 当驱动程序选择获取设备的“所有权”时，探测函数返回零，否则返回错误代码（负数）。 探测函数总是从进程上下文中调用，因此它可以休眠


check kernel cmd, 
cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-5.15.0-71-generic root=UUID=88917def-9077-4cb7-b470-a84668cc4c99 ro vga=792 console=tty0 console=ttyS0,115200n8 net.ifnames=0 noibrs crashkernel=0M-1G:0M,1G-4G:192M,4G-128G:384M,128G-:512M nvme_core.io_timeout=4294967295 nvme_core.admin_timeout=4294967295 iommu=pt

cat /boot/config-$(uname -r) | grep NOIOMMU
CONFIG_VFIO_NOIOMMU=y




IP-over-InfiniBand protocol (IPoIB)
modprobe ib_ipoib
module_init(ipoib_init_module);
    ib_register_client(&ipoib_client)
static struct ib_client ipoib_client = {
	.name   = "ipoib",
	.add    = ipoib_add_one,
	.remove = ipoib_remove_one,
	.get_net_dev_by_params = ipoib_get_net_dev_by_params,
};
ipoib_add_one
    ipoib_add_port | ipoib_vlan_add
        ipoib_intf_alloc
            ipoib_intf_init
                ipoib_build_priv
                    ipoib_mcast_join_task
                        ipoib_mcast_join
                            ib_sa_join_multicast
                                static struct mcast_group *acquire_group(struct mlx4_ib_demux_ctx *ctx,
                                    mlx4_ib_mcg_work_handler
                                        handle_join_req
                                            send_reply_to_slave
                                                send_mad_to_slave
                                                    wc.slid = rdma_ah_get_dlid(&ah_attr);  /* opensm lid */



BUILD_BUG_ON：只有条件condition为0时可编译，但不返回值，否则编译器报错。
BUILD_BUG_ON_ZERO：只有条件e为0时返回0，否则编译器报错



下面的结构体中包含了一个零长度的数组
struct ib_pkey_cache {
	int             table_len;
	u16             table[0];
};
那如果要定义一个这个结构体的指针的话，申请的空间改怎么计算呢？
可以参看下面的code
struct ib_pkey_cache      *pkey_cache = NULL；
pkey_cache = kmalloc(struct_size(pkey_cache, table, tprops->pkey_tbl_len),
			     GFP_KERNEL);
	if (!pkey_cache)
		goto err;
这段code 用tprops->pkey_tbl_len 来表示要给原本零长度数组分配的空间，然后用
struct_size来计算整个结构体的长度, 计算方式就是(数组元素大小 * 元素个数 + 结构体本身的大小)
其中struct_size 是一个宏，实现如下：
#define struct_size(p, member, n)					\
	__ab_c_size(n,							\
		    sizeof(*(p)->member) + __must_be_array((p)->member),\
		    sizeof(*(p)))

hmc impl, https://lore.kernel.org/linux-rdma/9DD61F30A802C4429A01CA4200E302A7AC7078BD@fmsmsx123.amr.corp.intel.com/T/#m20a8d733d6db084b45b235daf71041f01a273027
hmc.h
hmc.c

#define IRDMA_HMC_MAX_BP_COUNT			512

struct irdma_hmc_info {
	u32 signature;
	u8 hmc_fn_id;
	u16 first_sd_index;
	struct irdma_hmc_obj_info *hmc_obj;
	struct irdma_virt_mem hmc_obj_virt_mem;
	struct irdma_hmc_sd_table sd_table;
	u16 sd_indexes[IRDMA_HMC_MAX_SD_COUNT];
};


段描述符条目(SD Entry)
struct irdma_hmc_sd_entry {
	enum irdma_sd_entry_type entry_type;
	bool valid;
	union {
		struct irdma_hmc_pd_table pd_table; -> 页描述符 (PD)
		struct irdma_hmc_bp bp; -> 后备页(BP)
	} u;
};


static inline struct ib_qp *ib_create_qp(struct ib_pd *pd, struct ib_qp_init_attr *init_attr)
    ib_create_qp_kernel(pd, init_attr, KBUILD_MODNAME)
        if (qp_init_attr->cap.max_rdma_ctxs)
            ret = rdma_rw_init_mrs(qp, qp_init_attr)
                ret = ib_mr_pool_init(qp, &qp->rdma_mrs, nr_mrs, IB_MR_TYPE_MEM_REG, max_num_sg, 0)
                    mr = ib_alloc_mr(qp->pd, type, max_num_sg)
                        mr = pd->device->ops.alloc_mr(pd, mr_type, max_num_sg)
                    list_add_tail(&mr->qp_entry, list)




continue memory,
void __init dma_contiguous_reserve(phys_addr_t limit)
CONFIG_CMA_SIZE_MBYTES

free_area_init
memmap_init
memmap_init_range
memmap_init_zone_range


mlx5_ib_enable_driver
    mlx5_ib_test_wc


find linux kernel module entry:
find ./ -type f -name "*.c"|xargs grep "module_init" |grep kvm



struct rdma_hw_stats -> RDMA硬件状态, @lock - 互斥体，用于保护对计数器的生命周期和值的并行写入访问，这些计数器是 64 位的，不保证在 32 位系统上原子写入。 @timestamp - 由核心代码用来跟踪上次更新的时间 @lifespan - 由核心代码用来确定计数器在再次更新之前应该有多久。 存储在 jiffies 中，默认为 10 毫秒，驱动程序可以在分配例程期间指定自己的值来覆盖默认值。 @descs - 指向用于目录中计数器的静态描述符的指针数组。 @is_disabled - 指示每个计数器当前是否禁用的位图。 @num_counters - 有多少个硬件计数器。 如果名称短于此数字，则会导致内核错误。 鼓励驱动程序作者在代码中保留 BUILD_BUG_ON(ARRAY_SIZE(@name) < num_counters) 以防止出现这种情况。 @value - 由 sysfs 代码访问并由驱动程序 get_stats 例程填充的 u64 计数器数组
struct rdma_hw_stats {
	struct mutex	lock; /* Protect lifespan and values[] */
	unsigned long	timestamp;
	unsigned long	lifespan;
	const struct rdma_stat_desc *descs;
	unsigned long	*is_disabled;
	int		num_counters;
	u64		value[] __counted_by(num_counters); -> zero len array
};

static struct ib_mr *irdma_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
    stag = irdma_create_stag(iwdev)
    err_code = irdma_get_pble(iwdev->rf->pble_rsrc, palloc, iwmr->page_cnt,
    irdma_hw_alloc_stag(iwdev, iwmr)
        ...
        cqp_info->cqp_cmd = IRDMA_OP_ALLOC_STAG
        cqp_info->post_sq = 1
        status = irdma_handle_cqp_op(iwdev->rf, cqp_request)
        ...
    iwpbl->pbl_allocated = true



ib_verbs.h
__printf(3, 4) __cold

dynamic_ibdev_dbg(__dev, format, ##args)




nvmeof, module
drivers/nvme/host/rdma.c -> module_init(nvme_rdma_init_module)
    ib_register_client(&nvme_rdma_ib_client) -> nvme_rdma
    nvmf_register_transport(&nvme_rdma_transport)
        list_add_tail(&ops->entry, &nvmf_transports)



static struct nvmf_transport_ops nvme_rdma_transport = {
	.name		= "rdma",
	.module		= THIS_MODULE,
	.required_opts	= NVMF_OPT_TRADDR,
	.allowed_opts	= NVMF_OPT_TRSVCID | NVMF_OPT_RECONNECT_DELAY |
			  NVMF_OPT_HOST_TRADDR | NVMF_OPT_CTRL_LOSS_TMO |
			  NVMF_OPT_NR_WRITE_QUEUES | NVMF_OPT_NR_POLL_QUEUES |
			  NVMF_OPT_TOS,
	.create_ctrl	= nvme_rdma_create_ctrl,
};



nvmf_dev_write
    buf = memdup_user_nul(ubuf, count)
    nvmf_create_ctrl(nvmf_device, buf)
        nvmf_parse_options(opts, buf)
            strscpy(hostnqn, nvmf_default_host->nqn, NVMF_NQN_SIZE)
            opts->host = nvmf_host_add(hostnqn, &hostid)
                list_for_each_entry(host, &nvmf_hosts, list)
            nvmf_host_alloc
            list_add_tail(&host->list, &nvmf_hosts)
        request_module("nvme-%s", opts->transport)
        nvmf_check_required_opts
        nvmf_lookup_transport
        try_module_get
        nvmf_check_required_opts(opts, ops->required_opts)
        nvmf_check_allowed_opts(opts, NVMF_ALLOWED_OPTS | ops->allowed_opts | ops->required_opts)
        ctrl = ops->create_ctrl(dev, opts) -> nvme_rdma_create_ctrl
            INIT_LIST_HEAD(&ctrl->list)
            inet_pton_with_scope
            nvme_rdma_existing_controller
            INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work)
            INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work)
            INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work)
            nvme_init_ctrl
            nvme_change_ctrl_state
            nvme_rdma_setup_ctrl
            list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list)









drivers/nvme/host/fabrics.c
module_init(nvmf_init)
    nvmf_default_host = nvmf_host_default() -> nvme-fabrics：添加通用 NVMe over Fabrics 库，NVMe over Fabrics 库为传输和 nvme 核心提供接口，以处理独立于底层传输的特定于结构的命令和属性。 此外，fabrics 库添加了一个杂项设备接口，允许实际创建一个fabrics 控制器，因为我们不能像 PCI 情况那样自动发现它。 nvme-cli 实用程序已得到增强，可以使用此接口来支持结构连接和发现
        uuid_gen(&id)
        snprintf(nqn, NVMF_NQN_SIZE,"nqn.2014-08.org.nvmexpress:uuid:%pUb", &id)
        host = nvmf_host_alloc(nqn, &id)
            host = kmalloc(sizeof(*host),
        list_add_tail(&host->list, &nvmf_hosts)
    class_create("nvme-fabrics")
    device_create(nvmf_class, NULL, MKDEV(0, 0), NULL, "ctl")
    misc_register(&nvmf_misc)


static struct miscdevice nvmf_misc = {
	.minor		= MISC_DYNAMIC_MINOR,
	.name           = "nvme-fabrics",
	.fops		= &nvmf_dev_fops,
};
static const struct file_operations nvmf_dev_fops = {
	.owner		= THIS_MODULE,
	.write		= nvmf_dev_write,
	.read		= seq_read,
	.open		= nvmf_dev_open,
	.release	= nvmf_dev_release,
};


nvmf_dev_open
    single_open(file, nvmf_dev_show, NULL)
        res = seq_open(file, op)
nvmf_dev_show
    seq_printf(seq_file, "instance=%d,cntlid=%d\n", ctrl->instance, ctrl->cntlid)



ib_dispatch_event
    is_cache_update_event(event)
    else -> INIT_WORK(&work->work, ib_generic_event_task)


nvme_rdma_cm_handler
    case RDMA_CM_EVENT_ADDR_RESOLVED:
            cm_error = nvme_rdma_addr_resolved(queue)
                ret = nvme_rdma_create_queue_ib(queue)
                    ret = nvme_rdma_create_cq(ibdev, queue)
                        queue->ib_cq = ib_cq_pool_get(ibdev, queue->cq_size, comp_vector, IB_POLL_SOFTIRQ) -> ib_cq_pool_get() - 查找与给定的 cpu 提示匹配（或对于通配符亲和性最少使用）且适合 nr_cqe 的最少使用的完成队列。@dev：rdma 设备 @nr_cqe：所需的 cqe 条目数 @comp_vector_hint：驱动程序根据内部计数器分配 comp 向量的完成向量提示（-1）@poll_ctx：cq 轮询上下文查找满足 @comp_vector_hint 和 @nr_cqe 要求的 cq 并为我们声明其中的条目。如果没有可用的 cq，则分配一个符合要求的新 cq 并将其添加到设备池中。IB_POLL_DIRECT 不能用于共享 cq，因此它不是 @poll_ctx 的有效值
                            ret = ib_alloc_cqs(dev, nr_cqe, poll_ctx) -> ib_alloc_cq


ib_alloc_cq -> __ib_alloc_cq
    rdma_dim_init -> struct dim - 动态中断调节 (DIM) 的主要结构。用于保存有关特定 DIM 实例的所有信息。 @state：算法状态（见下文） @prev_stats：上一次迭代的测量速率（用于比较） @start_sample：当前迭代开始时的采样数据 @measuring_sample：用于更新当前事件的 &dim_sample @work：要执行的工作 @priv：指向暗淡结构的指针 @profile_ix：当前审核配置文件 @mode：CQ 周期计数模式 @tune_state：算法调整状态（见下文） @steps_right：迈向更高审核的步数 @steps_left ：降低适度程度所采取的步数 @tired：停车深度计数器 -> 内核文档: https://docs.kernel.org/networking/net_dim.html
        dim->state = DIM_START_MEASURE -> enum dim_tune_state - DIM 算法调整状态，这些将确定算法应执行哪个操作。 @DIM_PARKING_ON_TOP：算法找到了一个局部最高点 - 出现显着差异时退出 @DIM_PARKING_TIRED：算法找到了一个深度最高点 - 如果累了 > 0，则不退出 @DIM_GOING_RIGHT：算法当前正在尝试更高的调节级别 @DIM_GOING_LEFT：算法当前正在尝试更低的调节级别 调节级别 */ m dim_tune_state {
        INIT_WORK(&dim->work, ib_cq_rdma_dim_work) -> DMA/核心：为 ULP 提供 RDMA DIM 支持，在 infiniband 驱动程序中添加了应用 rdma_dim 自适应调节的接口。 现在有一个特殊函数用于分配使用 rdma_dim 的 ib_cq。   使用 null_blk 设备在 Mellanox 交换机上的两个具有 56 个内核的同等终端主机之间运行基于 NVMf 的 FIO 基准测试的性能改进（ConnectX-5 100GbE、x86）, rdma_dim 算法旨在以通用方式测量流量调节的有效性，因此应适用于所有 RDMA 存储协议。   根据大量测试后看到的性能改进，将 rdma_dim 配置为默认选项
            cq->device->ops.modify_cq(cq, comps, usec) -> mlx5_ib_modify_cq
                mlx5_core_modify_cq_moderation
                    mlx5_core_modify_cq
                        MLX5_CMD_OP_MODIFY_CQ
                        return mlx5_cmd_exec(dev, in, inlen, out, sizeof(out))


ib_cq_poll_work
    rdma_dim
        dim_update_sample_with_comps
            dim_update_sample
        switch (dim->state)
        case DIM_START_MEASURE
            dim->state = DIM_MEASURE_IN_PROGRESS;
            dim_update_sample_with_comps
        case DIM_MEASURE_IN_PROGRESS:
            dim_calc_stats -> dim_calc_stats - 计算两个样本之间的差异， @start：开始样本 @end：结束样本 @curr_stats：样本之间的增量 * 计算两个样本之间的增量（以数据速率表示）。   考虑计数器环绕。   返回boolean表示curr_stats是否可靠
                delta_us = ktime_us_delta -> 可重用的时间差函数
                npkts = BIT_GAP(BITS_PER_TYPE(u32), end->pkt_ctr, start->pkt_ctr)
                curr_stats->cpe_ratio = DIV_ROUND_DOWN_ULL(curr_stats->cpms * 100, curr_stats->epms)
            if (rdma_dim_decision(&curr_stats, dim))
                schedule_work(&dim->work)
        schedule_work(&dim->work)


module_init(p9_trans_rdma_init);
    v9fs_register_trans(&p9_rdma_trans)
        list_add_tail(&m->list, &v9fs_trans_list)
static struct p9_trans_module p9_rdma_trans = {
	.name = "rdma",
	.maxsize = P9_RDMA_MAXSIZE,
	.pooled_rbuffers = true,
	.def = 0,
	.owner = THIS_MODULE,
	.create = rdma_create_trans,
	.close = rdma_close,
	.request = rdma_request,
	.cancel = rdma_cancel,
	.cancelled = rdma_cancelled,
	.show_options = p9_rdma_show_options,
};
rdma_create_trans



struct file_system_type v9fs_fs_type = {
	.name = "9p",
	.mount = v9fs_mount,
	.kill_sb = v9fs_kill_super,
	.owner = THIS_MODULE,
	.fs_flags = FS_RENAME_DOES_D_MOVE,
};



module_init(init_v9fs)
    static int __init init_v9fs(void)
        v9fs_cache_register
            v9fs_init_inode_cache
                v9fs_inode_cache = kmem_cache_create("v9fs_inode_cache"
        v9fs_sysfs_init
        register_filesystem(&v9fs_fs_type)
            v9fs_mount
                fid = v9fs_session_init(v9ses, dev_name, data)
                    p9_client_create
                        err = clnt->trans_mod->create(clnt, dev_name, options)  -> rdma_create_trans -> create rdma transport layer
                            rdma = alloc_rdma(&opts)
                            rdma->cm_id = rdma_create_id(&init_net, p9_cm_event_handler, client, RDMA_PS_TCP, IB_QPT_RC)
                            if (opts.privport) 
                                p9_rdma_bind_privport
                                    rdma_bind_addr(rdma->cm_id, (struct sockaddr *)&cl)
                            err = rdma_resolve_addr(rdma->cm_id, NULL, (struct sockaddr *)&rdma->addr, rdma->timeout)
                            wait_for_completion_interruptible -> P9_RDMA_ADDR_RESOLVED
                            err = rdma_resolve_route(rdma->cm_id, rdma->timeout)
                            P9_RDMA_ROUTE_RESOLVED
                            rdma->cq = ib_alloc_cq_any(rdma->cm_id->device, client, opts.sq_depth + opts.rq_depth + 1, IB_POLL_SOFTIRQ) -> __ib_alloc_cq_any
                                if (dev->num_comp_vectors > 1)
                                    comp_vector = atomic_inc_return(&counter) %	min_t(int, dev->num_comp_vectors, num_online_cpus())
                                __ib_alloc_cq(dev, private, nr_cqe, comp_vector, poll_ctx, caller)
                                    cq = rdma_zalloc_drv_obj(dev, ib_cq)
                                    cq->comp_vector = comp_vector
                                    cq->wc = kmalloc_array(IB_POLL_BATCH, sizeof(*cq->wc), GFP_KERNEL) -> 16
                                    ret = dev->ops.create_cq(cq, &cq_attr, NULL) -> 
                                    rdma_dim_init
                                    switch (cq->poll_ctx)
                                    case IB_POLL_DIRECT:
                                        cq->comp_handler = ib_cq_completion_direct
                                    case IB_POLL_SOFTIRQ
                                        cq->comp_handler = ib_cq_completion_softirq
                                            irq_poll_sched(&cq->iop)
                                                list_add_tail(&iop->list, this_cpu_ptr(&blk_cpu_iopoll))
                                                raise_softirq_irqoff(IRQ_POLL_SOFTIRQ)
                                        irq_poll_init(&cq->iop, IB_POLL_BUDGET_IRQ, ib_poll_handler) -> IB：添加适当的完成队列抽象，这添加了一个抽象，允许 ULP 简单地向每个提交的 WR 传递完成对象和完成回调，并让 RDMA 核心处理如何处理完成中断和轮询 CQ 的具体细节。   详细来说，有一个新的 ib_cqe 结构，它只包含完成回调，并且可用于使用 container_of 获取包含对象。 WR 和 WC 指出它作为 wr_id 字段的替代项，类似于有多少 ULP 已经使用该字段通过强制转换来存储指针。   使用新的完成回调的驱动程序使用新的 ib_create_cq API 分配它的 CQ，除了 CQE 的数量和完成向量之外，它还采用我们如何轮询 CQE 的模式。 提供三种模式：直接用于从不接受 CQ 中断并仅轮询它们的驱动程序，软中断使用重命名的 blk-iopoll 基础设施从软中断上下文进行轮询，该基础设施负责重新配置和预算，或者为想要的消费者提供工作队列 从用户上下文中调用。   非常感谢 Sagi Grimberg，他帮助审查了 API，编写了当前版本的工作队列代码，因为我之前的两次尝试太糟糕了，并将 iSER 启动器转换为新的 API
                                            iop->poll = ib_poll_handler <- irq_poll_softirq -> poll()
                                                completed = __ib_process_cq(cq, budget, cq->wc, IB_POLL_BATCH)
                                                    trace_cq_process(cq)
                                                    while ((n = __poll_cq(cq, min_t(u32, batch,
                                                        ib_poll_cq(cq, num_entries, wc)
                                                            return cq->device->ops.poll_cq(cq, num_entries, wc)
														wc->wr_cqe->done(cq, wc)
                                                if (completed < budget)
                                                    irq_poll_complete(&cq->iop)
                                                    irq_poll_sched(&cq->iop)
                                                rdma_dim
                                        ib_req_notify_cq(cq, IB_CQ_NEXT_COMP)
                                    case IB_POLL_WORKQUEUE
                                    case IB_POLL_UNBOUND_WORKQUEUE
                                        cq->comp_handler = ib_cq_completion_workqueue
                                        INIT_WORK(&cq->work, ib_cq_poll_work)
                            rdma->pd = ib_alloc_pd(rdma->cm_id->device, 0) -> __ib_alloc_pd
                                device->ops.alloc_pd(pd, NULL)
                                mr = pd->device->ops.get_dma_mr(pd, mr_access_flags)
                            qp_attr.event_handler = qp_event_handler
                            qp_attr.cap.max_send_sge = P9_RDMA_SEND_SGE
                            qp_attr.qp_type = IB_QPT_RC
                            rdma_create_qp
                            rdma_connect
                            wait_for_completion_interruptible P9_RDMA_CONNECTED
                            client->status = Connected
                    p9_is_proto_dotl
                    v9fs_parse_options
                    p9_client_attach
                    list_add(&v9ses->slist, &v9fs_sessionlist)
                v9fs_fill_super
                inode = v9fs_get_inode(sb, S_IFDIR | mode, 0)
                root = d_make_root(inode)
                v9fs_proto_dotl
                p9_client_stat
                v9fs_get_acl
                v9fs_fid_add(root, &fid)


/* See IBTA Annex A11, servies ID bytes 4 & 5 */
enum rdma_ucm_port_space {
	RDMA_PS_IPOIB = 0x0002,
	RDMA_PS_IB    = 0x013F,
	RDMA_PS_TCP   = 0x0106,
	RDMA_PS_UDP   = 0x0111,
};


可测量的动态中断调节比率
struct dim_stats {
	int ppms; /* packets per msec */
	int bpms; /* bytes per msec */
	int epms; /* events per msec */
	int cpms; /* completions per msec */
	int cpe_ratio; /* ratio of completions to events */
};



#define IS_SIGNIFICANT_DIFF(val, ref) \
	((ref) && (((100UL * abs((val) - (ref))) / (ref)) > 10)) -> 真实值与参考值之间的差异是否证明采取行动是合理的。 我们认为 10% 的差异是显着的



rdma_dim - 运行自适应调节。@dim：审核结构。 @completions：本轮收集的完成数。 每次调用 rdma_dim 都会获取已收集的最新完成数量，并将其计为新事件。 一旦收集到足够的事件，算法就会决定新的审核级别
配置 DIM 参数列表, 动态中断调节（DIM, Dynamic Interrupt Moderation）是一种在 Linux 内核中实现的技术，以 NetDIM 库的形式提供，旨在优化和调整中断频率，以提升网络设备的性能和效率，其被广泛应用于各现代网卡中。  然而，不同的网卡具有多样化的候选中断参数列表需求，尤其是 virtio 网卡的后端可能由不同软件设备或硬件设备实现，所以 DIM 在 virtio-net 上的性能并未符合预期，这在一定程度上正是不当的中断配置列表造成的。同时，其他的现代网卡例如 ICE、IDPF 等也对其 DIM 参数列表进行了自定义配置。  基于此，高性能网络小组正在推动基于用户态工具 ethtool 配置中断列表的方法 [2]，以允许用户针对各自设备的特定需求，灵活地调节 DIM，从而实现最好的网络吞吐和时延。  [2] https://lore.kernel.org/all/1712059988-7705-1-git-send-email-hengqi@linux.alibaba.com/
NetDIM 库为许多现代网卡提供了出色的加速。 然而，DIM 的默认配置文件限制了其针对不同 NIC 的最大功能，因此有必要提供一种可以自定义配置 NIC 的方法。   目前与驱动程序的交互仍然基于常用的“ethtool -C”。 驱动程序根据.supported_coalesce_params声明其支持的参数，并在.set_coalesce和.get_coalesce中实现驱动程序相关的自定义限制




HTB, 分层 QoS 硬件卸载, https://lore.kernel.org/netdev/FC053E80-74C9-4884-92F1-4DBEB5F0C81A@mellanox.com/
mlx5 support htb commit: https://github.com/ssbandjl/linux/commit/214baf22870cfa437522f3bd4fbae56338674b04
发件人：Yossi Kuperman <yossiku@mellanox.com>,收件人：“netdev@vger.kernel.org”<netdev@vger.kernel.org> 抄送：Jamal Hadi Salim <jhs@mojatatu.com>， 	Jiri Pirko <jiri@mellanox.com>、Rony Efraim <ronye@mellanox.com>、 	Maxim Mikityanskiy <maximmi@mellanox.com>， 	约翰·法斯塔本德（John Fastabend）<john.fastabend@gmail.com>， 	Eran Ben Elisha <eranbe@mellanox.com> 主题：[RFC] 分层 QoS 硬件卸载 (HTB) 日期：2020 年 1 月 30 日星期四 16:20:38 +0000	 [线程概述] 消息 ID：<FC053E80-74C9-4884-92F1-4DBEB5F0C81A@mellanox.com>（原始）  下面简要描述了我们卸载 HTB 功能的计划。  HTB qdisc 允许您使用一条物理链路模拟多条较慢的链路。这是通过配置分层 QoS 树来实现的；每个树节点对应一个类。过滤器用于将流分类到不同的类。HTB 非常灵活且用途广泛，但需要付出代价。HTB 无法扩展，并且会消耗大量 CPU 和内存。我们的目标是将 HTB 功能卸载到硬件上，并为用户提供 TC 子系统提供的灵活性和常规工具，同时扩展到数千个流量类别并保持线速性能。   Mellanox 硬件可以支持分层速率限制；速率限制是按硬件队列进行的。在我们提出的解决方案中，流分类是在软件中进行的。通过将分类移至线程安全且不需要锁定的 clsact 出口挂钩，我们避免了由单个 qdisc 锁引起的争用。此外，在选择网络设备的 TX 队列之前执行 clsact 过滤器，使驱动程序有机会将类转换为适当的硬件队列。请注意，用户需要以略有不同的方式配置过滤器；将它们应用于 clsact 而不是 HTB 本身，并将优先级设置为所需的类 ID。  例如，以下两个过滤器是等效的： 	1. tc filter add dev eth0 parent 1:0 protocol ip flower dst_port 80 classid 1:10 	2. tc filter add dev eth0 egress protocol ip flower dst_port 80 action skbedit priority 1:10  注意：为了支持上述过滤器，不需要对上游内核或 iproute2 包进行任何代码更改。  此外，当前 HTB 实现最令人担忧的方面是它缺乏对多队列的支持。所有网络设备的 TX 队列都指向同一个 HTB 实例，导致高自旋锁争用。这种争用（可能）抵消了引入卸载所预期的整体性能提升。我们应该修改 HTB，使其呈现为 mq qdisc。默认情况下，mq qdisc 为每个由下层设备公开的 TX 队列分配一个简单的 fifo qdisc。这仅在配置了硬件卸载时才会发生，否则，HTB 的行为将与往常一样。数据路径上没有 HTB 代码；与常规流量相比，唯一的开销是在 clsact 处进行的分类。请注意，此设计会导致完全卸载——不会回退到软件；无论如何，考虑到兄弟之间的借用，部分卸载分层树并非易事。   总结一下：对于每个 HTB 叶类，驱动程序将分配一个特殊队列并将其与相应的网络设备 TX 队列匹配（增加 real_num_tx_queues）。任何此类 TX 队列都将附加一个唯一的 fifo qdisc。分类仍将在软件中进行，但在 clsact egress 钩子上进行。这样，我们可以扩展到数千个类，同时保持线速性能并减少 CPU 开销。  任何反馈都将非常感谢。  干杯， 库珀曼



dcb: DCB (Data Center Bridging)




io_uring,
https://www.scylladb.com/2020/05/05/how-io_uring-and-ebpf-will-revolutionize-programming-in-linux/
tools/testing/selftests/net/io_uring_zerocopy_tx.c -> main
    ...
    io_uring_submit


DORMANT: 休眠状态


mlx5_ib_stage_init_init
    dev->ib_dev.node_type = RDMA_NODE_IB_CA
    dev->ib_dev.lag_flags = RDMA_LAG_FLAGS_HASH_ALL_SLAVES
    mlx5r_cmd_query_special_mkeys -> MLX5_CMD_OP_QUERY_SPECIAL_CONTEXTS
    mlx5r_macsec_init_gids_and_devlist
        mlx5_is_macsec_roce_supported
        dev->port[i].reserved_gids = kcalloc
        INIT_LIST_HEAD(&dev->macsec.macsec_devices_list)
    mlx5_ib_init_multiport_master -> {net, IB}/mlx5：管理多端口 RoCE 的端口关联，调用 mlx5_ib_add 时确定要添加的 mlx5 核心设备是否能够进行双端口 RoCE 操作。 如果是，请使用 num_vhca_ports 和affiliate_nic_vport_criteria 功能确定它是主设备还是从设备。   如果该设备是从属设备，请尝试找到与其关联的主设备。 可以关联的设备将共享系统映像 GUID。 如果没有找到，请将其放入非关联端口列表中。 如果找到主设备，则通过在 NIC vport 上下文中配置端口从属关系将端口绑定到它。   同样，当调用 mlx5_ib_remove 时确定端口类型。 如果它是从端口，则将其与主设备取消关联，否则只需将其从非关联端口列表中删除即可。   即使第二个端口不可用于关联，IB 设备也会注册为多端口设备。 当第二个端口稍后附属时，必须刷新 GID 缓存才能获取缓存中第二个端口的默认 GID。 导出roce_rescan_device以提供在绑定新端口后刷新缓存的机制。   在多端口配置中，所有 IB 对象（QP、MR、PD 等）相关命令应流过主站 mlx5_core_dev，其他命令必须发送到从端口 mlx5_core_mdev，提供一个接口来获取非 IB 对象命令的正确 mdev
        mlx5_core_is_mp_master
        mlx5_query_nic_vport_system_image_guid
        mlx5_nic_vport_enable_roce
        list_add_tail(&dev->ib_dev_list, &mlx5_ib_dev_list)
    set_has_smi_cap
    mlx5_query_max_pkeys
        mlx5_query_mad_ifc_smp_attr_node_info
            ib_init_query_mad
                mad->method = IB_MGMT_METHOD_GET
            MLX5_CMD_OP_MAD_IFC
    mlx5_use_mad_ifc
    get_ext_port_caps
    mlx5_comp_vectors_max
    INIT_LIST_HEAD(&dev->qp_list)


linux rdma or infiniband patch: https://patchwork.kernel.org/project/linux-rdma/list/?submitter=55441&state=*&archive=both&param=4&order=delegate


mlx5_ib_fs_init
    ib_set_device_ops(&dev->ib_dev, &flow_ops)
static const struct ib_device_ops flow_ops = {
	.create_flow = mlx5_ib_create_flow,
	.destroy_flow = mlx5_ib_destroy_flow,
	.destroy_flow_action = mlx5_ib_destroy_flow_action,
};



mlx5_ib_reg_user_mr
    IB_ACCESS_ON_DEMAND
        create_user_odp_mr
            mlx5r_odp_create_eq -> RDMA/mlx5：仅在创建 ODP MR 时创建 ODP EQ，如果用户不使用 ODP MR，则无需创建 ODP EQ。 因此，仅在创建第一个 ODP MR 时创建它。 仅当设备卸载时，该 EQ 才会被破坏。 这将减少每个设备创建的均衡器数量。 例如：如果我们创建 1K 设备（SF/VF/等），那么我们会将 EQ 数量减少 1K
                INIT_WORK(&eq->work, mlx5_ib_eq_pf_action)
                    mempool_refill(eq->pool)
                    mlx5_ib_eq_pf_process
                        mlx5_eq_get_eqe
                        INIT_WORK(&pfault->work, mlx5_ib_eqe_pf_action)
                        mlx5_eq_update_cc
                        mlx5_eq_update_ci
                eq->pool = mempool_create_kmalloc_pool
                eq->wq = alloc_workqueue("mlx5_ib_page_fault",
                mlx5_eq_create_generic
                mlx5_eq_enable
            mlx5_ib_alloc_implicit_mr
                mlx5_mr_cache_alloc
                    struct mlx5_cache_ent *ent = mkey_cache_ent_from_rb_key(dev, rb_key)
                ib_umem_odp_alloc_implicit
                mlx5r_umr_update_xlt
                mlx5r_store_odp_mkey
            mlx5r_umr_can_load_pas
            ib_umem_odp_get
            alloc_cacheable_mr
            xa_init(&mr->implicit_children)
            mlx5r_store_odp_mkey
            mlx5_ib_init_odp_mr
    umem = ib_umem_get
    create_real_mr
        mlx5r_umr_can_load_pas
        alloc_cacheable_mr
            mlx5_umem_dmabuf_default_pgsz
            ent = mkey_cache_ent_from_rb_key(dev, rb_key)
            _mlx5_mr_cache_alloc(dev, ent, access_flags)
        reg_create
        mlx5r_umr_update_mr_pas


huawei, hisilicon, 
module_init(hns_roce_hw_v2_init);


static const struct ib_device_ops hns_roce_dev_ops = {
	.owner = THIS_MODULE,
	.driver_id = RDMA_DRIVER_HNS,
	.uverbs_abi_ver = 1,
	.uverbs_no_driver_id_binding = 1,

	.get_dev_fw_str = hns_roce_get_fw_ver,
	.add_gid = hns_roce_add_gid,
	.alloc_pd = hns_roce_alloc_pd,
	.alloc_ucontext = hns_roce_alloc_ucontext,
	.create_ah = hns_roce_create_ah,
	.create_user_ah = hns_roce_create_ah,
	.create_cq = hns_roce_create_cq,
	.create_qp = hns_roce_create_qp,
	.dealloc_pd = hns_roce_dealloc_pd,
	.dealloc_ucontext = hns_roce_dealloc_ucontext,
	.del_gid = hns_roce_del_gid,
	.dereg_mr = hns_roce_dereg_mr,
	.destroy_ah = hns_roce_destroy_ah,
	.destroy_cq = hns_roce_destroy_cq,
	.disassociate_ucontext = hns_roce_disassociate_ucontext,
	.get_dma_mr = hns_roce_get_dma_mr,
	.get_link_layer = hns_roce_get_link_layer,
	.get_port_immutable = hns_roce_port_immutable,
	.mmap = hns_roce_mmap,
	.mmap_free = hns_roce_free_mmap,
	.modify_device = hns_roce_modify_device,
	.modify_qp = hns_roce_modify_qp,
	.query_ah = hns_roce_query_ah,
	.query_device = hns_roce_query_device,
	.query_pkey = hns_roce_query_pkey,
	.query_port = hns_roce_query_port,
	.reg_user_mr = hns_roce_reg_user_mr,

	INIT_RDMA_OBJ_SIZE(ib_ah, hns_roce_ah, ibah),
	INIT_RDMA_OBJ_SIZE(ib_cq, hns_roce_cq, ib_cq),
	INIT_RDMA_OBJ_SIZE(ib_pd, hns_roce_pd, ibpd),
	INIT_RDMA_OBJ_SIZE(ib_qp, hns_roce_qp, ibqp),
	INIT_RDMA_OBJ_SIZE(ib_ucontext, hns_roce_ucontext, ibucontext),
};


dma_buf_export
drivers/gpu/drm/i915/gem/selftests/mock_dmabuf.c


i915_gem_dmabuf_mock_selftests
    static const struct i915_subtest tests[] = {
		SUBTEST(igt_dmabuf_export),
		SUBTEST(igt_dmabuf_import_self),
		SUBTEST(igt_dmabuf_import),
		SUBTEST(igt_dmabuf_import_ownership),
		SUBTEST(igt_dmabuf_export_vmap),
	};
    mock_gem_device
        pdev = kzalloc(sizeof(*pdev), GFP_KERNEL)
        pdev->class = PCI_BASE_CLASS_DISPLAY << 16
        dma_coerce_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64))
        devres_open_group
        devm_drm_dev_alloc
        i915_params_copy
        intel_device_info_driver_create
        intel_display_device_probe
        ...
    i915_subtests(tests, i915) -> __i915_subtests

    mock_destroy_device

selftest(dmabuf, i915_gem_dmabuf_mock_selftests)

drivers/gpu/drm/i915/selftests/i915_mock_selftests.h

drivers/gpu/drm/i915/i915_module.c
module_init(i915_init);
    for (i = 0; i < ARRAY_SIZE(init_funcs); i++)
        err = init_funcs[i].init() -> i915_mock_selftests
static const struct {
   int (*init)(void);
   void (*exit)(void);
} init_funcs[] = {
	{ .init = i915_check_nomodeset },
	{ .init = i915_active_module_init,
	  .exit = i915_active_module_exit },
	{ .init = i915_context_module_init,
	  .exit = i915_context_module_exit },
	{ .init = i915_gem_context_module_init,
	  .exit = i915_gem_context_module_exit },
	{ .init = i915_objects_module_init,
	  .exit = i915_objects_module_exit },
	{ .init = i915_request_module_init,
	  .exit = i915_request_module_exit },
	{ .init = i915_scheduler_module_init,
	  .exit = i915_scheduler_module_exit },
	{ .init = i915_vma_module_init,
	  .exit = i915_vma_module_exit },
	{ .init = i915_vma_resource_module_init,
	  .exit = i915_vma_resource_module_exit },
	{ .init = i915_mock_selftests },
	{ .init = i915_pmu_init,
	  .exit = i915_pmu_exit },
	{ .init = i915_pci_register_driver,
	  .exit = i915_pci_unregister_driver },
	{ .init = i915_perf_sysctl_register,
	  .exit = i915_perf_sysctl_unregister },
};

int i915_mock_selftests(void)
    run_selftests(mock, NULL) -> __run_selftests(#x, x##_selftests, ARRAY_SIZE(x##_selftests), data) -> __run_selftests("mock", mock_selftests, ARRAY_SIZE(mock_selftests), ((void *)0))

        i915_selftest.random_seed = get_random_u32()
        set_default_test_all
        cond_resched
        signal_pending
        st->live(data)
        else
            st->mock()
        
#define selftest(n, f) [mock_##n] = { .name = #n, { .mock = f } },
static struct selftest mock_selftests[] = {
#include "i915_mock_selftests.h"
};
#undef selftest

selftest(sanitycheck, i915_mock_sanitycheck) /* keep first (igt selfcheck) */
selftest(shmem, shmem_utils_mock_selftests)
selftest(fence, i915_sw_fence_mock_selftests)
selftest(scatterlist, scatterlist_mock_selftests)
selftest(syncmap, i915_syncmap_mock_selftests)
selftest(uncore, intel_uncore_mock_selftests)
selftest(ring, intel_ring_mock_selftests)
selftest(engine, intel_engine_cs_mock_selftests)
selftest(timelines, intel_timeline_mock_selftests)
selftest(requests, i915_request_mock_selftests)
selftest(objects, i915_gem_object_mock_selftests)
selftest(phys, i915_gem_phys_mock_selftests)
selftest(dmabuf, i915_gem_dmabuf_mock_selftests)
selftest(vma, i915_vma_mock_selftests)
selftest(evict, i915_gem_evict_mock_selftests)
selftest(gtt, i915_gem_gtt_mock_selftests)
selftest(hugepages, i915_gem_huge_page_mock_selftests)
selftest(memory_region, intel_memory_region_mock_selftests)


igt_dmabuf_export
    obj = i915_gem_object_create_shmem(i915, PAGE_SIZE)
    i915_gem_prime_export
        drm_gem_dmabuf_export
            dma_buf_export
            drm_dev_get
            drm_gem_object_get
    i915_gem_object_put
    dma_buf_put(dmabuf)

scatterlist_mock_selftests
    igt_sg_alloc
        for_each_prime_number(prime, max_order) -> for_each_prime_number - 迭代每个素数直至达到某个值 @prime：本次迭代中的当前素数 @max：上限 从第一个素数 2 开始迭代每个素数直至达到 @max 值。在每次迭代中，@prime 都设置为当前素数。@max 应小于 ULONG_MAX 以确保终止。若要在第一次迭代中将 @prime 设置为 1，请使用 for_each_prime_number_from()
            prandom_seed_state
            alloc_table
            expect_pfn_sgtable
                expect_pfn_sg
                expect_pfn_sg_page_iter
                expect_pfn_sgtiter
    igt_sg_trim



infiniband
Kconfig
menuconfig INFINIBAND
INFINIBAND_ADDR_TRANS_CONFIGFS -> IB/cma：为 rdma_cm 添加 configfs，用户希望控制 rdma_cm 的行为。例如，未设置所需 RoCE gid 类型的旧应用程序可以在 RoCE V2 网络类型上执行。为了支持此配置，我们为 rdma_cm 实现了一个 configfs。为了使用 configfs，需要将其挂载并在 rdma_cm 目录中 mkdir <IB 设备名称>。该补丁增加了对单个配置文件 default_roce_mode 的支持。模式可以是“IB/RoCE v1”​​或“RoCE v2”



module_init(ib_umad_init)
    register_chrdev_region(base_umad_dev, IB_UMAD_NUM_FIXED_MINOR * 2, umad_class.name)
    alloc_chrdev_region(&dynamic_umad_dev, 0, IB_UMAD_NUM_DYNAMIC_MINOR * 2, umad_class.name)
    class_register(&umad_class)
    ib_register_client(&umad_client)
    ib_register_client(&issm_client)

static struct ib_client umad_client = {
	.name   = "umad",
	.add    = ib_umad_add_one,
	.remove = ib_umad_remove_one,
	.get_nl_info = ib_umad_get_nl_info,
};



virtio,
core_initcall(virtio_init)
    bus_register(&virtio_bus)





static struct bus_type virtio_bus = {
	.name  = "virtio",
	.match = virtio_dev_match,
	.dev_groups = virtio_dev_groups,
	.uevent = virtio_uevent,
        add_uevent_var
	.probe = virtio_dev_probe,
	.remove = virtio_dev_remove,
};


struct virtio_pci_common_cfg

VirtIO-PCI初始化 virtio-blk基于virtio-pci，virtio-pci基于pci，所以virtio-blk初始化要从pci设备初始化说起 PCI初始化 PCI驱动框架初始化在内核中有两个入口，分别如下： arch_initcall(pci_arch_init)，体系结构的初始化，包括初始化IO地址0XCF8 subsys_initcall(pci_subsys_init)，PCI子系统初始化，这个过程会完成PIC总线树上设备的枚举，Host bridge会为PCI设备分配地址空间并将其写入BAR寄存器。体系结构初始化不做介绍，这张主要介绍PCI总线树的枚举和配置

arch_initcall(pci_arch_init)
    type = pci_direct_probe()
    pci_mmcfg_early_init
    pcbios = x86_init.pci.arch_init()
    x86_create_pci_msi_domain
    pci_pcbios_init
    pci_direct_init
    dmi_check_pciprobe
    dmi_check_skip_isa_align



subsys_initcall(pci_subsys_init);
	x86_init.pci.init	=>	x86_default_pci_init	
	pci_legacy_init
		pcibios_scan_root
			x86_pci_root_bus_resources	// 为Host bridge分配资源，通常情况下就是64K IO空间地址和内存空间地址就在这里划分
			pci_scan_root_bus			// 枚举总线树上的设备
				pci_create_root_bus		// 创建Host bridge
				pci_scan_child_bus		// 扫描总线树上所有设备，如果有pci桥，递归扫描下去
					pci_scan_slot
						pci_scan_single_device	// 扫描设备，读取vendor id和device id
							pci_scan_device
								pci_setup_device
									pci_read_bases
										__pci_read_base	// 读取bar空间大小
                            pci_device_add(dev, bus)


resource代表一个资源，可以是一段IO地址区间，或者Mem地址区间，总线树上每枚举一个设备，Host bridge就根据设备的BAR空间大小分配合适的资源给这个PCI设备用，这里的资源就是IO或者内存空间的物理地址。PCI设备BAR寄存器的值就是从这里申请得来的。申请的流程如下
pci_read_bases
	/* 遍历每个BAR寄存器，读取其内容，并为其申请物理地址空间 */
	for (pos = 0; pos < howmany; pos++) {
        struct resource *res = &dev->resource[pos];	// 申请的地址空间放在这里面
        reg = PCI_BASE_ADDRESS_0 + (pos << 2);
        pos += __pci_read_base(dev, pci_bar_unknown, res, reg);
    }
    	region.start = l64;	
    	region.end = l64 + sz64;
		/* 申请资源，将申请到的资源放在res中， region存放PCI设备BAR空间区间 */
    	pcibios_bus_to_resource(dev->bus, res, &region);  	



pci_subsys_init
	pcibios_resource_survey             
    	pcibios_allocate_bus_resources(&pci_root_buses);		// 首先将整个资源按照总线再分成一段段空间
    	pcibios_allocate_resources(0);							// 检查资源是否统一并且不冲突
    	pcibios_allocate_resources(1);
    	pcibios_assign_resources();								// 写入地址到BAR寄存器
    		pci_assign_resource
    			_pci_assign_resource
    				__pci_assign_resource
    					pci_bus_alloc_resource
    					pci_update_resource
    						pci_std_update_resource
    							pci_write_config_dword(dev, reg, new)	// 往BAR寄存器写入起始地址

#############################################
module_pci_driver(virtio_pci_driver); -> PCI驱动和其它linux驱动一样，需要定义init和exit两个函数作为加载模块的入口点和卸载模块的出口点，可以使用 module_pci_driver 宏，只要将pci_driver结构体变量作为参数调用这个宏，就会自动定义init和exit这两个函数，好处是代码整洁，缺点就是不够灵活，我这里仍然用传统的方法使用 module_init 和 module_exit 两个宏
static struct pci_driver virtio_pci_driver = {
	.name		= "virtio-pci",
	.id_table	= virtio_pci_id_table,
PCI driver for virtio devices
modulename: virtio_pci.ko
configname: CONFIG_VIRTIO_PCI
Linux Kernel Configuration
└─>Device Drivers
    └─>Virtio drivers
        └─>PCI support
            └─>PCI driver for virtio devices
drivers/virtio/virtio_pci_common.c -> module_pci_driver(virtio_pci_driver); -> PCI驱动和其它linux驱动一样，需要定义init和exit两个函数作为加载模块的入口点和卸载模块的出口点，可以使用 module_pci_driver 宏，只要将pci_driver结构体变量作为参数调用这个宏，就会自动定义init和exit这两个函数，好处是代码整洁，缺点就是不够灵活，我这里仍然用传统的方法使用 module_init 和 module_exit 两个宏
static struct pci_driver virtio_pci_driver = {
	.name		= "virtio-pci",
	.id_table	= virtio_pci_id_table, -> { PCI_DEVICE(PCI_VENDOR_ID_REDHAT_QUMRANET, PCI_ANY_ID) } -> 0x1af4
	.probe		= virtio_pci_probe,
	.remove		= virtio_pci_remove,
#ifdef CONFIG_PM_SLEEP
	.driver.pm	= &virtio_pci_pm_ops,
#endif
	.sriov_configure = virtio_pci_sriov_configure,
};


static int virtio_pci_probe(struct pci_dev *pci_dev, const struct pci_device_id *id)
    vp_dev = kzalloc
    vp_dev->vdev.dev.parent = &pci_dev->dev
    vp_dev->vdev.dev.release = virtio_pci_release_dev
    INIT_LIST_HEAD(&vp_dev->virtqueues)
    pci_enable_device
        pci_enable_device_flags(dev, IORESOURCE_MEM | IORESOURCE_IO)	// 打开内存和IO访问权限
            do_pci_enable_device
                pcibios_enable_device
                    pci_enable_resources
                        pci_write_config_word(dev, PCI_COMMAND, cmd)	// 向command寄存器字段写1
    virtio_pci_legacy_probe -> virtio spec 0.95
        vp_legacy_probe(ldev)
            ...
            rc = pci_request_region(pci_dev, 0, "virtio-pci-legacy")
            ldev->ioaddr = pci_iomap(pci_dev, 0, 0) -> pci_iomap函数完成PCI BAR的映射，第一个参数是pci设备的指针，第二个参数指定我们要映射的是0号BAR，第三个参数确定要映射的BAR空间多大，当第三个参数为0时，就将整个BAR空间都映射到内存空间上。VirtioPCI设备的0号BAR指向的就是配置空间的寄存器空间，也就是配置空间上用于消息通知的寄存器
            ldev->isr = ldev->ioaddr + VIRTIO_PCI_ISR
        vp_dev->vdev.config = &virtio_pci_config_ops;
        vp_dev->config_vector = vp_config_vector;
        vp_dev->setup_vq = setup_vq;
        vp_dev->del_vq = del_vq;
        vp_dev->is_legacy = true;
    virtio_pci_modern_probe -> #对于virtio modern，通过capability方式报告配置数据结构的位置，配置数据结构有5种类型
        vp_modern_probe(mdev) -> 现代 PCI 设备实现。此模块实现基于现代 PCI 设备（可能带有供应商特定的扩展）的设备的基本探测和控制。选择此模块的任何模块都必须依赖于 PCI
            check_offsets()
            devid = mdev->device_id_check(pci_dev)
            common = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_COMMON_CFG, IORESOURCE_IO | IORESOURCE_MEM, &mdev->modern_bars)
                for (pos = pci_find_capability(dev, PCI_CAP_ID_VNDR)
                    pci_find_next_capability
                    pci_read_config_byte(dev, pos + offsetof(struct virtio_pci_cap, cfg_type), &type)
                    pci_read_config_byte(dev, pos + offsetof(struct virtio_pci_cap, bar), &bar)
                    *bars |= (1 << bar)
            isr = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_ISR_CFG,
            VIRTIO_PCI_CAP_NOTIFY_CFG
            VIRTIO_PCI_CAP_DEVICE_CFG
            pci_request_selected_regions(pci_dev, mdev->modern_bars, "virtio-pci-modern") -> 为设备内存分配物理地址空间, 保留选定的 PCI I/O 和内存资源, 标记MMIO所需要的内存空间，实际上最终调用的是 __pci_request_region()
            mdev->common = vp_modern_map_capability(mdev, common, sizeof(struct virtio_pci_common_cfg), 4, 0, offsetofend(struct virtio_pci_modern_common_cfg, admin_queue_num) -> 将virtio配置结构(公共common部分)所在的BAR空间MAP到内核地址空间
                pci_read_config_byte(dev, off + offsetof(struct virtio_pci_cap,bar), &bar) -> 读取PCIE配置空间的值到bar变量中
                pci_read_config_dword(dev, off + offsetof(struct virtio_pci_cap, offset), &offset)
                p = pci_iomap_range(dev, bar, offset, length) -> 为PCI BAR创建一个虚拟映射
                *pa = pci_resource_start(dev, bar) + offset -> kick_addr? set common config addr
            mdev->isr = vp_modern_map_capability(mdev, isr, sizeof(u8), 1, 0, 1, NULL, NULL)
            pci_read_config_dword(pci_dev, notify + offsetof(struct virtio_pci_notify_cap, notify_off_multiplier), &mdev->notify_offset_multiplier)
            mdev->notify_base = vp_modern_map_capability(mdev, notify, 2, 2, 0, notify_length, &mdev->notify_len, &mdev->notify_pa) -> 我们事先不知道要映射多少个 VQ。如果通知长度较小，则现在映射所有 VQ。否则，稍后单独映射每个 VQ -> 将pci总线域 notify 地址通过 ioremap 映射到存储器域
            mdev->device = vp_modern_map_capability(mdev, device, 0, 4, 0, PAGE_SIZE, &mdev->device_len, NULL)
        if (mdev->device)
            vp_dev->vdev.config = &virtio_pci_config_ops
        else 
            vp_dev->vdev.config = &virtio_pci_config_nodev_ops -> virtio-pci-modern：分解出现代设备初始化逻辑，此补丁将现代设备初始化逻辑分解为辅助程序。请注意，它仍然依赖于调用者来启用 pci 设备，这允许调用者使用例如 devres
        vp_dev->setup_vq = setup_vq
        vp_dev->del_vq = del_vq
    register_virtio_device(&vp_dev->vdev)
        dev->dev.bus = &virtio_bus
        device_initialize(&dev->dev)
        dev_set_name(&dev->dev, "virtio%u", dev->index)
        virtio_device_of_init(dev) -> open firmware device tree
            char compat[] = "virtio,deviceXXXXXXXX"
            count = of_get_available_child_count(pnode) -> 查看节点的 compatible 属性是否有包含 compat 指定的字符串，也就是检查设备节点的兼容性
            of_get_next_available_child
        INIT_LIST_HEAD(&dev->vqs)
        virtio_reset_device(dev) -> dev->config->reset(dev)
        virtio_add_status(dev, VIRTIO_CONFIG_S_ACKNOWLEDGE)
        device_add(&dev->dev) -> match net/fs/block... driver
            if (dev->init_name) <- drivers/base/core.c
                dev_set_name(dev, "%s", dev->init_name)
            




config VIRTIO_BLK -> Virtio block driver
module_init(virtio_blk_init);
    major = register_blkdev(0, "virtblk")
    register_virtio_driver(&virtio_blk)
        driver->driver.bus = &virtio_bus
        return driver_register(&driver->driver)

static struct virtio_driver virtio_blk = {
	.feature_table			= features,
	.feature_table_size		= ARRAY_SIZE(features),
	.feature_table_legacy		= features_legacy,
	.feature_table_size_legacy	= ARRAY_SIZE(features_legacy),
	.driver.name			= KBUILD_MODNAME,
	.driver.owner			= THIS_MODULE,
	.id_table			= id_table,
	.probe				= virtblk_probe,
	.remove				= virtblk_remove,
	.config_changed			= virtblk_config_changed,
#ifdef CONFIG_PM_SLEEP
	.freeze				= virtblk_freeze,
	.restore			= virtblk_restore,
#endif
};

static int virtblk_probe(struct virtio_device *vdev)
    ida_alloc_range(&vd_index_ida, 0,
    查询BLK设备支持的分段数量
    virtio_cread_feature(vdev, VIRTIO_BLK_F_SEG_MAX, struct virtio_blk_config, seg_max, &sg_elems) -> #define virtio_cread_feature(vdev, fbit, structname, member, ptr)
        virtio_has_feature(vdev, fbit)
        virtio_cread((vdev), structname, member, ptr)
            vdev->config->get((vdev),
                static void vp_get
            or
            __virtio_cread_many
    vdev->priv = vblk = kmalloc(sizeof(*vblk), GFP_KERNEL)
    INIT_WORK(&vblk->config_work, virtblk_config_changed_work) -> virtio_blk：允许在运行时重新读取配置空间，连接 virtio_driver config_changed 方法以获取有关主机引发的配置更改的通知。目前，我们只是重新读取设备大小以支持设备在线调整大小，但一旦我们添加更多可能更改的属性，它们也可以添加。请注意，config_changed 方法是从 irq 上下文调用的，因此我们必须使用工作队列基础结构为我们的更改提供适当的用户上下文
        virtblk_update_capacity(vblk, true) -> virtio_blk：探测时打印容量 在探测驱动程序时打印块设备的容量。许多用户都期望这一点，因为 SCSI 磁盘 (sd) 会这样做。此外，内核 dmesg 输出是故障排除信息的主要来源，因此将磁盘大小包含在内会很有帮助。当发生调整大小事件时，virtio_blk 已经打印了容量。提取代码并从 virtblk_probe() 中重用它。此补丁还将块设备名称添加到消息中，以便可以将其与特定设备关联：virtio_blk virtio0：[vda] 20971520 512 字节逻辑块（10.7 GB/10.0 GiB）
            virtio_cread(vdev, struct virtio_blk_config, capacity, &capacity)
            nblocks = DIV_ROUND_UP_ULL(capacity, queue_logical_block_size(q) >> 9)
            dev_notice(&vdev->dev, "[%s] %s%llu %d-byte logical blocks (%s/%s)\n", vblk->disk->disk_name, resize ? "new size: " : "", nblocks, queue_logical_block_size(q), cap_str_10,cap_str_2);
                dmesg ref: [273680.379128] virtio_blk virtio0: [vda] 262144 512-byte logical blocks (134 MB/128 MiB)
            set_capacity_and_notify(vblk->disk, capacity)
    init_vq(vblk)
        virtio_cread_feature(vdev, VIRTIO_BLK_F_MQ, struct virtio_blk_config, num_queues, &num_vqs) -> /* 读取pci配置空间中的磁盘队列数 */, multi_q, 多队列支持
        num_vqs = min_t(unsigned int, min_not_zero(num_request_queues, nr_cpu_ids),num_vqs) -> 取配置队列数与CPU总数的最小值，作为队列数
        vblk->vqs = kmalloc_array(num_vqs, sizeof(*vblk->vqs), GFP_KERNEL)
        virtio_find_vqs(vdev, num_vqs, vqs, callbacks, names, &desc)
    vblk->tag_set.ops = &virtio_mq_ops
    blk_mq_alloc_tag_set(&vblk->tag_set)
    vblk->disk = blk_mq_alloc_disk(&vblk->tag_set, vblk)
    virtblk_name_format("vd", index, vblk->disk->disk_name, DISK_NAME_LEN) -> vda/vdb...vdz
    vblk->disk->fops = &virtblk_fops
	vblk->tag_set.nr_hw_queues = vblk->num_vqs
    virtblk_update_cache_mode -> virtio-blk：允许在写回和写通之间切换主机缓存，此补丁增加了对新 VIRTIO_BLK_F_CONFIG_WCE 功能的支持，该功能在配置空间中公开缓存模式并允许驱动程序对其进行修改。缓存模式通过 sysfs 公开。即使主机不支持新功能，缓存模式也是可见的（得益于现有的 VIRTIO_BLK_F_WCE），但不可修改
        blk_queue_write_cache(vblk->disk->queue, writeback, false) -> 强制单元访问(FUA)
    set_disk_ro(vblk->disk, 1)?
    blk_queue_max_segments(q, sg_elems)
        q->limits.max_segments = max_segments
    blk_queue_max_hw_sectors
    virtio_cread_feature(vdev, VIRTIO_BLK_F_SIZE_MAX,
    blk_queue_max_segment_size(q, max_size)
        q->limits.max_segment_size = max_size
    virtio_cread_feature(vdev, VIRTIO_BLK_F_BLK_SIZE,
    queue_logical_block_size
    virtblk_update_capacity
    virtio_device_ready
        dev->config->set_status(dev, status | VIRTIO_CONFIG_S_DRIVER_OK)
    device_add_disk(&vdev->dev, vblk->disk, virtblk_attr_groups) -> block/genhd.c -> add disk information to kernel list
        dev_set_name(ddev, "%s", disk->disk_name)
        device_add(ddev)
        disk_alloc_events(disk)
        sysfs_create_link(block_depr, &ddev->kobj, kobject_name(&ddev->kobj))
        bdev_add(disk->part0, ddev->devt)
			insert_inode_hash(bdev->bd_inode)
        if (get_capacity(disk))
            disk_scan_partitions(disk, BLK_OPEN_READ)
				handle = bdev_open_by_dev(disk_devt(disk), mode & ~BLK_OPEN_EXCL, NULL,
					devcgroup_check_permission
					bdev = blkdev_get_no_open
						inode = ilookup(blockdev_superblock, dev)
        disk_uevent(disk, KOBJ_ADD)
        disk_update_readahead(disk)
        disk_add_events(disk)

# 前端驱动写Status位，val & VIRTIO_CONFIG_S_DRIVER_OK, 这时候前端驱动已经ready
然后QEMU/KVM后端的处理流程如下：
virtio_pci_config_write  --> virtio_ioport_write --> virtio_pci_start_ioeventfd
--> virtio_bus_set_host_notifier --> virtio_bus_start_ioeventfd --> virtio_device_start_ioeventfd_impl
--> virtio_bus_set_host_notifier
    --> virtio_pci_ioeventfd_assign
        --> memory_region_add_eventfd
            --> memory_region_transaction_commit
              --> address_space_update_ioeventfds
                --> address_space_add_del_ioeventfds
                  --> kvm_io_ioeventfd_add/vhost_eventfd_add
                    --> kvm_set_ioeventfd_pio
                      --> kvm_vm_ioctl(kvm_state, KVM_IOEVENTFD, &kick)


半虚拟化 virtio-blk 设备的使用。此示例显示具有 4 个 I/O 队列的 virtio-blk，后端设备为 disk.qcow2
qemu sim blk dev:
qemu-system-x86_64 -machine accel=kvm -vnc :0 -smp 4 -m 4096M \
    -net nic -net user,hostfwd=tcp::5023-:22 \
    -hda ol7.qcow2 -serial stdio \
    -device virtio-blk-pci,drive=drive0,id=virtblk0,num-queues=4 \
    -drive file=disk.qcow2,if=none,id=drive0

# ls /sys/block/vda/mq/
0  1  2  3


virtio_ioport_write


#kvm内核代码virt/kvm/eventfd.c中
kvm_vm_ioctl(KVM_IOEVENTFD)
  --> kvm_ioeventfd
    --> kvm_assign_ioeventfd
      --> kvm_assign_ioeventfd_idx
    case KVM_SET_USER_MEMORY_REGION
        SANITY_CHECK_MEM_REGION_FIELD(guest_phys_addr)
        SANITY_CHECK_MEM_REGION_FIELD(memory_size)
        SANITY_CHECK_MEM_REGION_FIELD(userspace_addr)
        kvm_vm_ioctl_set_memory_region(kvm, &mem)
            kvm_set_memory_region(kvm, mem) -> __kvm_set_memory_region(kvm, mem)
                check_memory_region_flags(kvm, mem)
                slots = __kvm_memslots(kvm, as_id)
                old = id_to_memslot(slots, id)
                kvm_set_memslot(kvm, old, NULL, KVM_MR_DELETE)
    ...
    case KVM_IRQFD
        kvm_irqfd(kvm, &data)
            if (args->flags & KVM_IRQFD_FLAG_DEASSIGN)
                kvm_irqfd_deassign(kvm, args)
            kvm_irqfd_assign(kvm, args)
                INIT_WORK(&irqfd->inject, irqfd_inject);
                    virqfd->thread(virqfd->opaque, virqfd->data)
                INIT_WORK(&irqfd->shutdown, irqfd_shutdown)
                eventfd = eventfd_ctx_fileget(f.file)
                init_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup)
                init_poll_funcptr(&irqfd->pt, irqfd_ptable_queue_proc)
                irqfd_update(kvm, irqfd)
                events = vfs_poll(f.file, &irqfd->pt)
                schedule_work(&irqfd->inject)
                #ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
                    irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
                    irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
                    irq_bypass_register_consumer(&irqfd->consumer)

# MMIO处理流程中（handle_ept_misconfig）最后会调用到ioeventfd_write通知QEMU。
/* MMIO/PIO writes trigger an event if the addr/val match */
static int
ioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,
                int len, const void *val)
{
        struct _ioeventfd *p = to_ioeventfd(this);

        if (!ioeventfd_in_range(p, addr, len, val))
                return -EOPNOTSUPP;

        eventfd_signal(p->eventfd, 1);
        return 0;
}

PCI_BASE_ADDRESS_SPACE_IO    

virtio_pci_legacy.c
static const struct virtio_config_ops virtio_pci_config_ops = {
	.get		= vp_get,
	.set		= vp_set,
	.get_status	= vp_get_status,
	.set_status	= vp_set_status,
	.reset		= vp_reset,
	.find_vqs	= vp_find_vqs,
	.del_vqs	= vp_del_vqs,
	.synchronize_cbs = vp_synchronize_vectors,
	.get_features	= vp_get_features,
	.finalize_features = vp_finalize_features,
	.bus_name	= vp_bus_name,
	.set_vq_affinity = vp_set_vq_affinity,
	.get_vq_affinity = vp_get_vq_affinity,
};


virtio_pci_modern.c
static const struct virtio_config_ops virtio_pci_config_ops = {
	.get		= vp_get,
	.set		= vp_set,
	.generation	= vp_generation, -> vp_modern_generation(&vp_dev->mdev)
        struct virtio_pci_common_cfg __iomem *cfg = mdev->common
        vp_ioread8(&cfg->config_generation) -> 当外设IO寄存器通过ioport_map()函数映射后，Linux内核如何要读写这些IO寄存器就必须通过 ioread8()、ioread16()、ioread32()等这类函数读写
	.get_status	= vp_get_status,
	.set_status	= vp_set_status,
	.reset		= vp_reset,
	.find_vqs	= vp_modern_find_vqs,
	.del_vqs	= vp_del_vqs,
	.synchronize_cbs = vp_synchronize_vectors,
	.get_features	= vp_get_features, -> vp_modern_get_features -> 驱动先写 device_feature_select 为0/1，然后再读取device_feature字段，来读取设备特性的低/高32位信息 -> vp_legacy_get_features -> ioread32(读寄存器 VIRTIO_PCI_HOST_FEATURES )
        vp_iowrite32(0, &cfg->device_feature_select)
        features = vp_ioread32(&cfg->device_feature)
        vp_iowrite32(1, &cfg->device_feature_select)
        features |= ((u64)vp_ioread32(&cfg->device_feature) << 32)
	.finalize_features = vp_finalize_features,
	.bus_name	= vp_bus_name,
	.set_vq_affinity = vp_set_vq_affinity,
	.get_vq_affinity = vp_get_vq_affinity,
		pci_irq_get_affinity(vp_dev->pci_dev, vp_dev->vqs[index]->msix_vector)
	.get_shm_region  = vp_get_shm_region,
        virtio_pci_find_shm_cap(pci_dev, id, &bar, &offset, &len) -> get share mem cap off
        phys_addr = pci_resource_start(pci_dev, bar) -> 得到某段寄存器映射到内存的地址(获取PCI设备资源起始地址)
        region->addr = (u64) phys_addr + offset -> set shm region addr
	.disable_vq_and_reset = vp_modern_disable_vq_and_reset, -> support VIRTIO_F_RING_RESET -> 步骤: 1. 通知设备重置队列，2. 回收已提交的缓冲区 3. 重置 vring（可能重新分配） 4. 将 vring mmap 到设备，并启用队列
        vp_modern_set_queue_reset(mdev, vq->index)
            vp_iowrite16(index, &cfg->cfg.queue_select)
            vp_iowrite16(1, &cfg->queue_reset)
            while (vp_ioread16(&cfg->cfg.queue_enable))
        list_del(&info->node) -> delete vq from irq handler
        INIT_LIST_HEAD(&info->node)
        __virtqueue_break(vq)
            WRITE_ONCE(vq->broken, true)
        synchronize_irq(pci_irq_vector(vp_dev->pci_dev, info->msix_vector))
	.enable_vq_after_reset = vp_modern_enable_vq_after_reset,
        vp_modern_get_queue_reset
        vp_modern_get_queue_enable
        vp_active_vq(vq, info->msix_vector)
        list_add(&info->node, &vp_dev->virtqueues)
        else
            INIT_LIST_HEAD(&info->node)
        __virtqueue_unbreak(vq)
        vp_modern_set_queue_enable(&vp_dev->mdev, index, true)
	.create_avq = vp_modern_create_avq,
	.destroy_avq = vp_modern_destroy_avq,
};


前端驱动通知后端：内核流程mark一下，PCI设备驱动流程这个后面可以学习一下，先扫描PCI bus发现是virtio设备再扫描virtio-bus
worker_thread 
    --> process_one_work 
    --> pciehp_power_thread 
    --> pciehp_enable_slot 
    --> pciehp_configure_device 
    --> pci_bus_add_devices 
    --> pci_bus_add_device 
    --> device_attach 
    --> __device_attach 
    --> bus_for_each_drv 
    --> __device_attach_driver 
    --> driver_probe_device 
    --> pci_device_probe 
    --> local_pci_probe 
    --> virtio_pci_probe 
    --> register_virtio_device 
    --> device_register 
    --> device_add 
    --> bus_probe_device 
    --> device_initial_probe 
    --> __device_attach 
    --> bus_for_each_drv 
    --> __device_attach_driver 
    --> driver_probe_device 
    --> virtio_dev_probe 
    --> virtnet_probe (网卡设备驱动加载的入口)

pci扫描执行完成后，在/sys/devices/pci0000:00目录下，创建virtio pci设备。并且在/sys/bus/pci/devices/目录下，创建相应对于pci设备的符号连接，同时在/sys/bus/pci/drivers/目录下，创建virtio-pci目录，目录下存在支持设备符号连接文件
static int virtio_dev_probe(struct device *_d) -> 上述注册函数调用执行完成，在/sys/bus/目录下，创建了一个新的目录virtio，在该目录下同时创建了两个文件夹为devices和drivers。表示创建virtio总线，总线支持设备与驱动devices和drivers目录下
    virtio_add_status(dev, VIRTIO_CONFIG_S_DRIVER) -> ioread8(ldev->ioaddr + VIRTIO_PCI_STATUS)
        dev->config->set_status(dev, dev->config->get_status(dev) | status)
    device_features = dev->config->get_features(dev) -> vp_get_features
    for (i = 0; i < drv->feature_table_size; i++) -> Figure out what features the driver supports
    为 virtio_ring, virtio_pci 保留标志位
    dev->config->finalize_features(dev) -> virtio：访问前确认所有功能(先清空,再设置)，功能协商的设计方式使得设备能够知道驱动程序将访问哪些配置字段 -> static int vp_finalize_features -> 客户机不应在未确认功能的情况下访问配置空间
        vring_transport_features(vdev)
            __virtio_clear_bit(vdev, i)
                vdev->features &= ~BIT_ULL(fbit)
        vp_transport_features(vdev, features)
            __virtio_set_bit(vdev, VIRTIO_F_SR_IOV)
            __virtio_set_bit(vdev, VIRTIO_F_RING_RESET)
            __virtio_set_bit(vdev, VIRTIO_F_ADMIN_VQ)
        vp_check_common_size
        vp_modern_set_features(&vp_dev->mdev, vdev->features)
            vp_iowrite32(0, &cfg->guest_feature_select)
                iowrite32(value, addr)
            ...
            vp_iowrite32(features >> 32, &cfg->guest_feature) -> ignore low 32 bit
    drv->validate(dev) -> allow drivers to validate features
    virtio_features_ok
        virtio_add_status(dev, VIRTIO_CONFIG_S_FEATURES_OK)
            vp_modern_set_status
                struct virtio_pci_common_cfg __iomem *cfg = mdev->common
                vp_iowrite8(status, &cfg->device_status)
        status = dev->config->get_status(dev)
    create_avq -> vp_modern_create_avq -> virtio-pci：引入管理 virtqueue 引入对管理 virtqueue 的支持。通过协商 VIRTIO_F_ADMIN_VQ 功能，驱动程序检测功能并创建一个管理 virtqueue。virtio pci 通用层中的管理 virtqueue 实现使多种类型的上层驱动程序（如 vfio、net、blk）能够利用它
        admin_q_num = vp_modern_avq_num(&vp_dev->mdev)
        vq = vp_dev->setup_vq(vp_dev, &vp_dev->admin_vq.info, avq->vq_index, NULL, avq->name, NULL, VIRTIO_MSI_NO_VECTOR)
            notify = vp_notify_with_data
                vring_notification_data
                    iowrite32(data, (void __iomem *)vq->priv)
            or notify = vp_notify -> iowrite16(vq->index, (void __iomem *)vq->priv)
            vq = vring_create_virtqueue(index, num, SMP_CACHE_BYTES, &vp_dev->vdev, true, true, ctx, notify, callback, name)
            vp_active_vq
                vp_modern_set_queue_size
                vp_modern_queue_address
                vp_modern_queue_vector
            vq->priv = (void __force *)vp_modern_map_vq_notify(mdev, index, NULL)
        vp_modern_set_queue_enable(&vp_dev->mdev, avq->info.vq->index, true)
    err = drv->probe(dev) -> virtblk_probe
    virtio_device_ready(dev)
        在设置 DRIVER_OK 之前，传输应确保 vq->broken 的可见性。请参阅传输特定的 set_status() 方法的注释。行为良好的设备将仅在 DRIVER_OK 之后通知 virtqueue，这意味着设备应该“看到”将 vq->broken 设置为 false 的一致内存写入，这是驱动程序在看到 DRIVER_OK 时完成的，然后以下驱动程序的 vring_interrupt() 将看到 vq->broken 为 false，因此我们不会丢失任何通知
        dev->config->set_status(dev, status | VIRTIO_CONFIG_S_DRIVER_OK)
    drv->scan(dev)
    virtio_config_enable
        dev->config_enabled = true
        __virtio_config_changed(dev) -> drv->config_changed(dev) -> virtblk_config_changed


3. virtio核心代码分析，以virtio-net为例,这里我们已virtio-net网卡为例，在没有使用vhost的情况下（网卡后端收发包都走QEMU处理）， 后端收发包走vhost的情况下有些不同，后面单独分析。  3.1 前后端握手流程 QEM模拟PCI设备对GuestOS进行呈现，设备驱动加载的时候尝试去初始化设备
# 先在PCI总线上调用probe设备，调用了virtio_pci_probe，然后再virtio-bus上调用virtio_dev_probe
# virtio_dev_probe最后调用到virtnet_probe
pci_device_probe --> local_pci_probe --> virtio_pci_probe --> register_virtio_device --> 
device_register --> device_add --> bus_probe_device --> device_initial_probe 
--> __device_attach --> bus_for_each_drv --> __device_attach_driver --> driver_probe_device --> 
virtio_dev_probe --> virtnet_probe


include/uapi/linux/virtio_pci.h



前端驱动加载(probe)的过程中，会去初始化 virtqueue，这个时候会去申请MSIx中断并注册中断处理函数：
virtnet_probe
  --> init_vqs
    --> virtnet_find_vqs
      --> vi->vdev->config->find_vqs [vp_modern_find_vqs]
        --> vp_find_vqs
          --> vp_find_vqs_msix // 为每virtqueue申请一个MSIx中断，通常收发各一个队列
            --> vp_request_msix_vectors // 主要的MSIx中断申请逻辑都在这个函数里面
              --> pci_alloc_irq_vectors_affinity // 申请MSIx中断描述符(__pci_enable_msix_range)
                --> request_irq  // 注册中断处理函数
	            // virtio-net网卡至少申请了3个MSIx中断：
                // 一个是configuration change中断（配置空间发生变化后，QEMU通知前端）
                // 发送队列1个MSIx中断，接收队列1MSIx中断




virtio_pci, commit: https://github.com/ssbandjl/linux/commit/3343660d8c62c6b00b2f15324ef3fcb6be207bfa


struct virtio_pci_cap {


static int virtnet_probe(struct virtio_device *vdev)
{
       // check后端是否支持多队列，并按情况创建队列
       /* Allocate ourselves a network device with room for our info */
        dev = alloc_etherdev_mq(sizeof(struct virtnet_info), max_queue_pairs);
        // 定义一个网络设备并配置一些属性，例如MAC地址
        dev->netdev_ops = &virtnet_netdev
        dev->ethtool_ops = &virtnet_ethtool_ops;
	    SET_NETDEV_DEV(dev, &vdev->dev);
        INIT_WORK(&vi->config_work, virtnet_config_changed_work)
        virtnet_set_big_packets
            vi->big_packets_num_skbfrags = guest_gso ? MAX_SKB_FRAGS : DIV_ROUND_UP(mtu, PAGE_SIZE)
        virtnet_init_settings -> ethtool：添加对 800Gbps 链路模式的支持，添加对 800Gbps 速度、每通道 100Gbps 链路模式的支持。如 IEEE 文档 [1] 幻灯片 21 中所述，所有采用 100G/通道的 802.3df 铜缆和光纤 PMD 基线都将得到支持。添加 IEEE 文档 [1] 第 5 页中提到并于 2022 年 10 月批准的相关 PMD [2]：BP - KR8 Cu 电缆 - CR8 MMF 50m - VR8 MMF 100m - SR8 SMF 500m - DR8 SMF 2km - DR8-2 [1]：https://www.ieee802.org/3/df/public/22_10/22_1004/shrikhande_3df_01a_221004.pdf [2]：https://ieee802.org/3/df/KeyMotions_3df_221005.pdf
            vi->speed = SPEED_UNKNOWN;
            vi->duplex = DUPLEX_UNKNOWN;
        // 初始化virtqueue
        err = init_vqs(vi);
        // 注册一个网络设备
        err = register_netdev(dev);
        // 写状态位DRIVER_OK，告诉后端，前端已经ready
        virtio_device_ready(vdev);
        _virtnet_set_queues
            virtnet_send_command(vi, VIRTIO_NET_CTRL_MQ, VIRTIO_NET_CTRL_MQ_VQ_PAIRS_SET, &sg)
                sg_init_one(&hdr, &vi->ctrl->hdr, sizeof(vi->ctrl->hdr))
                virtqueue_add_sgs(vi->cvq, sgs, out_num, 1, vi, GFP_ATOMIC)
        virtnet_cpu_notif_add
        netif_carrier_off(dev)
        // 将网卡up起来
        netif_carrier_on(dev);
}


network speed:
#define SPEED_10		10
#define SPEED_100		100
#define SPEED_1000		1000
#define SPEED_2500		2500
#define SPEED_5000		5000
#define SPEED_10000		10000
#define SPEED_14000		14000
#define SPEED_20000		20000
#define SPEED_25000		25000
#define SPEED_40000		40000
#define SPEED_50000		50000
#define SPEED_56000		56000
#define SPEED_100000		100000
#define SPEED_200000		200000
#define SPEED_400000		400000
#define SPEED_800000		800000

#define SPEED_UNKNOWN		-1


其中关键的流程是init_vqs，在vp_find_vqs_msix流程中会尝试去申请MSIx中断，这里前面已经有分析过了。 其中，"configuration changed" 中断服务程序vp_config_changed， virtqueue队列的中断服务程序是 vp_vring_interrupt。
init_vqs --> virtnet_find_vqs --> vi->vdev->config->find_vqs --> vp_modern_find_vqs
--> vp_find_vqs --> vp_find_vqs_msix
static int vp_find_vqs_msix(struct virtio_device *vdev, unsigned nvqs,
		struct virtqueue *vqs[], vq_callback_t *callbacks[],
		const char * const names[], bool per_vq_vectors,
		const bool *ctx,
		struct irq_affinity *desc)
{
        /* 为configuration change申请MSIx中断 */
	err = vp_request_msix_vectors(vdev, nvectors, per_vq_vectors,
			      per_vq_vectors ? desc : NULL);
        for (i = 0; i < nvqs; ++i) {
		 // 创建队列 --> vring_create_virtqueue --> vring_create_virtqueue_split --> vring_alloc_queue -> qemu初始化时，virtio的后端有数据结构VirtIODevice、VirtQueue和vring一模一样，前端和后端对应起来，都应该指向刚才创建的那一段内存。现在的问题是，刚才分配的内存在客户机的内核里面，如何告知qemu来访问这段内存呢？qemu模拟出来的virtio block device只是一个PCI设备，对于客户机来讲这是一个外部设备，可以通过给外部设备发送指令的方式告知外部设备，这就是代码中vp_iowrite16的作用，它会调用专门给外部设备发送指令的函数iowrite，告诉外部的PCI设备, 告知的有三个地址virtqueue_get_desc_addr、virtqueue_get_avail_addr、virtqueue_get_used_addr。从客户机角度来看，这里面的地址都是物理地址即GPA（Guest Physical Address），因为只有物理地址才是客户机和qemu程序都认可的地址，本来客户机的物理内存也是qemu模拟出来的。在qemu中，对PCI总线添加一个设备的时候，会调用virtio_pci_device_plugged
	         vqs[i] = vp_setup_vq(vdev, queue_idx++, callbacks[i], names[i],
                                ctx ? ctx[i] : false,
                                msix_vec);
		// 每个队列申请一个MSIx中断
                err = request_irq(pci_irq_vector(vp_dev->pci_dev, msix_vec),
                                  vring_interrupt, 0,
                                  vp_dev->msix_names[msix_vec],
                                  vqs[i]);
	}
vp_setup_vq流程再往下走就开始分配共享内存页，至此建立起共享内存通信通道。 值得注意的是一路传下来的callbacks参数其实传入了发送队列和接收队列的回调处理函数， 好家伙，从virtnet_find_vqs一路传递到了__vring_new_virtqueue中最终赋值给了vq->vq.callback。
static struct virtqueue *vring_create_virtqueue_split(
        unsigned int index,
        unsigned int num,
        unsigned int vring_align,
        struct virtio_device *vdev,
        bool weak_barriers,
        bool may_reduce_num,
        bool context,
        bool (*notify)(struct virtqueue *),
        void (*callback)(struct virtqueue *),
        const char *name)
{
       /* TODO: allocate each queue chunk individually */
        for (; num && vring_size(num, vring_align) > PAGE_SIZE; num /= 2) {
		// 申请物理页，地址赋值给queue
                queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
                                          &dma_addr,
                                          GFP_KERNEL|__GFP_NOWARN|__GFP_ZERO);
        }
        queue_size_in_bytes = vring_size(num, vring_align);
        vring_init(&vring, num, queue, vring_align); // 确定 descriptor table, available ring, used ring的位置。
}
我们看下如果virtqueue队列如果收到MSIx中断消息后，会调用哪个hook来处理？
irqreturn_t vring_interrupt(int irq, void *_vq)
{
        struct vring_virtqueue *vq = to_vvq(_vq);
        if (!more_used(vq)) {
                pr_debug("virtqueue interrupt with no work for %p\n", vq);
                return IRQ_NONE;
        }
        if (unlikely(vq->broken))
                return IRQ_HANDLED;
        pr_debug("virtqueue callback for %p (%p)\n", vq, vq->vq.callback);
        if (vq->vq.callback)
                vq->vq.callback(&vq->vq); -> virtblk_done
        return IRQ_HANDLED;
}
EXPORT_SYMBOL_GPL(vring_interrupt);
不难想到中断服务程序里面会调用队列上的callback。 我们再回过头来看下virtnet_find_vqs，原来接受队列的回调函数是skb_recv_done，发送队列的回调函数是skb_xmit_done。
static int virtnet_find_vqs(struct virtnet_info *vi)
{
       /* Allocate/initialize parameters for send/receive virtqueues */
        for (i = 0; i < vi->max_queue_pairs; i++) {
		callbacks[rxq2vq(i)] = skb_recv_done;
		callbacks[txq2vq(i)] = skb_xmit_done;
	}
}
OK，这个小节就到这里。Are you clear ?
3.2 virtio-net网卡收发在virtqueue上的实现
这里以virtio-net为例（非vhost-net模式）来分析一下网卡收发报文在virtio协议上的具体实现。 virtio-net模式下网卡收发包的流程为：
收包：Hardware => Host Kernel => Qemu => Guest
发包：Guest => Host Kernel => Qemu => Host Kernel => Hardware
3.2.1 virtio-net网卡发包
前面我们看到virtio-net设备初始化的时候会创建一个net_device设备： virtnet_probe -> alloc_etherdev_mq注册了netdev_ops = &virtnet_netdev， 这里virtnet_netdev是网卡驱动的回调函数集合（收发包和参数设置）。
static const struct net_device_ops netdev_ops = {
        .ndo_open               = rio_open,
        .ndo_start_xmit = start_xmit,
        .ndo_stop               = rio_close,
        .ndo_get_stats          = get_stats,
        .ndo_validate_addr      = eth_validate_addr,
        .ndo_set_mac_address    = eth_mac_addr,
        .ndo_set_rx_mode        = set_multicast,
        .ndo_do_ioctl           = rio_ioctl,
        .ndo_tx_timeout         = rio_tx_timeout,
};
网卡发包的时候调用ndo_start_xmit，将TCP/IP上层协议栈扔下来的数据发送出去。 对应到virtio网卡的回调函数就是start_xmit，从代码看就是将skb发送到virtqueue中， 然后调用virtqueue_kick通知qemu后端将数据包发送出去。
Guest内核里面的virtio-net驱动发包：
内核驱动 virtio_net.c
static netdev_tx_t start_xmit
    skb_tx_timestamp(skb)
	// 将skb放到virtqueue队列中
 	-> xmit_skb(sq, skb) -> sg_init_table,virtqueue_add_outbuf -> virtqueue_add
        virtqueue_add_packed(_vq, sgs, total_sg,
        or virtqueue_add_split(_vq, sgs, total_sg,
            virtqueue_kick
                // kick通知qemu后端去取
                virtqueue_kick_prepare && virtqueue_notify 
    check_sq_full_and_disable(vi, dev, sq)
    virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)
	// kick次数加1
	sq->stats.kicks++
Guest Kick后端从KVM中VMExit出来退出到Qemu用户态（走的是ioeventfd）由Qemu去将数据发送出去。 大致调用的流程是： virtio_queue_host_notifier_read -> virtio_net_handle_tx_bh -> virtio_net_flush_tx -> virtqueue_pop拿到发包(skb) -> qemu_sendv_packet_async
Qemu代码virtio-net相关代码:
virtio_queue_host_notifier_read -> virtio_queue_notify_vq
    -> vq->handle_output -> virtio_net_handle_tx_bh 队列注册的时候，回注册回调函数
        -> qemu_bh_schedule -> virtio_net_tx_bh
            -> virtio_net_flush_tx
	        -> virtqueue_pop
		-> qemu_sendv_packet_async // 报文放到发送队列上，写tap设备的fd去发包
		    -> tap_receive_iov -> tap_write_packet
		    
// 最后调用 tap_write_packet 把数据包发给tap设备投递出去
3.2.2 virtio-net网卡收包
网卡收包的时候，tap设备先收到报文，对应的virtio-net网卡tap设备fd变为可读， Qemu主循环收到POLL_IN事件调用回调函数收包。
tap_send -> qemu_send_packet_async -> qemu_send_packet_async_with_flags
    -> qemu_net_queue_send
        -> qemu_net_queue_deliver
	    -> qemu_deliver_packet_iov
	        -> nc_sendv_compat
		    -> virtio_net_receive
		        -> virtio_net_receive_rcu
virtio-net网卡收报最终调用了virtio_net_receive_rcu， 和发包类似都是调用virtqueue_pop从前端获取virtqueue element， 将报文数据填充到vring中然后virtio_notify注入中断通知前端驱动取结果。
这里不得不吐槽一下，为啥收包函数取名要叫tap_send。





Thread 3 hit Breakpoint 3, virtblk_probe (vdev=0xffff88811a36e400) at drivers/block/virtio_blk.c:693
693     {
(gdb) bt
#0  virtblk_probe (vdev=0xffff88811a36e400) at drivers/block/virtio_blk.c:693
#1  0xffffffff81765deb in virtio_dev_probe (_d=0xffff88811a36e410) at drivers/virtio/virtio.c:273
#2  0xffffffff8181a721 in call_driver_probe (drv=0xffffffffc000d0a0 <virtio_blk>, drv=0xffffffffc000d0a0 <virtio_blk>, dev=0xffff88811a36e410) at drivers/base/dd.c:517
#3  really_probe (drv=0xffffffffc000d0a0 <virtio_blk>, dev=0xffff88811a36e410) at drivers/base/dd.c:596
#4  really_probe (dev=0xffff88811a36e410, drv=0xffffffffc000d0a0 <virtio_blk>) at drivers/base/dd.c:541
#5  0xffffffff8181aa45 in __driver_probe_device (drv=drv@entry=0xffffffffc000d0a0 <virtio_blk>, dev=dev@entry=0xffff88811a36e410) at drivers/base/dd.c:751
#6  0xffffffff8181aae3 in driver_probe_device (drv=drv@entry=0xffffffffc000d0a0 <virtio_blk>, dev=dev@entry=0xffff88811a36e410) at drivers/base/dd.c:781
#7  0xffffffff8181aece in __device_attach_driver (drv=0xffffffffc000d0a0 <virtio_blk>, _data=<optimized out>) at drivers/base/dd.c:898
#8  0xffffffff81818462 in bus_for_each_drv (bus=<optimized out>, start=start@entry=0x0 <fixed_percpu_data>, data=data@entry=0xffffc90001cdf930, fn=fn@entry=0xffffffff8181ae20 <__device_attach_driver>)
    at drivers/base/bus.c:427
#9  0xffffffff8181a48d in __device_attach (dev=dev@entry=0xffff88811a36e410, allow_async=allow_async@entry=true) at drivers/base/dd.c:969
#10 0xffffffff8181b0a3 in device_initial_probe (dev=dev@entry=0xffff88811a36e410) at drivers/base/dd.c:1016
#11 0xffffffff8181947f in bus_probe_device (dev=dev@entry=0xffff88811a36e410) at drivers/base/bus.c:487
#12 0xffffffff81816b2a in device_add (dev=dev@entry=0xffff88811a36e410) at drivers/base/core.c:3396
#13 0xffffffff81765939 in register_virtio_device (dev=dev@entry=0xffff88811a36e400) at drivers/virtio/virtio.c:423
#14 0xffffffff8176d74f in virtio_pci_probe (pci_dev=0xffff8881302f0000, id=<optimized out>) at drivers/virtio/virtio_pci_common.c:552
#15 0xffffffff81688068 in local_pci_probe (_ddi=_ddi@entry=0xffffc90001cdfab8) at drivers/pci/pci-driver.c:323 -> pci_drv->probe(pci_dev, ddi->id)
#16 0xffffffff81689ba2 in pci_call_probe (id=<optimized out>, dev=0xffff8881302f0000, drv=0xffffffff830f0ee0 <virtio_pci_driver>) at drivers/pci/pci-driver.c:380
#17 __pci_device_probe (pci_dev=0xffff8881302f0000, drv=0xffffffff830f0ee0 <virtio_pci_driver>) at drivers/pci/pci-driver.c:405
#18 pci_device_probe (dev=0xffff8881302f00d0) at drivers/pci/pci-driver.c:448
#19 0xffffffff8181a721 in call_driver_probe (drv=0xffffffff830f0f58 <virtio_pci_driver+120>, drv=0xffffffff830f0f58 <virtio_pci_driver+120>, dev=0xffff8881302f00d0) at drivers/base/dd.c:517
#20 really_probe (drv=0xffffffff830f0f58 <virtio_pci_driver+120>, dev=0xffff8881302f00d0) at drivers/base/dd.c:596
#21 really_probe (dev=0xffff8881302f00d0, drv=0xffffffff830f0f58 <virtio_pci_driver+120>) at drivers/base/dd.c:541
#22 0xffffffff8181aa45 in __driver_probe_device (drv=drv@entry=0xffffffff830f0f58 <virtio_pci_driver+120>, dev=dev@entry=0xffff8881302f00d0) at drivers/base/dd.c:751
#23 0xffffffff8181aae3 in driver_probe_device (drv=drv@entry=0xffffffff830f0f58 <virtio_pci_driver+120>, dev=dev@entry=0xffff8881302f00d0) at drivers/base/dd.c:781
#24 0xffffffff8181aece in __device_attach_driver (drv=0xffffffff830f0f58 <virtio_pci_driver+120>, _data=<optimized out>) at drivers/base/dd.c:898
#25 0xffffffff81818462 in bus_for_each_drv (bus=<optimized out>, start=start@entry=0x0 <fixed_percpu_data>, data=data@entry=0xffffc90001cdfc00, fn=fn@entry=0xffffffff8181ae20 <__device_attach_driver>)
    at drivers/base/bus.c:427
#26 0xffffffff8181a48d in __device_attach (dev=dev@entry=0xffff8881302f00d0, allow_async=allow_async@entry=false) at drivers/base/dd.c:969
#27 0xffffffff8181a530 in device_attach (dev=dev@entry=0xffff8881302f00d0) at drivers/base/dd.c:1010
#28 0xffffffff8167c26f in pci_bus_add_device (dev=dev@entry=0xffff8881302f0000) at drivers/pci/bus.c:324
#29 0xffffffff8167c2c1 in pci_bus_add_devices (bus=bus@entry=0xffff888100dcf800) at drivers/pci/bus.c:347
#30 0xffffffff8167f4ca in pci_rescan_bus (bus=bus@entry=0xffff888100dcf800) at drivers/pci/probe.c:3249
#31 0xffffffff8168a458 in rescan_store (bus=<optimized out>, buf=<optimized out>, count=2) at drivers/pci/pci-sysfs.c:421
#32 0xffffffff81817fc4 in bus_attr_store (kobj=<optimized out>, attr=0xffffc90000031014, buf=0xffffc90000031014 "\v", count=4294967296) at drivers/base/bus.c:122
#33 0xffffffff8142c9ab in sysfs_kf_write (of=<optimized out>, buf=0x100000000 <error: Cannot access memory at address 0x100000000>, count=38, pos=<optimized out>) at fs/sysfs/file.c:139
#34 0xffffffff8142ba78 in kernfs_fop_write_iter (iocb=0xffffc90001cdfd88, iter=<optimized out>) at fs/kernfs/file.c:296
#35 0xffffffff8136441a in call_write_iter (file=0xffff888116e53300, iter=0xffffc90001cdfd60, kio=0xffffc90001cdfd88) at ./include/linux/fs.h:2163
#36 new_sync_write (filp=filp@entry=0xffff888116e53300, buf=buf@entry=0x5567d57f2080 "1\nd-symbol-file ./drivers/block/virtio_blk.ko 0xffffffffc000a000 -s .data 0xffffffffc000d000 -s .bss 0xffffffffc000d700\n", 
    len=len@entry=2, ppos=ppos@entry=0xffffc90001cdfe18) at fs/read_write.c:507
#37 0xffffffff81366c85 in vfs_write (pos=0xffffc90001cdfe18, count=2, buf=0x5567d57f2080 "1\nd-symbol-file ./drivers/block/virtio_blk.ko 0xffffffffc000a000 -s .data 0xffffffffc000d000 -s .bss 0xffffffffc000d700\n", 
    file=0xffff888116e53300) at fs/read_write.c:594
#38 vfs_write (file=0xffff888116e53300, buf=0x5567d57f2080 "1\nd-symbol-file ./drivers/block/virtio_blk.ko 0xffffffffc000a000 -s .data 0xffffffffc000d000 -s .bss 0xffffffffc000d700\n", count=<optimized out>, 
    pos=0xffffc90001cdfe18) at fs/read_write.c:574
#39 0xffffffff81366ee7 in ksys_write (fd=<optimized out>, buf=0x5567d57f2080 "1\nd-symbol-file ./drivers/block/virtio_blk.ko 0xffffffffc000a000 -s .data 0xffffffffc000d000 -s .bss 0xffffffffc000d700\n", count=2)
    at fs/read_write.c:647
#40 0xffffffff81366f7a in __do_sys_write (count=<optimized out>, buf=<optimized out>, fd=<optimized out>) at fs/read_write.c:659
#41 __se_sys_write (count=<optimized out>, buf=<optimized out>, fd=<optimized out>) at fs/read_write.c:656
#42 __x64_sys_write (regs=<optimized out>) at fs/read_write.c:656
#43 0xffffffff81cff979 in do_syscall_x64 (nr=<optimized out>, regs=0xffffc90001cdff58) at arch/x86/entry/common.c:50
#44 do_syscall_64 (regs=0xffffc90001cdff58, nr=<optimized out>) at arch/x86/entry/common.c:80
#45 0xffffffff81e0007c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:113
#48 0x00007f41692506a0 in ?? () at drivers/block/virtio_blk.c:1027
#49 0x0000000000000002 in fixed_percpu_data ()
Backtrace stopped: Cannot access memory at address 0xffffc90001ce0000
(gdb) 


modern
static const struct virtio_config_ops virtio_pci_config_nodev_ops = {
	.get		= NULL,
	.set		= NULL,
	.generation	= vp_generation,
	.get_status	= vp_get_status,
	.set_status	= vp_set_status,
	.reset		= vp_reset,
	.find_vqs	= vp_modern_find_vqs,
	.del_vqs	= vp_del_vqs,
	.get_features	= vp_get_features,
	.finalize_features = vp_finalize_features,
	.bus_name	= vp_bus_name,
	.set_vq_affinity = vp_set_vq_affinity,
	.get_vq_affinity = vp_get_vq_affinity,
	.get_shm_region  = vp_get_shm_region,
};

virtblk_probe
    virtio_cread_feature
        static void vp_get
            switch (len)
            case 2:
                w = cpu_to_le16(ioread16(device + offset))
                memcpy(buf, &w, sizeof w)
    num_vqs = min_t(unsigned int, nr_cpu_ids, num_vqs) -> 8
    for (i = 0; i < num_vqs; i++)
        callbacks[i] = virtblk_done -> virtblk_done(struct virtqueue *vq) <- vring_interrupt
            virtqueue_disable_cb(vq)
            while ((vbr = virtqueue_get_buf(vblk->vqs[qid].vq, &len)) -> virtqueue_get_buf_ctx -> virtqueue_get_buf_ctx_packed -> update ptr
                struct request *req = blk_mq_rq_from_pdu(vbr)
                blk_mq_complete_request(req)
                    if (!blk_mq_complete_request_remote(rq))
                            blk_mq_raise_softirq(rq)
                                raise_softirq(BLOCK_SOFTIRQ) -> blk_done_softirq
                        rq->q->mq_ops->complete(rq) -> virtblk_request_done
                blk_mq_start_stopped_hw_queues(vblk->disk->queue, true)
    int virtio_find_vqs -> virtqueue 是客户机前端对于队列管理的数据结构，在客户机的linux内核中通过kmalloc_array进行分配。而队列的实体需要通过函数 virtio_find_vqs 查找或者生成，所以这里还把callback函数指定为virtblk_done，当buffer使用发生变化的时候，需要调用这个callback函数进行通知
        vdev->config->find_vqs(vdev, nvqs, vqs, callbacks, names, NULL, desc) -> static int vp_modern_find_vqs
            vp_find_vqs(vdev, nvqs, vqs, callbacks, names, ctx, desc) -> Try MSI-X with one vector per queue
                vp_find_vqs_msix true
                    vp_request_msix_vectors
                        pci_alloc_irq_vectors_affinity(vp_dev->pci_dev, nvectors, nvectors, flags, desc)
                            struct irq_affinity msi_default_affd = {0}
                            affd = &msi_default_affd -> desc = NULL or {0}
                        request_irq(pci_irq_vector(vp_dev->pci_dev, v), vp_config_changed, 0, vp_dev->msix_names[v], vp_dev)
                        vp_dev->config_vector(vp_dev, v)
                        request_irq(pci_irq_vector(vp_dev->pci_dev, v), vp_vring_interrupt, 0, vp_dev->msix_names[v], vp_dev)
                    vp_setup_vq -> static struct virtqueue *setup_vq
                        notify = vp_notify_with_data
                        or notify = vp_notify
                        vring_create_virtqueue -> create the vring
                            if (virtio_has_feature(vdev, VIRTIO_F_RING_PACKED))
                                vring_create_virtqueue_packed -> default
                            vring_create_virtqueue_split
                                vring_init
                                __vring_new_virtqueue
                        err = vp_active_vq(vq, msix_vec)
                            vp_modern_set_queue_size(mdev, index, virtqueue_get_vring_size(vq))
                                vp_iowrite16(index, &mdev->common->queue_select)
                                vp_iowrite16(size, &mdev->common->queue_size)
                            vp_modern_queue_address(mdev, index, virtqueue_get_desc_addr(vq), virtqueue_get_avail_addr(vq), virtqueue_get_used_addr(vq)) -> 写qemu中的pci设备配置空间, 告知qemu, 然后qemu和虚机共用这段内存空间 -> virtio的后端有数据结构VirtIODevice、VirtQueue和vring一模一样，前端和后端对应起来，都应该指向刚才创建的那一段内存。现在的问题是，刚才分配的内存在客户机的内核里面，如何告知qemu来访问这段内存呢？qemu模拟出来的virtio block device只是一个PCI设备，对于客户机来讲这是一个外部设备，可以通过给外部设备发送指令的方式告知外部设备，这就是代码中vp_iowrite16的作用，它会调用专门给外部设备发送指令的函数iowrite，告诉外部的PCI设备 -> write vq info to device config space
                                vp_iowrite16(index, &cfg->queue_select)
                                vp_iowrite64_twopart(desc_addr, &cfg->queue_desc_lo, &cfg->queue_desc_hi)
                                vp_iowrite64_twopart(driver_addr, &cfg->queue_avail_lo, &cfg->queue_avail_hi)
                                vp_iowrite64_twopart(device_addr, &cfg->queue_used_lo, &cfg->queue_used_hi)
                            msix_vec = vp_modern_queue_vector(mdev, index, msix_vec)
                        vq->priv = (void __force *)vp_modern_map_vq_notify(mdev, index, NULL)
                            u16 off = vp_modern_get_queue_notify_off(mdev, index) -> vp_ioread16(&mdev->common->queue_notify_off)
                            vp_iowrite16(index, &cfg->queue_select)
                            vp_iowrite16(vector, &cfg->queue_msix_vector)
                    request_irq(pci_irq_vector(vp_dev->pci_dev, msix_vec), vring_interrupt, 0, vp_dev->msix_names[msix_vec], vqs[i])
                vp_find_vqs_msix false
                vp_find_vqs_intx(vdev, nvqs, vqs, callbacks, names, ctx) -> Finally fall back to regular interrupts -> 通过request_irq注册一个中断处理函数vp_interrupt，当设备的配置信息发生改变会产生一个中断，当设备向队列中写入信息时也会产生一个中断，称为vq中断，中断处理函数需要调用相应的队列的回调函数。然后，根据队列的数目依次调用vp_setup_vq，完成virtqueue、vring的分配和初始化
                    request_irq(vp_dev->pci_dev->irq, vp_interrupt, IRQF_SHARED, dev_name(&vdev->dev), vp_dev) -> virtio_pci：将 vp_try_to_find_vqs 拆分为 INTx 和 MSI-X 变体，vp_try_to_find_vqs 中的 INTx 和 MSI-X 情况之间基本上没有共享逻辑，因此将函数拆分为两个并稍微清理一下。同时删除相当无意义的 vp_request_intx 包装器
                        isr = ioread8(vp_dev->isr)
                        vp_config_changed(irq, opaque)
                            drv->config_changed(dev) -> virtblk_config_changed
                                queue_work(virtblk_wq, &vblk->config_work)
                        vp_vring_interrupt(irq, opaque)
                            list_for_each_entry(info, &vp_dev->virtqueues, node)
                                vring_interrupt(irq, info->vq)
                    vqs[i] = vp_setup_vq(vdev, queue_idx++, callbacks[i], names[i], ctx ? ctx[i] : false, VIRTIO_MSI_NO_VECTOR)
            list_for_each_entry(vq, &vdev->vqs, list) -> 选择并激活所有队列。必须最后完成：一旦我们这样做，除了重置外没有其他办法可以返回。
                vp_modern_set_queue_enable(&vp_dev->mdev, vq->index, true)
                    vp_iowrite16(index, &mdev->common->queue_select)
                    vp_iowrite16(enable, &mdev->common->queue_enable)



static const struct sysfs_ops bus_sysfs_ops = {
	.show	= bus_attr_show,
	.store	= bus_attr_store,
        bus_attr->store(subsys_priv->bus, buf, count) -> rescan_store
};


static void vp_reset(struct virtio_device *vdev)
    vp_modern_set_status
    vp_modern_get_status
    vp_modern_avq_deactivate -> virtio-pci：引入管理命令发送功能，添加通过管理 virtqueue 接口发送管理命令的支持。设备重置完成后，中止任何正在进行的管理命令。设备准备就绪时激活管理队列；设备重置时停用。为了符合以下规范声明 [1]，仅在设置 DRIVER_OK 状态后，管理 virtqueue 才会为上层用户激活。[1] 在设置 DRIVER_OK 之前，驱动程序不得向设备发送任何缓冲区可用通知
        __virtqueue_break(admin_vq->info.vq)
            WRITE_ONCE(vq->broken, true) -> 用于描述符环形缓冲区实现的 Virtio 辅助例程，这些辅助例程为想要使用 virtio 环形的虚拟机管理程序提供大部分 virtqueue_ops。与之前的 lguest 实现不同：1) 环形大小可变（2^n-1 个元素）。2) 它们每个 sg 元素的上限为 65535 字节。3) 页码始终为 64 位（PAE 有人知道吗？）4) 它们不再将 used[] 放在单独的页面上，而只是单独的缓存行。5) 我们对变量进行取模。如果我们在意的话，我们可能会很棘手。6) 使用环形内的标志抑制中断和通知。用户只需获取环形页面并提供通知挂钩（KVM 希望客户分配环形，lguest 可以明智地完成此操作）
	vp_synchronize_vectors(vdev) -> flush vq/config cb, wait for pending irq handlers
        synchronize_irq(vp_dev->pci_dev->irq) -> 对 virtqueues 使用共享中断 -> 用于等待PENDING状态的中断处理函数结束（中断处理包括硬中断的处理以及中断线程的处理）
            struct irq_desc *desc = irq_to_desc(irq) -> get irq desc
            __synchronize_irq(desc) -> wait hw irq complete -> 其中 __synchronize_hardirq()函数用于等待硬中断的处理完成，在函数handle_irq_event()中处理硬中断之前设置IRQD_IRQ_INPROGRESS，在处理完成后清IRQD_IRQ_INPROGRESS。 __synchronize_hardirq()检查IRQD_IRQ_INPROGRESS是否清除
                __synchronize_hardirq(desc, true)
                wait_event(desc->wait_for_threads, !atomic_read(&desc->threads_active)) -> 等待中断线程处理结束  <- irq_thread -> wake_threads_waitq(desc); //唤醒等待队列
        or synchronize_irq(pci_irq_vector(vp_dev->pci_dev, i))


static int irq_thread(void *data)
    handler_fn = irq_thread_fn
    irq_thread_check_affinity(desc, action)
    wake_threads_waitq(desc)



static const struct blk_mq_ops virtio_mq_ops = {
	.queue_rq	= virtio_queue_rq,
	.queue_rqs	= virtio_queue_rqs,
	.commit_rqs	= virtio_commit_rqs,
	.complete	= virtblk_request_done,
        virtblk_unmap_data(req, vbr)
            if (blk_rq_nr_phys_segments(req)) -> 发送到设备的物理段数。通常，这是提交者发送的不连续数据段数。但对于像丢弃这样的无数据命令，我们可能没有提交实际的数据段，但驱动程序可能必须添加自己的特殊有效负载。在这种情况下，我们仍然在这里返回 1，以便映射此特殊有效负载
                    if (rq->rq_flags & RQF_SPECIAL_PAYLOAD) -> resturn 1
                    return rq->nr_phys_segments
                sg_free_table_chained(&vbr->sg_table, VIRTIO_BLK_INLINE_SG_CNT) -> virtio-blk：避免为数据预分配大 SGL，不再需要为 IO SGL 预分配大缓冲区。如果设备有大量深队列，则 sg 列表的预分配可能会消耗大量内存。对于 HW virtio-blk 设备，nr_hw_queues 可以是 64 或 128，每个队列的深度可能是 128。这意味着数据 SGL 的最终预分配很大。对于长度超过 2 个条目的列表，切换到 SGL 的运行时分配。这是 NVMe 驱动程序使用的方法，因此对于 virtio 块也应该是合理的。运行时 SGL 分配一直是传统 I/O 路径的情况，所以这并不是什么新鲜事。预分配的小 SGL 依赖于 SG_CHAIN，因此如果 ARCH 不支持 SG_CHAIN，则仅使用 SGL 的运行时分配。重新组织 IO 请求的设置以适应新的 sg 链机制。未看到性能下降（具有 16 个作业和 128 个 iodepth 的 fio libaio 引擎）：
                    __sg_free_table(table, SG_CHUNK_SIZE, nents_first_chunk, sg_pool_free, table->orig_nents)
        virtblk_cleanup_cmd(req)
            kfree(bvec_virt(&req->special_vec))
        blk_mq_end_request(req, status) -> virtio-blk：添加对分区块设备的支持，此补丁为内核 virtio-blk 驱动程序添加了对分区块设备 (ZBD) 的支持。此补丁随附了目前正在提议标准化的 virtio-blk ZBD 支持草案。草案的最新版本链接为 https://github.com/oasis-tcs/virtio-spec/issues/143 。实现这些协议扩展的 QEMU 分区设备代码由 Sam Li 开发，目前正在 QEMU 邮件列表中进行审核。引入了许多 virtblk 请求结构更改，以适应特定于分区块设备的功能，最重要的是，为将设备中的分区附加扇区值连同请求状态一起传回驱动程序腾出空间。补丁中的区域特定代码受到 drivers/nvme/host/zns.c 中 NVMe ZNS 代码的很大影响，但它更简单，因为提议的 virtio ZBD 草案仅涵盖与 Linux 块层提供的区域功能相关的区域设备功能。包括以下修复：virtio-blk：修复没有 CONFIG_BLK_DEV_ZONED 的探测在没有 CONFIG_BLK_DEV_ZONED 的情况下构建时，VIRTIO_BLK_F_ZONED 被排除在驱动程序功能数组之外。因此，virtio_has_feature 在 vi​​rtio_check_driver_offered_feature 中发生恐慌，因为这在设计上会验证我们正在检查的功能是否列在功能数组中。要修复，请用存根替换对 virtio_has_feature 的调用 -> block：引入新的块状态代码类型，目前我们在块层中使用普通的 Linux errno 值，虽然我们接受任何错误，但其中一些具有重载的魔法含义。此补丁反而引入了一个新的 blk_status_t 值，该值包含块层特定的状态代码并明确解释其含义。目前提供了从以前的特殊含义转换的帮助程序，但我怀疑我们想从长远来看摆脱它们 - 那些具有 errno 输入（例如网络）的驱动程序通常会得到不知道特殊块层重载的 errnos，同样将它们返回到用户空间通常会返回一些严格来说对文件系统操作不正确的东西，但这留待以后再做练习。目前，错误集是一个非常有限的集合，与以前重载的 errno 值非常接近，但有一些容易实现的改进方法。 blk_status_t (ab) 使用稀疏 __bitwise 注释来进行稀疏类型检查，这样我们就可以轻松捕获传递错误值的地方
            blk_update_request(rq, error, blk_rq_bytes(rq))
            __blk_mq_end_request(rq, error)
                blk_mq_sched_completed_request
                blk_mq_finish_request
                    q->elevator->type->ops.finish_request(rq)
	.map_queues	= virtblk_map_queues,
		blk_mq_virtio_map_queues
			for (queue = 0; queue < qmap->nr_queues; queue++)
				mask = vdev->config->get_vq_affinity(vdev, first_vec + queue)
				for_each_cpu(cpu, mask)
					qmap->mq_map[cpu] = qmap->queue_offset + queue
	.poll		= virtblk_poll, -> static int virtblk_poll(struct blk_mq_hw_ctx *hctx, struct io_comp_batch *iob)

};

struct virtblk_req {
    struct virtio_blk_outhdr out_hdr
    ...
    struct sg_table sg_table;
    struct scatterlist sg[];
}


virtio_queue_rq
    int qid = hctx->queue_num -> support multi_q
    struct virtio_blk *vblk = hctx->queue->queuedata;
    struct virtblk_req *vbr = blk_mq_rq_to_pdu(req)
    virtblk_prep_rq(hctx, vblk, req, vbr)
        virtblk_setup_cmd(vblk->vdev, req, vbr)
            case REQ_OP_READ:
		        type = VIRTIO_BLK_T_IN;
            case REQ_OP_WRITE
                type = VIRTIO_BLK_T_OUT
                sector = blk_rq_pos(req)
        num = virtblk_map_data(hctx, req, vbr)
            vbr->sg_table.sgl = vbr->sg
            sg_alloc_table_chained(&vbr->sg_table, blk_rq_nr_phys_segments(req), vbr->sg_table.sgl, VIRTIO_BLK_INLINE_SG_CNT) -> 分配内存以保存数据 SG 的信息，并调用 blk_rq_map_sg 映射数据 SG
            blk_rq_map_sg(hctx->queue, req, vbr->sg_table.sgl) -> request映射, 使用blk_rq_map_sg把request映射到一个scatterlist，在__blk_segment_map_sg中，BIOVEC_PHYS_MERGEABLE检测相邻bio_vec是否可以合并，BIOVEC_SEG_BOUNDARY用于检测是否超过边界，比如某些硬件不能跨64MB边界传输，其单次发起DMA的最大大小就是64MB，可以通过queue_max_segment_size来控制scatterlist的表项
                //block\blk-merge.c
                __blk_bios_map_sg
                    __blk_segment_map_sg
        blk_mq_start_request(req)
            WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT)
            rq->mq_hctx->tags->rqs[rq->tag] = rq
    virtblk_add_req(vblk->vqs[qid].vq, vbr) -> virtio-blk：支持 mq_ops->queue_rqs()，此补丁支持 mq_ops->queue_rqs() 钩子。它具有批量提交给 virtio-blk 驱动程序的优势。它还有助于轮询 I/O，因为轮询使用块层的批量完成。queue_rqs() 中的批量提交可以提高轮询性能。在queue_rqs()中，它迭代 plug->mq_list，收集属于同一 HW 队列的请求，直到遇到来自其他 HW 队列的请求或看到列表末尾。然后，virtio-blk 将请求添加到 virtqueue 并踢出 virtqueue 提交请求。如果有错误，它会将错误请求插入 requeue_list 并将其传递给普通块层路径。为了验证，我做了 fio 测试
        sg_init_one(&out_hdr, &vbr->out_hdr, sizeof(vbr->out_hdr))
        virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC) -> scatter-list成员入virtqueue的vring
            virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs, data, NULL, gfp)
    virtqueue_kick_prepare(vblk->vqs[qid].vq)
        virtqueue_kick_prepare_packed
            needs_kick = vring_need_event(event_idx, new, old) -> virtio_ring：利用打包环中的事件 IDX，利用打包环中的 EVENT_IDX 功能在可用时抑制事件
                return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old) -> old是add_sg之前的avail.idx，而new是当前的avail.idx, 理解为当前可用buffer小于原来可用buffer时, 需要通知后端 ->  old表示上次kick后的avail.idx，new是当前的avail.idx，两者的差值就是vq->num_added，也就是自上次kick后端后前端又积累的desc chain数量。另外vq->event是在vq初始化的时候设置的 -> 后端处理的位置event_idx超过了old，表示后端QEMU处理的速度够快，索引返回true，通知(kick)后端，通知后端有新的avail 逻辑buf，请你继续处理 -> 后端处理的位置event_idx落后于上次添加avail ring的位置，说明后端处理较慢，返回false，那么前端就先不通知(kick)，积攒一下，反正后端正处理不过来，下次退出的时候，让后端一起尽情处理
        or virtqueue_kick_prepare_split
    virtqueue_notify(vblk->vqs[qid].vq) -> bool virtqueue_notify(struct virtqueue *_vq) -> vm kernel notify qemu
        vq->notify(_vq)) -> 
            vp_notify
                iowrite16(vq->index, (void __iomem *)vq->priv) -> qemu: virtio_pci_notify_write -> 参考QEMU前端通知机制: https://blog.csdn.net/huang987246510/article/details/105496843
            or vp_notify_with_data
                u32 data = vring_notification_data(vq)
                iowrite32(data, (void __iomem *)vq->priv)
        vq->broken = true


int kvm_cpu_exec(CPUState *cpu)
{
    struct kvm_run *run = cpu->kvm_run;
    int ret, run_ret;
......
    run_ret = kvm_vcpu_ioctl(cpu, KVM_RUN, 0);
......
    switch (run->exit_reason) {
        case KVM_EXIT_IO:
            DPRINTF("handle_io\n");
            /* Called outside BQL */
            kvm_handle_io(run->io.port, attrs,
                          (uint8_t *)run + run->io.data_offset,
                          run->io.direction,
                          run->io.size,
                          run->io.count);
            ret = 0;
            break;
    }
......
}



mailbox
ref: https://blog.csdn.net/u010961173/article/details/96422441
drivers/mailbox/mailbox-test.c
mbox_create(&mbox, 32)
mbox_send_message(&mbox, "Hello World!", sizeof("Hello World!"))
    add_to_rbuf(chan, mssg)
    msg_submit(chan)
        chan->cl->tx_prepare(chan->cl, data)
        chan->mbox->ops->send_data(chan, data)
        hrtimer_start(&chan->mbox->poll_hrt, 0, HRTIMER_MODE_REL)
    wait = msecs_to_jiffies
    ret = wait_for_completion_timeout(&chan->tx_complete, wait)
    tx_tick(chan, t)
        complete(&chan->tx_complete)

static const struct file_operations mbox_test_message_ops = {
	.write	= mbox_test_message_write,
	.read	= mbox_test_message_read,
	.fasync	= mbox_test_message_fasync,
	.poll	= mbox_test_message_poll,
	.open	= simple_open,
	.llseek	= generic_file_llseek,
};


drivers/mailbox/hi6220-mailbox.c -> core_initcall(hi6220_mbox_init) -> mbox：Hi6220：添加mbox驱动 添加Hi6220mbox驱动，mbox与MCU通信；发送数据时，可以支持两种底层实现方式：一种是使用中断作为确认，另一种是自动模式，无需任何确认。这两种方式在驱动中都已支持。接收数据时，将依靠中断来通知通道有消息传入。现在mbox驱动用于向MCU发送消息，以控制CPU、GPU和DDR的动态电压和频率调节
    ...
    hi6220_mbox_probe
        mbox->ipc = devm_platform_ioremap_resource(pdev, 0)
        devm_request_irq(dev, mbox->irq, hi6220_mbox_interrupt, 0, dev_name(dev), mbox)
            state = readl(ACK_INT_STAT_REG(mbox->ipc))
            intr_bit = __ffs(state)
            mbox_chan_txdone(chan, 0)
            mbox_chan_received_data(chan, (void *)msg)
        devm_mbox_controller_register(dev, &mbox->controller)
        platform_set_drvdata(pdev, mbox)







int vp_set_vq_affinity(struct virtqueue *vq, const struct cpumask *cpu_mask)
    irq = pci_irq_vector(vp_dev->pci_dev, info->msix_vector)
    if (!cpu_mask)
        irq_update_affinity_hint(irq, NULL)
            __irq_set_affinity(irq, m, false)
                irq_set_affinity_locked(irq_desc_get_irq_data(desc), mask, force)
                    irq_try_set_affinity(data, mask, force)
    else
        cpumask_copy(mask, cpu_mask)
        irq_set_affinity_and_hint(irq, mask) -> __irq_apply_affinity_hint
            __irq_set_affinity(irq, m, false)



virtio_blk IO路径/全流程
总结一下存储虚拟化的场景下，整个写入的过程，如下图所示：
（1）在虚拟机里面，应用层调用write系统调用写入文件。
（2）write系统调用进入虚拟机里面的内核，经过VFS、通用块设备层、I/O调度层，到达块设备驱动。
（3）虚拟机里面的块设备驱动是virtio_blk，它和通用的块设备驱动一样有一个request queue，另外有一个函数 make_request_fn 会被设置为blk_mq_make_request，这个函数用于将请求放入队列。
（4）虚拟机里面的块设备驱动是 virtio_blk，会注册一个中断处理函数 vp_interrupt 。当qemu写入完成之后，它会通知虚拟机里面的块设备驱动。
（5）blk_mq_make_request 最终调用 virtqueue_add，将请求添加到传输队列 virtqueue 中，然后调用 virtqueue_notify 通知 qemu。
   ------------------------------------------------------------------------------------------------------------
（6）在qemu中，本来虚拟机正处于KVM_RUN的状态即处于客户机状态。
（7）qemu收到通知后，通过VM exit指令退出客户机状态进入宿主机状态，根据退出原因得知有I/O需要处理。
（8）qemu调用 virtio_blk_handle_output/vhost_user_blk_handle_output, 最终调用 virtio_blk_handle_vq。
（9）virtio_blk_handle_vq 里面有一个循环，在循环中 virtio_blk_get_request 函数从传输队列中拿出请求，然后调用 virtio_blk_handle_request 处理请求。
（10）virtio_blk_handle_request 会调用 blk_aio_pwritev，通过BlockBackend驱动写入qcow2文件。
（11）写入完毕之后，virtio_blk_req_complete 会调用 virtio_notify 通知虚拟机里面的驱动，数据写入完成，刚才注册的中断处理函数 vp_interrupt 会收到这个通知




网络虚拟化场景下网络包的发送过程总结一下，如下图所示：
（1）在虚拟机里面的用户态，应用程序通过write系统调用写入socket。
（2）写入的内容经过VFS层、内核协议栈，到达虚拟机里面内核的网络设备驱动即virtio_net。
（3）virtio_net网络设备有一个操作结构struct net_device_ops，里面定义了发送一个网络包调用的函数为start_xmit。
（4）在virtio_net的前端驱动和qemu中的后端驱动之间，有两个队列virtqueue，一个用于发送一个用于接收。然后，需要在start_xmit中调用virtqueue_add，将网络包放入发送队列，然后调用virtqueue_notify通知qemu。
（5）qemu本来处于KVM_RUN的状态，收到通知后通过VM exit指令退出客户机模式，进入宿主机模式。发送网络包的时候，virtio_net_handle_tx_bh函数会被调用。
（6）接下来是一个for循环，需要在循环中调用virtqueue_pop，从传输队列中获取要发送的数据，然后调用qemu_sendv_packet_async进行发送。
（7）qemu会调用writev向字符设备文件写入，进入宿主机的内核。
（8）在宿主机内核中字符设备文件的file_operations里面的write_iter会被调用，即会调用tun_chr_write_iter。
（9）在tun_chr_write_iter函数中，tun_get_user将要发送的网络包从qemu拷贝到宿主机内核里面来，然后调用netif_rx_ni开始调用宿主机内核协议栈进行处理。
（10）宿主机内核协议栈处理完毕之后，会发送给tap虚拟网卡，完成从虚拟机里面到宿主机的整个发送过程。
简化的数据流转流程为：客户机用户态{write 数据} -> 客户机内核态{协议栈处理为网络包} -> qemu{循环从队列中获取网络包，再向字符设备写入数据流} -> 宿主机用户态{dev/net/tun字符设备接收数据流} -> 宿主机内核态{将网络包拷贝到宿主机内核里面来，协议栈处理} -> 宿主机用户态{虚拟网卡发送}。


vdpa, linux commit: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4c8cf31885f69e86be0b5b9e6677a26797365e1d
modprobe vdpa
core_initcall(vdpa_init) -> vDPA：介绍 vDPA 总线，vDPA 设备是一种使用符合 virtio 规范的数据路径并带有供应商特定控制路径的设备。vDPA 设备既可以物理位于硬件上，也可以通过软件模拟。vDPA 硬件设备通常通过 PCIE 实现，具有以下类型： - PF（物理功能） - 单个物理功能 - VF（虚拟功能） - 支持单根 I/O 虚拟化（SR-IOV）的设备。其虚拟功能（VF）代表设备的虚拟化实例，可以分配给不同的分区 - ADI（可分配设备接口）及其等效项 - 利用 Intel 可扩展 IOV 等技术，由主机操作系统利用一个或多个 ADI 组成的虚拟设备（VDEV）。或者其等效项，如 Mellanox 的 SF（子功能）。 >从驱动程序的角度来看，根据 DMA 转换的方式和位置，vDPA 设备分为两种类型： - 平台特定的 DMA 转换 - 从驱动程序的角度来看，设备可以在设备访问内存中的数据受到限制和/或转换的平台上使用。一个例子是 PCIE vDPA，其 DMA 请求通过总线（例如 PCIE）特定的方式标记。DMA 转换和保护在 PCIE 总线 IOMMU 级别完成。 - 设备特定的 DMA 转换 - 设备通过其自己的逻辑实现 DMA 隔离和保护。一个例子是使用片上 IOMMU 的 vDPA 设备。为了隐藏 vDPA 设备/IOMMU 选项的上述类型的差异和复杂性，并为了向上层展示通用的 virtio 设备，需要一个设备无关的框架。此补丁引入了软件 vDPA 总线，它抽象了 vDPA 设备、vDPA 总线驱动程序的通用属性以及 vDPA 设备抽象和 vDPA 总线驱动程序之间的通信方法 (vdpa_config_ops)。这允许多种类型的驱动程序用于 vDPA 设备，例如 virtio_vdpa 和 vhost_vdpa 驱动程序在总线上运行，并允许内核 virtio 驱动程序或用户空间 vhost 驱动程序使用 vDPA 设备： -> commit: https://github.com/ssbandjl/linux/commit/961e9c84077f6c8579d7a628cbe94a675cb67ae4, 通过对 vDPA 总线和 vDPA 总线操作的抽象，底层硬件的差异和复杂性对上层隐藏起来。上层的 vDPA 总线驱动程序可以使用统一的 vdpa_config_ops 来控制不同类型的 vDPA 设备
virtio drivers  vhost drivers
        |             |
[virtio bus]   [vhost uAPI]
        |             |
virtio device   vhost device
virtio_vdpa drv vhost_vdpa drv
            \       /
        [vDPA bus]
                |
        vDPA device
        hardware drv
                |
        [hardware bus]
                |
        vDPA hardware


root@host101:/dev# tree -L 100 /sys/bus/vdpa/
/sys/bus/vdpa/
├── devices
├── drivers
├── drivers_autoprobe
├── drivers_probe
└── uevent


modprobe -a virtio-vdpa vhost-vdpa
root@host101:/sys/bus/vdpa# tree -L 10
.
├── devices
├── drivers
│   ├── vhost_vdpa
│   │   ├── bind
│   │   ├── module -> ../../../../module/vhost_vdpa
│   │   ├── uevent
│   │   └── unbind
│   └── virtio_vdpa
│       ├── bind
│       ├── module -> ../../../../module/virtio_vdpa
│       ├── uevent
│       └── unbind
├── drivers_autoprobe
├── drivers_probe
└── uevent

root@host101:/sys/bus/vdpa# vdpa dev add mgmtdev vdpasim_blk name vdpa0
root@host101:/sys/bus/vdpa# tree -L 10
.
├── devices
│   └── vdpa0 -> ../../../devices/vdpa0
├── drivers
│   ├── vhost_vdpa
│   │   ├── bind
│   │   ├── module -> ../../../../module/vhost_vdpa
│   │   ├── uevent
│   │   └── unbind
│   └── virtio_vdpa
│       ├── bind
│       ├── module -> ../../../../module/virtio_vdpa
│       ├── uevent
│       ├── unbind
│       └── vdpa0 -> ../../../../devices/vdpa0
├── drivers_autoprobe
├── drivers_probe
└── ueven


static int vdpa_init(void)
    bus_register(&vdpa_bus)
    genl_register_family(&vdpa_nl_family) -> netlink -> vdpa mgmtdev show -jp
        .ops = vdpa_nl_ops

static const struct genl_ops vdpa_nl_ops[] = {
	{
		.cmd = VDPA_CMD_MGMTDEV_GET,
		.doit = vdpa_nl_cmd_mgmtdev_get_doit,
		.dumpit = vdpa_nl_cmd_mgmtdev_get_dumpit,
	},
	{
		.cmd = VDPA_CMD_DEV_NEW,
		.doit = vdpa_nl_cmd_dev_add_set_doit,
            memcpy(config.net.mac, macaddr, sizeof(config.net.mac))
            mtu
            max_vq_pairs
            mdev = vdpa_mgmtdev_get_from_attr(info->attrs)
            err = mdev->ops->dev_add(mdev, name, &config) -> mlx5_vdpa_dev_add
		.flags = GENL_ADMIN_PERM,
	},
	{
		.cmd = VDPA_CMD_DEV_DEL,
		.doit = vdpa_nl_cmd_dev_del_set_doit,
		.flags = GENL_ADMIN_PERM,
	},
	{
		.cmd = VDPA_CMD_DEV_GET,
		.doit = vdpa_nl_cmd_dev_get_doit,
		.dumpit = vdpa_nl_cmd_dev_get_dumpit,
	},
	{
		.cmd = VDPA_CMD_DEV_CONFIG_GET,
		.doit = vdpa_nl_cmd_dev_config_get_doit,
		.dumpit = vdpa_nl_cmd_dev_config_get_dumpit,
	},
	{
		.cmd = VDPA_CMD_DEV_VSTATS_GET,
		.doit = vdpa_nl_cmd_dev_stats_get_doit,
		.flags = GENL_ADMIN_PERM,
	},
};


static struct bus_type vdpa_bus = {
	.name  = "vdpa",
	.dev_groups = vdpa_dev_groups,
	.match = vdpa_dev_match,
	.probe = vdpa_dev_probe,
        dma_set_mask_and_coherent
        max_num = ops->get_vq_num_max(vdev)
        ret = drv->probe(vdev) -> virtio probe
	.remove = vdpa_dev_remove,
};



modprobe vhost_vdpa, insmod vhost_vdpa.ko
module_init(vhost_vdpa_init); -> vhost：引入基于 vDPA 的后端，此补丁引入了基于 vDPA 的 vhost 后端。此后端建立在 virtio-vDPA 中定义的相同接口之上，并为用户空间提供通用 vhost 接口以加速客户机中的 virtio 设备。此后端作为 vDPA 设备驱动程序实现，基于 virtio-vDPA 中使用的相同操作。它将创建名为 vhost-vdpa-$index 的字符设备条目供用户空间使用。用户空间可以在此字符设备上使用 vhost ioctls 来设置后端。Vhost ioctls 经过扩展，使其与类型无关并像 virtio 设备一样运行，这有助于消除类型特定的 API，例如 vhost_net/scsi/vsock 所做的那样, 对于内存映射，IOTLB API 是 vhost-vDPA 的强制要求，这意味着用户空间驱动程序需要使用 VHOST_IOTLB_UPDATE/VHOST_IOTLB_INVALIDATE 来添加或删除特定用户空间内存区域的映射。vhost-vDPA API 被设计为类型无关的，但它只允许当前阶段的网络设备。由于缺乏控制 virtqueue 支持，一些功能被 vhost-vdpa 过滤掉。我们将在不久的将来启用更多功能和设备, commit -> https://github.com/ssbandjl/linux/commit/4c8cf31885f69e86be0b5b9e6677a26797365e1d
    alloc_chrdev_region(&vhost_vdpa_major, 0, VHOST_VDPA_DEV_MAX, "vhost-vdpa")
    vdpa_register_driver(&vhost_vdpa_driver) -> vhost_vdpa_probe -> iproute2 add vdpa device, then match driver and divice, exec this probe
        device_initialize(&v->dev)
        v->dev.release = vhost_vdpa_release_dev
        dev_set_name(&v->dev, "vhost-vdpa-%u", minor) -> char_dev
        cdev_init(&v->cdev, &vhost_vdpa_fops)
        cdev_device_add(&v->cdev, &v->dev)
        init_completion(&v->completion)
        vdpa_set_drvdata(vdpa, v)

vdpa_register_driver
    __vdpa_register_driver
        drv->driver.bus = &vdpa_bus
        driver_register(&drv->driver)

ls -l  /sys/bus/vdpa/drivers
ls -l  /sys/bus/vdpa/devices/vdpa0
readlink /sys/bus/vdpa/devices/vdpa0/driver
$ ls /sys/bus/vdpa/devices/vdpa0/vhost-*
vhost-vdpa-0

mlx use vdpa: https://github.com/Mellanox/scalablefunctions/wiki/Upstream-how-to-use-SF-kernel-vdpa-device
1.7 Assign VHOST VDPA device to VM
$ sudo x86_64-softmmu/qemu-system-x86_64 \
        -hda Fedora-Cloud-Base-32-1.6.x86_64.qcow2 \
        -netdev type=vhost-vdpa,vhostdev=/dev/vhost-vdpa-0,id=vhost-vdpa1 \
        -device virtio-net-pci,netdev=vhost-vdpa1,mac=00:e8:ca:33:ba:05,\
        disable-modern=off,page-per-vq=on \
        -enable-kvm \
        -nographic \
        -m 4G \
        -cpu host \
        2>&1 | tee vm.log


VDUSE（vDPA Device in Userspace）技术是字节跳动2020年10月向 Linux 内核社区正式开源的一项技术，通过VDUSE可以在一个用户进程实现一个软件定义的 vDPA 设备，并可以通过上述 vDPA 框架接入 virtio 或者 vhost 子系统，供容器或者虚机使用。此技术将在Linux 5.15 版本中首次引入：https://blog.csdn.net/hbuxiaofei/article/details/120932071



drivers/vdpa/Kconfig
config MLX5_VDPA_NET
ConnectX6 及更新版本的 VDPA 网络驱动程序。提供 virtio 网络数据路径的卸载，以便放在环上的描述符将由硬件执行。它还支持各种无状态卸载，具体取决于实际使用的设备和固件版本


static const struct file_operations vhost_vdpa_fops = {
	.owner		= THIS_MODULE,
	.open		= vhost_vdpa_open,
        v = container_of(inode->i_cdev, struct vhost_vdpa, cdev)
        vhost_vdpa_reset(v) -> vdpa_reset(vdpa, flags)
            const struct vdpa_config_ops *ops = vdev->config
            ops->reset(vdev)
        for (i = 0; i < nvqs; i++)
            vqs[i]->handle_kick = handle_vq_kick
        vhost_dev_init(dev, vqs, nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg) -> dev->msg_handler = msg_handler
            if (msg->type == VHOST_IOTLB_UPDATE || msg->type == VHOST_IOTLB_BATCH_BEGIN)
                as = vhost_vdpa_find_alloc_as(v, asid)
            else
                iotlb = asid_to_iotlb(v, asid)
            case VHOST_IOTLB_UPDATE
                vhost_vdpa_process_iotlb_update(v, iotlb, msg)
                    vhost_iotlb_itree_first(iotlb, msg->iova, msg->iova + msg->size - 1) -> return the first overlapped range
                         vhost_iotlb_itree_iter_first(&iotlb->root, start, last)
                    if (vdpa->use_va)
                        return vhost_vdpa_va_map(v, iotlb, msg->iova, msg->size, msg->uaddr, msg->perm)
                            vma = find_vma(dev->mm, uaddr) -> 查找 addr 所在的第一个 VMA，如果没有则为 NULL，应使用 mm->mmap_lock 进行调用，至少保持读锁定
                            map_file = kzalloc(sizeof(*map_file), GFP_KERNEL)
                            vhost_vdpa_map(v, iotlb, map_iova, map_size, uaddr, perm, map_file)
                                vhost_iotlb_add_range_ctx(iotlb, iova, iova + size - 1, pa, perm, opaque)
                                if (ops->dma_map)
                                    ops->dma_map(vdpa, asid, iova, size, pa, perm, opaque)
                                else iommu_map(v->domain, iova, pa, size, perm_to_iommu_flags(perm), GFP_KERNEL_ACCOUNT)
                    vhost_vdpa_pa_map(v, iotlb, msg->iova, msg->size, msg->uaddr, msg->perm)
                        page_list = (struct page **) __get_free_page(GFP_KERNEL)
                        npages = PFN_UP(size + (iova & ~PAGE_MASK))
                        pinned = pin_user_pages(cur_base, sz2pin, gup_flags, page_list)
                        map_pfn = page_to_pfn(page_list[0])
                        while (npages)
                            vhost_vdpa_map(v, iotlb, iova, csize, PFN_PHYS(map_pfn), perm, NULL)
                        vhost_vdpa_map(v, iotlb, iova, PFN_PHYS(last_pfn - map_pfn + 1), PFN_PHYS(map_pfn), perm, NULL) -> Pin the rest chunk
            case VHOST_IOTLB_BATCH_BEGIN
                v->batch_asid = asid
                v->in_batch = true;
            case VHOST_IOTLB_BATCH_END
                ops->set_map(vdpa, asid, iotlb)
        vhost_vdpa_alloc_domain(v)
            v->domain = iommu_domain_alloc(bus) -> 在我的理解中，domain这个词是从intel的VT-d文档中继承下来的，其他平台有各自的叫法，比如ARM下叫context。一个domain应该是指一个独立的iommu映射上下文。处于同一个domain中的设备使用同一套映射做地址转换（对于mmio来说就是独立的页表）。core层中使用struct iommu_domain表示一个domain
            iommu_attach_device(v->domain, dma_dev)
        vhost_vdpa_set_iova_range
            struct vdpa_iova_range *range = &v->range
            *range = ops->get_iova_range(vdpa) -> vdpasim_get_iova_range
                struct vdpa_iova_range range = {
                    .first = 0ULL,
                    .last = ULLONG_MAX,
                };
	.release	= vhost_vdpa_release,
	.write_iter	= vhost_vdpa_chr_write_iter,
        vhost_chr_write_iter
            case VHOST_IOTLB_MSG
            case VHOST_IOTLB_MSG_V2
            ret = dev->msg_handler(dev, asid, &msg) -> vhost_vdpa_process_iotlb_msg
            ret = vhost_process_iotlb_msg(dev, asid, &msg)
                case VHOST_IOTLB_UPDATE
                    vhost_iotlb_notify_vq
                        vhost_poll_queue(&node->vq->poll)
	.unlocked_ioctl	= vhost_vdpa_unlocked_ioctl,
        struct vhost_vdpa *v = filep->private_data
        if (cmd == VHOST_SET_BACKEND_FEATURES)
            if ((features & BIT_ULL(VHOST_BACKEND_F_IOTLB_PERSIST)) && !vhost_vdpa_has_persistent_map(v))
                (!ops->set_map && !ops->dma_map) || ops->reset_map || vhost_vdpa_get_backend_features(v) & BIT_ULL(VHOST_BACKEND_F_IOTLB_PERSIST) -> [PATCH v3 3/5] vhost-vdpa：引入 IOTLB_PERSIST 后端功能位，由 Si-Wei Liu 于 8 个月 3 周前发布 用户空间需要此功能标志来区分内核中的 vhost-vdpa iotlb 是否可以信任以在 vDPA 重置期间保留 IOTLB 映射。没有它，用户空间就无法区分它是否在旧内核上运行，这可能会在 vDPA 重置期间默默删除所有 iotlb 映射，尤其是在 .reset 驱动程序操作的父驱动程序实现损坏的情况下。损坏的驱动程序可能会错误地在 .reset 期间删除其自己的所有映射，从而无意中导致 vhost-vdpa 用户空间和内核之间的映射状态损坏。作为一种解决方法，为了使映射行为在重置期间可预测，用户空间必须在 vDPA 重置之前主动删除所有映射，然后在之后恢复所有映射。由于父驱动程序实现问题且没有区分方法，目前所有父驱动程序都无条件地执行此解决方法。自从相应的 vhost-vdpa 用户空间后端问世的第一天起，QEMU 就使用了此解决方法。在 3 种情况下后端可能会声明此功能位：- 必须与平台 IOMMU 配合使用的父设备- 具有片上 IOMMU 的父设备，其驱动程序中具有预期的 .reset_map 支持- 具有供应商特定 IOMMU 实现的父设备，该实现已经具有持久 IOTLB 映射，必须专门声明此后端功能 .reset_map 之所以成为持久 iotlb 的先决条件之一，是因为如果没有它，vhost-vdpa 以后无法将 iotlb 切换回初始状态，尤其是对于从设备创建时的身份映射开始的片上 IOMMU 情况。 virtio-vdpa 需要片上 IOMMU 执行从 PA 到 IOVA 的 1:1 直通转换，而 .reset_map 是 vhost-vdpa 消失后将 iotlb 恢复到身份映射模式的唯一方法。行为上的差异并不重要，因为 QEMU 取消了所有内存的映射，并在 vhost_vdpa_dev_start（started = false）处取消注册内存侦听器，但后端确认此功能标志允许 QEMU 确保在 vhost 停止和启动循环的情况下可以安全地跳过此取消映射和映射。从这个意义上说，此功能标志实际上是让用户空间知道驱动程序错误已解决的信号。不提供它表示用户空间无法相信内核会保留映射
            vhost_set_backend_features(&v->vdev, features)
                for (i = 0; i < dev->nvqs; ++i)
                    vq->acked_backend_features = features
        switch (cmd) {
        case VHOST_VDPA_GET_DEVICE_ID:
            vhost_vdpa_get_device_id
                device_id = ops->get_device_id(vdpa)
        case VHOST_VDPA_GET_STATUS
            vhost_vdpa_get_status
                status = ops->get_status(vdpa)
        case VHOST_VDPA_SET_STATUS:
		    r = vhost_vdpa_set_status(v, argp)
                if ((status_old & VIRTIO_CONFIG_S_DRIVER_OK) && !(status & VIRTIO_CONFIG_S_DRIVER_OK)) -> status_ok -> status_no_ok
                    vhost_vdpa_unsetup_vq_irq(v, i)
                        irq_bypass_unregister_producer(&vq->call_ctx.producer) -> unregister IRQ bypass producer
                            if (consumer->token == producer->token) {
				                __disconnect(producer, consumer)
                _compat_vdpa_reset -> vdpa_reset(vdpa, flags)
                else vdpa_set_status(vdpa, status) -> vdev->config->set_status(vdev, status)
                if ((status & VIRTIO_CONFIG_S_DRIVER_OK) && !(status_old & VIRTIO_CONFIG_S_DRIVER_OK)) -> status_no_ok -> status_ok
                    vhost_vdpa_setup_vq_irq(v, i)
                        irq = ops->get_vq_irq(vdpa, qid)
                        irq_bypass_unregister_producer
                        irq_bypass_register_producer
                            __connect(producer, consumer)
        case VHOST_VDPA_GET_CONFIG:
            vhost_vdpa_get_config -> vdpa_get_config -> vdpa_get_config_unlocked -> ops->get_config
        case VHOST_VDPA_SET_CONFIG
            vhost_vdpa_set_config
        case VHOST_GET_FEATURES:
		    r = vhost_vdpa_get_features(v, argp) -> get_device_features
        case VHOST_SET_FEATURES:
		    r = vhost_vdpa_set_features(v, argp) -> mlx5_vdpa_set_driver_features
                verify_driver_features(mvdev, features)
                update_cvq_info(mvdev)
        case VHOST_VDPA_SET_CONFIG_CALL
            vhost_vdpa_set_config_call
                cb.callback = vhost_vdpa_config_cb
                    eventfd_signal(config_ctx) -> 增加count的值并唤醒wqh中的元素
                ctx = fd == VHOST_FILE_UNBIND ? NULL : eventfd_ctx_fdget(fd)
                    ctx = eventfd_ctx_fileget(f.file)
                        ctx = file->private_data
                v->vdpa->config->set_config_cb(v->vdpa, &cb)
        case VHOST_VDPA_GET_IOVA_RANGE
            vhost_vdpa_get_iova_range
                .first = v->range.first,
		        .last = v->range.last,
        case VHOST_VDPA_GET_CONFIG_SIZE:
		r = vhost_vdpa_get_config_size(v, argp); -> size = ops->get_config_size(vdpa)
        case VHOST_VDPA_GET_VQS_COUNT: -> vdpa: support exposing the count of vqs to userspace
            r = vhost_vdpa_get_vqs_count(v, argp);
                copy_to_user(argp, &vdpa->nvqs, sizeof(vdpa->nvqs))
        case VHOST_VDPA_SUSPEND:
            r = vhost_vdpa_suspend(v); -> ioctl 成功返回后，设备不得处理更多 virtqueue 描述符。设备可以像未暂停一样响应配置字段的读取或写入。特别是，向“queue_enable”写入值为 1 不会使设备开始处理缓冲区
                ops->suspend(vdpa)
        case VHOST_VDPA_RESUME:
            r = vhost_vdpa_resume(v); -> ops->resume(vdpa)
        vhost_dev_ioctl(&v->vdev, cmd, argp) -> default
            case VHOST_SET_MEM_TABLE -> vhost_net：内核级 virtio 服务器 它是什么：vhost net 是一种字符设备，可用于减少 virtio 网络中涉及的系统调用数量。现有的 virtio net 代码无需修改即可在客户机中使用。它与 vringfd 有​​相似之处，但有一些不同之处，并且范围有所缩小 - 使用 eventfd 发送信号 - 结构可以随时在内存中移动（有利于迁移，在用户空间中解决错误） - 支持写入日志记录（有利于迁移） - 支持内存表而不仅仅是偏移量（kvm 需要） 通用 virtio 相关代码已放在单独的文件 vhost.c 中，如果/当出现更多后端时，可以将其制成单独的模块。我使用 Rusty 的 lguest.c 作为开发这部分的源代码：这为我提供了一些我自己无法写的诙谐评论。 它不是什么：vhost net 不是总线，也不是通用的新系统调用。没有对客户机如何执行超级调用做出任何假设。支持用户空间虚拟机管理程序以及 kvm。 工作原理：基本上，我们将 virtio 前端（由用户空间配置）连接到后端。后端可以是网络设备或 tap 设备。后端也由用户空间配置，包括 vlan/mac 等。 状态：这对我来说很有效，我没有看到任何崩溃。与用户空间相比，人们报告说延迟有所改善（因为我每个数据包节省了最多 4 个系统调用），以及更好的带宽和 CPU 利用率。 我计划在未来研究的功能： - 可合并缓冲区 - 零拷贝 - 可扩展性调整：找出要使用的最佳线程模型 关于 RCU 使用的注意事项（这也记录在 vhost.h 中，靠近 private_pointer，这是受此 RCU 变体保护的值）：发生的情况是 rcu_dereference() 正在工作队列项中使用。 rcu_read_lock() 的作用由工作队列项的执行开始承担，rcu_read_unlock() 的作用由工作队列项的执行结束承担，syncnize_rcu() 的作用由 flush_workqueue()/flush_work() 承担。将来，我们可能需要将一些 gcc 属性或稀疏注释应用于传递给 INIT_WORK() 的函数。下面 Paul 的确认适用于此 RCU 用法
                vhost_set_memory(d, argp)
                    struct vhost_memory mem, *newmem
                    newmem = kvzalloc(struct_size(newmem, regions, mem.nregions), GFP_KERNEL)
                    memcpy(newmem, &mem, size)
                    newumem = iotlb_alloc()
                        vhost_iotlb_alloc(max_iotlb_entries, VHOST_IOTLB_FLAG_RETIRE) -> max 2048
                            kzalloc
                            vhost_iotlb_init(iotlb, limit, flags)
                                iotlb->root = RB_ROOT_CACHED
                                iotlb->limit = limit
                                INIT_LIST_HEAD(&iotlb->list)
                    vhost_iotlb_add_range(newumem, region->guest_phys_addr, region->guest_phys_addr + region->memory_size - 1, region->userspace_addr, VHOST_MAP_RW)
                        vhost_iotlb_add_range_ctx(iotlb, start, last, addr, perm, NULL)
                            vhost_iotlb_add_range_ctx
                            vhost_iotlb_itree_insert(map, &iotlb->root) -> #define INTERVAL_TREE_DEFINE -> Template for implementing interval trees
                            list_add_tail(&map->link, &iotlb->list)
            case VHOST_SET_LOG_BASE
                void __user *base = (void __user *)(unsigned long)p
                vq->log_base = base
            case VHOST_SET_LOG_FD
                ctx = fd == VHOST_FILE_UNBIND ? NULL : eventfd_ctx_fdget(fd)
        vhost_vdpa_vring_ioctl
            case VHOST_VDPA_SET_VRING_ENABLE
                ops->set_vq_ready(vdpa, idx, s.num)
            case VHOST_VDPA_GET_VRING_GROUP
                s.num = ops->get_vq_group(vdpa, idx)
            case VHOST_VDPA_GET_VRING_DESC_GROUP
                s.num = ops->get_vq_desc_group(vdpa, idx)
            case VHOST_VDPA_SET_GROUP_ASID
                ops->set_group_asid(vdpa, idx, s.num)
            case VHOST_GET_VRING_BASE
                ops->get_vq_state(v->vdpa, idx, &vq_state)
                if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED))
                    vq->last_avail_idx = vq_state.packed.last_avail_idx | (vq_state.packed.last_avail_counter << 15)
                    vq->last_used_idx = vq_state.packed.last_used_idx | (vq_state.packed.last_used_counter << 15)
            r = vhost_vring_ioctl(&v->vdev, cmd, argp)
            case VHOST_SET_VRING_ADDR
                if (ops->set_vq_address(vdpa, idx,
                            (u64)(uintptr_t)vq->desc,
                            (u64)(uintptr_t)vq->avail,
                            (u64)(uintptr_t)vq->used))
            case VHOST_SET_VRING_CALL -> irq to userspace
                ops->set_vq_cb(vdpa, idx, &cb)
                vhost_vdpa_setup_vq_irq(v, idx)
                    irq = ops->get_vq_irq(vdpa, qid) -> int irq = vp_vdpa->vring[idx].irq
                    irq_bypass_unregister_producer(&vq->call_ctx.producer)
                        __disconnect(producer, consumer)
                            prod->stop(prod)
                            cons->stop(cons)
                            cons->del_producer(cons, prod)
                            cons->start(cons)
                            prod->start(prod)
                    vq->call_ctx.producer.irq = irq
                    irq_bypass_register_producer(&vq->call_ctx.producer)
        case VHOST_SET_OWNER
            vhost_vdpa_bind_mm
                ops->bind_mm(vdpa, v->vdev.mm)
            vhost_dev_reset_owner
#ifdef CONFIG_MMU
	.mmap		= vhost_vdpa_mmap,
        notify = ops->get_vq_notification(vdpa, index)
        vma->vm_ops = &vhost_vdpa_vm_ops
#endif /* CONFIG_MMU */
	.compat_ioctl	= compat_ptr_ioctl,
};


static const struct vm_operations_struct vhost_vdpa_vm_ops = {
	.fault = vhost_vdpa_fault,
        notify = ops->get_vq_notification(vdpa, index)
        vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot) -> vhost_vdpa：通过 mmap 支持门铃映射，目前门铃通过 eventfd 中继，由于 vmexits 或 syscall 的成本，这可能会产生很大的开销。此补丁引入了基于 mmap() 的门铃映射，可以消除 vmexit 或 syscall 造成的开销。为了简化门铃布局（通常是 virtio-pci）的用户空间建模，此补丁从每页门铃模型开始。Vhost-vdpa 仅支持位于页面边界且不与其他寄存器共享页面的硬件门铃。每个 virtqueue 的门铃必须单独映射，pgoff 是 virtqueue 的索引。这允许用户空间映射门铃的子集，这可能对将来实现软件辅助 virtqueue（控制 vq）有用
        remap_pfn_range(vma, vmf->address & PAGE_MASK, PFN_DOWN(notify.addr), PAGE_SIZE, vma->vm_page_prot)
            remap_pfn_range_notrack(vma, addr, pfn, size, prot)
};

modprobe virtio_vdpa, insmod virtio_vdpa.ko
module_vdpa_driver(virtio_vdpa_driver)
static struct vdpa_driver virtio_vdpa_driver = {
	.driver = {
		.name	= "virtio_vdpa",
	},
	.probe	= virtio_vdpa_probe,
	.remove = virtio_vdpa_remove,
};
virtio_vdpa_probe
    vd_dev->vdev.id.device = ops->get_device_id(vdpa)
    vd_dev->vdev.id.vendor = ops->get_vendor_id(vdpa)
    vd_dev->vdev.config = &virtio_vdpa_config_ops
    register_virtio_device(&vd_dev->vdev)
    vdpa_set_drvdata(vdpa, vd_dev)



static int virtio_vdpa_find_vqs(struct virtio_device *vdev, unsigned int nvqs,
				struct virtqueue *vqs[],
				vq_callback_t *callbacks[],
				const char * const names[],
				const bool *ctx,
				struct irq_affinity *desc)
    if (has_affinity)
        masks = create_affinity_masks(nvqs, desc ? desc : &default_affd)
    for (i = 0; i < nvqs; ++i)
        vqs[i] = virtio_vdpa_setup_vq(vdev, queue_idx++, callbacks[i], names[i], ctx ? ctx[i] : false)
            bool (*notify)(struct virtqueue *vq) = virtio_vdpa_notify
                ops->kick_vq(vdpa, vq->index)
            or notify = virtio_vdpa_notify_with_data
            dma_dev = ops->get_vq_dma_dev(vdpa, index)
            vq = vring_create_virtqueue_dma(index, max_num, align, vdev, true, may_reduce_num, ctx, notify, callback, name, dma_dev)
                vring_create_virtqueue_packed
                    vring_alloc_queue_packed(&vring_packed, vdev, num, dma_dev)
                    vq->notify = notify
                    vq->indirect = virtio_has_feature(vdev, VIRTIO_RING_F_INDIRECT_DESC)
                        virtio_check_driver_offered_feature(vdev, fbit)
                        __virtio_test_bit(vdev, fbit)
                            vdev->features & BIT_ULL(fbit)
                    vring_alloc_state_extra_packed(&vring_packed)
                    virtqueue_vring_init_packed(&vring_packed, !!callback)
                    virtqueue_init(vq, num)
                    virtqueue_vring_attach_packed(vq, &vring_packed)
                    list_add_tail(&vq->vq.list, &vdev->vqs)
                or vring_create_virtqueue_split
            desc_addr = virtqueue_get_desc_addr(vq);
            driver_addr = virtqueue_get_avail_addr(vq);
            device_addr = virtqueue_get_used_addr(vq);
            ops->set_vq_address(vdpa, index, desc_addr, driver_addr, device_addr)
            ops->set_vq_state(vdpa, index, &state)
            ops->set_vq_ready(vdpa, index, 1)
            list_add(&info->node, &vd_dev->virtqueues)
        ops->set_vq_affinity(vdpa, i, &masks[i])
    cb.callback = virtio_vdpa_config_cb
    ops->set_config_cb(vdpa, &cb)
    

static const struct virtio_config_ops virtio_vdpa_config_ops = {
	.get		= virtio_vdpa_get,
        vdpa_get_config(vdpa, offset, buf, len) -> vdpa_get_config_unlocked(vdev, offset, buf, len)
            ops->get_config(vdev, offset, buf, len)
	.set		= virtio_vdpa_set,
	.generation	= virtio_vdpa_generation,
	.get_status	= virtio_vdpa_get_status,
	.set_status	= virtio_vdpa_set_status,
	.reset		= virtio_vdpa_reset,
	.find_vqs	= virtio_vdpa_find_vqs,
	.del_vqs	= virtio_vdpa_del_vqs,
	.get_features	= virtio_vdpa_get_features,
	.finalize_features = virtio_vdpa_finalize_features,
	.bus_name	= virtio_vdpa_bus_name,
	.set_vq_affinity = virtio_vdpa_set_vq_affinity,
        ops->set_vq_affinity(vdpa, index, cpu_mask)
	.get_vq_affinity = virtio_vdpa_get_vq_affinity,
};



smc-r, SMC socket protocol family, SMC-R 提供“RDMA 套接字”解决方案，利用 RDMA over Converged Ethernet (RoCE) 技术透明地升级 AF_INET TCP 连接。SMC-R 解决方案的 Linux 实现设计为单独的套接字系列 SMC
net/smc


net/smc/af_smc.c -> module_init(smc_init);
    register_pernet_subsys(&smc_net_ops) -> 注册网络命名空间
    register_pernet_subsys(&smc_net_stat_ops)
    smc_ism_init
        smc_ism_create_system_eid
            memcpy(seid->seid_string, "IBM-SYSZ-ISMSEID00000000", 24)
            get_cpu_id(&id)
        ism_register_client(&smc_ism_client)
            ism_setup_forwarding(client, ism)
                ism->subs[client->id] = client
    smc_clc_init -> Connection Layer Control
    smc_nl_init -> net/smc：引入通用 netlink 接口用于诊断目的，引入通用 netlink 接口基础设施以公开有关 smc 链路组、链路和设备的诊断信息
        genl_register_family(&smc_gen_nl_family)
    smc_pnet_init
        genl_register_family(&smc_pnet_nl_family)
        register_netdevice_notifier(&smc_netdev_notifier)
        genl_unregister_family(&smc_pnet_nl_family)
    smc_core_init
    smc_llc_init
    smc_cdc_init
    proto_register(&smc_proto, 1)
    proto_register(&smc_proto6, 1)
    sock_register(&smc_sock_family_ops);
    smc_ib_register_client()
    tcp_register_ulp(&smc_ulp_ops)
    static_branch_enable(&tcp_have_smc) -> smc：添加 SMC 会合协议，SMC 协议 [1] 使用会合协议在对等端之间协商 SMC 功能。当前 Linux 实现尚未使用此会合协议，因此不符合 RFC7609 且与其他 SMC 实现（如 zOS 中的实现）不兼容。此补丁添加了对 SMC 会合协议的支持。它使用新的 TCP 实验选项。使用此选项，在 TCP 三向握手期间，对等端之间会交换 SMC 功能。[1] SMC-R 信息 RFC：http://www.rfc-editor.org/info/rfc7609

static const struct net_proto_family smc_sock_family_ops = {
	.family	= PF_SMC,
	.owner	= THIS_MODULE,
	.create	= smc_create,
};
smc_create
__smc_create
smc_sock_alloc
smc_tcp_listen_work
smc_listen_work
smc_listen_find_device
smc_listen_ism_init
    smc_conn_create
        smc_lgr_create
            smc_llc_lgr_init
                smc_llc_add_link_work
                    smc_llc_process_cli_add_link
                        smc_llc_cli_add_link
                            smc_ib_ready_link
                                smc_ib_modify_qp_init
                                    ib_modify_qp

smcr_tx_rdma_writes
    smc_tx_rdma_write
        ib_post_send




pcibios_scan_root（）---->pci_scan_bus_parented()---->pci_scan_child_bus()--->pci_scan_slot()--->pci_scan_single_device()----->pci_device_add()



ib_register_peer_memory_client -> in ofed lib





static struct pernet_operations smc_net_ops = {
	.init = smc_net_init,
        smc_sysctl_net_init
            net->smc.sysctl_autocorking_size
        smc_pnet_net_init
            smc_pnet_create_pnetids_list
                smc_pnet_add_base_pnetid
	.exit = smc_net_exit,
	.id   = &smc_net_id,
	.size = sizeof(struct smc_net),
};

static struct pernet_operations smc_net_stat_ops = {
	.init = smc_net_stat_init,
        smc_stats_init
            net->smc.fback_rsn = kzalloc
            net->smc.smc_stats = alloc_percpu(struct smc_stats)
	.exit = smc_net_stat_exit,
};


static struct ism_client smc_ism_client = {
	.name = "SMC-D",
	.add = smcd_register_dev,
        const struct smcd_ops *ops = ism_get_smcd_ops
            return &ism_ops
        smcd_alloc_dev
            smcd->conn = devm_kcalloc
            INIT_LIST_HEAD(&smcd->vlan) -> net/smc：为 SMC-D 和 ISM 添加基础架构，SMC 支持两种变体：SMC-R 和 SMC-D。对于数据传输，SMC-R 使用 RDMA 设备，SMC-D 使用所谓的内部共享内存 (ISM) 设备。ISM 设备仅允许同一台机器上的 SMC 实例之间进行共享内存通信。例如，这允许同一主机上的虚拟机通过 SMC 进行通信，而无需 RDMA 设备。此补丁将 SMC-D 和 ISM 设备的基础架构添加到现有 SMC 代码中。它包含以下内容：* ISM 驱动程序接口：此接口允许 ISM 驱动程序在 SMC 中注册 ISM 设备。在此过程中，驱动程序为每个设备提供一组设备操作。SMC 使用这些操作在设备上执行 SMC 特定的操作或通过设备传输数据。* 核心 SMC-D 链路组、连接和缓冲区支持：链路组、SMC 连接和 SMC 缓冲区（在 smc_core 中）已扩展以支持 SMC-D。 * SMC 类型检查：添加了一些类型检查，以防止将 SMC-R 特定代码用于 SMC-D，反之亦然。要实际使用 SMC-D，需要对 pnetid、CLC、CDC 等进行额外更改。这些将在后续补丁中添加
	.remove = smcd_unregister_dev,
	.handle_event = smcd_handle_event,
	.handle_irq = smcd_handle_irq,
};

static const struct smcd_ops ism_ops = {
	.query_remote_gid = smcd_query_rgid,
	.register_dmb = smcd_register_dmb,
	.unregister_dmb = smcd_unregister_dmb,
	.add_vlan_id = smcd_add_vlan_id,
	.del_vlan_id = smcd_del_vlan_id,
	.set_vlan_required = smcd_set_vlan_required,
	.reset_vlan_required = smcd_reset_vlan_required,
	.signal_event = smcd_signal_ieq,
	.move_data = smcd_move,
	.supports_v2 = smcd_supports_v2,
	.get_local_gid = smcd_get_local_gid,
	.get_chid = smcd_get_chid,
	.get_dev = smcd_get_dev,
};


struct genl_family smc_gen_nl_family __ro_after_init = {
	.hdrsize =	0,
	.name =		SMC_GENL_FAMILY_NAME,
	.version =	SMC_GENL_FAMILY_VERSION,
	.maxattr =	SMC_CMD_MAX_ATTR,
	.policy =	smc_gen_nl_policy,
	.netnsok =	true,
	.module =	THIS_MODULE,
	.ops =		smc_gen_nl_ops,
	.n_ops =	ARRAY_SIZE(smc_gen_nl_ops),
	.resv_start_op = SMC_NETLINK_DISABLE_HS_LIMITATION + 1,
};


rdma inline
MLX5_SCATTER_TO_CQE
MLX5_QP_FLAG_SCATTER_CQE
create_qp
    switch (qp->type)
    create_dci


RDMA/mlx5：允许提供额外的散射 CQE QP 标志，散射 CQE 功能依赖于两个标志 MLX5_QP_FLAG_SCATTER_CQE 和 MLX5_QP_FLAG_ALLOW_SCATTER_CQE
process_vendor_flags

create_user_qp
    if ((qp->flags_en & MLX5_QP_FLAG_SCATTER_CQE) &&
	    (init_attr->qp_type == IB_QPT_RC ||
	     init_attr->qp_type == IB_QPT_UC))
        int rcqe_sz = mlx5_ib_get_cqe_size(init_attr->recv_cq) -> 128 | 64 (default)
        rcqe_sz == 128 ? MLX5_RES_SCAT_DATA64_CQE : MLX5_RES_SCAT_DATA32_CQE -> 根据接收CQE大小设置内联64还是32
	if (qp->type == MLX5_IB_QPT_DCI || qp->type == IB_QPT_RC))
		configure_requester_scat_cqe(dev, qp, init_attr, qpc)
            IB_SIGNAL_ALL_WR -> IB/mlx5：允许在不向全局 WR 发送信号的前提下向 CQE 发送散射，请求者向 CQE 发送散射仅限于配置为向所有 WR 发送信号的 QP。此补丁添加了在请求者中启用向 cqe 发送散射（强制启用）的功能，无需 sig_all，适用于不希望向所有 WR 发送信号的用户，而只希望向在 CQE 中找到其数据的 WR 发送信号
            if (scqe_sz == 128)
                MLX5_SET(qpc, qpc, cs_req, MLX5_REQ_SCAT_DATA64_CQE) -> 设置到QPC中



handle_good_req


build module, driver
make clean
cd drivers/scsi/
make -C /lib/modules/$(uname -r)/build M=$(pwd) modules
# make -C ../../ M=$(pwd) modules


驱动/模块初始化顺序
core_initcall 的函数比 subsys_initcall 的函数执行地更早

core_initcall(fn) --->.initcall1.init  
postcore_initcall(fn) --->.initcall2.init  
arch_initcall(fn) --->.initcall3.init  
subsys_initcall(fn) --->.initcall4.init  
fs_initcall(fn) --->.initcall5.init  
device_initcall(fn) --->.initcall6.init  
late_initcall(fn) --->.initcall7.init

Linux 内核启动期间调用如下函数
start_kernel -->
    arch_call_rest_init -->
        rest_init -->
            kthread_create(kernel_init) -->
                kernel_init_freeable -->
                    do_basic_setup -->
                        do_initcalls
pure_initcall: 最先运行的，不依赖于任何其他初始化函数。
core_initcall
core_initcall_sync
postcore_initcall
postcore_initcall_sync
arch_initcall
arch_initcall_sync
subsys_initcall
subsys_initcall_sync
fs_initcall
fs_initcall_sync
rootfs_initcall
device_initcall
device_initcall_sync
late_initcall
late_initcall_sync
可以看一下x86中pci初始化的代码，首先是 pci_access_init ，这个函数是 arch_initcall 的，然后是 pci_legacy_init，这个函数是 subsys_initcall 的，然后是 pcibios_irq_init ，这个函数也是 subsys_initcall 的，然后是 pcibios_assign_resources这个函数是fs_initcall的，最后才是 pci_init，这个函数是device_initcall的，这样就把整个pci初始化过程分开了


struct virtio_blk_config {
	/* The capacity (in 512-byte sectors). */
	__virtio64 capacity;
	/* The maximum segment size (if VIRTIO_BLK_F_SIZE_MAX) */
	__virtio32 size_max;
	/* The maximum number of segments (if VIRTIO_BLK_F_SEG_MAX) */
	__virtio32 seg_max;
	/* geometry of the device (if VIRTIO_BLK_F_GEOMETRY) */ -> 盘式几何体
	struct virtio_blk_geometry {
		__virtio16 cylinders;
		__u8 heads;
		__u8 sectors;
	} geometry;

	/* block size of device (if VIRTIO_BLK_F_BLK_SIZE) */
	__virtio32 blk_size;

	/* the next 4 entries are guarded by VIRTIO_BLK_F_TOPOLOGY  */
	/* exponent for physical block per logical block. */
	__u8 physical_block_exp;
	/* alignment offset in logical blocks. */
	__u8 alignment_offset;
	/* minimum I/O size without performance penalty in logical blocks. */
	__virtio16 min_io_size;
	/* optimal sustained I/O size in logical blocks. */
	__virtio32 opt_io_size;

	/* writeback mode (if VIRTIO_BLK_F_CONFIG_WCE) */
	__u8 wce;
	__u8 unused;

	/* number of vqs, only available when VIRTIO_BLK_F_MQ is set */
	__virtio16 num_queues;

	/* the next 3 entries are guarded by VIRTIO_BLK_F_DISCARD */
	/*
	 * The maximum discard sectors (in 512-byte sectors) for
	 * one segment.
	 */
	__virtio32 max_discard_sectors;
	/*
	 * The maximum number of discard segments in a
	 * discard command.
	 */
	__virtio32 max_discard_seg;
	/* Discard commands must be aligned to this number of sectors. */
	__virtio32 discard_sector_alignment;

	/* the next 3 entries are guarded by VIRTIO_BLK_F_WRITE_ZEROES */
	/*
	 * The maximum number of write zeroes sectors (in 512-byte sectors) in
	 * one segment.
	 */
	__virtio32 max_write_zeroes_sectors;
	/*
	 * The maximum number of segments in a write zeroes
	 * command.
	 */
	__virtio32 max_write_zeroes_seg;
	/*
	 * Set if a VIRTIO_BLK_T_WRITE_ZEROES request may result in the
	 * deallocation of one or more of the sectors.
	 */
	__u8 write_zeroes_may_unmap;

	__u8 unused1[3];

	/* the next 3 entries are guarded by VIRTIO_BLK_F_SECURE_ERASE */
	/*
	 * The maximum secure erase sectors (in 512-byte sectors) for
	 * one segment.
	 */
	__virtio32 max_secure_erase_sectors;
	/*
	 * The maximum number of secure erase segments in a
	 * secure erase command.
	 */
	__virtio32 max_secure_erase_seg;
	/* Secure erase commands must be aligned to this number of sectors. */
	__virtio32 secure_erase_sector_alignment;

	/* Zoned block device characteristics (if VIRTIO_BLK_F_ZONED) */
	struct virtio_blk_zoned_characteristics {
		__virtio32 zone_sectors;
		__virtio32 max_open_zones;
		__virtio32 max_active_zones;
		__virtio32 max_append_sectors;
		__virtio32 write_granularity;
		__u8 model;
		__u8 unused2[3];
	} zoned;
} __attribute__((packed));




enum vhost_user_request {
	VHOST_USER_GET_FEATURES = 1,
	VHOST_USER_SET_FEATURES = 2,
	VHOST_USER_SET_OWNER = 3,
	VHOST_USER_RESET_OWNER = 4,
	VHOST_USER_SET_MEM_TABLE = 5,
	VHOST_USER_SET_LOG_BASE = 6,
	VHOST_USER_SET_LOG_FD = 7,
	VHOST_USER_SET_VRING_NUM = 8,
	VHOST_USER_SET_VRING_ADDR = 9,
	VHOST_USER_SET_VRING_BASE = 10,
	VHOST_USER_GET_VRING_BASE = 11,
	VHOST_USER_SET_VRING_KICK = 12,
	VHOST_USER_SET_VRING_CALL = 13,
	VHOST_USER_SET_VRING_ERR = 14,
	VHOST_USER_GET_PROTOCOL_FEATURES = 15,
	VHOST_USER_SET_PROTOCOL_FEATURES = 16,
	VHOST_USER_GET_QUEUE_NUM = 17,
	VHOST_USER_SET_VRING_ENABLE = 18,
	VHOST_USER_SEND_RARP = 19,
	VHOST_USER_NET_SEND_MTU = 20,
	VHOST_USER_SET_SLAVE_REQ_FD = 21,
	VHOST_USER_IOTLB_MSG = 22,
	VHOST_USER_SET_VRING_ENDIAN = 23,
	VHOST_USER_GET_CONFIG = 24,
	VHOST_USER_SET_CONFIG = 25,
	VHOST_USER_VRING_KICK = 35,
};



virtio_cwrite8
/* Config space accessors. */
#define virtio_cwrite(vdev, structname, member, ptr)
    vdev->config->set((vdev), offsetof(structname, member),	&virtio_cwrite_v, sizeof(virtio_cwrite_v)) -> vp_set



cache_type_store
    virtio_cwrite8(vdev, offsetof(struct virtio_blk_config, wce), i)


virtio_fs:
config VIRTIO_FS -> Virtio Filesystem allows guests to mount file systems from the host
module_init(virtio_fs_init)
    register_virtio_driver(&virtio_fs_driver)
    register_filesystem(&virtio_fs_type)
static struct virtio_driver virtio_fs_driver = {
	.driver.name		= KBUILD_MODNAME,
	.driver.owner		= THIS_MODULE,
	.id_table		= id_table,
	.feature_table		= feature_table,
	.feature_table_size	= ARRAY_SIZE(feature_table),
	.probe			= virtio_fs_probe, -> virtio-fs：添加 virtiofs 文件系统，为 virtio-fs 添加基本文件系统模块。这还不包含主机和客户机之间的共享数据支持或元数据一致性加速。但是它已经比 virtio-9p 快得多。设计概述，为了设计具有更好性能和本地文件系统语义的东西，提出了许多想法。- 使用 fuse 协议（而不是 9p）进行客户机和主机之间的通信。客户机内核将是 fuse 客户端，而 fuse 服务器将在主机上运行以处理请求。- 对于客户机内部的数据访问，在 QEMU 地址空间中 mmap 文件的部分，客户机使用 dax 访问此内存。这样，客户机页面缓存被绕过，并且只有一个数据副本（在主机上）。这还将在客户机之间启用 mmap（MAP_SHARED）。- 对于元数据一致性，有一个共享内存区域，其中包含与元数据关联的版本号，任何更改元数据的客户机都会更新版本号，其他客户机在下次访问时刷新元数据。这尚未实现。 virtio-fs 与现有方法的不同之处，virtio-fs 背后的独特理念是利用虚拟机和虚拟机管理程序的共置来避免通信 (vmexits)。DAX 允许在不与虚拟机管理程序通信的情况下访问文件内容。元数据的共享内存区域避免了在元数据不变的常见情况下的通信。通过用更便宜的共享内存访问取代昂贵的通信，我们期望实现比基于网络文件系统协议的方法更好的性能。此外，这也使得实现本地文件系统语义 (一致性) 变得更容易。这些技术不适用于网络文件系统协议，因为通过利用本地机器上的共享内存可以绕过通信通道。这就是我们决定构建 virtio-fs 而不是专注于 9P 或 NFS 的原因。缓存模式：与 virtio-9p 一样，支持不同的缓存模式，这也决定了一致性级别。“cache=FOO”和“writeback”选项控制客户机和主机文件系统之间的一致性级别。 - cache=none 元数据、数据和路径名查找不会缓存在客户机中。它们始终从主机获取，任何更改都会立即推送到主机。 - cache=always 元数据、数据和路径名查找缓存在客户机中，永不过期。 - cache=auto 元数据和路径名查找缓存在配置的时间后过期（默认为 1 秒）。文件打开时缓存数据（接近打开一致性）。 - writeback/no_writeback 这些选项控制写回策略。如果禁用写回，则正常写入将立即与主机 fs 同步。如果启用写回，则写入可能会缓存在客户机中，直到文件关闭或执行 fsync(2)。此选项对 mmap 写入或通过 DAX 机制的写入没有影响
        virtio_fs_read_tag(vdev, fs) -> mount_tag
            virtio_cread_bytes(vdev, offsetof(struct virtio_fs_config, tag),  &tag_buf, sizeof(tag_buf))
                __virtio_cread_many
                    vdev->config->generation(vdev)
                    vdev->config->get(vdev, offset + bytes * i, buf + i * bytes, bytes)
        virtio_fs_setup_vqs(vdev, fs)
            fs->nvqs = VQ_REQUEST + fs->num_request_queues
            callbacks[VQ_HIPRIO] = virtio_fs_vq_done -> high priority -> virtio_fs_hiprio_done_work
            virtio_fs_init_vq(&fs->vqs[VQ_HIPRIO], "hiprio", VQ_HIPRIO) -> init work queue
			INIT_WORK(&fsvq->done_work, virtio_fs_requests_done_work)
			INIT_DELAYED_WORK(&fsvq->dispatch_work, virtio_fs_request_dispatch_work)
				req = list_first_entry_or_null(&fsvq->queued_reqs,
				ret = virtio_fs_enqueue_req(fsvq, req, true)
					total_sgs = sg_count_fuse_req(req)
					sg_init_one(&sg[out_sgs++], &req->in.h, sizeof(req->in.h))
					out_sgs += sg_init_fuse_args(&sg[out_sgs], req,
					ret = virtqueue_add_sgs(vq, sgs, out_sgs, in_sgs, req, GFP_ATOMIC)
					list_add_tail(&req->list, fpq->processing)
					notify = virtqueue_kick_prepare(vq)
					if (notify)
						virtqueue_notify(vq)
            virtio_fs_init_vq(&fs->vqs[i], vq_name, VQ_REQUEST)
            callbacks[i] = virtio_fs_vq_done -> schedule_work(&fsvq->done_work) -> virtio_fs_requests_done_work
            virtio_find_vqs(vdev, fs->nvqs, vqs, callbacks, names, NULL)
            virtio_fs_start_all_queues(fs) -> fsvq->connected = true
        virtio_fs_setup_dax -> virtiofs：设置 virtio_fs dax_device 设置 dax 设备。使用 shm 功能查找缓存条目并映射它。DAX 窗口由 fs/dax.c 基础架构访问，并且必须具有 struct pages（至少在 x86 上）。使用 devm_memremap_pages() 映射 DAX 窗口 PCI BAR 并分配 struct page
            struct virtio_shm_region cache_reg
            have_cache = virtio_get_shm_region(vdev, &cache_reg, (u8)VIRTIO_FS_SHMCAP_ID_CACHE) -> vdev->config->get_shm_region(vdev, region, id)
            devm_request_mem_region(&vdev->dev, cache_reg.addr, cache_reg.len, dev_name(&vdev->dev)) -> 申请使用这段IO内存， 再使用 ioremap() 将其映射出来， 供用户空间使用
            fs->window_kaddr = devm_memremap_pages(&vdev->dev, pgmap) -> devm_memremap_pages() 是一种可以为任意范围创建 struct page 条目的工具，它使驱动程序能够颠覆页面管理的核心方面。具体来说，该工具与内核的内存热插拔功能紧密集成。它将 altmap 参数深深地注入到特定于体系结构的 vmemmap 实现中，以允许从特定的保留页面进行分配，并且它具有与 get_user_pages() 和 get_user_pages_fast() 相关的页面结构引用计数的 Linux 特定假设。这是一个疏忽和错误，它从一开始就没有标记为 EXPORT_SYMBOL_GPL。同样，devm_memremap_pagex() 公开并依赖于核心内核内部假设，并将随着“struct page”、内存热插拔和对新内存类型/拓扑的支持而不断发展。只有内核内 GPL 驱动程序才能跟上这一持续的发展。此接口以及从此接口派生的功能不适用于内核外部驱动程序
            fs->window_phys_addr = (phys_addr_t) cache_reg.addr
            devm_add_action_or_reset(&vdev->dev, virtio_fs_cleanup_dax, fs->dax_dev) -> 辅助函数 devm_add_action_or_reset() 将在内部调用 devm_add_action()，如果 devm_add_action() 失败，它将执行所提到的操作并返回错误代码。因此，使用 devm_add_action_or_reset() 代替 devm_add_action() 来简化错误处理，减少代码
                kill_dax(dax_dev)
        virtio_device_ready(vdev)
	.remove			= virtio_fs_remove,
#ifdef CONFIG_PM_SLEEP
	.freeze			= virtio_fs_freeze,
	.restore		= virtio_fs_restore,
#endif
};

static struct file_system_type virtio_fs_type = {
	.owner		= THIS_MODULE,
	.name		= "virtiofs",
	.init_fs_context = virtio_fs_init_fs_context,
         fuse_init_fs_context_submount(fsc)
            fsc->ops = &fuse_context_submount_ops
         fsc->ops = &virtio_fs_context_ops
	.kill_sb	= virtio_kill_sb,
};


virtio_fs_hiprio_dispatch_work
	send_forget_request

SYSCALL_DEFINE2(fsopen, const char __user *, _fs_name, unsigned int, flags)
    fs_context_for_mount
        alloc_fs_context -> Create a filesystem context
            init_fs_context(fc)

static const struct fs_context_operations virtio_fs_context_ops = {
	.free		= virtio_fs_free_fsc,
	.parse_param	= virtio_fs_parse_param,
	.get_tree	= virtio_fs_get_tree,
		virtio_fs_find_instance
		virtqueue_get_vring_size
		WARN_ON(virtqueue_size <= FUSE_HEADER_OVERHEAD)
		fuse_conn_init(fc, fm, fsc->user_ns, &virtio_fs_fiq_ops, fs)
			atomic_set(&fc->dev_count, 1)
			fc->max_background = FUSE_DEFAULT_MAX_BACKGROUND -> 12
			fc->max_pages = FUSE_DEFAULT_MAX_PAGES_PER_REQ -> 32
			fc->max_pages_limit = FUSE_MAX_MAX_PAGES -> 256
		Tell FUSE to split requests that exceed the virtqueue's size
		fc->max_pages_limit = min_t(unsigned int, fc->max_pages_limit, virtqueue_size - FUSE_HEADER_OVERHEAD)
		sb = sget_fc(fsc, virtio_fs_test_super, set_anon_super_fc)
};



 |  "rm /mnt/fuse/file"               |  FUSE filesystem daemon
 |                                    |
 |                                    |  >sys_read()
 |                                    |    >fuse_dev_read()
 |                                    |      >request_wait()
 |                                    |        [sleep on fc->waitq]
 |                                    |
 |  >sys_unlink()                     |
 |    >fuse_unlink()                  |
 |      [get request from             |
 |       fc->unused_list]             |
 |      >request_send()               |
 |        [queue req on fc->pending]  |
 |        [wake up fc->waitq]         |        [woken up]
 |        >request_wait_answer()      |
 |          [sleep on req->waitq]     |
 |                                    |      <request_wait()
 |                                    |      [remove req from fc->pending]
 |                                    |      [copy req to read buffer]
 |                                    |      [add req to fc->processing]
 |                                    |    <fuse_dev_read()
 |                                    |  <sys_read()
 |                                    |
 |                                    |  [perform unlink]
 |                                    |
 |                                    |  >sys_write()
 |                                    |    >fuse_dev_write()
 |                                    |      [look up req in fc->processing]
 |                                    |      [remove from fc->processing]
 |                                    |      [copy write buffer to req]
 |          [woken up]                |      [wake up req->waitq]
 |                                    |    <fuse_dev_write()
 |                                    |  <sys_write()
 |        <request_wait_answer()      |
 |      <request_send()               |
 |      [add request to               |
 |       fc->unused_list]             |
 |    <fuse_unlink()                  |
 |  <sys_unlink()                     |

fuse_dev_write
	fuse_dev_do_write
	对请求写一个回复。首先从写入缓冲区复制标头。然后根据标头中找到的唯一 ID 在处理列表中搜索请求。如果找到，则将其从列表中删除，并将缓冲区的其余部分复制到请求中。通过调用 fuse_request_end() 完成请求
		fuse_copy_one(cs, &oh, sizeof(oh))
		request_find
			list_for_each_entry(req, &fpq->processing[hash], list)
		fuse_request_end
			flush_bg_queue(fc)
				queue_request_and_unlock(fiq, req)

fuse_queue_forget
	fiq->ops->wake_forget_and_unlock(fiq)


static const struct fuse_iqueue_ops virtio_fs_fiq_ops = {
	.wake_forget_and_unlock		= virtio_fs_wake_forget_and_unlock,
	.wake_interrupt_and_unlock	= virtio_fs_wake_interrupt_and_unlock,
	.wake_pending_and_unlock	= virtio_fs_wake_pending_and_unlock,
	.release			= virtio_fs_fiq_release,
};


new:
static const struct fuse_iqueue_ops virtio_fs_fiq_ops = {
	.send_forget	= virtio_fs_send_forget,
	.send_interrupt	= virtio_fs_send_interrupt,
	.send_req	= virtio_fs_send_req,
		ret = virtio_fs_enqueue_req(fsvq, req, false, GFP_ATOMIC)
	.release	= virtio_fs_fiq_release,
};


uml: UserMode Linux, 
module_init(virtio_uml_init); -> um：驱动程序：添加 virtio vhost-user 驱动程序，此模块允许通过 vhost-user 套接字使用 virtio 设备。
static struct platform_driver virtio_uml_driver = {
	.probe = virtio_uml_probe,
	.remove = virtio_uml_remove,
	.driver = {
		.name = "virtio-uml",
		.of_match_table = virtio_uml_match,
	},
	.suspend = virtio_uml_suspend,
	.resume = virtio_uml_resume,
};

virtio_uml_probe
    vu_dev->vdev.config = &virtio_uml_config_ops
    vhost_user_init(vu_dev)
    register_virtio_device(&vu_dev->vdev)

static const struct virtio_config_ops virtio_uml_config_ops = {
	.get = vu_get,
	.set = vu_set,
	.get_status = vu_get_status,
	.set_status = vu_set_status,
	.reset = vu_reset,
	.find_vqs = vu_find_vqs,
        vhost_user_set_mem_table(vu_dev)
            struct vhost_user_msg msg = {
                .header.request = VHOST_USER_SET_MEM_TABLE,
                .header.size = sizeof(msg.payload.mem_regions),
                .payload.mem_regions.num = 1,
            };
            vhost_user_init_mem_region(reserved, physmem_size - reserved, &fds[0], &msg.payload.mem_regions.regions[0])
                phys_mapping(addr + size - 1, &mem_offset)
            vhost_user_send(vu_dev, false, &msg, fds, msg.payload.mem_regions.num)
                full_sendmsg_fds(vu_dev->sock, msg, size, fds, num_fds)
                    os_sendmsg_fds(fd, buf, len, fds, fds_num)
                vhost_user_recv_u64
        for (i = 0; i < nvqs; ++i)
            vqs[i] = vu_setup_vq(vdev, queue_idx++, callbacks[i], names[i], ctx ? ctx[i] : false)
                vq = vring_create_virtqueue(index, num, PAGE_SIZE, vdev, true, true, ctx, vu_notify, callback, info->name)
                vu_setup_vq_call_fd(vu_dev, vq)
                vhost_user_set_vring_num
                vhost_user_set_vring_base
                    vhost_user_set_vring_state(vu_dev, VHOST_USER_SET_VRING_BASE, index, offset)
                vhost_user_set_vring_addr(vu_dev, index, virtqueue_get_desc_addr(vq), virtqueue_get_used_addr(vq), virtqueue_get_avail_addr(vq), (u64) -1)
                    .header.request = VHOST_USER_SET_VRING_ADDR,
        list_for_each_entry(vq, &vdev->vqs, list)
            vhost_user_set_vring_kick(vu_dev, vq->index, info->kick_fd)
            vhost_user_set_vring_enable(vu_dev, vq->index, true)
	.del_vqs = vu_del_vqs,
	.get_features = vu_get_features,
	.finalize_features = vu_finalize_features,
	.bus_name = vu_bus_name,
};

tools/virtio/vringh_test.c
    ...
    host_map = mmap(NULL, mapsize, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);
    guest_map = mmap(NULL, mapsize, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);
    ...

kernel:
drivers/vhost/test.c
module_misc_device(vhost_test_misc); -> 将模块作为杂项设备自动注册 -> https://blog.csdn.net/qq_37858281/article/details/125702163
static struct miscdevice vhost_test_misc = {
	MISC_DYNAMIC_MINOR,
	"vhost-test", -> /dev/vhost-test
	&vhost_test_fops,
};
static const struct file_operations vhost_test_fops = {
	.owner          = THIS_MODULE,
	.release        = vhost_test_release,
	.unlocked_ioctl = vhost_test_ioctl,
        switch (ioctl)
        case VHOST_TEST_RUN
            vhost_test_run(n, test)
                vhost_dev_check_owner
                vhost_vq_access_ok
                vhost_vq_get_backend
                vhost_vq_set_backend
                vhost_vq_init_access
                    vhost_init_is_le(vq) -> vhost：对旧设备的跨端支持，此补丁在用于实现旧式 virtio 设备时为 vhost 带来跨端支持。由于这种情况相对罕见，因此该功能的可用性由内核配置选项控制（默认情况下未设置）。添加了 vq->is_le 布尔字段以缓存用于环访问的字节序。它默认为本机字节序，如旧式 virtio 设备所期望的那样。当环激活时，如果设备是现代的，我们会强制使用小端。当环停用时，我们会恢复为本机字节序默认值。如果编译了跨端，则会添加 vq->user_be 布尔字段，以便用户空间可以请求特定的字节序。此字段用于在激活旧式设备的环时覆盖默认值。它对现代设备没有影响
                    vhost_update_used_flags
                        log_used(vq, (used - (void __user *)vq->used), sizeof vq->used->flags)
                            log_write(vq->log_base, vq->log_addr + used_offset, len)
                                set_bit_to_user(bit, (void __user *)(unsigned long)log)
                                    pin_user_pages_fast(log, 1, FOLL_WRITE, &page)
                                    set_bit(bit, base)
                            translate_desc(vq, (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO) -> vhost：正确记录脏页，Vhost 脏页记录 API 旨在通过 GPA 同步。但我们尝试在启用设备 IOTLB 时记录 GIOVA。这是错误的，可能会导致迁移后数据丢失。为了解决这个问题，在启用设备 IOTLB 的情况下记录时，我们将：1）重用设备 IOTLB 转换结果 GIOVA->HVA 映射来获取 HVA，对于可写描述符，通过 iovec 获取 HVA。对于已使用的环更新，将其 GIOVA 转换为 HVA 2）遍历 GPA->HVA 映射以获取可能的 GPA 并通过 GPA 记录。请注意，这种反向映射不能保证是唯一的，因此在这种情况下我们应该记录每个可能的 GPA。这修复了迁移期间 scp 到客户机的失败。在 -next 中，我们可能会支持传递 GIOVA->GPA 而不是 GIOVA->HVA
                                map = vhost_iotlb_itree_first(umem, addr, last) -> return the first overlapped range
                                    vhost_iotlb_itree_iter_first(&iotlb->root, start, last)
                                vhost_iotlb_miss(vq, addr, access)
                                    msg->type = VHOST_IOTLB_MISS
                                    vhost_enqueue_msg(dev, &dev->read_list, node) -> list_add_tail(&node->node, head)
                            log_write_hva(vq,	(uintptr_t)iov[i].iov_base, iov[i].iov_len)
                                r = log_write(vq->log_base, u->start + start - u->addr, l)
                    vhost_get_used_idx(vq, &last_used_idx) -> vhost：细粒度用户空间内存访问器，用于隐藏 virtqueue 助手的元数据地址。这将允许实现基于 vmap 的元数据快速访问
                        vhost_get_used(vq, *idx, &vq->used->idx) -> VHOST_ADDR_USED (VHOST_ACCESS_WO)
                vhost_test_flush -> vhost_dev_flush
                    xa_for_each(&dev->worker_xa, i, worker)
                        vhost_worker_flush(worker)
                            init_completion(&flush.wait_event)
                            vhost_work_init(&flush.work, vhost_flush_work)
                                complete(&s->wait_event)
                            vhost_worker_queue(worker, &flush.work)
	.compat_ioctl   = compat_ptr_ioctl,
	.open           = vhost_test_open,
        struct vhost_test *n = kmalloc
        vqs = kmalloc_array
        n->vqs[VHOST_TEST_VQ].handle_kick = handle_vq_kick
            ops->kick_vq(v->vdpa, vq - v->vqs) -> mlx5_vdpa_kick_vq -> iowrite16(idx, ndev->mvdev.res.kick_addr) -> or kick_vq_with_data -> call notify or async work call
                or handle_vq(n)
                    private = vhost_vq_get_backend(vq)
                    vhost_disable_notify(&n->dev, vq)
                    for (;;)
                        head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL)
                        vhost_enable_notify(&n->dev, vq)
                        len = iov_length(vq->iov, out)
                        vhost_add_used_and_signal(&n->dev, vq, head, 0)
                        vhost_exceeds_weight(vq, 0, total_len) -> over weight
        vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL)
            dev->vqs = vqs
            dev->nvqs = nvqs
            INIT_LIST_HEAD(&dev->read_list)
            INIT_LIST_HEAD(&dev->pending_list)
            for (i = 0; i < dev->nvqs; ++i)
                vhost_vq_reset(dev, vq)
                    vhost_vring_call_reset(&vq->call_ctx) -> memset(&call_ctx->producer, 0x0, sizeof(struct irq_bypass_producer))
                    __vhost_vq_meta_reset(vq) -> vhost：引入 O(1) vq 元数据缓存，当启用设备 IOTLB 时，所有地址转换都存储在间隔树中。由于 virtqueue 元数据（可用、已使用和描述符）的访问频率比其他地址高得多，因此 O(lgN) 搜索时间可能很慢。因此，此补丁引入了一个 O(1) 数组，该数组指向存储 vq 元数据转换的间隔树节点。这些数组在 vq IOTLB 预取期间更新，并在每次失效和 tlb 更新期间重置。每次我们想要访问 vq 元数据时，都会在间隔树之前查询这个小数组。这对于静态映射来说已经足够，但对于动态映射则不够，我们可以在上面进行优化。测试是在客户机（2M 大页面）中使用 l2fwd 进行的：
                        vq->meta_iotlb[j] = NULL
                vhost_poll_init(&vq->poll, vq->handle_kick,	EPOLLIN, dev, vq) -> work->fn(work)
                    init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup)
                        vhost_poll_queue(poll) -> handle_vq_kick
                    init_poll_funcptr(&poll->table, vhost_poll_func)
                        pt->_qproc = qproc -> vhost_poll_func -> add_wait_queue(wqh, &poll->wait) -> 将一个进程添加到等待队列，首先获得队列的自旋锁，然后调用__add_wait_queue()实现将新的等待进程添加等待队列（添加到等待队列的头部），然后解锁 -> wait wake_up
                    vhost_work_init(&poll->work, fn)
                        work->fn = fn -> vq->handle_kick
        f->private_data = n
	.llseek		= noop_llseek,
};


init_waitqueue_func_entry(struct wait_queue_entry *wq_entry, wait_queue_func_t func)
    wq_entry->func		= func

module_init(vhost_vsock_init);
    vsock_core_register(&vhost_transport.transport, VSOCK_TRANSPORT_F_H2G)
    misc_register(&vhost_vsock_misc)


static struct miscdevice vhost_vsock_misc = {
	.minor = VHOST_VSOCK_MINOR,
	.name = "vhost-vsock",
	.fops = &vhost_vsock_fops,
};
static const struct file_operations vhost_vsock_fops = {
	.owner          = THIS_MODULE,
	.open           = vhost_vsock_dev_open,
	.release        = vhost_vsock_dev_release,
	.llseek		= noop_llseek,
	.unlocked_ioctl = vhost_vsock_dev_ioctl,
	.compat_ioctl   = compat_ptr_ioctl,
	.read_iter      = vhost_vsock_chr_read_iter,
        vhost_chr_read_iter
            node = vhost_dequeue_msg(dev, &dev->read_list)
	.write_iter     = vhost_vsock_chr_write_iter,
	.poll           = vhost_vsock_chr_poll,
        vhost_chr_poll(file, dev, wait)
            poll_wait(file, &dev->wait, wait) -> poll_wait 不会挂起当前进程,而是把自己注册到某个事件等待队列中
            mask |= EPOLLIN | EPOLLRDNORM
};



userspace:
tools/virtio/virtio_test.c
    vdev_info_init
        dev->buf_size = 1024
        dev->buf = malloc(dev->buf_size)
        dev->control = open("/dev/vhost-test", O_RDWR) -> vhost_test_open
        ioctl(dev->control, VHOST_SET_OWNER, NULL) -> vhost_test_ioctl
        dev->mem->regions[0].guest_phys_addr = (long)dev->buf
        dev->mem->regions[0].userspace_addr = (long)dev->buf
        ioctl(dev->control, VHOST_SET_MEM_TABLE, dev->mem)
    vq_info_add -> 256 vring_size
        info->kick = eventfd(0, EFD_NONBLOCK)
        info->call = eventfd(0, EFD_NONBLOCK)
        posix_memalign(&info->ring, 4096, vring_size(num, 4096))
        vhost_vq_setup
            struct vhost_vring_addr addr = {
                .index = info->idx,
                .desc_user_addr = (uint64_t)(unsigned long)info->vring.desc,
                .avail_user_addr = (uint64_t)(unsigned long)info->vring.avail,
                .used_user_addr = (uint64_t)(unsigned long)info->vring.used,
            };
            ioctl(dev->control, VHOST_SET_FEATURES, &features)
            ioctl(dev->control, VHOST_SET_VRING_NUM, &state)
            ioctl(dev->control, VHOST_SET_VRING_BASE, &state)
            ioctl(dev->control, VHOST_SET_VRING_ADDR, &addr)
            ioctl(dev->control, VHOST_SET_VRING_KICK, &file)
            ioctl(dev->control, VHOST_SET_VRING_CALL, &file)
    run_test(&dev, &dev.vqs[0], delayed, batch, reset, 0x100000)
        ioctl(dev->control, VHOST_TEST_RUN, &test)
        for (;;)
            virtqueue_disable_cb(vq->vq)
                virtqueue_disable_cb_packed
                    vq->packed.event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE
                    vq->packed.vring.driver->flags = cpu_to_le16(vq->packed.event_flags_shadow)
                or virtqueue_disable_cb_split
            batch = (random() % vq->vring.num) + 1
            sg_init_one(&sl, dev->buf, dev->buf_size) -> init 1024B buf to sgl
            virtqueue_add_outbuf(vq->vq, &sl, 1, dev->buf + started, GFP_ATOMIC)
            virtqueue_kick(vq->vq) -> update after add_buf, kick to other side
                virtqueue_kick_prepare(vq)
                virtqueue_notify(vq)
            ioctl(dev->control, VHOST_TEST_SET_BACKEND
            virtqueue_get_buf(vq->vq, &len)
            virtqueue_enable_cb(vq->vq) -> restart callbacks after disable_cb
                unsigned int last_used_idx = virtqueue_enable_cb_prepare(_vq)
                    virtqueue_enable_cb_prepare_packed -> 我们乐观地重新打开中断，然后检查是否还有更多事情要做
                        return vq->last_used_idx
                    or virtqueue_enable_cb_prepare_split
                return !virtqueue_poll(_vq, last_used_idx)
                    virtqueue_poll_packed(_vq, last_used_idx)
                        is_used_desc_packed(vq, used_idx, wrap_counter)
                            flags = le16_to_cpu(vq->packed.vring.desc[idx].flags)
                            avail = !!(flags & (1 << VRING_PACKED_DESC_F_AVAIL))
                            used = !!(flags & (1 << VRING_PACKED_DESC_F_USED))
                    or virtqueue_poll_split(_vq, last_used_idx)
            wait_for_interrupt(dev)
        r = ioctl(dev->control, VHOST_TEST_RUN, &test)



module_init(vhost_scsi_init);
vhost_vring_ioctl
    vhost_vring_set_num_addr
        case VHOST_SET_VRING_ADDR
            vhost_vring_set_addr(d, vq, argp)
                vq->desc = (void __user *)(unsigned long)a.desc_user_addr
                vq->avail = (void __user *)(unsigned long)a.avail_user_addr
                vq->log_addr = a.log_guest_addr
                vq->used = (void __user *)(unsigned long)a.used_user_addr
    case VHOST_SET_VRING_BUSYLOOP_TIMEOUT
        vq->busyloop_timeout = s.num
    if (pollstart && vq->handle_kick)
        vhost_poll_start(&vq->poll, vq->kick)
            mask = vfs_poll(file, &poll->table) -> file->f_op->poll(file, pt)
            vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask))
    case VHOST_SET_VRING_KICK
        eventfp = f.fd == VHOST_FILE_UNBIND ? NULL : eventfd_fget(f.fd)
        filep = eventfp




modprobe mlx5_vdpa
insmod mlx5_vdpa.ko
module_auxiliary_driver(mlx5v_driver)
    mlx5v_probe
        mgtdev->mgtdev.ops = &mdev_ops
        mgtdev->mgtdev.supported_features = get_supported_features(mdev) -> vdpa mgmtdev show
        mgtdev->vdpa_ops = mlx5_vdpa_ops
        vdpa_mgmtdev_register(&mgtdev->mgtdev) -> vdpa dev add mgmtdev pci/0000:3b:00.1 name vdpa0 -> vdpa dev show
            list_add_tail(&mdev->list, &mdev_head)
        auxiliary_set_drvdata(adev, mgtdev)


mlx5 support vdpa, commit: https://github.com/ssbandjl/linux/commit/29064bfdabd5ef49eac6909d3a36a075e3b52255
config MLX5_VDPA

static const struct vdpa_mgmtdev_ops mdev_ops = {
	.dev_add = mlx5_vdpa_dev_add,
        vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mgtdev->vdpa_ops, MLX5_VDPA_NUMVQ_GROUPS, MLX5_VDPA_NUM_AS, name, false) -> vdpa/mlx5：启用硬件对 vq 描述符映射的支持，通过填写包含到 hw vq 的描述符映射的附加 mkey，硬件支持 Vq 描述符映射。本系列中的上一个补丁增加了对 ASID 1 的 hw mkey (mr) 创建的支持。此补丁根据组 ASID 映射填写 vq 数据和 vq 描述符 mkey。该功能通过 .get_vq_desc_group op 的存在向 vdpa 核心发出信号
            vdev = kzalloc(size, GFP_KERNEL)
            vdev->dev.bus = &vdpa_bus
            device_initialize(&vdev->dev)
        init_mvqs(ndev)
        allocate_irqs(ndev)
            ndev->irqp.entries = kcalloc(ndev->mvdev.max_vqs, sizeof(*ndev->irqp.entries), GFP_KERNEL)
            for (i = 0; i < ndev->mvdev.max_vqs; i++)
                ent->map = pci_msix_alloc_irq_at(ndev->mvdev.mdev->pdev, MSI_ANY_INDEX, NULL)
        config_func_mtu
        query_mtu
        get_link_state
        memcpy(ndev->config.mac, add_config->net.mac, ETH_ALEN)
        mlx5_query_nic_vport_mac_address
        pfmdev = pci_get_drvdata(pci_physfn(mdev->pdev))
        mvdev->vdev.dma_dev = &mdev->pdev->dev
        ndev = mlx5_vdpa_alloc_resources(struct mlx5_vdpa_dev *mvdev)
            u64 offset = MLX5_CAP64_DEV_VDPA_EMULATION(mvdev->mdev, doorbell_bar_offset)
            mlx5_get_uars_page
            kick_addr = mdev->bar_addr + offset -> dev->bar_addr = pci_resource_start(pdev, 0)
            res->phys_kick_addr = kick_addr
            res->kick_addr = ioremap(kick_addr, PAGE_SIZE) -> map device mem to kernel
            init_ctrl_vq(mvdev)
                mvdev->cvq.iotlb = vhost_iotlb_alloc(0, 0)
                vringh_set_iotlb(&mvdev->cvq.vring, mvdev->cvq.iotlb, &mvdev->cvq.iommu_lock) -> #if IS_REACHABLE(CONFIG_VHOST_IOTLB)
                    vrh->iotlb = iotlb;
                    vrh->iotlb_lock = iotlb_lock;
        INIT_LIST_HEAD(&mvdev->mr_list_head)
        mlx5_vdpa_create_dma_mr(mvdev)
        alloc_resources(ndev)
             mlx5_vdpa_alloc_transport_domain(&ndev->mvdev, &res->tdn) -> MLX5_CMD_OP_ALLOC_TRANSPORT_DOMAIN
        INIT_WORK(&ndev->cvq_ent.work, mlx5_cvq_kick_handler)
        _vdpa_register_device(&mvdev->vdev, max_vqs + 1)
	.dev_del = mlx5_vdpa_dev_del,
};



static const struct vdpa_config_ops mlx5_vdpa_ops = {
	.set_vq_address = mlx5_vdpa_set_vq_address,
        struct mlx5_vdpa_virtqueue *mvq
        mvq = &ndev->vqs[idx]
        mvq->desc_addr = desc_area
        mvq->device_addr = device_area
        mvq->driver_addr = driver_area
        mvq->modified_fields |= MLX5_VIRTQ_MODIFY_MASK_VIRTIO_Q_ADDRS
	.set_vq_num = mlx5_vdpa_set_vq_num,
	.kick_vq = mlx5_vdpa_kick_vq,
        iowrite16(idx, ndev->mvdev.res.kick_addr)
	.set_vq_cb = mlx5_vdpa_set_vq_cb,
        ndev->event_cbs[idx] = *cb
        mvdev->cvq.event_cb = *cb
	.set_vq_ready = mlx5_vdpa_set_vq_ready,
	.get_vq_ready = mlx5_vdpa_get_vq_ready,
	.set_vq_state = mlx5_vdpa_set_vq_state,
	.get_vq_state = mlx5_vdpa_get_vq_state,
	.get_vendor_vq_stats = mlx5_vdpa_get_vendor_vq_stats,
	.get_vq_notification = mlx5_get_vq_notification,
        addr = (phys_addr_t)ndev->mvdev.res.phys_kick_addr
	.get_vq_irq = mlx5_get_vq_irq,
        mvq = &ndev->vqs[idx]
        mvq->map.virq
	.get_vq_align = mlx5_vdpa_get_vq_align,
	.get_vq_group = mlx5_vdpa_get_vq_group,
	.get_vq_desc_group = mlx5_vdpa_get_vq_desc_group, /* Op disabled if not supported. */
	.get_device_features = mlx5_vdpa_get_device_features, -> return ndev->mvdev.mlx_features
	.get_backend_features = mlx5_vdpa_get_backend_features,
	.set_driver_features = mlx5_vdpa_set_driver_features,
	.get_driver_features = mlx5_vdpa_get_driver_features,
	.set_config_cb = mlx5_vdpa_set_config_cb, -> ndev->config_cb = *cb
	.get_vq_num_max = mlx5_vdpa_get_vq_num_max,
	.get_device_id = mlx5_vdpa_get_device_id,
        return VIRTIO_ID_NET
	.get_vendor_id = mlx5_vdpa_get_vendor_id,
	.get_status = mlx5_vdpa_get_status,
	.set_status = mlx5_vdpa_set_status,
        if ((status ^ ndev->mvdev.status) & VIRTIO_CONFIG_S_DRIVER_OK)
            setup_cvq_vring(mvdev) -> vdpa/mlx5：仅初始化一次 CVQ vring(控制虚拟队列)，目前，CVQ vringh 在 setup_virtqueues() 内初始化，每次完成内存更新时都会调用此函数。这是不可取的，因为它会重置 vring 的所有上下文，包括可用和已使用的索引。当设置 VIRTIO_CONFIG_S_DRIVER_OK 时，将初始化移至 mlx5_vdpa_set_status()
                u16 idx = cvq->vring.last_avail_idx
                vringh_init_iotlb(&cvq->vring, mvdev->actual_features, MLX5_CVQ_MAX_ENT, false,
					(struct vring_desc *)(uintptr_t)cvq->desc_addr,
					(struct vring_avail *)(uintptr_t)cvq->driver_addr,
					(struct vring_used *)(uintptr_t)cvq->device_addr) -> initialize a vringh for a ring with IOTLB -> #if IS_REACHABLE(CONFIG_VHOST_IOTLB)
                cvq->vring.last_avail_idx = cvq->vring.last_used_idx = idx
            register_link_notifier(ndev) -> vdpa/mlx5：避免丢失链接状态更新，如果未协商 VIRTIO_NET_F_STATUS，当前代码将忽略链接状态更新。但是，在功能协商完成之前可能会收到链接状态更新，因此导致链接状态事件丢失，可能导致链接状态关闭。修改代码，以便在协商 DRIVER_OK 后注册链接状态通知程序，并且仅在协商 VIRTIO_NET_F_STATUS 时才进行注册。重置设备时取消注册通知程序
                ndev->nb.notifier_call = event_handler
                mlx5_notifier_register(ndev->mvdev.mdev, &ndev->nb)
                queue_link_work(ndev)
                    INIT_WORK(&wqent->work, update_carrier) -> ndev->config_cb.callback(ndev->config_cb.private) -> virtio_vdpa_config_cb
                        virtio_config_changed(&vd_dev->vdev) -> __virtio_config_changed(dev)
            setup_driver(mvdev)
                mlx5_vdpa_add_debugfs(ndev)
                read_umem_params(ndev) -> vdpa/mlx5：修复创建 1k VQ 时出现的固件错误，切换到 switchdev 模式后，在 PF 上配置 9k MTU，然后使用具有更大（1k）环的 vdpa 设备时，会触发固件错误：mlx5_cmd_out_err：CREATE_GENERAL_OBJECT(0xa00) op_mod(0xd) 失败，状态为坏资源（0x5），综合征（0xf6db90），err(-22) 这是因为 hw VQ 大小参数是根据 umem_1/2/3_buffer_param_a/b 功能计算的，并且当驱动程序移至 switchdev 模式时，所有设备功能都是只读的。有问题的配置流程如下所示：1) 创建 VF 2) 解除 VF 绑定 3) 将 PF 切换到 switchdev 模式。 4) 绑定 VF 5) 将 PF MTU 设置为 9k 6) 创建 vDPA 设备 7) 使用 vDPA 设备和 1K 队列大小启动 VM 请注意，在步骤 3) 之前设置 MTU 不会触发此问题。此补丁会在创建设备的 VQ 之前尽可能晚地读取上述 umem 参数。v2：- 使用 kmalloc 分配输出以减少堆栈帧大小。- 从 cc 中删除稳定版本
                    MLX5_CMD_OP_QUERY_HCA_CAP
                setup_virtqueues(mvdev)
                    for (i = 0; i < mvdev->max_vqs; i++)
                        setup_vq(ndev, &ndev->vqs[i])
                            cq_create(ndev, idx, mvq->num_ent)
                            qp_create(ndev, mvq, &mvq->fwqp)
                            qp_create(ndev, mvq, &mvq->vqqp)
                            connect_qps(ndev, mvq)
                            counter_set_alloc(ndev, mvq)
                            alloc_vector(ndev, mvq)
                                for (i = 0; i < irqp->num_ent; i++)
                                    ent->dev_id = &ndev->event_cbs[mvq->index]
                                    request_irq(ent->map.virq, mlx5_vdpa_int_handler, 0, ent->name, ent->dev_id) -> vdpa/mlx5：支持中断绕过，添加对从设备直接向 VM 和 VCPU 生成中断的支持，从而避免主机 CPU 的开销。当支持时，驱动程序将尝试为每个数据虚拟队列分配向量。如果无法提供虚拟队列的向量，它将使用 QP 模式，其中通知通过驱动程序。此外，我们添加了关机回调，以确保在关机时释放分配的中断，从而允许干净关机
                                        cb->callback(cb->private)
                            create_virtqueue(ndev, mvq)
                                umems_create(ndev, mvq)
                                MLX5_CMD_OP_CREATE_GENERAL_OBJECT MLX5_OBJ_TYPE_VIRTIO_NET_Q
                                MLX5_SET64(virtio_q, vq_ctx, desc_addr, mvq->desc_addr);
                                MLX5_SET64(virtio_q, vq_ctx, used_addr, mvq->device_addr);
                                MLX5_SET64(virtio_q, vq_ctx, available_addr, mvq->driver_addr);
                            modify_virtqueue_state(ndev, mvq, MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY)
                                mvq->modified_fields |= MLX5_VIRTQ_MODIFY_MASK_STATE
                                modify_virtqueue(ndev, mvq, state) -> vdpa/mlx5：允许在一个修改命令中修改多个 vq 字段，添加一个位掩码变量，用于跟踪 hw vq 字段更改，这些更改应在下一个 hw vq 更改命令中修改。这对于在恢复 vq 时设置多个 vq 字段很有用
                                    MLX5_CMD_OP_MODIFY_GENERAL_OBJECT
                create_rqt(ndev) -> RQ 表 (RQT)，RQT 对象保存一个 RQ 表。TIR 指向 RQT 并将其用作间接表。RQT 允许仅将数据包分派到表中的单个 RQ，通过表索引访问。RQT 上下文通过 CREATE_RQT 命令创建。然后可以通过 MODIFY_RQT 命令修改上下文。可以通过 QUERY_RQT 命令查询上下文。完成后，RQT 上下文通过 DESTROY_RQT 命令销毁。通过 QUERY_HCA 命令通过 log_max_rqt 参数报告 RQT 的支持情况和支持的 RQT 数量
                    MLX5_CMD_OP_CREATE_RQT
                create_tir(ndev) -> 传输接口接收 (TIR)，传输接口接收 (TIR) 对象负责在接收端执行所有与传输相关的操作。TIR 执行数据包处理和重组，还负责将数据包解复用为不同的 RQ。解复用支持将数据包传送到列出的一个或多个 RQ 中，或者从列表中基于哈希选择单个 RQ（例如接收端缩放）。TIR 上下文是通过 CREATE_TIR 命令创建的。然后可以通过 MODIFY_TIR 命令根据 TIR 状态机修改上下文。可以通过 QUERY_TIR 命令对其进行查询。完成之后，TIR 上下文通过 DESTROY_TIR 命令销毁。通过 QUERY_HCA 命令通过 log_max_tir 参数报告对 TIR 的支持以及支持的 TIR 数量
                    static const u8 rx_hash_toeplitz_key[] = { 0x2c, 0xc6, 0x81, 0xd1, 0x5b, 0xdb, 0xf4, 0xf7,
                    MLX5_CMD_OP_CREATE_TIR
                setup_steering(ndev) -> vdpa/mlx5：添加 RX MAC VLAN 过滤器支持，支持 MAC/VLAN 数据包的 HW 卸载过滤。为此，我们添加了一个处理程序来处理通过控制 VQ 的 VLAN 配置。支持两种操作。1. 添加 VLAN - 在这种情况下，将向 RX 流表添加一个条目，以允许将 MAC/VLAN 的组合转发到 TIR。2. 删除 VLAN - 将从流表中删除条目，从而有效地阻止此类数据包通过。目前，控制 VQ 不会将更改传播到 VLAN 设备的 MAC，因此我们始终使用父设备的 MAC。示例：1. 创建 vlan 设备：$ ip link add link ens1 name ens1.8 type vlan id 8
                    mlx5_get_flow_namespace(ndev->mvdev.mdev, MLX5_FLOW_NAMESPACE_BYPASS)
                    mlx5_create_auto_grouped_flow_table
                    mlx5_vdpa_add_rx_flow_table
                    mac_vlan_add(ndev, ndev->config.mac, 0, false)
        ndev->mvdev.status = status
	.reset = mlx5_vdpa_reset, -> mlx5_vdpa_compat_reset
	.compat_reset = mlx5_vdpa_compat_reset,
        ndev->mvdev.status = 0

	.get_config_size = mlx5_vdpa_get_config_size,
        return sizeof(struct virtio_net_config)
	.get_config = mlx5_vdpa_get_config,
        memcpy(buf, (u8 *)&ndev->config + offset, len)
	.set_config = mlx5_vdpa_set_config, -> /* not supported */
	.get_generation = mlx5_vdpa_get_generation, -> return mvdev->generation
	.set_map = mlx5_vdpa_set_map,
        set_map_data(mvdev, iotlb, asid)
            if (vhost_iotlb_itree_first(iotlb, 0, U64_MAX))
                new_mr = mlx5_vdpa_create_mr(mvdev, iotlb) -> vdpa/mlx5：改进 mr 更新流程，当前更新 mr 的流程直接在 mvdev->mr 上进行，这使得处理多个新 mr 结构变得很麻烦。此补丁通过让 mlx5_vdpa_create_mr 返回一个将更新旧 mr（如果有）的新 mr，使流程更加简单。旧 mr 将被删除并与 mvdev 解除链接。对于 iotlb 为空（非 NULL）的情况，将清除旧 mr。此更改为添加不同 ASID 的 mrs 铺平了道路。不再需要初始化的 bool，因为 mr 现在是 mlx5_vdpa_dev 结构中的指针，未初始化时将为 NULL
                    _mlx5_vdpa_create_mr
                        if (iotlb)
                            create_user_mr(mvdev, mr, iotlb)
                        else create_dma_mr(mvdev, mr)
                        mr->iotlb = vhost_iotlb_alloc(0, 0)
                        dup_iotlb(mr->iotlb, iotlb)
                        list_add_tail(&mr->mr_list, &mvdev->mr_list_head)
            mlx5_vdpa_update_mr(mvdev, new_mr, asid)
            or mlx5_vdpa_change_map(mvdev, new_mr, asid)
            mlx5_vdpa_update_cvq_iotlb(mvdev, iotlb, asid)
                prune_iotlb(mvdev->cvq.iotlb)
                dup_iotlb(mvdev->cvq.iotlb, iotlb)
	.reset_map = mlx5_vdpa_reset_map,
	.set_group_asid = mlx5_set_group_asid,
        mvdev->group2asid[group] = asid
	.get_vq_dma_dev = mlx5_get_vq_dma_dev, -> mvdev->vdev.dma_dev
	.free = mlx5_vdpa_free,
	.suspend = mlx5_vdpa_suspend,
        for (i = 0; i < ndev->cur_num_vqs; i++)
            suspend_vq(ndev, mvq)
                modify_virtqueue_state(ndev, mvq, MLX5_VIRTIO_NET_Q_OBJECT_STATE_SUSPEND)
                    opcode, MLX5_CMD_OP_MODIFY_GENERAL_OBJECT -> op to hw
                query_virtqueue(ndev, mvq, &attr)
                    opcode, MLX5_CMD_OP_QUERY_GENERAL_OBJECT)
                mvq->avail_idx = attr.available_index
                mvq->used_idx = attr.used_index
        mlx5_vdpa_cvq_suspend(mvdev)
	.resume = mlx5_vdpa_resume, /* Op disabled if not supported. */
        resume_vqs(ndev) -> resume_vq
            modify_virtqueue_state(ndev, mvq, MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY)
};


vdpa:
drivers/vdpa/vdpa_sim/vdpa_sim_net.c
modprobe vdpa-sim-net | insmod vdpa-sim-net.ko -> module_init(vdpasim_net_init)
    device_register(&vdpasim_net_mgmtdev)
    vdpa_mgmtdev_register(&mgmt_dev)

static struct vdpa_mgmt_dev mgmt_dev = {
	.device = &vdpasim_net_mgmtdev,
	.id_table = id_table,
	.ops = &vdpasim_net_mgmtdev_ops,
};


# Add `vdpa-net1` device through `vdpasim_net` management device
$ vdpa dev add name vdpa-net1 mgmtdev vdpasim_net

# Add `vdpa-blk1` device through `vdpasim_blk` management device
$ vdpa dev add name vdpa-blk1 mgmtdev vdpasim_blk

# List all vdpa devices on the system
$ vdpa dev show
vdpa-net1: type network mgmtdev vdpasim_net vendor_id 0 max_vqs 2 max_vq_size 256
vdpa-blk1: type block mgmtdev vdpasim_blk vendor_id 0 max_vqs 1 max_vq_size 256

static const struct vdpa_mgmtdev_ops vdpasim_net_mgmtdev_ops = {
	.dev_add = vdpasim_net_dev_add,
        dev_attr.id = VIRTIO_ID_NET
        dev_attr.get_config = vdpasim_net_get_config
            net_config->status = cpu_to_vdpasim16(vdpasim, VIRTIO_NET_S_LINK_UP)
        dev_attr.work_fn = vdpasim_net_work
            vdpasim_handle_cvq(vdpasim)
                vringh_getdesc_iotlb -> get next available descriptor from ring with IOTLB
                vringh_iov_pull_iotlb -> copy bytes from vring_iov
                vdpasim_handle_ctrl_mac
                    vringh_iov_pull_iotlb(&cvq->vring, &cvq->in_iov, vio_config->mac, ETH_ALEN)
                vringh_iov_push_iotlb -> copy bytes into vring_iov
                vringh_complete_iotlb ->  finished with descriptor, publish it
                u64_stats_update_begin
                u64_stats_update_end
            vringh_getdesc_iotlb(&txq->vring, &txq->out_iov, NULL, &txq->head, GFP_ATOMIC)
            vringh_iov_pull_iotlb(&txq->vring, &txq->out_iov, net->buffer, PAGE_SIZE)
            receive_filter(vdpasim, read)
                is_broadcast_ether_addr
                is_multicast_ether_addr
            vringh_getdesc_iotlb(&rxq->vring, NULL, &rxq->in_iov, &rxq->head, GFP_ATOMIC)
            vringh_iov_push_iotlb(&rxq->vring, &rxq->in_iov, net->buffer, read)
            vdpasim_net_complete(txq, 0);
            vdpasim_net_complete(rxq, write)
            vdpasim_schedule_work(vdpasim)
        simdev = vdpasim_create(&dev_attr, config)
        vdpasim_net_setup_config(simdev, config) -> set mac and mtu
            memcpy(vio_config->mac, config->net.mac, ETH_ALEN)
            vio_config->mtu = cpu_to_vdpasim16(vdpasim, config->net.mtu)
        _vdpa_register_device(&simdev->vdpa, VDPASIM_NET_VQ_NUM)
	.dev_del = vdpasim_net_dev_del
};

# Add `vdpa-net1` device through `vdpasim_net` management device
$ vdpa dev add name vdpa-net1 mgmtdev vdpasim_net



modprobe vdpa-sim-blk, insmod vdpa_sim_blk.ko
drivers/vdpa/vdpa_sim/vdpa_sim_blk.c
module_init(vdpasim_blk_init) -> sim
    module_param(shared_backend, bool, 0444)
    device_register(&vdpasim_blk_mgmtdev)
    vdpa_mgmtdev_register(&mgmt_dev)
    if (shared_backend)
        shared_buffer = kvzalloc(VDPASIM_BLK_CAPACITY << SECTOR_SHIFT, GFP_KERNEL)
static struct vdpa_mgmt_dev mgmt_dev = {
	.device = &vdpasim_blk_mgmtdev,
	.id_table = id_table,
	.ops = &vdpasim_blk_mgmtdev_ops,
};

static const struct vdpa_mgmtdev_ops vdpasim_blk_mgmtdev_ops = {
	.dev_add = vdpasim_blk_dev_add,
	.dev_del = vdpasim_blk_dev_del
};



# Add `vdpa-blk1` device through `vdpasim_blk` management device
$ vdpa dev add name vdpa-blk1 mgmtdev vdpasim_blk
static int vdpasim_blk_dev_add(struct vdpa_mgmt_dev *mdev, const char *name, const struct vdpa_dev_set_config *config)
    dev_attr.id = VIRTIO_ID_BLOCK
    dev_attr.supported_features = VDPASIM_BLK_FEATURES
    dev_attr.nvqs = VDPASIM_BLK_VQ_NUM -> 1
    dev_attr.get_config = vdpasim_blk_get_config
        blk_config->capacity = cpu_to_vdpasim64(vdpasim, VDPASIM_BLK_CAPACITY) -> 0x40000 -> print(0x40000) 262144 * 512 = 134217728 = 128MB -> lsblk vda    252:0    0   128M  0 block:virtio:vdpa
        blk_config->size_max = cpu_to_vdpasim32(vdpasim, VDPASIM_BLK_SIZE_MAX) -> 4096
        blk_config->seg_max = cpu_to_vdpasim32(vdpasim, VDPASIM_BLK_SEG_MAX) -> 32
        blk_config->num_queues = cpu_to_vdpasim16(vdpasim, VDPASIM_BLK_VQ_NUM)
        blk_config->blk_size = cpu_to_vdpasim32(vdpasim, SECTOR_SIZE) -> 1<<9 = 512
        blk_config->discard_sector_alignment
        blk_config->max_discard_sectors
        blk_config->max_discard_seg
        blk_config->max_write_zeroes_sectors
        blk_config->max_write_zeroes_seg
        ...
    dev_attr.work_fn = vdpasim_blk_work
        for (i = 0; i < VDPASIM_BLK_VQ_NUM; i++)
            while (vdpasim_blk_handle_req(vdpasim, vq))
                vdpasim_blk_handle_req
                    struct virtio_blk_outhdr hdr
                    vringh_getdesc_iotlb(&vq->vring, &vq->out_iov, &vq->in_iov, &vq->head, GFP_ATOMIC) -> 不再需要使用 riov 和 wiov 时，您应该通过调用 vringh_kiov_cleanup() 来清理它们以释放内存
                        __vringh_iov(vrh, *head, riov, wiov, no_range_check, NULL,gfp, copydesc_iotlb)
                    to_push = vringh_kiov_length(&vq->in_iov) - 1
                    to_pull = vringh_kiov_length(&vq->out_iov)
                    bytes = vringh_iov_pull_iotlb(&vq->vring, &vq->out_iov, &hdr, sizeof(hdr)) -> copy bytes from vring_iov to hdr(从GUEST获取消息头)
                        vringh_iov_xfer(vrh, riov, dst, len, xfer_from_iotlb)
                            err = xfer(vrh, iov->iov[iov->i].iov_base, ptr, partlen) -> xfer_from_iotlb
                                copy_from_iotlb(vrh, dst, src, len)
                    type = vdpasim32_to_cpu(vdpasim, hdr.type);
                    sector = vdpasim64_to_cpu(vdpasim, hdr.sector);
                    offset = sector << SECTOR_SHIFT;
                    status = VIRTIO_BLK_S_OK;
                    switch (type)
                    case VIRTIO_BLK_T_IN -> GUEST READ
                        vringh_iov_push_iotlb(&vq->vring, &vq->in_iov, blk->buffer + offset, to_push) -> copy bytes into vring_iov -> vringh_iov_xfer(vrh, wiov, (void *)src, len, xfer_to_iotlb) -> read from blk->buffer + offset
                            err = xfer(vrh, iov->iov[iov->i].iov_base, ptr, partlen)
                                copy_to_iotlb(vrh, dst, src, len)
                                    ret = iotlb_translate(vrh, (u64)(uintptr_t)dst
                                    copy_to_iter
                    case VIRTIO_BLK_T_OUT: ->  GUEST WRITE
                        bytes = vringh_iov_pull_iotlb(&vq->vring, &vq->out_iov, blk->buffer + offset, to_pull) ->  copy bytes from vring_iov to blk buffer(将IO写入blk指向的缓冲区(偏移))
                    case VIRTIO_BLK_T_GET_ID
                        bytes = vringh_iov_push_iotlb(&vq->vring, &vq->in_iov, vdpasim_blk_id, VIRTIO_BLK_ID_BYTES)
                    case VIRTIO_BLK_T_FLUSH:
                        break
                    case VIRTIO_BLK_T_DISCARD:
                    case VIRTIO_BLK_T_WRITE_ZEROES:
                        struct virtio_blk_discard_write_zeroes range
                        bytes = vringh_iov_pull_iotlb(&vq->vring, &vq->out_iov, &range, to_pull) -> vring -> range
                        vdpasim_blk_check_range(vdpasim, sector, num_sectors, VDPASIM_BLK_DWZ_MAX_SECTORS)
                        memset(blk->buffer + offset, 0, num_sectors << SECTOR_SHIFT) -> reset blk buffer
                smp_wmb()
                local_bh_disable
                if (vringh_need_notify_iotlb(&vq->vring) > 0)
                    vringh_notify(&vq->vring)
                        vrh->notify(vrh)
                local_bh_enable()
        if (reschedule)
            vdpasim_schedule_work
    simdev = vdpasim_create(&dev_attr, config)
        ops = &vdpasim_batch_config_ops
        or ops = &vdpasim_config_ops
        vdpa = __vdpa_alloc_device(NULL, ops, dev_attr->ngroups, dev_attr->nas, dev_attr->alloc_size, dev_attr->name, use_va) -> allocate and initilaize a vDPA device
            vdev->dev.bus = &vdpa_bus
            vdev->config = config -> ops
            device_initialize(&vdev->dev)
        kthread_init_work(&vdpasim->work, vdpasim_work_fn)
        vdpasim->worker = kthread_create_worker(0, "vDPA sim worker: %s", dev_attr->name)
        if (dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64)))
        vdpasim->vqs = kcalloc(dev_attr->nvqs
        vdpasim->iommu = kmalloc_array(vdpasim->dev_attr.nas, sizeof(*vdpasim->iommu), GFP_KERNEL)
        vdpasim->iommu_pt = kmalloc_array(vdpasim->dev_attr.nas, sizeof(*vdpasim->iommu_pt), GFP_KERNEL)
        for (i = 0; i < vdpasim->dev_attr.nas; i++)
            vhost_iotlb_init(&vdpasim->iommu[i], max_iotlb_entries, 0) -> 2048 -> initialize a vhost IOTLB
            vhost_iotlb_add_range                vdpasim->iommu_pt[i] = true
        for (i = 0; i < dev_attr->nvqs; i++)
            vringh_set_iotlb(&vdpasim->vqs[i].vring, &vdpasim->iommu[0], &vdpasim->iommu_lock) -> initialize a vringh for a ring with IOTLB, associated vring and iotlb
    blk = sim_to_blk(simdev)
    blk->shared_backend = shared_backend
    blk->buffer = kvzalloc
    _vdpa_register_device(&simdev->vdpa, VDPASIM_BLK_VQ_NUM)


static const struct vdpa_config_ops vdpasim_config_ops = {
	.set_vq_address         = vdpasim_set_vq_address,
        vq->desc_addr = desc_area;
        vq->driver_addr = driver_area
        vq->device_addr = device_area
	.set_vq_num             = vdpasim_set_vq_num, -> vq->num = num
	.kick_vq                = vdpasim_kick_vq, -> vdpasim_schedule_work(vdpasim) -> vdpasim_work_fn -> 关键函数, 当用户态Guest准备好IO BUF后通知VQ后执行此IO处理函数
        vdpasim->dev_attr.work_fn(vdpasim) -> vdpasim_blk_work -> handle IO
	.set_vq_cb              = vdpasim_set_vq_cb,
        vq->cb = cb->callback
        vq->private = cb->private
	.set_vq_ready           = vdpasim_set_vq_ready,
        vdpasim_queue_ready(vdpasim, idx)
            vq->vring.last_avail_idx = last_avail_idx;
            vq->vring.last_used_idx = last_avail_idx
            vq->vring.notify = vdpasim_vq_notify
                vq->cb(vq->private)
	.get_vq_ready           = vdpasim_get_vq_ready,
	.set_vq_state           = vdpasim_set_vq_state,
        vrh->last_avail_idx = state->split.avail_index
	.get_vendor_vq_stats    = vdpasim_get_vq_stats,
	.get_vq_state           = vdpasim_get_vq_state,
	.get_vq_align           = vdpasim_get_vq_align, -> #define VDPASIM_QUEUE_ALIGN PAGE_SIZE -> 4K
	.get_vq_group           = vdpasim_get_vq_group,
	.get_device_features    = vdpasim_get_device_features,
        vdpasim->dev_attr.supported_features
	.get_backend_features   = vdpasim_get_backend_features,
        BIT_ULL(VHOST_BACKEND_F_ENABLE_AFTER_DRIVER_OK)
	.set_driver_features    = vdpasim_set_driver_features,
	.get_driver_features    = vdpasim_get_driver_features,
	.set_config_cb          = vdpasim_set_config_cb,
	.get_vq_num_max         = vdpasim_get_vq_num_max,
	.get_device_id          = vdpasim_get_device_id,
	.get_vendor_id          = vdpasim_get_vendor_id,
	.get_status             = vdpasim_get_status,
	.set_status             = vdpasim_set_status,
        vdpasim->status = status
	.reset			= vdpasim_reset,
	.compat_reset		= vdpasim_compat_reset,
        vdpasim_do_reset(vdpasim, flags)
            for (i = 0; i < vdpasim->dev_attr.nvqs; i++)
                vdpasim_vq_reset(vdpasim, &vdpasim->vqs[i])
                    vq->ready = false;
                    vq->desc_addr = 0;
                    vq->driver_addr = 0;
                    vq->device_addr = 0;
                    vq->cb = NULL;
                    vq->private = NULL;
                    vringh_init_iotlb(&vq->vring, vdpasim->dev_attr.supported_features, VDPASIM_QUEUE_MAX, false, NULL, NULL, NULL);
                    vq->vring.notify = NULL;
                vringh_set_iotlb(&vdpasim->vqs[i].vring, &vdpasim->iommu[0], &vdpasim->iommu_lock)
            for (i = 0; i < vdpasim->dev_attr.nas; i++)
                vhost_iotlb_reset(&vdpasim->iommu[i])
                vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX, 0, VHOST_MAP_RW)
	.suspend		= vdpasim_suspend,
        vdpasim->running = false
	.resume			= vdpasim_resume,
        if (vdpasim->pending_kick)
            for (i = 0; i < vdpasim->dev_attr.nvqs; ++i)
                vdpasim_kick_vq(vdpa, i)
	.get_config_size        = vdpasim_get_config_size,
        vdpasim->dev_attr.config_size
	.get_config             = vdpasim_get_config,
        vdpasim->dev_attr.get_config(vdpasim, vdpasim->config)
        memcpy(buf, vdpasim->config + offset, len)
	.set_config             = vdpasim_set_config,
        vdpasim->dev_attr.set_config(vdpasim, vdpasim->config) -> 中转
	.get_generation         = vdpasim_get_generation,
	.get_iova_range         = vdpasim_get_iova_range,
	.set_group_asid         = vdpasim_set_group_asid,
	.dma_map                = vdpasim_dma_map,
        vhost_iotlb_add_range_ctx(&vdpasim->iommu[asid], iova, iova + size - 1, pa, perm, opaque)
	.dma_unmap              = vdpasim_dma_unmap,
        vhost_iotlb_reset(&vdpasim->iommu[asid])
        vhost_iotlb_del_range(&vdpasim->iommu[asid], iova, iova + size - 1)
	.reset_map              = vdpasim_reset_map,
	.bind_mm		= vdpasim_bind_mm,
        mm_work.mm_to_bind = mm
        vdpasim_worker_change_mm_sync(vdpasim, &mm_work)
            vdpasim_mm_work_fn
                vdpasim->mm_bound = mm_work->mm_to_bind
	.unbind_mm		= vdpasim_unbind_mm,
	.free                   = vdpasim_free,
};


virtio_vdpa_notify_with_data
    kick_vq_with_data -> snet_kick_vq_with_data
        iowrite32((data & 0xFFFF0000) | SNET_KICK_VAL, snet->vqs[idx]->kick_ptr)


static const struct vdpa_config_ops vp_vdpa_ops = {
	.get_device_features = vp_vdpa_get_device_features,
	.set_driver_features = vp_vdpa_set_driver_features,
	.get_driver_features = vp_vdpa_get_driver_features,
	.get_status	= vp_vdpa_get_status,
	.set_status	= vp_vdpa_set_status,
	.reset		= vp_vdpa_reset,
	.get_vq_num_max	= vp_vdpa_get_vq_num_max,
	.get_vq_state	= vp_vdpa_get_vq_state,
	.get_vq_notification = vp_vdpa_get_vq_notification,
	.set_vq_state	= vp_vdpa_set_vq_state,
	.set_vq_cb	= vp_vdpa_set_vq_cb,
	.set_vq_ready	= vp_vdpa_set_vq_ready,
	.get_vq_ready	= vp_vdpa_get_vq_ready,
	.set_vq_num	= vp_vdpa_set_vq_num,
	.set_vq_address	= vp_vdpa_set_vq_address,
	.kick_vq	= vp_vdpa_kick_vq,
        vp_iowrite16(qid, vp_vdpa->vring[qid].notify)
	.get_generation	= vp_vdpa_get_generation,
	.get_device_id	= vp_vdpa_get_device_id,
	.get_vendor_id	= vp_vdpa_get_vendor_id,
	.get_vq_align	= vp_vdpa_get_vq_align,
	.get_config_size = vp_vdpa_get_config_size,
	.get_config	= vp_vdpa_get_config,
	.set_config	= vp_vdpa_set_config,
	.set_config_cb  = vp_vdpa_set_config_cb,
	.get_vq_irq	= vp_vdpa_get_vq_irq,
};


module_pci_driver(vp_vdpa_driver) -> vdpa：引入 virtio pci 驱动程序，此补丁引入了用于 virtio-pci 设备的 vDPA 驱动程序。它将 virtio-pci 控制命令桥接到 vDPA 总线。这将用于功能原型设计和测试。请注意，不支持获取/恢复 virtqueue 状态，这需要在 virtio 规范上进行扩展
vp_vdpa_probe
    mgtdev->ops = &vp_vdpa_mdev_ops
    pcim_enable_device(pdev)
    vp_modern_probe
    vdpa_mgmtdev_register




static const struct vdpa_mgmtdev_ops vp_vdpa_mdev_ops = {
	.dev_add = vp_vdpa_dev_add,
        vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa, dev, &vp_vdpa_ops, 1, 1, name, false)
        device_features = vp_modern_get_features(mdev)
        devm_add_action_or_reset(dev, vp_vdpa_free_irq_vectors, pdev)
        for (i = 0; i < vp_vdpa->queues; i++)
            vp_vdpa->vring[i].notify = vp_modern_map_vq_notify(mdev, i, &vp_vdpa->vring[i].notify_pa) -> vp_vdpa：报告门铃地址，此补丁将每个 vq 门铃位置和大小报告给 vDPA 总线。然后，用户空间可以通过 vhost-vDPA 总线驱动程序通过 mmap() 映射门铃
                *pa = mdev->notify_pa + off * mdev->notify_offset_multiplier
                return mdev->notify_base + off * mdev->notify_offset_multiplier -> 从规范的介绍来看，notify地址是notify cap在bar空间内的偏移，加上common cap的queue_notify_off字段与notify cap的notify_off_multiplier的乘积。再看一次之前的地址计算公式，就是规范里面介绍的计算方法
        ret = _vdpa_register_device(&vp_vdpa->vdpa, vp_vdpa->queues)
            dev = bus_find_device(&vdpa_bus, NULL, dev_name(&vdev->dev), vdpa_name_match)
            device_add(&vdev->dev)
	.dev_del = vp_vdpa_dev_del,
};


vdpa, commit: https://patchwork.kernel.org/project/kvm/patch/20200131033651.103534-1-tiwei.bie@intel.com/
此补丁引入了基于 vDPA 的 vhost 后端。此后端建立在 virtio-vDPA 中定义的相同接口之上，并为用户空间提供通用 vhost 接口，以加速客户机中的 virtio 设备。此后端作为 vDPA 设备驱动程序实现，基于 virtio-vDPA 中使用的相同操作。它将创建名为 vhost-vdpa/$vdpa_device_index 的字符设备条目供用户空间使用。用户空间可以在此字符设备上使用 vhost ioctls 来设置后端



module_init(vhost_net_init)
static const struct file_operations vhost_net_fops = {
	.owner          = THIS_MODULE,
	.release        = vhost_net_release,
	.read_iter      = vhost_net_chr_read_iter,
	.write_iter     = vhost_net_chr_write_iter,
	.poll           = vhost_net_chr_poll,
	.unlocked_ioctl = vhost_net_ioctl,
	.compat_ioctl   = compat_ptr_ioctl,
	.open           = vhost_net_open,
        n->vqs[VHOST_NET_VQ_TX].xdp = xdp
        n->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick
        n->vqs[VHOST_NET_VQ_RX].vq.handle_kick = handle_rx_kick
        handle_tx_kick
            handle_tx
                handle_tx_zerocopy
                    get_tx_bufs
                        vhost_net_tx_get_vq_desc
                            vhost_net_busy_poll
        vhost_dev_init
        vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev, vqs[VHOST_NET_VQ_TX])
        vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev,	vqs[VHOST_NET_VQ_RX])
	.llseek		= noop_llseek,
};




vdpa alibaab
drivers/vdpa/alibaba/eni_vdpa.c
module_pci_driver(eni_vdpa_driver) -> eni_vdpa_probe
    eni_vdpa = vdpa_alloc_device(struct eni_vdpa, vdpa, dev, &eni_vdpa_ops, 1, 1, NULL, false)
    vp_legacy_probe(ldev)
    eni_vdpa->queues = eni_vdpa_get_num_queues(eni_vdpa) -> read register
        eni_vdpa_get_config(&eni_vdpa->vdpa, offsetof(struct virtio_net_config, max_virtqueue_pairs), &max_virtqueue_pairs,	sizeof(max_virtqueue_pairs))
        void __iomem *ioaddr = ldev->ioaddr + VIRTIO_PCI_CONFIG_OFF(eni_vdpa->vectors) + offset
        *p++ = ioread8(ioaddr + i)
    for (i = 0; i < eni_vdpa->queues; i++)
        eni_vdpa->vring[i].irq = VIRTIO_MSI_NO_VECTOR
        eni_vdpa->vring[i].notify = ldev->ioaddr + VIRTIO_PCI_QUEUE_NOTIFY
    vdpa_register_device(&eni_vdpa->vdpa, eni_vdpa->queues)

static const struct vdpa_config_ops eni_vdpa_ops = {
	.get_device_features = eni_vdpa_get_device_features,
	.set_driver_features = eni_vdpa_set_driver_features,
	.get_driver_features = eni_vdpa_get_driver_features,
	.get_status	= eni_vdpa_get_status,
	.set_status	= eni_vdpa_set_status,
	.reset		= eni_vdpa_reset,
	.get_vq_num_max	= eni_vdpa_get_vq_num_max,
	.get_vq_num_min	= eni_vdpa_get_vq_num_min,
	.get_vq_state	= eni_vdpa_get_vq_state,
	.set_vq_state	= eni_vdpa_set_vq_state,
	.set_vq_cb	= eni_vdpa_set_vq_cb,
	.set_vq_ready	= eni_vdpa_set_vq_ready,
	.get_vq_ready	= eni_vdpa_get_vq_ready,
	.set_vq_num	= eni_vdpa_set_vq_num,
	.set_vq_address	= eni_vdpa_set_vq_address, -> eni_vdpa：为阿里巴巴 ENI 添加 vDPA 驱动程序，此补丁为阿里巴巴 ENI（弹性网络接口）添加了一个新的 vDPA 驱动程序，该驱动程序基于 virtio 0.9.5 规范构建。并且此驱动程序目前仅在 X86 主机上启用
        vp_legacy_set_queue_address(ldev, qid, pfn) -> vs modern -> vp_modern_queue_address
            iowrite16(index, ldev->ioaddr + VIRTIO_PCI_QUEUE_SEL) -> select queue
            iowrite32(queue_pfn, ldev->ioaddr + VIRTIO_PCI_QUEUE_PFN)
	.kick_vq	= eni_vdpa_kick_vq,
        iowrite16(qid, eni_vdpa->vring[qid].notify)
	.get_device_id	= eni_vdpa_get_device_id,
	.get_vendor_id	= eni_vdpa_get_vendor_id,
	.get_vq_align	= eni_vdpa_get_vq_align,
	.get_config_size = eni_vdpa_get_config_size,
	.get_config	= eni_vdpa_get_config,
	.set_config	= eni_vdpa_set_config,
	.set_config_cb  = eni_vdpa_set_config_cb,
	.get_vq_irq	= eni_vdpa_get_vq_irq,
};



qemu驱动commit: https://github.com/wangzhou/linux/commit/87695695e4d3ea72e60d9c5da5fc5804ae71fb48
如何在qemu里增加一个虚拟设备: https://wangzhou.github.io/%E5%A6%82%E4%BD%95%E5%9C%A8qemu%E9%87%8C%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E8%99%9A%E6%8B%9F%E8%AE%BE%E5%A4%87/

#动态模块
obj-m += helloworld.o


env:
install make, gcc


qa?
模块验证失败：缺少签名和/或所需密钥 - 污染内核


show devices:
ls -alh /sys/devices/pci0000:00/0000:`lspci|grep "1234:3456"|awk '{print$1}'`/
cat /sys/devices/pci0000:00/0000:`lspci|grep "1234:3456"|awk '{print$1}'`/copy_size
echo 128 > /sys/devices/pci0000:00/0000:`lspci|grep "1234:3456"|awk '{print$1}'`/copy_size


flow:
pci_register_driver

关键宏:(sysfs/debugfs), 属性文件, read/write fd will trigger copy_size_show/copy_size_store
static DEVICE_ATTR_RW(copy_size);

write:
echo 128 > /sys/devices/pci0000:00/0000:00:05.0/copy_size
copy_size_store
    do_dma_copy_test(hw)
        src = kmalloc(buf_size * 2, GFP_KERNEL) -> malloc 80 * 2 = 160
        dma_src = dma_map_single(dev, src, buf_size * 2, DMA_BIDIRECTIONAL) -> map va(dram) -> ha(device)
        do_dma_copy(hw, dma_src, dma_src + buf_size, buf_size)
            void *base = hw->io_va
            start DMA
            writeq(src, base + IO_OFFSET_CHAN_SRC) -> mmio -> dma_engine_io_write
            ...
                pci_dma_read(pdev, chan->src + offset, buf, buf_size)
                pci_dma_write(pdev, chan->dts + offset, buf, buf_size);
            readl_relaxed_poll_timeout(base + IO_OFFSET_CHAN_DONE, val, (val == 1), 10, 1000)
    dma_unmap_single
    memcmp(src, src + buf_size, buf_size)

read:
cat /sys/devices/pci0000:00/0000:00:05.0/copy_siz
copy_size_show
    sysfs_emit(buf, "0x%lx\n", hw->copy_size)



readl_relaxed_poll_timeout delay_us, timeout_us




watch "dmesg | tail -20"

real_time dmesg log:
tail -f /var/log/{dmesg,syslog}



sudo strace insmod dma_engine.ko



debug driver:
root@vm:/home/xb/project/dma_engine_driver# lsmod|grep engine
dma_engine             16384  -1
root@vm:/home/xb/project/dma_engine_driver# lsmod |less
root@vm:/home/xb/project/dma_engine_driver# cat /proc/modules |grep dma
dma_engine 16384 -1 - Unloading 0xffffffffc045f000 (OE-)

cat /sys/module/dma_engine/refcnt



dma_engine.c
module_init(dma_init)
static struct pci_driver dma_pci_drv = {
	.name     = DMA_ENGINE,
	.id_table = dma_pci_tbl,
	.probe    = dma_pci_probe,
        pci_enable_device
        dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64))
        pci_alloc_irq_vectors
        pci_irq_vector
        pci_set_master(pdev) -> 设置设备具有获得总线的能力，即调用这个函数，使设备具备申请使用PCI总线的能力
        hw = dma_hw_init(pdev, irq)
            hw = devm_kzalloc(dev, sizeof(*hw), GFP_KERNEL)
            hw->io_va = devm_ioremap_resource(dev, &pdev->resource[0]) -> get bar0 addr -> 将设备寄存器地址映射为内核空间的虚拟地址HA -> VA -> 申请内存资源:devm_request_mem_region, 映射为虚拟地址:devm_ioremap
            devm_request_irq(dev, irq, dma_irq_handler, 0, DMA_ENGINE, hw) -> 申请中断号
                dev_info(&hw->pdev->dev, "dma interrupt! "FFL_FMT"\n", FFL) -> print irq info
            sysfs_create_file(&dev->kobj, &dev_attr_copy_size.attr) 创建系统文件 copy_size -> 通过sysfs接口向用户态暴露了一个叫 copy_size 的文件，向这个文件写入一个数值将触发一次该数值大小的DMA数据拷贝，为了方便起见，我们在这个驱动的内部生成需要拷贝的数据 -> /sys/devices/pci0000:00/0000:00:05.0/copy_size
        pci_set_drvdata(pdev, hw)
	.remove   = dma_pci_remove,
	.shutdown = dma_pci_shutdown,
};


edu, dma
print log level:
cat /proc/sys/kernel/printk

sudo echo 7 > /proc/sys/kernel/printk

dmesg -wH

read config edu:
configure address 0x0, read data is 11e81234

mmio_base = pci_iomap(device, 0, pci_resource_len(device, 0))


driver:
// Config
#define DMA_REG_SRC    0x80
#define DMA_REG_DST    0x88
#define DMA_REG_LEN    0x90
#define DMA_REG_CMD    0x98

// DMA
#define DMA_FILE_OFF   0x40000
#define DMA_REG_SRC    0x80
#define DMA_REG_DST    0x88
#define DMA_REG_LEN    0x90
#define DMA_REG_CMD    0x98

#define DMA_CMD_SEND   0x01
#define DMA_CMD_RECV   0x03
#define DMA_CMD_IRQ    0x04

// MMIO
#define MMIO_FILE_OFF     0x214
static void __iomem *mmio_base;

module_init(edu_init);
    pci_register_driver(&pci_driver)

static struct pci_driver pci_driver = {
    .name = "edu",
    .id_table = pci_id_table,
    .probe = edu_probe,
    .remove = edu_remove
};

edu_probe
    major = register_chrdev(0, "edu", &file_ops);
    pci_request_region(device, 0, "region")
    dma_set_mask_and_coherent(&(device->dev),  DMA_BIT_MASK(28))
    mmio_base = pci_iomap(device, 0, pci_resource_len(device, 0))
    request_irq(device->irq, dma_irq_handler, IRQF_SHARED, "pci_irq_handler0", &major)



static struct file_operations file_ops = {
    .owner     = THIS_MODULE,
    .llseek    = edu_llseek,
    .read      = edu_read,
    .write     = edu_write
};

test:
test\test.c
    open("/dev/edu", O_RDWR)
    write(fd, buffer, length) -> edu_write
        switch(get_ops_type(*off))
        case CONFIG_OPS:
            retVal = edu_config_write(filp, buf, len, off);
                pci_write_config_dword(pci_device, *off - MMIO_FILE_OFF, memory)
        case MMIO_OPS:
            retVal = edu_reg_write(filp, buf, len, off);
                iowrite32(memory, mmio_base + *off - MMIO_FILE_OFF)
        case DMA_OPS:
            edu_dma_write(filp, buf,len,off);
                vaddr = dma_alloc_coherent(&(pci_device->dev), len, &dma_handle, GFP_ATOMIC)
                if(raw_copy_from_user(vaddr, buf, len)
                iowrite32(dma_handle, mmio_base + DMA_REG_SRC);
                iowrite32(*off, mmio_base + DMA_REG_DST);
                iowrite32(len, mmio_base + DMA_REG_LEN);
                iowrite32(DMA_CMD_SEND|DMA_CMD_IRQ, mmio_base + DMA_REG_CMD); -> start DMA
                while(ioread32(mmio_base + 0x98) & 0x1 )
                dma_free_coherent(&(pci_device->dev), len, vaddr, dma_handle)



bytedance, vduse, commit: https://github.com/ssbandjl/linux/commit/c8a6153b6c59d95c0e091f053f6f180952ade91e
kernel_doc: https://docs.kernel.org/userspace-api/vduse.html
vduse：在用户空间引入 VDUSE - vDPA 设备，此 VDUSE 驱动程序支持在用户空间实现软件模拟的 vDPA 设备。vDPA 设备由 /dev/vduse/control 上的 ioctl(VDUSE_CREATE_DEV) 创建。然后将字符设备接口 (/dev/vduse/$NAME) 导出到用户空间进行设备模拟。为了使设备模拟更安全，设备的控制路径在内核中处理。引入了一种消息机制来将一些数据平面相关的控制消息转发到用户空间。并且在数据路径中，DMA 缓冲区将通过不同的方式映射到用户空间地址空间，具体取决于 vDPA 设备所连接的 vDPA 总线。在 virtio-vdpa 情况下，使用基于 MMU 的软件 IOTLB 来实现这一点。在 vhost-vdpa 情况下，DMA 缓冲区位于用户空间内存区域中，可以通过传输 shmfd 将其共享给 VDUSE 用户空间进程。有关 VDUSE 设计和使用的更多详细信息，请参阅后续文档提交
modprobe vduse
insmod vduse.ko
drivers/vdpa/vdpa_user/vduse_dev.c
module_init(vduse_init)
    class_register(&vduse_class)
        .name = "vduse"
        "vduse/%s"
    alloc_chrdev_region(&vduse_major, 0, VDUSE_DEV_MAX, "vduse")
    cdev_init(&vduse_ctrl_cdev, &vduse_ctrl_fops)
    cdev_add(&vduse_ctrl_cdev, vduse_major, 1)
    dev = device_create(&vduse_class, NULL, vduse_major, NULL, "control") -> /dev/vduse/control
    cdev_init(&vduse_cdev, &vduse_dev_fops) -> /dev/vduse/$DEVICE
    cdev_add(&vduse_cdev, MKDEV(MAJOR(vduse_major), 1), VDUSE_DEV_MAX - 1)
    vduse_irq_wq = alloc_workqueue("vduse-irq", WQ_HIGHPRI | WQ_SYSFS | WQ_UNBOUND, 0)
    vduse_irq_bound_wq = alloc_workqueue("vduse-irq-bound", WQ_HIGHPRI, 0)
    vduse_domain_init
        iova_cache_get() -> 内核启动时，通过该函数，声明了 struct iova 结构体专用的slab分配器
            iova_cache = kmem_cache_create("iommu_iova", sizeof(struct iova), 0, SLAB_HWCACHE_ALIGN, NULL)
    vduse_mgmtdev_init
        vduse_mgmt->mgmt_dev.ops = &vdpa_dev_mgmtdev_ops
        vdpa_mgmtdev_register(&vduse_mgmt->mgmt_dev)


static const struct file_operations vduse_ctrl_fops = {
	.owner		= THIS_MODULE,
	.open		= vduse_open,
        control = kmalloc(sizeof(struct vduse_control), GFP_KERNEL)
        file->private_data = control
	.release	= vduse_release,
	.unlocked_ioctl	= vduse_ioctl,
        switch (cmd)
        case VDUSE_CREATE_DEV -> #define VDUSE_CREATE_DEV	_IOW(VDUSE_BASE, 0x02, struct vduse_dev_config)
            vduse_create_dev(&config, buf, control->api_version)
                dev = vduse_dev_create()
                    INIT_LIST_HEAD(&dev->send_list)
                    INIT_LIST_HEAD(&dev->recv_list)
                    INIT_WORK(&dev->inject, vduse_dev_irq_inject)
                        dev->config_cb.callback(dev->config_cb.private)
                dev->device_id = config->device_id <- from qemu -> vblk_exp->dev = vduse_dev_create(vblk_opts->name, VIRTIO_ID_BLOCK, 0,features, num_queues,sizeof(struct virtio_blk_config),(char *)&config, &vduse_blk_ops,vblk_exp)
                dev->bounce_size = VDUSE_BOUNCE_SIZE -> 64MB
                idr_alloc(&vduse_idr, dev, 1, VDUSE_DEV_MAX, GFP_KERNEL)
                dev->dev = device_create_with_groups(&vduse_class, NULL, MKDEV(MAJOR(vduse_major), dev->minor), dev, vduse_dev_groups, "%s", config->name) -> new kernel -> creates a device and registers it with sysfs
                vduse_dev_init_vqs(dev, config->vq_align, config->vq_num)
                    dev->vqs = kcalloc(dev->vq_num, sizeof(*dev->vqs), GFP_KERNEL)
                    for (i = 0; i < vq_num; i++)
                        dev->vqs[i]->irq_effective_cpu = IRQ_UNBOUND
                        INIT_WORK(&dev->vqs[i]->inject, vduse_vq_irq_inject)
                            vq->cb.callback(vq->cb.private)
                        INIT_WORK(&dev->vqs[i]->kick, vduse_vq_kick_work) -> vduse_vq_kick(vq)
                            if (vq->kickfd)
                                eventfd_signal(vq->kickfd) -> vq->kickfd = ctx <- vduse_kickfd_setup
                                    -> to qemu -> on_vduse_vq_kick
                            else
                                vq->kicked = true
                        kobject_add(&dev->vqs[i]->kobj, &dev->dev->kobj, "vq%d", i)
                __module_get(THIS_MODULE) -> 增加模块的引用计数
        case VDUSE_DESTROY_DEV
            name[VDUSE_NAME_MAX - 1] = '\0'
            vduse_destroy_dev(name)
                vduse_dev_reset(dev)
                device_destroy(&vduse_class, MKDEV(MAJOR(vduse_major), dev->minor))
                idr_remove(&vduse_idr, dev->minor)
                vduse_dev_deinit_vqs(dev)
                vduse_domain_destroy(dev->domain)
                module_put(THIS_MODULE)
	.compat_ioctl	= compat_ptr_ioctl,
	.llseek		= noop_llseek,
};

static const struct file_operations vduse_dev_fops = {
	.owner		= THIS_MODULE,
	.open		= vduse_dev_open,
	.release	= vduse_dev_release,
	.read_iter	= vduse_dev_read_iter,
	.write_iter	= vduse_dev_write_iter,
	.poll		= vduse_dev_poll,
	.unlocked_ioctl	= vduse_dev_ioctl,
        case VDUSE_IOTLB_GET_FD -> those IOVA regions can be mapped into userspace
            map = vhost_iotlb_itree_first(dev->domain->iotlb, entry.start, entry.last)
            receive_fd(f, NULL, perm_to_file_flags(entry.perm))
        case VDUSE_DEV_GET_FEATURES
            ret = put_user(dev->driver_features, (u64 __user *)argp)
        case VDUSE_DEV_SET_CONFIG
            copy_from_user(dev->config + config.offset, argp + size, config.length)
        case VDUSE_DEV_INJECT_CONFIG_IRQ
             vduse_dev_queue_irq_work(dev, &dev->inject, IRQ_UNBOUND)
        case VDUSE_VQ_SETUP
            index = array_index_nospec(config.index, dev->vq_num)
            dev->vqs[index]->num_max = config.max_size
        case VDUSE_VQ_GET_INFO -> Get the specified virtqueue’s information with this ioctl, including the size, the IOVAs of descriptor table, available ring and used ring, the state and the ready status
            vq = dev->vqs[index];
            vq_info.desc_addr = vq->desc_addr;
            vq_info.driver_addr = vq->driver_addr;
            vq_info.device_addr = vq->device_addr;
            vq_info.num = vq->num;
        case VDUSE_VQ_SETUP_KICKFD -> Setup the kick eventfd for the specified virtqueues, The kick eventfd is used by VDUSE kernel module to notify userspace to consume the available ring. This is optional since userspace can choose to poll the available ring instead
            vduse_kickfd_setup(dev, &eventfd)
                ctx = eventfd_ctx_fdget(eventfd->fd)
                vq->kickfd = ctx
        case VDUSE_VQ_INJECT_IRQ -> Inject an interrupt for specific virtqueue with this ioctl, after the used ring is filled
            vduse_dev_queue_irq_work(dev, &dev->vqs[index]->inject,	dev->vqs[index]->irq_effective_cpu)
                queue_work(vduse_irq_wq, irq_work)
                or queue_work_on(irq_effective_cpu, vduse_irq_bound_wq, irq_work)
        case VDUSE_IOTLB_REG_UMEM
            vduse_dev_reg_umem(dev, umem.iova, umem.uaddr, umem.size)
                npages = size >> PAGE_SHIFT;
	            page_list = __vmalloc(array_size(npages, sizeof(struct page *)), GFP_KERNEL_ACCOUNT);
	            umem = kzalloc(sizeof(*umem), GFP_KERNEL)
                pinned = pin_user_pages(uaddr, npages, FOLL_LONGTERM | FOLL_WRITE, page_list) -> pin user pages in memory for use by other devices
                vduse_domain_add_user_bounce_pages(dev->domain, page_list, pinned)
                    memcpy_to_page(pages[i], 0, page_address(map->bounce_page), PAGE_SIZE)
                mmgrab(current->mm)
        case VDUSE_IOTLB_DEREG_UMEM
            vduse_dev_dereg_umem(dev, umem.iova, umem.size)
        case VDUSE_IOTLB_GET_INFO
            map = vhost_iotlb_itree_first(dev->domain->iotlb, info.start, info.last)
	.compat_ioctl	= compat_ptr_ioctl,
	.llseek		= noop_llseek,
};

static const struct vdpa_mgmtdev_ops vdpa_dev_mgmtdev_ops = {
	.dev_add = vdpa_dev_add,
        dev = vduse_find_dev(name)
            idr_for_each_entry(&vduse_idr, dev, id) <- static DEFINE_IDR(vduse_idr
        vduse_dev_init_vdpa(dev, name)
            vdev = vdpa_alloc_device(struct vduse_vdpa, vdpa, dev->dev, &vduse_vdpa_config_ops, 1, 1, name, true)
            set_dma_ops(&vdev->vdpa.dev, &vduse_dev_dma_ops)
        dev->domain = vduse_domain_create(VDUSE_IOVA_SIZE - 1, dev->bounce_size)
            domain->iotlb = vhost_iotlb_alloc(0, 0)
            file = anon_inode_getfile("[vduse-domain]", &vduse_domain_fops, domain, O_RDWR)
            init_iova_domain(&domain->stream_iovad,	PAGE_SIZE, IOVA_START_PFN)
            iova_domain_init_rcaches(&domain->stream_iovad)
        _vdpa_register_device(&dev->vdev->vdpa, dev->vq_num) -> __vdpa_register_device(vdev, nvqs)
            dev = bus_find_device(&vdpa_bus, NULL, dev_name(&vdev->dev), vdpa_name_match)
            device_add(&vdev->dev)
	.dev_del = vdpa_dev_del,
};


static const struct dma_map_ops vduse_dev_dma_ops = {
	.map_page = vduse_dev_map_page,
        vduse_domain_map_page(domain, page, offset, size, dir, attrs)
            dma_addr_t iova = vduse_domain_alloc_iova(iovad, size, limit)
                iova_pfn = alloc_iova_fast(iovad, iova_len, limit >> shift, true) -> allocates an iova from rcache -> 首先尝试从缓存中查找满足条件的 I/O 虚拟地址空间地址段，如果找到就成功返回；否则，调用 alloc_iova() 函数分配 IOVA 内存段，如果需要刷新缓存，则释放 CPU 缓存的所有 IOVA 范围
                    iova_pfn = iova_rcache_get
                    new_iova = alloc_iova(iovad, size, limit_pfn, true)
                return (dma_addr_t)iova_pfn << shift
            vduse_domain_init_bounce_map
                vduse_iotlb_add_range(domain, 0, domain->bounce_size - 1, 0, VHOST_MAP_RW, domain->file, 0)
                    map_file->file = get_file(file)
                    vhost_iotlb_add_range_ctx
            vduse_domain_map_bounce_page
                map->bounce_page = alloc_page(GFP_ATOMIC)
            vduse_domain_bounce
	.unmap_page = vduse_dev_unmap_page,
	.alloc = vduse_dev_alloc_coherent,
        addr = vduse_domain_alloc_coherent(domain, size, (dma_addr_t *)&iova, flag, attrs)
            dma_addr_t iova = vduse_domain_alloc_iova(iovad, size, limit)
            alloc_pages_exact -> 分配精确数量的物理连续页面
            vduse_iotlb_add_range virt_to_phys
        *dma_addr = (dma_addr_t)iova
	.free = vduse_dev_free_coherent,
	.max_mapping_size = vduse_dev_max_mapping_size,
};

static const struct vdpa_config_ops vduse_vdpa_config_ops = {
	.set_vq_address		= vduse_vdpa_set_vq_address,
        vq->desc_addr = desc_area;
        vq->driver_addr = driver_area;
        vq->device_addr = device_area;
	.kick_vq		= vduse_vdpa_kick_vq,
        schedule_work(&vq->kick)
        vduse_vq_kick(vq)
	.set_vq_cb		= vduse_vdpa_set_vq_cb,
	.set_vq_num             = vduse_vdpa_set_vq_num,
	.set_vq_ready		= vduse_vdpa_set_vq_ready,
	.get_vq_ready		= vduse_vdpa_get_vq_ready,
	.set_vq_state		= vduse_vdpa_set_vq_state,
	.get_vq_state		= vduse_vdpa_get_vq_state,
	.get_vq_align		= vduse_vdpa_get_vq_align,
	.get_device_features	= vduse_vdpa_get_device_features,
	.set_driver_features	= vduse_vdpa_set_driver_features,
	.get_driver_features	= vduse_vdpa_get_driver_features,
	.set_config_cb		= vduse_vdpa_set_config_cb,
	.get_vq_num_max		= vduse_vdpa_get_vq_num_max,
	.get_device_id		= vduse_vdpa_get_device_id,
	.get_vendor_id		= vduse_vdpa_get_vendor_id,
	.get_status		= vduse_vdpa_get_status,
	.set_status		= vduse_vdpa_set_status,
	.get_config_size	= vduse_vdpa_get_config_size,
	.get_config		= vduse_vdpa_get_config,
	.set_config		= vduse_vdpa_set_config,
	.get_generation		= vduse_vdpa_get_generation,
	.set_vq_affinity	= vduse_vdpa_set_vq_affinity,
        if (cpu_mask)
            cpumask_copy(&dev->vqs[idx]->irq_affinity, cpu_mask)
        else
            cpumask_setall(&dev->vqs[idx]->irq_affinity)
	.get_vq_affinity	= vduse_vdpa_get_vq_affinity,
	.reset			= vduse_vdpa_reset,
	.set_map		= vduse_vdpa_set_map,
	.free			= vduse_vdpa_free,
};


static const struct file_operations vduse_domain_fops = {
	.owner = THIS_MODULE,
	.mmap = vduse_domain_mmap,
	.release = vduse_domain_release,
};


kvm test,
tools/testing/selftests/kvm/aarch64/page_fault_test.c
    vm = ____vm_create(VM_SHAPE(mode))
    setup_memslots(vm, p)
    kvm_vm_elf_load
    setup_ucall
    vm_vcpu_add(vm, 0, guest_code)
    setup_gva_maps
        region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA)
        virt_pg_map(vm, TEST_GVA, region->region.guest_phys_addr) -> virt_arch_pg_map(vm, vaddr, paddr)
            __virt_pg_map(vm, vaddr, paddr, PG_LEVEL_4K)
                pml4e = virt_create_upper_pte(vm, &vm->pgd, vaddr, paddr, PG_LEVEL_512G, level)
                    virt_get_pte
                    vm_alloc_page_table
                        vm_phy_page_alloc(vm, KVM_GUEST_PAGE_TABLE_MIN_PADDR, vm->memslots[MEM_REGION_PT])
                            vm_phy_pages_alloc(vm, 1, paddr_min, memslot)
                pdpe = virt_create_upper_pte(vm, pml4e, vaddr, paddr, PG_LEVEL_1G, level)
                pde = virt_create_upper_pte(vm, pdpe, vaddr, paddr, PG_LEVEL_2M, level)
                pte = virt_get_pte(vm, pde, vaddr, PG_LEVEL_4K)
        pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA))
    vcpu_run_loop



vu_notify
    vhost_user_set_vring_state(vu_dev, VHOST_USER_VRING_KICK, vq->index, 0)



中断和kick vq
Interrupts and virtqueue kicks are relayed via eventfd (two yellow arrows going to and from the “vhost subsystem” box 如下图两个黄框). The vhost-vDPA adapter sets up eventfds via VHOST_SET_VRING_KICK (for virtqueue kick) and VHOST_SET_VRING_CALL (for virtqueue interrupt). When vhost-vDPA adapter needs to kick a specific virtqueue, it will write to that eventfd. The vhost-vDPA device will be notified and forward the kick request to the vDPA VF driver (which is one example of a vDPA device driver) via vDPA bus operations. When an interrupt is raised by a specific virtqueue, the vhost-vDPA device writes to the eventfd and wakes up the vhost-vDPA adapter in the userspace




vdpa mgmtdev show
vdpa/vdpa.c -> main
    vdpa = vdpa_alloc()
        vdpa->indent = alloc_indent_mem()
    vdpa_init(vdpa)
        mnlu_gen_socket_open(&vdpa->nlg, VDPA_GENL_NAME, VDPA_GENL_VERSION)
            nlg->nl = mnlu_socket_open(NETLINK_GENERIC)
        new_json_obj_plain(vdpa->json_output)
            _jw = jsonw_new(stdout)
            jsonw_pretty(_jw, true)
    vdpa_cmd(vdpa, argc, argv)
        cmd_mgmtdev(vdpa, argc - 1, argv + 1)
            cmd_mgmtdev_show(vdpa, argc - 1, argv + 1)
                mnlu_gen_socket_cmd_prepare(&vdpa->nlg, VDPA_CMD_MGMTDEV_GET, flags)
                mnlu_gen_socket_sndrcv(&vdpa->nlg, nlh, cmd_mgmtdev_show_cb, vdpa)
        or cmd_dev(vdpa, argc - 1, argv + 1)




enum vdpa_command {
	VDPA_CMD_UNSPEC,
	VDPA_CMD_MGMTDEV_NEW,
	VDPA_CMD_MGMTDEV_GET,		/* can dump */
	VDPA_CMD_DEV_NEW, <- vdpa dev add mgmtdev vdpasim_net name foo2
	VDPA_CMD_DEV_DEL,
	VDPA_CMD_DEV_GET,		/* can dump */
};


red_black_tree, rb_tree:
static inline bool vhost_iotlb_itree_augment_compute_max(struct vhost_iotlb_map *node, bool exit) { 
    struct vhost_iotlb_map *child; 
    __u64 max = ((node)->last); 
    if (node->rb.rb_left) { 
        child = container_of(node->rb.rb_left, struct vhost_iotlb_map, rb); 
        if (child->__subtree_last > max) 
            max = child->__subtree_last; 
    } 
    if (node->rb.rb_right) { 
        child = container_of(node->rb.rb_right, struct vhost_iotlb_map, rb); 
        if (child->__subtree_last > max) 
            max = child->__subtree_last; 
    } 
    if (exit && node->__subtree_last == max) 
        return true; 
    node->__subtree_last = max; 
    return false; 
} 

static inline void vhost_iotlb_itree_augment_propagate(struct rb_node *rb, struct rb_node *stop) { 
    while (rb != stop) { 
        struct vhost_iotlb_map *node = container_of(rb, struct vhost_iotlb_map, rb); 
        if (vhost_iotlb_itree_augment_compute_max(node, true)) 
            break; 
        rb = ((struct rb_node *)((&node->rb)->__rb_parent_color & ~3)); 
    } 
} 

static inline void vhost_iotlb_itree_augment_copy(struct rb_node *rb_old, struct rb_node *rb_new) { 
    struct vhost_iotlb_map *old = container_of(rb_old, struct vhost_iotlb_map, rb); 
    struct vhost_iotlb_map *new = container_of(rb_new, struct vhost_iotlb_map, rb); 
    new->__subtree_last = old->__subtree_last; 
} 

static void vhost_iotlb_itree_augment_rotate(struct rb_node *rb_old, struct rb_node *rb_new) { 
    struct vhost_iotlb_map *old = container_of(rb_old, struct vhost_iotlb_map, rb); 
    struct vhost_iotlb_map *new = container_of(rb_new, struct vhost_iotlb_map, rb); 
    new->__subtree_last = old->__subtree_last; 
    vhost_iotlb_itree_augment_compute_max(old, false); 
} 

static const struct rb_augment_callbacks vhost_iotlb_itree_augment = { 
    .propagate = vhost_iotlb_itree_augment_propagate, 
    .copy = vhost_iotlb_itree_augment_copy, 
    .rotate = vhost_iotlb_itree_augment_rotate 
}; 



INTERVAL_TREE_DEFINE(struct vhost_iotlb_map,
		     rb, __u64, __subtree_last,
		     START, LAST, static inline, vhost_iotlb_itree);
static inline void vhost_iotlb_itree_insert(struct vhost_iotlb_map *node, struct rb_root_cached *root) { 
    struct rb_node **link = &root->rb_root.rb_node, 
    *rb_parent = NULL; 
    __u64 start = ((node)->start), 
    last = ((node)->last); 
    struct vhost_iotlb_map *parent; 
    bool leftmost = true; 
    while (*link) { 
        rb_parent = *link; parent = container_of(rb_parent, struct vhost_iotlb_map, rb); 
        if (parent->__subtree_last < last) 
            parent->__subtree_last = last; 
        if (start < ((parent)->start)) 
            link = &parent->rb.rb_left; 
        else { 
            link = &parent->rb.rb_right; 
            leftmost = false; 
        } 
    } 
    node->__subtree_last = last; 
    rb_link_node(&node->rb, rb_parent, link); 
    rb_insert_augmented_cached(&node->rb, root, leftmost, &vhost_iotlb_itree_augment); 
} 

static inline void vhost_iotlb_itree_remove(struct vhost_iotlb_map *node, struct rb_root_cached *root) { 
    rb_erase_augmented_cached(&node->rb, root, &vhost_iotlb_itree_augment); 
} 

static struct vhost_iotlb_map * vhost_iotlb_itree_subtree_search(struct vhost_iotlb_map *node, __u64 start, __u64 last) { 
    while (true) { 
        if (node->rb.rb_left) { 
            struct vhost_iotlb_map *left = container_of(node->rb.rb_left, struct vhost_iotlb_map, rb); 
            if (start <= left->__subtree_last) { 
                node = left; 
                continue; 
            } 
        } 
        if (((node)->start) <= last) { 
            if (start <= ((node)->last)) 
                return node; 
            if (node->rb.rb_right) { 
                node = container_of(node->rb.rb_right, struct vhost_iotlb_map, rb); 
                if (start <= node->__subtree_last) 
                    continue; 
            } 
        } 
        return NULL; 
    } 
} 

static inline struct vhost_iotlb_map * vhost_iotlb_itree_iter_first(struct rb_root_cached *root, __u64 start, __u64 last) { 
    struct vhost_iotlb_map *node, *leftmost; 
    if (!root->rb_root.rb_node) 
        return NULL; 
    node = container_of(root->rb_root.rb_node, struct vhost_iotlb_map, rb); 
    if (node->__subtree_last < start) 
        return NULL; 
    leftmost = container_of(root->rb_leftmost, struct vhost_iotlb_map, rb); 
    if (((leftmost)->start) > last) 
        return NULL; 
    return vhost_iotlb_itree_subtree_search(node, start, last); 
} 

static inline struct vhost_iotlb_map * vhost_iotlb_itree_iter_next(struct vhost_iotlb_map *node, __u64 start, __u64 last) { 
    struct rb_node *rb = node->rb.rb_right, *prev; 
    while (true) { 
        if (rb) { 
            struct vhost_iotlb_map *right = container_of(rb, struct vhost_iotlb_map, rb); 
            if (start <= right->__subtree_last) 
                return vhost_iotlb_itree_subtree_search(right, start, last); 
        } 
        do { 
            rb = ((struct rb_node *)((&node->rb)->__rb_parent_color & ~3)); 
            if (!rb) 
                return NULL; 
            prev = &node->rb; 
            node = container_of(rb, struct vhost_iotlb_map, rb); 
            rb = node->rb.rb_right; 
        } while (prev == rb); 
        if (last < ((node)->start)) 
            return NULL; 
        else if (start <= ((node)->last)) 
            return node; 
    } 
}



module_virtio_driver(virtio_balloon_driver)
virtio_balloon_driver



print_val_func
    virtqueue_add_outbuf(vq, &sg, 1, vb, GFP_KERNEL)
    virtqueue_kick(vq)


insmod virtio_scsi.ko -> drivers/scsi/virtio_scsi.c -> module_init(virtio_scsi_init)
    virtscsi_cmd_cache = KMEM_CACHE(virtio_scsi_cmd, 0)
    virtscsi_cmd_pool = mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ, virtscsi_cmd_cache)
    register_virtio_driver(&virtio_scsi_driver)


static struct virtio_driver virtio_scsi_driver = {
	.feature_table = features,
	.feature_table_size = ARRAY_SIZE(features),
	.driver.name = KBUILD_MODNAME,
	.driver.owner = THIS_MODULE,
	.id_table = id_table, -> #define VIRTIO_ID_SCSI			8 /* virtio scsi */
	.probe = virtscsi_probe,
        num_queues = virtscsi_config_get(vdev, num_queues) ? : 1
        shost = scsi_host_alloc(&virtscsi_host_template, struct_size(vscsi, req_vqs, num_queues))
            shost->shost_gendev.bus = &scsi_bus_type;
	        shost->shost_gendev.type = &scsi_host_type;
            shost->ehandler = kthread_run(scsi_error_handler, shost, "scsi_eh_%d", shost->host_no)
            ...
        virtscsi_init(vdev, vscsi)
            callbacks[0] = virtscsi_ctrl_done
            callbacks[1] = virtscsi_event_done
            callbacks[i] = virtscsi_req_done
                virtscsi_complete_cmd
            virtio_find_vqs(vdev, num_vqs, vqs, callbacks, names, &desc)
            virtscsi_init_vq(&vscsi->ctrl_vq, vqs[0])
            virtscsi_init_vq(&vscsi->event_vq, vqs[1])
            virtscsi_init_vq(&vscsi->req_vqs[i - VIRTIO_SCSI_VQ_BASE], vqs[i])
        shost->can_queue = virtqueue_get_vring_size(vscsi->req_vqs[0].vq)
        scsi_add_host(shost, &vdev->dev) -> scsi_add_host_with_dma
            scsi_init_sense_cache(shost)
            scsi_mq_setup_tags(shost)
            device_add(&shost->shost_gendev)
            scsi_sysfs_add_host(shost)
        virtio_device_ready(vdev)
        scsi_scan_host(shost)
            do_scsi_scan_host
                scsi_scan_host_selected
                    scsi_scan_channel -> __scsi_scan_target
                        starget = scsi_alloc_target(parent, channel, id)
                        scsi_probe_and_add_lun
                            scsi_probe_lun
                            scsi_add_lun
                                transport_configure_device(&sdev->sdev_gendev)
                                scsi_device_set_state(sdev, SDEV_RUNNING)
                                scsi_sysfs_add_sdev
#ifdef CONFIG_PM_SLEEP
	.freeze = virtscsi_freeze,
	.restore = virtscsi_restore,
#endif
	.remove = virtscsi_remove,
};



static const struct scsi_host_template virtscsi_host_template = {
	.module = THIS_MODULE,
	.name = "Virtio SCSI HBA",
	.proc_name = "virtio_scsi",
	.this_id = -1,
	.cmd_size = sizeof(struct virtio_scsi_cmd),
	.queuecommand = virtscsi_queuecommand, <- scsi_dispatch_cmd
        dev_dbg(&sc->device->sdev_gendev, "cmd %p CDB: %#02x\n", sc, sc->cmnd[0])
        virtio_scsi_init_hdr_pi(vscsi->vdev, &cmd->req.cmd_pi, sc)
        or virtio_scsi_init_hdr(vscsi->vdev, &cmd->req.cmd, sc)
            cmd->lun[2] = (sc->device->lun >> 8) | 0x40
            cmd->task_attr = VIRTIO_SCSI_S_SIMPLE
        kick = (sc->flags & SCMD_LAST) != 0
        memcpy(cmd->req.cmd.cdb, sc->cmnd, sc->cmd_len)
        virtscsi_add_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd), kick) -> add a virtio_scsi_cmd to a virtqueue, optionally kick it
            __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size)
                sg_init_one(&req, &cmd->req, req_size)
                sg_init_one(&resp, &cmd->resp, resp_size)
                virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC) -> add io buf to vq, then choose to kick qemu
            needs_kick = virtqueue_kick_prepare(vq->vq)
            if (needs_kick)
                virtqueue_notify(vq->vq) -> check qemu project func: virtio_add_queue in "hw/scsi/virtio-scsi.c"
	.mq_poll = virtscsi_mq_poll,
	.commit_rqs = virtscsi_commit_rqs,
	.change_queue_depth = virtscsi_change_queue_depth,
	.eh_abort_handler = virtscsi_abort,
	.eh_device_reset_handler = virtscsi_device_reset,
        virtscsi_tmf -> [SCSI] virtio-scsi：基于 QEMU 的虚拟机的 SCSI 驱动程序 virtio-scsi HBA 是基于 QEMU 的虚拟机（包括 KVM）的替代存储堆栈的基础。与 virtio-blk 相比，它更具可扩展性，因为它支持单个 PCI 插槽上的多个 LUN）、功能更强大（它更容易支持主机设备到客户机的直通）并且更易于扩展（QEMU 实现的新 SCSI 功能不需要更新客户机中的驱动程序 -> commit: https://github.com/ssbandjl/linux/commit/4fe74b1cb051dc9d47a80e263c388cf1651783d4
            if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
	.eh_timed_out = virtscsi_eh_timed_out,
	.slave_alloc = virtscsi_device_alloc,
	.dma_boundary = UINT_MAX,
	.map_queues = virtscsi_map_queues,
	.track_queue_depth = 1,
};



blk_mq_dispatch_rq_list
    blk_mq_commit_rqs
        scsi_commit_rqs
            commit_rqs(shost, hctx->queue_num)



intel ifcvf, Intel IFC VF VDPA driver, vdpa, Intel FPGA 100G VF (IFCVF)
modprobe ifcvf
insmod ifcvf.ko
drivers/vdpa/ifcvf/ifcvf_main.c
module_pci_driver(ifcvf_driver) -> ifcvf_probe
    pcim_enable_device
    pcim_iomap_regions(pdev, BIT(0) | BIT(2) | BIT(4), IFCVF_DRIVER_NAME) -> "ifcvf"
    dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64))
    devm_add_action_or_reset(dev, ifcvf_free_irq_vectors, pdev)
    pci_set_master(pdev)
    ifcvf_mgmt_dev = kzalloc(sizeof(struct ifcvf_vdpa_mgmt_dev), GFP_KERNEL)
    vf->base = pcim_iomap_table(pdev) -> array of mapped addresses indexed by BAR -> here get first addr
    ifcvf_init_hw(vf, pdev) -> vDPA/ifcvf：支持用户空间查询管理设备的功能和 MQ 适应当前的 netlink 接口，此提交允许用户空间查询管理设备的功能位和 MQ 功能。目前 vDPA 设备和管理设备都是 VF 本身，因此此 ifcvf 应在设置 struct vdpa_mgmt_dev 之前在probe() 中初始化 virtio 功能
        pci_read_config_byte(pdev, PCI_CAPABILITY_LIST, &pos)
        ifcvf_read_config_range(pdev, (u32 *)&cap, sizeof(cap), pos)
            pci_read_config_dword(dev, where + i, val + i / 4)
        hw->common_cfg = get_cap_addr(hw, &cap)
        hw->notify_base = get_cap_addr(hw, &cap)
        hw->isr = get_cap_addr(hw, &cap)
        hw->nr_vring = vp_ioread16(&hw->common_cfg->num_queues)
        hw->vring = kzalloc(sizeof(struct vring_info) * hw->nr_vring, GFP_KERNEL)
        vp_iowrite16(i, &hw->common_cfg->queue_select)
        IFCVF_DBG(pdev, "PCI capability mapping: common cfg: %p, notify base: %p\n, isr cfg: %p, device cfg: %p, multiplier: %u\n",hw->common_cfg, hw->notify_base, hw->isr, hw->dev_cfg, hw->notify_off_multiplier)
    case VIRTIO_ID_BLOCK:
        ifcvf_mgmt_dev->mdev.id_table = id_table_blk; -> VIRTIO_ID_BLOCK
    ifcvf_mgmt_dev->mdev.ops = &ifcvf_vdpa_mgmt_dev_ops
        .dev_add = ifcvf_vdpa_dev_add,
            adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa, &pdev->dev, &ifc_vdpa_ops, 1, 1, NULL, false)
            dev_set_name(&vdpa_dev->dev, "vdpa%u", vdpa_dev->index)
            _vdpa_register_device(&adapter->vdpa, vf->nr_vring)
        .dev_del = ifcvf_vdpa_dev_del
    vdpa_mgmtdev_register(&ifcvf_mgmt_dev->mdev)
    pci_set_drvdata(pdev, ifcvf_mgmt_dev)



static const struct vdpa_config_ops ifc_vdpa_ops = {
	.get_device_features = ifcvf_vdpa_get_device_features,
	.set_driver_features = ifcvf_vdpa_set_driver_features,
	.get_driver_features = ifcvf_vdpa_get_driver_features,
	.get_status	= ifcvf_vdpa_get_status,
        vp_ioread8(&hw->common_cfg->device_status)
	.set_status	= ifcvf_vdpa_set_status,
        vp_iowrite8(status, &hw->common_cfg->device_status)
	.reset		= ifcvf_vdpa_reset,
        ifcvf_stop(vf)
            ifcvf_synchronize_irq(hw)
                for (i = 0; i < nvectors; i++)
                    irq = pci_irq_vector(pdev, i)
                    synchronize_irq(irq)
            ifcvf_reset_vring(hw)
                hw->vring[qid].cb.callback = NULL
                hw->vring[qid].cb.private = NULL
                ifcvf_set_vq_vector(hw, qid, VIRTIO_MSI_NO_VECTOR)
                    vp_iowrite16(qid, &cfg->queue_select);
                    vp_iowrite16(vector, &cfg->queue_msix_vector);
            ifcvf_reset_config_handler(hw)
                hw->config_cb.callback = NULL
                vp_iowrite16(vector,  &cfg->msix_config)
        ifcvf_reset(vf)
            ifcvf_set_status(hw, 0)
	.get_vq_num_max	= ifcvf_vdpa_get_vq_num_max,
        queue_size = vp_ioread16(&hw->common_cfg->queue_size)
	.get_vq_state	= ifcvf_vdpa_get_vq_state,
	.set_vq_state	= ifcvf_vdpa_set_vq_state,
	.set_vq_cb	= ifcvf_vdpa_set_vq_cb,
	.set_vq_ready	= ifcvf_vdpa_set_vq_ready,
	.get_vq_ready	= ifcvf_vdpa_get_vq_ready,
	.set_vq_num	= ifcvf_vdpa_set_vq_num,
	.set_vq_address	= ifcvf_vdpa_set_vq_address,
        vp_iowrite16(qid, &cfg->queue_select);
        vp_iowrite64_twopart(desc_area, &cfg->queue_desc_lo, &cfg->queue_desc_hi);
        vp_iowrite64_twopart(driver_area, &cfg->queue_avail_lo, &cfg->queue_avail_hi);
        vp_iowrite64_twopart(device_area, &cfg->queue_used_lo, &cfg->queue_used_hi);
	.get_vq_irq	= ifcvf_vdpa_get_vq_irq,
        return vf->vring[qid].irq
	.kick_vq	= ifcvf_vdpa_kick_vq,
        ifcvf_notify_queue(vf, qid)
            vp_iowrite16(qid, hw->vring[qid].notify_addr)
	.get_generation	= ifcvf_vdpa_get_generation,
	.get_device_id	= ifcvf_vdpa_get_device_id,
	.get_vendor_id	= ifcvf_vdpa_get_vendor_id,
	.get_vq_align	= ifcvf_vdpa_get_vq_align,
	.get_vq_group	= ifcvf_vdpa_get_vq_group,
	.get_config_size	= ifcvf_vdpa_get_config_size,
	.get_config	= ifcvf_vdpa_get_config,
        vp_ioread8(&hw->common_cfg->config_generation)
	.set_config	= ifcvf_vdpa_set_config,
        vp_iowrite8(*p++, hw->dev_cfg + offset + i)
	.set_config_cb  = ifcvf_vdpa_set_config_cb,
	.get_vq_notification = ifcvf_get_vq_notification,
        area.addr = vf->vring[idx].notify_pa
        area.size = PAGE_SIZE 
        or area.size = vf->notify_off_multiplier
};





/boot – Stores actual kernel and related file such system man and initrd images.
/etc or /boot/grub – Stores grub.conf file (most distro use /boot/grub these days).
/lib/modules/KERNEL-VERSION/* (/lib/modules/$(uname -r))- Linux device drivers (modules)

remove kernel:
dpkg --list | grep 5.4.0-25
dpkg --list | grep kernel-image
sudo apt --purge remove linux-headers_xxx


vfio:
/sys/bus/pci/drivers/vfio-pci





add virtio_blk disk stack:
[ 1209.382500] INFO: task systemd-udevd:154 blocked for more than 1087 seconds.
[ 1209.382508]       Not tainted 5.15.0-117-generic #127~20.04.1-Ubuntu
[ 1209.382511] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[ 1209.382512] task:systemd-udevd   state:D stack:    0 pid:  154 ppid:   136 flags:0x00004004
[ 1209.382517] Call Trace:
[ 1209.382519]  <TASK>
[ 1209.382522]  __schedule+0x2cd/0x890
[ 1209.382539]  ? submit_bh_wbc+0x18a/0x1c0
[ 1209.382544]  schedule+0x69/0x110
[ 1209.382548]  io_schedule+0x16/0x40
[ 1209.382550]  wait_on_page_bit_common+0x16e/0x3b0
[ 1209.382555]  ? scan_shadow_nodes+0x40/0x40
[ 1209.382559]  ? filemap_invalidate_unlock_two+0x50/0x50
[ 1209.382562]  do_read_cache_page+0x1e0/0x410
[ 1209.382565]  read_cache_page+0x12/0x20
[ 1209.382568]  read_part_sector+0x3b/0x100
[ 1209.382573]  read_lba+0xfa/0x220
[ 1209.382576]  find_valid_gpt+0xe0/0x6d0
[ 1209.382578]  ? vmap_small_pages_range_noflush+0x315/0x480
[ 1209.382584]  ? find_valid_gpt+0x6d0/0x6d0
[ 1209.382586]  efi_partition+0x7a/0x370
[ 1209.382588]  ? snprintf+0x49/0x70
[ 1209.382594]  ? find_valid_gpt+0x6d0/0x6d0
[ 1209.382595]  check_partition+0x15a/0x2a0
[ 1209.382599]  bdev_disk_changed+0xea/0x250
[ 1209.382603]  blkdev_get_whole+0x7b/0x90
[ 1209.382607]  blkdev_get_by_dev+0xd6/0x2d0
[ 1209.382609]  device_add_disk+0x3a3/0x400
[ 1209.382613]  virtblk_probe+0x4a3/0x805 [virtio_blk]
[ 1209.382629]  ? virtio_add_status+0x3d/0x50
[ 1209.382635]  virtio_dev_probe+0x195/0x230
[ 1209.382637]  really_probe.part.0+0xcb/0x380
[ 1209.382640]  really_probe+0x40/0x80
[ 1209.382641]  __driver_probe_device+0xe8/0x140
[ 1209.382643]  driver_probe_device+0x23/0xb0
[ 1209.382645]  __driver_attach+0xc5/0x180
[ 1209.382647]  ? __device_attach_driver+0x140/0x140
[ 1209.382649]  bus_for_each_dev+0x7e/0xd0
[ 1209.382654]  driver_attach+0x1e/0x30
[ 1209.382657]  bus_add_driver+0x178/0x220
[ 1209.382661]  driver_register+0x74/0xe0
[ 1209.382663]  register_virtio_driver+0x20/0x40
[ 1209.382670]  init+0x56/0x1000 [virtio_blk]
[ 1209.382673]  ? 0xffffffffc0123000
[ 1209.382675]  do_one_initcall+0x48/0x1e0
[ 1209.382680]  ? __cond_resched+0x19/0x40
[ 1209.382683]  ? kmem_cache_alloc_trace+0x15a/0x420
[ 1209.382688]  do_init_module+0x52/0x230
[ 1209.382694]  load_module+0x12ae/0x1520
[ 1209.382699]  __do_sys_finit_module+0xbf/0x120
[ 1209.382702]  ? __do_sys_finit_module+0xbf/0x120
[ 1209.382707]  __x64_sys_finit_module+0x1a/0x20
[ 1209.382708]  x64_sys_call+0x1ac3/0x1fa0
[ 1209.382712]  do_syscall_64+0x54/0xb0
[ 1209.382716]  ? syscall_exit_to_user_mode+0x2c/0x50
[ 1209.382718]  ? x64_sys_call+0x41b/0x1fa0
[ 1209.382722]  ? do_syscall_64+0x61/0xb0
[ 1209.382725]  ? irqentry_exit_to_user_mode+0xe/0x20
[ 1209.382727]  ? irqentry_exit+0x1d/0x30
[ 1209.382729]  ? exc_page_fault+0x89/0x170
[ 1209.382731]  entry_SYSCALL_64_after_hwframe+0x6c/0xd6
[ 1209.382735] RIP: 0033:0x7f25b21c973d
[ 1209.382737] RSP: 002b:00007ffd3f3204e8 EFLAGS: 00000246 ORIG_RAX: 0000000000000139
[ 1209.382740] RAX: ffffffffffffffda RBX: 000055bacfdb7420 RCX: 00007f25b21c973d
[ 1209.382741] RDX: 0000000000000000 RSI: 00007f25b20a9ded RDI: 0000000000000005
[ 1209.382743] RBP: 0000000000020000 R08: 0000000000000000 R09: 0000000000000000
[ 1209.382744] R10: 0000000000000005 R11: 0000000000000246 R12: 00007f25b20a9ded
[ 1209.382745] R13: 0000000000000000 R14: 000055bacfdb74e0 R15: 000055bacfdb7420
[ 1209.382748]  </TASK>



VHOST_USER_SLAVE_CONFIG_CHANGE_MSG
    config_changed_irq
    virtio_config_changed
VHOST_USER_SLAVE_VRING_HOST_NOTIFIER_MSG




[ 1245.197074] ---- virtqueue_kick_prepare virtio_queue_rq() /root/xb/linux-5.15.113/drivers/block/virtio_blk.c:360
[ 1245.197204] ---- virtblk_done() /root/xb/linux-5.15.113/drivers/block/virtio_blk.c:271
[ 1245.197206] CPU: 0 PID: 517 Comm: in:imklog Tainted: G           OE     5.15.0-117-generic #127~20.04.1-Ubuntu
[ 1245.197208] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.16.3-0-ga6ed6b701f0a-prebuilt.qemu.org 04/01/2014
[ 1245.197210] Call Trace:
[ 1245.197211]  <TASK>
[ 1245.197212]  dump_stack_lvl+0x4a/0x63
[ 1245.197214]  dump_stack+0x10/0x16
[ 1245.197216]  virtblk_done+0x62/0x114 [virtio_blk]
[ 1245.197220]  vring_interrupt+0xa7/0x120
[ 1245.197222]  __handle_irq_event_percpu+0x42/0x170 -> action->handler(irq, action->dev_id)
[ 1245.197226]  handle_irq_event_percpu+0x33/0x80
[ 1245.197229]  handle_irq_event+0x3b/0x60
[ 1245.197232]  handle_edge_irq+0x93/0x1c0
[ 1245.197234]  __common_interrupt+0x52/0xe0
[ 1245.197237]  common_interrupt+0x45/0xa0
[ 1245.197239]  asm_common_interrupt+0x27/0x40 -> DECLARE_IDTENTRY_IRQ(X86_TRAP_OTHER,	common_interrupt);
[ 1245.197242] RIP: 0033:0x7f5391915377
[ 1245.197243] Code: 00 4c 01 c3 48 89 1d 08 ce 11 00 83 f8 ff 0f 85 07 fe ff ff 45 31 c0 e9 58 ff ff ff 0f 1f 80 00 00 00 00 0f b7 0d 09 ce 11 00 <48> 69 c9 80 51 01 00 49 01 c8 e9 62 fe ff ff 66 2e 0f 1f 84 00 00
[ 1245.197245] RSP: 002b:00007f5391153258 EFLAGS: 00000246
[ 1245.197247] RAX: 00000000000007e8 RBX: 0000000000000014 RCX: 0000000000000000
[ 1245.197248] RDX: 0000000000000001 RSI: 00007f5391153340 RDI: 0000000066a7aeff
[ 1245.197250] RBP: 0000000000007080 R08: 0000000065920080 R09: 0000000000000000
[ 1245.197251] R10: 00000000000001f9 R11: 2ce33e6c02ce33e7 R12: 00007f5391a30430
[ 1245.197252] R13: 00007f5391153340 R14: 000000000000001d R15: 00007f53911556e4
[ 1245.197254]  </TASK>




irq:
u32	ndst: Notification Destination -> VM：VMX：将已发布的中断支持提取到单独的文件中，提取已发布的中断代码，以便可以将其重新用于信任域扩展 (TDX)，这需要已发布的中断并且可以几乎逐字逐句地使用 KVM VMX 的实现。TDX 与原始 VMX 有很大不同，因此非常希望在单独的文件中实现 TDX 的核心，即通过将 TDX 支持推入 vmx.c 来重用已发布的中断代码会很麻烦
struct vcpu_vmx {
    /* Posted interrupt descriptor */
    struct pi_desc pi_desc;
}


remove/unregister blk dev:
unregister_virtio_device
    device_unregister(&dev->dev)
        device_del(dev) -> device_del - 从系统中删除设备。，@dev：设备。这是设备注销序列的第一部分。这会从我们在此处控制的列表中删除设备，将其从在 device_add() 中添加的其他驱动程序模型子系统中移除，并将其从 kobject 层次结构中移除。注意：这应该手动调用 _iff_ device_add() 也是手动调用的
            kill_device(dev)
            bus_notify(dev, BUS_NOTIFY_DEL_DEVICE)
            device_remove_file(dev, &dev_attr_uevent)
        put_device(dev)

[  726.449271]  schedule+0x42/0xb0
[  726.449274]  blk_mq_freeze_queue_wait+0x4b/0xb0
[  726.449279]  ? __wake_up_pollfree+0x40/0x40
[  726.449282]  blk_freeze_queue+0x1b/0x20
[  726.449286]  blk_cleanup_queue+0x45/0xd0
[  726.449290]  virtblk_remove+0x4a/0xd0 [virtio_blk]
[  726.449295]  virtio_dev_remove+0x4b/0x80
[  726.449299]  device_release_driver_internal+0xf4/0x1d0
[  726.449302]  device_release_driver+0x12/0x20
[  726.449304]  bus_remove_device+0xe1/0x150
[  726.449308]  device_del+0x165/0x370
[  726.449312]  device_unregister+0x1b/0x60
[  726.449316]  unregister_virtio_device+0x18/0x30



[  726.449414] Call Trace:
[  726.449418]  __schedule+0x2e3/0x740
[  726.449420]  schedule+0x42/0xb0
[  726.449422]  io_schedule+0x16/0x40
[  726.449427]  __lock_page+0x16a/0x2a0
[  726.449432]  ? file_fdatawait_range+0x30/0x30
[  726.449437]  truncate_inode_pages_range+0x4fa/0x890
[  726.449442]  ? find_get_pages_range_tag+0x7e/0x300
[  726.449447]  ? cpumask_next+0x1b/0x20
[  726.449452]  ? native_send_call_func_ipi+0x69/0xc0
[  726.449455]  ? cpumask_next+0x1b/0x20
[  726.449459]  ? smp_call_function_many+0x1e0/0x270
[  726.449462]  ? iter_to_pipe+0x40/0x240
[  726.449466]  ? __brelse+0x30/0x30
[  726.449469]  ? __x64_sys_fsconfig+0x5c0/0x5c0
[  726.449473]  ? free_cpumask_var+0x9/0x10
[  726.449476]  ? on_each_cpu_cond_mask+0xb1/0x130
[  726.449481]  truncate_inode_pages+0x15/0x20
[  726.449484]  kill_bdev+0x32/0x40
[  726.449487]  __blkdev_put+0x89/0x200
[  726.449490]  blkdev_put+0x4e/0xe0
[  726.449494]  blkdev_close+0x26/0x30
[  726.449497]  __fput+0xcc/0x260
[  726.449501]  ____fput+0xe/0x10
[  726.449504]  task_work_run+0x8f/0xb0
[  726.449508]  do_exit+0x36e/0xaf0
[  726.449512]  ? file_fdatawait_range+0x30/0x30
[  726.449515]  do_group_exit+0x47/0xb0
[  726.449518]  get_signal+0x169/0x890
[  726.449520]  ? blkdev_read_iter+0x4a/0x60
[  726.449524]  do_signal+0x37/0x6d0
[  726.449528]  ? __vfs_read+0x29/0x40
[  726.449530]  ? vfs_read+0xab/0x160
[  726.449535]  exit_to_usermode_loop+0xbf/0x160
[  726.449539]  do_syscall_64+0x168/0x190
[  726.449543]  entry_SYSCALL_64_after_hwframe+0x61/0xc6



英特尔 MIC 用户空间守护进程的示例实现。此补丁引入了一个示例用户空间守护进程，该守护进程在主机上实现 virtio 设备后端。守护进程通过与英特尔 MIC 主机驱动程序通信来创建/删除/配置 virtio 设备后端。当前支持的 virtio 设备包括 virtio net、virtio console 和 virtio block。Virtio net 支持 TSO/GSO。守护进程还监控卡关闭状态并采取适当的措施，例如在卡关闭和崩溃时终止 virtio 后端并重置卡
https://github.com/torvalds/linux/commit/8d49751580db804a02caf6a5b7cebe2ff26c0d7e
Intel MIC X100 设备是一种 PCIe 规格的附加协处理器卡，基于运行 Linux 操作系统的 Intel 众核 (MIC) 架构。它是平台中的 PCIe 端点，因此实现了三个必需的标准地址空间，即配置、内存和 I/O。主机操作系统加载设备驱动程序，这通常是 PCIe 设备的典型做法。卡本身在重置后运行引导程序，将控制权转移到从主机驱动程序下载的卡操作系统。英特尔提供的卡操作系统是一个 Linux 内核，针对 X100 设备进行了修改。由于它是 PCIe 卡，因此它无法托管用于网络、存储和控制台的硬件设备。我们在 X100 协处理器上提供这些设备，从而为应用程序提供可自启动的等效环境。我们的解决方案的一个主要优点是它利用了标准 virtio 框架来处理网络、磁盘和控制台设备，尽管在我们的案例中，virtio 框架是在 PCIe 总线上使用的。以下是上述各种组件的框图。 virtio 后端位于主机上，而不是卡上，因为与 MIC 相比，主机的单线程性能更好，主机能够使用 MIC DMA 引擎启动卡上的 DMA，而且 virtio 块存储后端只能位于主机上




virtio_create_blk failed:
[17038.583717] Call Trace:
[17038.584581]  __schedule+0x2e3/0x740
[17038.585243]  schedule+0x42/0xb0
[17038.585864]  io_schedule+0x16/0x40
[17038.586482]  do_read_cache_page+0x407/0x860
[17038.587099]  ? file_fdatawait_range+0x30/0x30
[17038.587720]  read_cache_page+0x12/0x20
[17038.588512]  read_dev_sector+0x27/0xd0
[17038.589139]  read_lba+0xbd/0x220
[17038.589727]  ? kmem_cache_alloc_trace+0x177/0x240
[17038.590314]  efi_partition+0x1e0/0x700
[17038.590887]  ? vsnprintf+0x39e/0x4e0
[17038.591454]  ? snprintf+0x49/0x60
[17038.592014]  check_partition+0x154/0x250
[17038.592983]  rescan_partitions+0xae/0x280


vfio_pci_core_ioctl
    case VFIO_DEVICE_GET_REGION_INFO
        vfio_pci_ioctl_get_region_info



iommu:
static LIST_HEAD(iommu_device_list);

arch/arm64/boot/dts/arm/fvp-base-revc.dts
smmu: iommu@2b400000 {
	compatible = "arm,smmu-v3";
	reg = <0x0 0x2b400000 0x0 0x100000>;
	interrupts = <GIC_SPI 74 IRQ_TYPE_EDGE_RISING>,
		     <GIC_SPI 79 IRQ_TYPE_EDGE_RISING>,
		     <GIC_SPI 75 IRQ_TYPE_EDGE_RISING>,
		     <GIC_SPI 77 IRQ_TYPE_EDGE_RISING>;
	interrupt-names = "eventq", "gerror", "priq", "cmdq-sync";
	dma-coherent;
	#iommu-cells = <1>;
	msi-parent = <&its 0x10000>;
};

drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c
module_driver(arm_smmu_driver, platform_driver_register, arm_smmu_driver_unregister);
arm_smmu_device_probe
    ret = arm_smmu_device_dt_probe(pdev, smmu)
        parse_driver_options(smmu)
        if (of_dma_is_coherent(dev->of_node))
            smmu->features |= ARM_SMMU_FEAT_COHERENCY
    or ret = arm_smmu_device_acpi_probe(pdev, smmu)
    smmu->base = arm_smmu_ioremap(dev, ioaddr, ARM_SMMU_REG_SZ)
        devm_ioremap_resource(dev, &res)
    irq = platform_get_irq_byname_optional(pdev, "combined")
    ret = arm_smmu_device_hw_probe(smmu)
    ret = arm_smmu_init_structures(smmu)
        ret = arm_smmu_init_queues(smmu)
            ret = arm_smmu_init_one_queue(smmu, &smmu->cmdq.q, smmu->base, ARM_SMMU_CMDQ_PROD, ARM_SMMU_CMDQ_CONS, CMDQ_ENT_DWORDS, "cmdq")
                q->base = dmam_alloc_coherent(smmu->dev, qsz, &q->base_dma, GFP_KERNEL)
                q->prod_reg	= page + prod_off
                q->q_base  = Q_BASE_RWA
                q->q_base |= q->base_dma & Q_BASE_ADDR_MASK
                q->q_base |= FIELD_PREP(Q_BASE_LOG2SIZE, q->llq.max_n_shift)
                q->llq.prod = q->llq.cons = 0
            ... evtq, priq
        ret = arm_smmu_init_strtab(smmu)
            ret = arm_smmu_init_strtab_2lvl(smmu)
                cfg->l2.l1tab = dmam_alloc_coherent(smmu->dev, l1size, &cfg->l2.l1_dma, GFP_KERNEL)
            or ret = arm_smmu_init_strtab_linear(smmu)
                cfg->linear.table = dmam_alloc_coherent(smmu->dev, size, &cfg->linear.ste_dma, GFP_KERNEL)
                arm_smmu_init_initial_stes(cfg->linear.table, cfg->linear.num_ents)
                    arm_smmu_make_abort_ste(strtab)
                        target->data[0] = cpu_to_le64(STRTAB_STE_0_V | FIELD_PREP(STRTAB_STE_0_CFG, STRTAB_STE_0_CFG_ABORT))
    ret = iommu_device_sysfs_add(&smmu->iommu, dev, NULL, "smmu3.%pa", &ioaddr)
    iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev)
        iommu->ops = ops
        list_add_tail(&iommu->list, &iommu_device_list)
		err = bus_iommu_probe(iommu_buses[i])
			for_each_group_device(group, gdev)
				iommu_setup_dma_ops(gdev->dev)


device tree, dts file:
fvp-base-revc.dts
interrupt-names = "eventq", "gerror", "priq", "cmdq-sync"

static struct iommu_ops arm_smmu_ops = {
	.identity_domain	= &arm_smmu_identity_domain,
	.blocked_domain		= &arm_smmu_blocked_domain,
	.capable		= arm_smmu_capable,
	.hw_info		= arm_smmu_hw_info,
	.domain_alloc_paging    = arm_smmu_domain_alloc_paging,
	.domain_alloc_sva       = arm_smmu_sva_domain_alloc,
	.domain_alloc_paging_flags = arm_smmu_domain_alloc_paging_flags,
	.probe_device		= arm_smmu_probe_device,
	.release_device		= arm_smmu_release_device,
	.device_group		= arm_smmu_device_group,
	.of_xlate		= arm_smmu_of_xlate,
	.get_resv_regions	= arm_smmu_get_resv_regions,
	.remove_dev_pasid	= arm_smmu_remove_dev_pasid,
	.dev_enable_feat	= arm_smmu_dev_enable_feature,
	.dev_disable_feat	= arm_smmu_dev_disable_feature,
	.page_response		= arm_smmu_page_response,
	.def_domain_type	= arm_smmu_def_domain_type,
	.viommu_alloc		= arm_vsmmu_alloc,
	.user_pasid_table	= 1,
	.pgsize_bitmap		= -1UL, /* Restricted during device attach */
	.owner			= THIS_MODULE,
	.default_domain_ops = &(const struct iommu_domain_ops) {
		.attach_dev		= arm_smmu_attach_dev,
		.enforce_cache_coherency = arm_smmu_enforce_cache_coherency,
		.set_dev_pasid		= arm_smmu_s1_set_dev_pasid,
		.map_pages		= arm_smmu_map_pages,
		.unmap_pages		= arm_smmu_unmap_pages,
		.flush_iotlb_all	= arm_smmu_flush_iotlb_all,
		.iotlb_sync		= arm_smmu_iotlb_sync,
		.iova_to_phys		= arm_smmu_iova_to_phys,
		.free			= arm_smmu_domain_free_paging,
	}
};

static struct iommu_device *arm_smmu_probe_device(struct device *dev)



iommu_setup_dma_ops
    static int iommu_dma_init_domain(struct iommu_domain *domain, struct device *dev)
        init_iova_domain(iovad, 1UL << order, base_pfn)
            iovad->rbroot = RB_ROOT; -> red black tree
            iovad->cached_node = &iovad->anchor.node;
            iovad->cached32_node = &iovad->anchor.node;
            iovad->granule = granule;
            iovad->start_pfn = start_pfn;
            iovad->dma_32bit_pfn = 1UL << (32 - iova_shift(iovad));
            iovad->max32_alloc_size = iovad->dma_32bit_pfn;
            iovad->anchor.pfn_lo = iovad->anchor.pfn_hi = IOVA_ANCHOR;
            rb_link_node(&iovad->anchor.node, NULL, &iovad->rbroot.rb_node);
            rb_insert_color(&iovad->anchor.node, &iovad->rbroot);
        iommu_dma_init_options(&cookie->options, dev)
        ret = iova_reserve_iommu_regions(dev, domain)
            reserve_iova(iovad, lo, hi)
                for (node = rb_first(&iovad->rbroot); node; node = rb_next(node))
                iova = __insert_new_range(iovad, pfn_lo, pfn_hi)

echo 2 > /sys/bus/pci/devices/0000:5e:00.0/sriov_numvfs
sriov_numvfs_store
    kstrtou16(buf, 0, &num_vfs) -> to num
    num_vfs > pci_sriov_get_totalvfs(pdev)
    ret = pdev->driver->sriov_configure(pdev, num_vfs)


igb：PCI-Express 82575 千兆以太网驱动程序，我们很高兴向 Linux 社区宣布推出一款新的千兆以太网产品及其驱动程序。该产品是 Intel(R) 82575 千兆以太网适配器系列。物理适配器将很快向公众推出。这些适配器目前有 2 端口和 4 端口版本（铜 PHY）。其他变体将在稍后推出。82575 芯片组支持显著不同的功能，需要新的驱动程序。描述符格式（就像 ixgbe 驱动程序一样）不同。该设备可以使用多个 MSI-X 向量和多个队列进行发送和接收。与 e1000 支持的设备相比，这使我们能够专门优化一些驱动程序代码。此版本的 igb 驱动程序 no lnger 使用假网络设备并合并每个环的 napi_struct 成员来执行多队列轮询。默认情况下启用多队列，驱动程序仅支持 NAPI 模式。此版本中所有命名空间冲突也应该消失。寄存器宏已被压缩以提高可读性
https://github.com/torvalds/linux/commit/9d5c824399dea881779d78a6c147288bf2dccb6b
module_init(igb_init_module);
    pci_register_driver(&igb_driver)
static struct pci_driver igb_driver = {
	.name     = igb_driver_name,
	.id_table = igb_pci_tbl,
	.probe    = igb_probe,
        pci_enable_device_mem
        dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64))
        pci_request_mem_regions(pdev, igb_driver_name)
        netdev = alloc_etherdev_mq(sizeof(struct igb_adapter), IGB_MAX_TX_QUEUES)
        adapter->io_addr = pci_iomap(pdev, 0, 0)
        netdev->netdev_ops = &igb_netdev_ops
        igb_set_ethtool_ops(netdev)
        ei->get_invariants(hw) -> 初始化特定倾斜常数
        igb_sw_init
        igb_get_bus_info_pcie
        igb_check_reset_block
        adapter->en_mng_pt = igb_enable_mng_pass_thru(hw)
        hw->mac.ops.reset_hw(hw)
        switch (hw->mac.type)
        hw->nvm.ops.validate(hw)
        eth_platform_get_mac_address
        eth_hw_addr_set
        igb_set_default_mac_filter
        igb_set_fw_version
        timer_setup(&adapter->watchdog_timer, igb_watchdog, 0);
        timer_setup(&adapter->phy_info_timer, igb_update_phy_info, 0);
        INIT_WORK(&adapter->reset_task, igb_reset_task);
        INIT_WORK(&adapter->watchdog_task, igb_watchdog_task);
        igb_validate_mdi_setting
        hw->nvm.ops.read
        device_set_wakeup_enable
        igb_reset(adapter)
        igb_init_i2c
        igb_get_hw_control
        register_netdev(netdev)
        netif_carrier_off
        igb_setup_dca
        igb_init_mas
        igb_ptp_init
        dev_pm_set_driver_flags(&pdev->dev, DPM_FLAG_NO_DIRECT_COMPLETE)
        pm_runtime_put_noidle
	.remove   = igb_remove,
#ifdef CONFIG_PM
	.driver.pm = &igb_pm_ops,
#endif
	.shutdown = igb_shutdown,
	.sriov_configure = igb_pci_sriov_configure,
        if (num_vfs == 0)
            igb_disable_sriov(dev, true)
        else
            igb_enable_sriov(dev, num_vfs, true)
                for (i = 0; i < adapter->vfs_allocated_count; i++)
                    igb_vf_configure(adapter, i)
                        igb_set_vf_mac(adapter, vf, mac_addr)
                igb_sriov_reinit
                    igb_clear_interrupt_scheme
                    igb_init_queue_configuration
                        igb_set_flag_queue_pairs
                    igb_init_interrupt_scheme
                        igb_set_interrupt_capability
                            pci_enable_msix_range(adapter->pdev, adapter->msix_entries, numvecs, numvecs)
                        igb_alloc_q_vectors
                            igb_alloc_q_vector
                        igb_cache_ring_register
                    netif_running
                        igb_open(netdev) -> init irq
                pci_enable_sriov(pdev, adapter->vfs_allocated_count)
	.err_handler = &igb_err_handler
};


pci_device_add
    pci_configure_device(dev)
    device_initialize(&dev->dev)
    dma_set_max_seg_size(&dev->dev, 65536);
    dma_set_seg_boundary(&dev->dev, 0xffffffff)
    pcie_failed_link_retrain
    pci_fixup_device
    pci_reassigndev_resource_alignment
    pci_init_capabilities
        pci_ea_init
        pci_msi_init
        pci_msix_init
        pci_allocate_cap_save_buffers
        pci_pm_init(dev);		/* Power Management */
        pci_vpd_init(dev);		/* Vital Product Data */
        pci_configure_ari(dev);		/* Alternative Routing-ID Forwarding */
        pci_iov_init(dev);		/* Single Root I/O Virtualization */
            sriov_init
                iov->driver_max_VFs = total
                i = PAGE_SHIFT > 12 ? PAGE_SHIFT - 12 : 0
                pgsz &= ~((1 << i) - 1)
                pgsz &= ~(pgsz - 1)
                pci_write_config_dword(dev, pos + PCI_SRIOV_SYS_PGSIZE, pgsz)
        pci_ats_init(dev);		/* Address Translation Services */
        pci_pri_init(dev);		/* Page Request Interface */
        pci_pasid_init(dev);		/* Process Address Space ID */
        pci_acs_init(dev);		/* Access Control Services */
        pci_ptm_init(dev);		/* Precision Time Measurement */
        pci_aer_init(dev);		/* Advanced Error Reporting */
        pci_dpc_init(dev);		/* Downstream Port Containment */
        pci_rcec_init(dev);		/* Root Complex Event Collector */
        pci_doe_init(dev);		/* Data Object Exchange */
        pcie_report_downtraining(dev);
        pci_init_reset_methods(dev);
    list_add_tail(&dev->bus_list, &bus->devices)
    pci_set_msi_domain(dev)
    ret = device_add(&dev->dev)




igb_open -> __igb_open
    igb_request_irq
        igb_request_msix
            request_irq(adapter->msix_entries[vector].vector, igb_msix_other, 0, netdev->name, adapter)
            request_irq(adapter->msix_entries[vector].vector, igb_msix_ring, 0, q_vector->name, q_vector)
                napi_schedule(&q_vector->napi)





static const struct net_device_ops igb_netdev_ops = {
	.ndo_open		= igb_open,
	.ndo_stop		= igb_close,
	.ndo_start_xmit		= igb_xmit_frame,
        igb_xmit_frame_ring(skb, igb_tx_queue_mapping(adapter, skb))
            first = &tx_ring->tx_buffer_info[tx_ring->next_to_use]
            igb_tx_map(tx_ring, first, hdr_len)
                dma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE)
                tx_desc->read.buffer_addr = cpu_to_le64(dma)
                dma = skb_frag_dma_map(tx_ring->dev, frag, 0, size, DMA_TO_DEVICE)
                writel(i, tx_ring->tail)
                tx_ring->next_to_use = i
	.ndo_get_stats64	= igb_get_stats64,
	.ndo_set_rx_mode	= igb_set_rx_mode,
	.ndo_set_mac_address	= igb_set_mac,
	.ndo_change_mtu		= igb_change_mtu,
	.ndo_eth_ioctl		= igb_ioctl,
	.ndo_tx_timeout		= igb_tx_timeout,
	.ndo_validate_addr	= eth_validate_addr,
	.ndo_vlan_rx_add_vid	= igb_vlan_rx_add_vid,
	.ndo_vlan_rx_kill_vid	= igb_vlan_rx_kill_vid,
	.ndo_set_vf_mac		= igb_ndo_set_vf_mac,
	.ndo_set_vf_vlan	= igb_ndo_set_vf_vlan,
	.ndo_set_vf_rate	= igb_ndo_set_vf_bw,
	.ndo_set_vf_spoofchk	= igb_ndo_set_vf_spoofchk,
	.ndo_set_vf_trust	= igb_ndo_set_vf_trust,
	.ndo_get_vf_config	= igb_ndo_get_vf_config,
	.ndo_fix_features	= igb_fix_features,
	.ndo_set_features	= igb_set_features,
	.ndo_fdb_add		= igb_ndo_fdb_add,
	.ndo_features_check	= igb_features_check,
	.ndo_setup_tc		= igb_setup_tc,
	.ndo_bpf		= igb_xdp,
	.ndo_xdp_xmit		= igb_xdp_xmit,
};

linux mce的一些相关内容和用户态监控的设计方法
https://www.cnblogs.com/10087622blog/p/9790128.html
mce_notify_irq
    pr_info(HW_ERR "Machine check events logged\n")

/var/log/mcelog


PCI 设备的前64字节寄存器布局
/**
 *
 *   0x0  0x1  0x2  0x3   0x4  0x5  0x6  0x7  0x8  0x9 0xa 0xb   0xc  0xd  0xe  0xf
 *  +---------+---------+---------+---------+----+-------------+----+----+----+----+
 *  |  vender |  device |   cmd   |  state  |ver |    class    |    |    |    |BIST|
 *  |    ID   |    ID   |register | register| ID |    NO       |    |    |    |    |  0x00
 *  +---------+---------+---------+---------+----+-------------+----+----+----+----+
 *  |     base Addr0    |     base Addr1    |     base Addr2   |     base Addr3    |
 *  |                   |                   |                  |                   |  0x10
 *  +-------------------+-------------------+------------------+---------+---------+
 *  |     base Addr4    |     base Addr5    |     Cardbus      | subsys  |  subsys |
 *  |                   |                   |     CIS pointer  |venderID |venderID |  0x20
 *  +-------------------+-------------------+------------------+----+----+----+----+
 *  |     extend ROM    |               reserved               |irq |irq | Min| Max|
 *  |     base Addr     |                                      |Line|pin | Gnt| Lat|  0x30
 *  +-------------------+-------------------+------------------+----+----+----+----+
 *
 *  荣涛 2021年8月31日15:14:58
 */



uio:
uio_cif.c
ref: https://github.com/Johannes4Linux/pcittl32io_uio.git

forlinx@ok3588:~$ lspci -s 01:00.3 -vvv
01:00.3 Ethernet controller: Xilinx Corporation Device 9638
	Subsystem: Xilinx Corporation Device 0007
	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Region 0: Memory at 906000000 (64-bit, prefetchable) [size=32M]
	Region 2: Memory at 908060000 (64-bit, prefetchable) [size=64K]
	Region 4: Memory at 908070000 (64-bit, prefetchable) [size=64K]
	Capabilities: <access denied>
	Kernel driver in use: igb_uio

root@ok3588:/sys/class/uio/uio0/maps# ls|while read line;do echo -e "\n$line";cat $line/*;done
dir addr  name  offset  size
map0
0x0000000906000000
BAR0
0x0
0x0000000002000000

map1
0x0000000908060000
BAR2
0x0
0x0000000000010000

map2
0x0000000908070000
BAR4
0x0
0x0000000000010000

map3
0x00000000107c0000
TX
0x0
0x0000000000040000

map4
0x0000000010a00000
RX
0x0
0x000000000082d000

root@ok3588:/sys/class/uio/uio0# tree -L 10
.
├── dev
├── device -> ../../../0000:01:00.3
├── event
├── maps
│   ├── map0
│   │   ├── addr
│   │   ├── name
│   │   ├── offset
│   │   └── size
│   ├── map1
│   │   ├── addr
│   │   ├── name
│   │   ├── offset
│   │   └── size
│   ├── map2
│   │   ├── addr
│   │   ├── name
│   │   ├── offset
│   │   └── size
│   ├── map3
│   │   ├── addr
│   │   ├── name
│   │   ├── offset
│   │   └── size
│   └── map4
│       ├── addr
│       ├── name
│       ├── offset
│       └── size
├── name
├── power
│   ├── async
│   ├── autosuspend_delay_ms
│   ├── control
│   ├── runtime_active_kids
│   ├── runtime_active_time
│   ├── runtime_enabled
│   ├── runtime_status
│   ├── runtime_suspended_time
│   └── runtime_usage
├── subsystem -> ../../../../../../../../class/uio
├── uevent
└── version

9 directories, 34 files



modprobe uio_pci_generic
echo "8086 10f5" > /sys/bus/pci/drivers/uio_pci_generic/new_id

drivers/uio/uio_pci_generic.c
module_pci_driver(uio_pci_driver) -> probe
    pcim_enable_device(pdev)
    gdev = devm_kzalloc
    gdev->info.handler = irqhandler
    uiomem = &gdev->info.mem[0]
    for (i = 0; i < MAX_UIO_MAPS; ++i) -> 5
        struct resource *r = &pdev->resource[i] -> for each device bar
        uiomem->memtype = UIO_MEM_PHYS
        uiomem->addr = r->start & PAGE_MASK
        uiomem->offs = r->start & ~PAGE_MASK
        uiomem->size = (uiomem->offs + resource_size(r) + PAGE_SIZE - 1) & PAGE_MASK
        uiomem->name = r->name
        ++uiomem
    devm_uio_register_device(&pdev->dev, &gdev->info) -> __devm_uio_register_device -> __uio_register_device
        dev_set_name(&idev->dev, "uio%d", idev->minor)
        device_add(&idev->dev)
        uio_dev_add_attributes(idev)
            for (mi = 0; mi < MAX_UIO_MAPS; mi++)
                mem = &idev->info->mem[mi]
                idev->map_dir = kobject_create_and_add("maps", &idev->dev.kobj)
            for (pi = 0; pi < MAX_UIO_PORT_REGIONS; pi++)
                port = &idev->info->port[pi]
                idev->portio_dir = kobject_create_and_add("portio",
        request_irq(info->irq, uio_interrupt, info->irq_flags, info->name, idev)
            idev->info->handler(irq, idev->info) -> irqhandler




drivers/net/virtio_net.c -> module_init(virtio_net_driver_init) -> net/virtio-net：转换为热插拔状态机，通过状态机安装回调。驱动程序支持多个实例，因此使用新的 cpuhp_state_add_instance_nocalls() 基础结构。驱动程序当前使用 get_online_cpus() 来避免在调用 virtnet_set_affinity() 时错过 CPU 热插拔事件。这可以通过使用 cpuhp_state_add_instance() 变体来避免，该变体在注册期间持有热插拔锁并调用回调。这或多或少是当前代码的 1:1 转换
    cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN, "virtio/net:online",
    cpuhp_setup_state_multi(CPUHP_VIRT_NET_DEAD, "virtio/net:dead",
    register_virtio_driver(&virtio_net_driver)

static struct virtio_driver virtio_net_driver = {
	.validate =	virtnet_validate,
	.probe =	virtnet_probe,
	.remove =	virtnet_remove,
	.config_changed = virtnet_config_changed,
#ifdef CONFIG_PM_SLEEP
	.freeze =	virtnet_freeze,
	.restore =	virtnet_restore,
#endif
};


static const struct ethtool_ops virtnet_ethtool_ops = {
	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES |
		ETHTOOL_COALESCE_USECS | ETHTOOL_COALESCE_USE_ADAPTIVE_RX,
	.get_drvinfo = virtnet_get_drvinfo,
	.get_link = ethtool_op_get_link,
	.get_ringparam = virtnet_get_ringparam,
	.set_ringparam = virtnet_set_ringparam,
	.get_strings = virtnet_get_strings,
	.get_sset_count = virtnet_get_sset_count,
	.get_ethtool_stats = virtnet_get_ethtool_stats,
	.set_channels = virtnet_set_channels,
	.get_channels = virtnet_get_channels,
	.get_ts_info = ethtool_op_get_ts_info,
	.get_link_ksettings = virtnet_get_link_ksettings,
	.set_link_ksettings = virtnet_set_link_ksettings,
	.set_coalesce = virtnet_set_coalesce,
	.get_coalesce = virtnet_get_coalesce,
	.set_per_queue_coalesce = virtnet_set_per_queue_coalesce,
	.get_per_queue_coalesce = virtnet_get_per_queue_coalesce,
	.get_rxfh_key_size = virtnet_get_rxfh_key_size,
	.get_rxfh_indir_size = virtnet_get_rxfh_indir_size,
	.get_rxfh = virtnet_get_rxfh,
	.set_rxfh = virtnet_set_rxfh,
	.get_rxnfc = virtnet_get_rxnfc,
	.set_rxnfc = virtnet_set_rxnfc,
};

static const struct net_device_ops virtnet_netdev = {
	.ndo_open            = virtnet_open,
	.ndo_stop   	     = virtnet_close,
	.ndo_start_xmit      = start_xmit,
	.ndo_validate_addr   = eth_validate_addr,
	.ndo_set_mac_address = virtnet_set_mac_address,
	.ndo_set_rx_mode     = virtnet_set_rx_mode,
	.ndo_get_stats64     = virtnet_stats,
	.ndo_vlan_rx_add_vid = virtnet_vlan_rx_add_vid,
	.ndo_vlan_rx_kill_vid = virtnet_vlan_rx_kill_vid,
	.ndo_bpf		= virtnet_xdp,
	.ndo_xdp_xmit		= virtnet_xdp_xmit,
	.ndo_features_check	= passthru_features_check,
	.ndo_get_phys_port_name	= virtnet_get_phys_port_name,
	.ndo_set_features	= virtnet_set_features,
	.ndo_tx_timeout		= virtnet_tx_timeout,
};

udp-fragmentation-offload

ufo -> udp4_ufo_fragment
static const struct net_offload udpv4_offload = {
	.callbacks = {
		.gso_segment = udp4_ufo_fragment,
		.gro_receive  =	udp4_gro_receive,
		.gro_complete =	udp4_gro_complete,
	},
};




vfio
module_pci_driver(mlx5vf_pci_driver) -> mlx5vf_pci_probe
    vfio_alloc_device(mlx5vf_pci_core_device, core_device.vdev, &pdev->dev, &mlx5vf_pci_ops)
    vfio_pci_core_register_device(&mvdev->core_device)


static const struct vfio_device_ops mlx5vf_pci_ops = {
	.name = "mlx5-vfio-pci",
	.init = mlx5vf_pci_init_dev,
        vfio_pci_core_init_dev(core_vdev)
        mlx5vf_cmd_set_migratable(mvdev, &mlx5vf_pci_mig_ops, &mlx5vf_pci_log_ops)
            mlx5_sriov_blocking_notifier_register
	.release = mlx5vf_pci_release_dev,
	.open_device = mlx5vf_pci_open_device,
	.close_device = mlx5vf_pci_close_device,
	.ioctl = vfio_pci_core_ioctl,
	.device_feature = vfio_pci_core_ioctl_feature,
	.read = vfio_pci_core_read,
	.write = vfio_pci_core_write,
	.mmap = vfio_pci_core_mmap,
	.request = vfio_pci_core_request,
	.match = vfio_pci_core_match,
	.bind_iommufd = vfio_iommufd_physical_bind,
	.unbind_iommufd = vfio_iommufd_physical_unbind,
	.attach_ioas = vfio_iommufd_physical_attach_ioas,
	.detach_ioas = vfio_iommufd_physical_detach_ioas,
};




static const struct vfio_migration_ops mlx5vf_pci_mig_ops = {
	.migration_set_state = mlx5vf_pci_set_device_state,
	.migration_get_state = mlx5vf_pci_get_device_state,
	.migration_get_data_size = mlx5vf_pci_get_data_size,
};

static const struct vfio_log_ops mlx5vf_pci_log_ops = {
	.log_start = mlx5vf_start_page_tracker,
	.log_stop = mlx5vf_stop_page_tracker,
	.log_read_and_clear = mlx5vf_tracker_read_and_clear,
};




resource:
drivers/net/ethernet/intel/igb/igb_main.c
#define IORESOURCE_IO		0x00000100	/* PCI/ISA I/O ports */
#define IORESOURCE_MEM		0x00000200
#define IORESOURCE_REG		0x00000300	/* Register offsets */





module_init(cnic_init) -> [SCSI] cnic：添加新的 Broadcom CNIC 驱动程序。CNIC 驱动程序控制 iSCSI 使用的 BNX2 硬件环和资源。iSCSI 的大多数硬件资源与用于以太网网络的资源是分开的。iSCSI 使用单独的 MAC 地址和 IP 地址。CNIC 驱动程序创建 UIO 接口来处理用户空间中的非卸载数据包（如 ARP 等）
    ...
    uio_register_device



/* For PCI devices, the region numbers are assigned this way: */
enum {
	/* #0-5: standard PCI resources */
	PCI_STD_RESOURCES,
	PCI_STD_RESOURCE_END = PCI_STD_RESOURCES + PCI_STD_NUM_BARS - 1,

	/* #6: expansion ROM resource */
	PCI_ROM_RESOURCE,


write flag:
VIRTQ_DESC_F_WRITE




struct virtio_blk_req {
    le32 type;
    le32 reserved;
    le64 sector;
    u8 data[];
    u8 status;
};




struct fuse_req
    unsigned long flags



module_init(fuse_init);

const struct file_operations fuse_dev_operations = {
	.owner		= THIS_MODULE,
	.open		= fuse_dev_open,
	.llseek		= no_llseek,
	.read_iter	= fuse_dev_read,
	.splice_read	= fuse_dev_splice_read,
	.write_iter	= fuse_dev_write,
	.splice_write	= fuse_dev_splice_write,
	.poll		= fuse_dev_poll,
	.release	= fuse_dev_release,
	.fasync		= fuse_dev_fasync,
	.unlocked_ioctl = fuse_dev_ioctl,
	.compat_ioctl   = compat_ptr_ioctl,
};



fuse_simple_request
    fuse_get_req
        fuse_request_alloc
            struct fuse_req *req = kmem_cache_zalloc(fuse_req_cachep, flags)


fuse_simple_request
	__fuse_simple_request(&invalid_mnt_idmap, fm, args)
		__fuse_request_send(req)
			fuse_send_one(fiq, req)
				fiq->ops->send_req(fiq, req)





do_mpage_readpage -> mpage_readahead - 开始读取页面，@rac：描述要读取哪些页面。@get_block：文件系统的块映射器函数。此函数遍历页面和每个页面内的块，构建并发出大型 BIO。如果发生任何异常情况，例如：遇到具有缓冲区的页面遇到在空洞之后没有空洞的页面遇到具有不连续块的页面，则此代码将放弃并调用基于 buffer_head 的读取函数。它确实处理末尾有空洞的页面 - 这是常见情况：块大小 < PAGE_SIZE 上的文件末尾设置。BH_Boundary 解释：存在问题。mpage 读取代码组装了几个页面，获取它们的所有磁盘映射，然后提交它们。这很好，但获取磁盘映射可能需要 I/O。例如，读取间接块。因此，对 ext2 文件的前 16 个块进行 mpage 读取将导致按以下顺序提交 I/O：12 0 1 2 3 4 5 6 7 8 9 10 11 13 14 15 16，因为必须读取间接块才能获得块 13、14、15、16 的映射。显然，这会影响性能。因此，我们这样做是为了允许文件系统的 get_block() 函数在映射块 11 时设置 BH_Boundary。BH_Boundary 表示：映射此块之后的块将需要针对可能接近此块的块进行 I/O。因此，您应该推送当前积累的 I/O
    args.bio = do_mpage_readpage(&args)
        blk_opf_t opf = REQ_OP_READ





do_read_cache_page
    folio_file_page -> folio_page
        pfn_to_page





[  968.144150]  __schedule+0x2e3/0x740
[  968.144154]  schedule+0x42/0xb0
[  968.144157]  io_schedule+0x16/0x40
[  968.144165]  __blkdev_direct_IO_simple+0x216/0x2f0
[  968.144176]  ? do_user_addr_fault+0x216/0x440
[  968.144181]  ? pat_enabled+0x20/0x20
[  968.144185]  ? blkdev_iopoll+0x40/0x40
[  968.144188]  blkdev_direct_IO+0x66/0x70
[  968.144193]  generic_file_direct_write+0x9c/0x170
[  968.144196]  __generic_file_write_iter+0xbc/0x1d0
[  968.144199]  blkdev_write_iter+0xb8/0x160
[  968.144205]  new_sync_write+0x125/0x1c0
[  968.144208]  __vfs_write+0x29/0x40
[  968.144211]  vfs_write+0xb9/0x1a0
[  968.144217]  ksys_write+0x67/0xe0
[  968.144220]  __x64_sys_write+0x1a/0x20
[  968.144230]  do_syscall_64+0x57/0x190
[  968.144236]  entry_SYSCALL_64_after_hwframe+0x61/0xc6



no cache write:
dd if=/dev/zero of=/dev/vda bs=4096 count=1 oflag=direct seek=4096
...
do_syscall_64
ksys_write
vfs_write
new_sync_write
blkdev_direct_write
    written = blkdev_direct_IO(iocb, from)



const struct file_operations def_blk_fops = {
	.open		= blkdev_open,
	.release	= blkdev_release,
	.llseek		= blkdev_llseek,
	.read_iter	= blkdev_read_iter,
	.write_iter	= blkdev_write_iter,
	.iopoll		= iocb_bio_iopoll,
	.mmap		= blkdev_mmap,
	.fsync		= blkdev_fsync,
	.unlocked_ioctl	= blkdev_ioctl,
	.compat_ioctl	= compat_blkdev_ioctl,
	.splice_read	= filemap_splice_read,
	.splice_write	= iter_file_splice_write,
	.fallocate	= blkdev_fallocate,
};




blk readio timeout:
[  726.504362] INFO: task systemd-udevd:1875 blocked for more than 120 seconds.
[  726.504426]       Tainted: G           OE     5.4.0-196-generic #216-Ubuntu
[  726.504473] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[  726.504525] systemd-udevd   D    0  1875    896 0x80004326
[  726.504531] Call Trace:
[  726.504541]  __schedule+0x2e3/0x740
[  726.504545]  schedule+0x42/0xb0
[  726.504548]  io_schedule+0x16/0x40
[  726.504557]  __lock_page+0x16a/0x2a0
[  726.504562]  ? file_fdatawait_range+0x30/0x30
[  726.504569]  truncate_inode_pages_range+0x4fa/0x890
[  726.504576]  ? iter_to_pipe+0x49/0x240
[  726.504581]  truncate_inode_pages+0x15/0x20
[  726.504584]  kill_bdev+0x32/0x40
[  726.504587]  __blkdev_put+0x89/0x200
[  726.504591]  blkdev_put+0x4e/0xe0
[  726.504593]  blkdev_close+0x26/0x30
[  726.504599]  __fput+0xcc/0x260
[  726.504603]  ____fput+0xe/0x10
[  726.504609]  task_work_run+0x8f/0xb0
[  726.504615]  do_exit+0x36e/0xaf0
[  726.504619]  ? file_fdatawait_range+0x30/0x30
[  726.504623]  do_group_exit+0x47/0xb0
[  726.504626]  get_signal+0x169/0x890
[  726.504629]  ? blkdev_read_iter+0x4a/0x60
[  726.504636]  do_signal+0x37/0x6d0
[  726.504639]  ? vfs_read+0x12e/0x160
[  726.504648]  exit_to_usermode_loop+0xbf/0x160
[  726.504651]  do_syscall_64+0x168/0x190
[  726.504658]  entry_SYSCALL_64_after_hwframe+0x61/0xc6




[  197.306594] load virtio_blk driver, init() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:1041
[  250.968308] dev_init_name:vblk1, xw_new_virtio() virtio.c:540
[  250.968315] xw_virtio_reset() virtio.c:136
[  250.968319] xw_sw vblk1: Virtio device with id 1 does not exist
[  250.970191] sg_elems:1 virtblk_probe() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:761
[  250.970193] num_vqs:0 init_vq() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:518
[  250.970194] num_vqs:1 init_vq() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:524
[  250.970553] xw_virtio_enable_virtqueue() virtio.c:174
[  250.970896] virtblk_queue_depth:512 virtblk_probe() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:789
[  250.970902] virtblk_queue_depth:512 virtblk_probe() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:794
[  250.970903] num_vqs:1 virtblk_probe() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:805
[  250.971367] virtio_max_dma_size, max_size:-1 virtblk_probe() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:843
[  250.971369] VIRTIO_BLK_F_SIZE_MAX, max_size:-1 virtblk_probe() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:851
[  250.971371] VIRTIO_BLK_F_BLK_SIZE, blk_size:0 virtblk_probe() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:858
[  250.971372] blk_size:512 virtblk_probe() /root/xb/project/linux/v5.4/linux/drivers/block/virtio_blk.c:863
[  250.971379] virtio_blk virtio0: [vda] 1048576 512-byte logical blocks (537 MB/512 MiB)





[  164.708400] virtio_blk vblk1: [vda] 1048576 512-byte logical blocks (537 MB/512 MiB)
virtblk_update_capacity
	dev_notice(&vdev->dev,
		   "[%s] %s%llu %d-byte logical blocks (%s/%s)\n",
		   vblk->disk->disk_name,
		   resize ? "new size: " : "",
		   nblocks,
		   queue_logical_block_size(q),
		   cap_str_10,
		   cap_str_2);


磁盘枚举方式
ls -alh /dev/disk/by-path/    包含槽位号
ls -alh /dev/disk/by-id/      包含WWN,硬盘排列方式,从上到下, 从左到右等, 插槽编号
lshw -class disk
ls /sys/bus/virtio/devices/*/block/
hwinfo --disk
cat /proc/partitions
root@tipu-node15:~# lshw -class disk
  *-vblk1                   
       description: Virtual I/O device
       physical id: 0
       bus info: virtio@vblk1
       logical name: /dev/vda
       size: 512MiB (536MB)
       configuration: driver=virtio_blk logicalsectorsize=512 sectorsize=512
  *-vblk2
       description: Virtual I/O device
       physical id: 1
       bus info: virtio@vblk2
       logical name: /dev/vdb
       size: 512MiB (536MB)
       configuration: driver=virtio_blk logicalsectorsize=512 sectorsize=512
  *-disk
       description: ATA Disk
       product: BR 240GB
       physical id: 0.0.0
       bus info: scsi@2:0.0.0
       logical name: /dev/sda
       version: 8A0
       serial: AAPT0000000000001695
       size: 223GiB (240GB)
       capabilities: gpt-1.00 partitioned partitioned:gpt
       configuration: ansiversion=5 guid=2ddb52f9-91a6-48a7-93f8-922eb2ec7dd8 logicalsectorsize=512 sectorsize=512
root@tipu-node15:~# 





bl_open_path




struct uio_info

enum bpf_cmd {
	BPF_MAP_CREATE,
	BPF_MAP_LOOKUP_ELEM,
	BPF_MAP_UPDATE_ELEM,
	BPF_MAP_DELETE_ELEM,
	BPF_MAP_GET_NEXT_KEY,
	BPF_PROG_LOAD,
	BPF_OBJ_PIN,
	BPF_OBJ_GET,
	BPF_PROG_ATTACH,
	BPF_PROG_DETACH,
	BPF_PROG_TEST_RUN,
	BPF_PROG_RUN = BPF_PROG_TEST_RUN,
	BPF_PROG_GET_NEXT_ID,
	BPF_MAP_GET_NEXT_ID,
	BPF_PROG_GET_FD_BY_ID,
	BPF_MAP_GET_FD_BY_ID,
	BPF_OBJ_GET_INFO_BY_FD,
	BPF_PROG_QUERY,
	BPF_RAW_TRACEPOINT_OPEN,
	BPF_BTF_LOAD,
	BPF_BTF_GET_FD_BY_ID,
	BPF_TASK_FD_QUERY,
	BPF_MAP_LOOKUP_AND_DELETE_ELEM,
	BPF_MAP_FREEZE,
	BPF_BTF_GET_NEXT_ID,
	BPF_MAP_LOOKUP_BATCH,
	BPF_MAP_LOOKUP_AND_DELETE_BATCH,
	BPF_MAP_UPDATE_BATCH,
	BPF_MAP_DELETE_BATCH,
	BPF_LINK_CREATE,
	BPF_LINK_UPDATE,
	BPF_LINK_GET_FD_BY_ID,
	BPF_LINK_GET_NEXT_ID,
	BPF_ENABLE_STATS,
	BPF_ITER_CREATE,
	BPF_LINK_DETACH,
	BPF_PROG_BIND_MAP,
};

enum bpf_map_type {
	BPF_MAP_TYPE_UNSPEC,
	BPF_MAP_TYPE_HASH,
	BPF_MAP_TYPE_ARRAY,
	BPF_MAP_TYPE_PROG_ARRAY,
	BPF_MAP_TYPE_PERF_EVENT_ARRAY,
	BPF_MAP_TYPE_PERCPU_HASH,
	BPF_MAP_TYPE_PERCPU_ARRAY,
	BPF_MAP_TYPE_STACK_TRACE,
	BPF_MAP_TYPE_CGROUP_ARRAY,
	BPF_MAP_TYPE_LRU_HASH,
	BPF_MAP_TYPE_LRU_PERCPU_HASH,
	BPF_MAP_TYPE_LPM_TRIE,
	BPF_MAP_TYPE_ARRAY_OF_MAPS,
	BPF_MAP_TYPE_HASH_OF_MAPS,
	BPF_MAP_TYPE_DEVMAP,
	BPF_MAP_TYPE_SOCKMAP,
	BPF_MAP_TYPE_CPUMAP,
	BPF_MAP_TYPE_XSKMAP,
	BPF_MAP_TYPE_SOCKHASH,
	BPF_MAP_TYPE_CGROUP_STORAGE,
	BPF_MAP_TYPE_REUSEPORT_SOCKARRAY,
	BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE,
	BPF_MAP_TYPE_QUEUE,
	BPF_MAP_TYPE_STACK,
	BPF_MAP_TYPE_SK_STORAGE,
	BPF_MAP_TYPE_DEVMAP_HASH,
	BPF_MAP_TYPE_STRUCT_OPS,
	BPF_MAP_TYPE_RINGBUF,
	BPF_MAP_TYPE_INODE_STORAGE,
	BPF_MAP_TYPE_TASK_STORAGE,
};



bpf_map_create
bpf_create_map
samples/bpf/test_lru_dist.c

tools/lib/bpf/bpf_helpers.h
kernel/bpf/helpers.c


Start of BPF helper function descriptions

int bpf_map_update_elem(int fd, const void *key, const void *value, __u64 flags)
    union bpf_attr attr
    attr.key = ptr_to_u64(key)
    sys_bpf(BPF_MAP_UPDATE_ELEM, &attr, attr_sz)

bpf_spin_lock

mount -t bpf/sys/fs/bpf/sysfs/bpf

/sys/fsbpf/shared/ips


blk_mq_alloc_disk
	__blk_mq_alloc_disk
		blk_mq_init_queue_data


typedef void vq_callback_t(struct virtqueue *);

	


mlx5_ib_create_srq
    create_srq_user(ib_srq->pd, srq, &in, udata, buf_size)
        create_srq_split
            MLX5_CMD_OP_CREATE_SRQ
    or create_srq_kernel(dev, srq, &in, buf_size)
    

block req op:
enum req_op {
	/* read sectors from the device */
	REQ_OP_READ		= (__force blk_opf_t)0,
	/* write sectors to the device */
	REQ_OP_WRITE		= (__force blk_opf_t)1,
	/* flush the volatile write cache */
	REQ_OP_FLUSH		= (__force blk_opf_t)2,
	/* discard sectors */
	REQ_OP_DISCARD		= (__force blk_opf_t)3,
	/* securely erase sectors */
	REQ_OP_SECURE_ERASE	= (__force blk_opf_t)5,
	/* write data at the current zone write pointer */
	REQ_OP_ZONE_APPEND	= (__force blk_opf_t)7,
	/* write the zero filled sector many times */
	REQ_OP_WRITE_ZEROES	= (__force blk_opf_t)9,
	/* Open a zone */
	REQ_OP_ZONE_OPEN	= (__force blk_opf_t)10,
	/* Close a zone */
	REQ_OP_ZONE_CLOSE	= (__force blk_opf_t)11,
	/* Transition a zone to full */
	REQ_OP_ZONE_FINISH	= (__force blk_opf_t)12,
	/* reset a zone write pointer */
	REQ_OP_ZONE_RESET	= (__force blk_opf_t)13,
	/* reset all the zone present on the device */
	REQ_OP_ZONE_RESET_ALL	= (__force blk_opf_t)15,

	/* Driver private requests */
	REQ_OP_DRV_IN		= (__force blk_opf_t)34,
	REQ_OP_DRV_OUT		= (__force blk_opf_t)35,

	REQ_OP_LAST		= (__force blk_opf_t)36,
};



#define FUSE_HEADER_OVERHEAD    4
[Virtio-fs] [PATCH 2/3] virtiofs: split requests that exceed virtqueue size

unsigned int max_pages_limit


virtqueue_size = virtqueue_get_vring_size(fs->vqs[VQ_REQUEST].vq);
if (WARN_ON(virtqueue_size <= FUSE_HEADER_OVERHEAD))
	goto out_err;


理论上是的。某些配置可以更轻松地避免这种情况发生，例如使用间接描述符；但是，在这种情况下，virtio 规范表示，即使使用间接描述符，描述符链长度也不应该超过队列大小的长度。因此，让 FUSE 拆分请求也有助于维护该属性。

这是我对潜在循环问题的解读：

virtio_fs_wake_pending_and_unlock
调用
virtio_fs_enqueue_req
调用
virtqueue_add_sgs

如果没有足够的描述符可用，virtqueue_add_sgs 可以返回 -ENOSPC。

此错误会传播回 virtio_fs_wake_pending_and_unlock，它会检查此确切问题并将请求放在工作队列上以便稍后重试提交。

重新提交发生在 virtio_fs_request_dispatch_work 中，它执行类似的操作，如果请求失败并出现 -ENOSPC，它只会将其放回队列中。但是，对于足够大的请求，即使超出了 virtqueue 的容量（即使为空），无论重试多少次都无法使其适合。




virtio ids:
#define VIRTIO_ID_NET			1 /* virtio net */
#define VIRTIO_ID_BLOCK			2 /* virtio block */
#define VIRTIO_ID_CONSOLE		3 /* virtio console */
#define VIRTIO_ID_RNG			4 /* virtio rng */
#define VIRTIO_ID_BALLOON		5 /* virtio balloon */
#define VIRTIO_ID_IOMEM			6 /* virtio ioMemory */
#define VIRTIO_ID_RPMSG			7 /* virtio remote processor messaging */
#define VIRTIO_ID_SCSI			8 /* virtio scsi */
#define VIRTIO_ID_9P			9 /* 9p virtio console */
#define VIRTIO_ID_MAC80211_WLAN		10 /* virtio WLAN MAC */
#define VIRTIO_ID_RPROC_SERIAL		11 /* virtio remoteproc serial link */
#define VIRTIO_ID_CAIF			12 /* Virtio caif */
#define VIRTIO_ID_MEMORY_BALLOON	13 /* virtio memory balloon */
#define VIRTIO_ID_GPU			16 /* virtio GPU */
#define VIRTIO_ID_CLOCK			17 /* virtio clock/timer */
#define VIRTIO_ID_INPUT			18 /* virtio input */
#define VIRTIO_ID_VSOCK			19 /* virtio vsock transport */
#define VIRTIO_ID_CRYPTO		20 /* virtio crypto */
#define VIRTIO_ID_SIGNAL_DIST		21 /* virtio signal distribution device */
#define VIRTIO_ID_PSTORE		22 /* virtio pstore device */
#define VIRTIO_ID_IOMMU			23 /* virtio IOMMU */
#define VIRTIO_ID_MEM			24 /* virtio mem */
#define VIRTIO_ID_SOUND			25 /* virtio sound */
#define VIRTIO_ID_FS			26 /* virtio filesystem */
#define VIRTIO_ID_PMEM			27 /* virtio pmem */
#define VIRTIO_ID_RPMB			28 /* virtio rpmb */
#define VIRTIO_ID_MAC80211_HWSIM	29 /* virtio mac80211-hwsim */
#define VIRTIO_ID_VIDEO_ENCODER		30 /* virtio video encoder */
#define VIRTIO_ID_VIDEO_DECODER		31 /* virtio video decoder */
#define VIRTIO_ID_SCMI			32 /* virtio SCMI */
#define VIRTIO_ID_NITRO_SEC_MOD		33 /* virtio nitro secure module*/
#define VIRTIO_ID_I2C_ADAPTER		34 /* virtio i2c adapter */
#define VIRTIO_ID_WATCHDOG		35 /* virtio watchdog */
#define VIRTIO_ID_CAN			36 /* virtio can */
#define VIRTIO_ID_DMABUF		37 /* virtio dmabuf */
#define VIRTIO_ID_PARAM_SERV		38 /* virtio parameter server */
#define VIRTIO_ID_AUDIO_POLICY		39 /* virtio audio policy */
#define VIRTIO_ID_BT			40 /* virtio bluetooth */
#define VIRTIO_ID_GPIO			41 /* virtio gpio */

/*
 * Virtio Transitional IDs
 */

#define VIRTIO_TRANS_ID_NET		0x1000 /* transitional virtio net */
#define VIRTIO_TRANS_ID_BLOCK		0x1001 /* transitional virtio block */
#define VIRTIO_TRANS_ID_BALLOON		0x1002 /* transitional virtio balloon */
#define VIRTIO_TRANS_ID_CONSOLE		0x1003 /* transitional virtio console */
#define VIRTIO_TRANS_ID_SCSI		0x1004 /* transitional virtio SCSI */
#define VIRTIO_TRANS_ID_RNG		0x1005 /* transitional virtio rng */
#define VIRTIO_TRANS_ID_9P		0x1009 /* transitional virtio 9p console */



/*
 * Userspace usage phase for fsopen/fspick.
 */
enum fs_context_phase {
	FS_CONTEXT_CREATE_PARAMS,	/* Loading params for sb creation */
	FS_CONTEXT_CREATING,		/* A superblock is being created */
	FS_CONTEXT_AWAITING_MOUNT,	/* Superblock created, awaiting fsmount() */
	FS_CONTEXT_AWAITING_RECONF,	/* Awaiting initialisation for reconfiguration */
	FS_CONTEXT_RECONF_PARAMS,	/* Loading params for reconfiguration */
	FS_CONTEXT_RECONFIGURING,	/* Reconfiguring the superblock */
	FS_CONTEXT_FAILED,		/* Failed to correctly transition a context */
};



resize_cq:
IRDMA_MEMREG_TYPE_CQ
if (lvl)
    irdma_check_mem_contiguous(arr, req->cq_pages, pg_size)
hmc_p->addr = arr[0]




ib_resize_cq
    cq->device->ops.resize_cq ? cq->device->ops.resize_cq(cq, cqe, NULL) : -EOPNOTSUPP


nvme_tgt:
module_init(nvmet_rdma_init)
    ib_register_client(&nvmet_rdma_ib_client)
    nvmet_register_transport(&nvmet_rdma_ops)
        nvmet_transports[ops->type] = ops


configfs_symlink -> fs/configfs/symlink.c
    type->ct_item_ops->allow_link(parent_item, target_item) -> nvmet_port_subsys_allow_link
        list_for_each_entry(p, &port->subsystems, entry)
        nvmet_enable_port(port)
            ret = ops->add_port(port) -> nvmet_rdma_add_port
        list_add_tail(&link->entry, &port->subsystems)
        nvmet_port_disc_changed(port, subsys) -> nvmet：启用发现控制器 AEN，添加函数以查找请求发现更改事件的连接，并向维护显式持久连接且具有待处理的活动异步事件请求的主机发送通知。只有有权访问受更改影响的子系统的主机才会收到发现更改事件的通知。每次发生影响发现日志页面的 configfs 更改时，都调用这些函数。设置识别控制器响应中的 OAES 字段以宣传对异步事件通知的支持
            list_for_each_entry(ctrl, &nvmet_disc_subsys->ctrls, subsys_entry)
                nvmet_host_allowed
                __nvmet_disc_changed(port, ctrl)
                    nvmet_add_async_event(ctrl, NVME_AER_TYPE_NOTICE, NVME_AER_NOTICE_DISC_CHANGED, NVME_LOG_DISC)
                        list_add_tail(&aen->entry, &ctrl->async_events)
                        queue_work(nvmet_wq, &ctrl->async_event_work)
                port->tr_ops->discovery_chg(port)


static const struct nvmet_fabrics_ops nvmet_rdma_ops = {
	.owner			= THIS_MODULE,
	.type			= NVMF_TRTYPE_RDMA,
	.msdbd			= 1,
	.flags			= NVMF_KEYED_SGLS | NVMF_METADATA_SUPPORTED,
	.add_port		= nvmet_rdma_add_port,
	.remove_port		= nvmet_rdma_remove_port,
	.queue_response		= nvmet_rdma_queue_response,
	.delete_ctrl		= nvmet_rdma_delete_ctrl,
	.disc_traddr		= nvmet_rdma_disc_port_addr,
	.get_mdts		= nvmet_rdma_get_mdts,
	.get_max_queue_size	= nvmet_rdma_get_max_queue_size,
};

nvmet_rdma_queue_established


nvmet_rdma_add_port
    INIT_DELAYED_WORK(&port->repair_work, nvmet_rdma_repair_port_work)
    inet_pton_with_scope
    nvmet_rdma_enable_port
        cm_id = rdma_create_id(&init_net, nvmet_rdma_cm_handler, port, RDMA_PS_TCP, IB_QPT_RC)
        rdma_bind_addr(cm_id, addr)
        rdma_listen(cm_id, NVMET_RDMA_BACKLOG)
            nvmet_rdma_cm_handler
                case RDMA_CM_EVENT_CONNECT_REQUEST -> nvmet_rdma_queue_connect
                    nvmet_rdma_find_get_device
                        nvmet_rdma_init_srqs
                            nvmet_rdma_init_srq
                                nvmet_rdma_alloc_cmds
                                    nvmet_rdma_alloc_cmd
                                        c->sge[0].addr = ib_dma_map_single(ndev->device, c->nvme_cmd, sizeof(*c->nvme_cmd), DMA_FROM_DEVICE)
                                        if (!admin && nvmet_rdma_alloc_inline_pages(ndev, c))
                                                for (i = 0; i < ndev->inline_page_count; i++, sg++, sge++)
                                                    sg_assign_page(sg, pg) -> Assign a given page to an SG entry
                                                    sge->addr = ib_dma_map_page(ndev->device, pg, 0, PAGE_SIZE, DMA_FROM_DEVICE) -> Map a physical page to DMA address
                                                        if (ib_uses_virt_dma(dev))
                                                            return (uintptr_t)(page_address(page) + offset)
                                                        dma_map_page(dev->dma_device, page, offset, size, direction)
                                        c->cqe.done = nvmet_rdma_recv_done
                    queue = nvmet_rdma_alloc_queue(ndev, cm_id, event)
                        nvmet_sq_init(&queue->nvme_sq)
                        nvmet_rdma_parse_cm_connect_req
                        nvmet_rdma_alloc_rsps
                            nvmet_rdma_alloc_rsp
                                r->send_sge.addr = ib_dma_map_single(ndev->device, r->req.cqe, sizeof(*r->req.cqe), DMA_TO_DEVICE)
                                r->send_cqe.done = nvmet_rdma_send_done
                                r->read_cqe.done = nvmet_rdma_read_data_done
                                r->write_cqe.done = nvmet_rdma_write_data_done
                        nvmet_rdma_alloc_cmds
                        nvmet_rdma_create_queue_ib
                    nvmet_rdma_cm_accept(cm_id, queue, &event->param.conn)
                    list_add_tail(&queue->queue_list, &nvmet_rdma_queue_list)
                case RDMA_CM_EVENT_ESTABLISHED -> nvmet_rdma_queue_established

nvme err:
nvmet_rdma_recv_done
    nvmet_rdma_handle_command
        nvmet_req_init
            nvmet_parse_connect_cmd
                req->execute = nvmet_execute_io_connect
            nvmet_execute_io_connect
                d = kmalloc(sizeof(*d), GFP_KERNEL)
                status = nvmet_copy_from_sgl(req, 0, d, sizeof(*d))
                    sg_pcopy_to_buffer(req->sg, req->sg_cnt, buf, len, off) -> sg_copy_buffer
                        memcpy(buf + offset, miter.addr, len)



nvmet_rdma_create_queue_ib
    nvmet_rdma_post_recv
        ib_post_recv


e810, resize_cq:
IB_USER_VERBS_CMD_RESIZE_CQ -> ib_uverbs_resize_cq
    cq->device->ops.resize_cq(cq, cmd.cqe, &attrs->driver_udata) -> .resize_cq = irdma_resize_cq,
        #define IRDMA_RESIZE_CQ_MIN_REQ_LEN
        if (!(rf->sc_dev.hw_attrs.uk_attrs.feature_flags & IRDMA_FEATURE_CQ_RESIZE))
        if (udata && udata->inlen < IRDMA_RESIZE_CQ_MIN_REQ_LEN)
        if (udata) -> userspace
            iwpbl_buf = irdma_get_pbl((unsigned long)req.user_cq_buffer, &ucontext->cq_reg_mem_list) -> Retrieve pbl from a list given a virtual address
				list_for_each_entry (iwpbl, pbl_list, list)
					if (iwpbl->user_base == va)
						return iwpbl
            cqmr_buf = &iwpbl_buf->cq_mr
			if (iwpbl_buf->pbl_allocated)
				info.virtual_map = true
				info.pbl_chunk_size = 1
				info.first_pm_pbl_idx = cqmr_buf->cq_pbl.idx
			else
				info.cq_pa = cqmr_buf->cq_pbl.addr
        else -> kernel
            kmem_buf.va = dma_alloc_coherent(dev->hw->device, kmem_buf.size, &kmem_buf.pa, GFP_KERNEL)
            info.cq_base = kmem_buf.va
            info.cq_pa = kmem_buf.pa
        cqp_request = irdma_alloc_and_get_cqp_request(&rf->cqp, true)
			cqp_request = list_first_entry(&cqp->cqp_avail_reqs,
			cqp_request->waiting = wait
			refcount_set(&cqp_request->refcnt, 1)
			memset(&cqp_request->compl_info, 0, sizeof(cqp_request->compl_info))
        info.cq_resize = true
        cqp_info->cqp_cmd = IRDMA_OP_CQ_MODIFY -> notify to hw
		cqp_info->in.u.cq_modify.cq = &iwcq->sc_cq -> SC: Send queue's Comletion queue ?
		cqp_info->post_sq = 1
        ret = irdma_handle_cqp_op(rf, cqp_request)
		irdma_put_cqp_request(&rf->cqp, cqp_request)
        if (cq_buf) -> kernel mode
            memcpy(&cq_buf->cq_uk, &iwcq->sc_cq.cq_uk, sizeof(cq_buf->cq_uk))
            INIT_WORK(&cq_buf->work, irdma_free_cqbuf)
            list_add_tail(&cq_buf->list, &iwcq->resize_list)
        irdma_sc_cq_resize(&iwcq->sc_cq, &info)
            cq->virtual_map = info->virtual_map;
            cq->cq_pa = info->cq_pa;
            cq->first_pm_pbl_idx = info->first_pm_pbl_idx;
            cq->pbl_chunk_size = info->pbl_chunk_size;
            irdma_uk_cq_resize(&cq->cq_uk, info->cq_base, info->cq_size)
                cq->cq_base = cq_base; -> change origin cq's cq_base to new cq_base
                cq->cq_size = cq_size;
                IRDMA_RING_INIT(cq->cq_ring, cq->cq_size);
                cq->polarity = 1;
        ibcq->cqe = info.cq_size - 1




RXE kernel Resize CQ:
.resize_cq = rxe_resize_cq
	rxe_cq_chk_attr(rxe, cq, cqe, 0)
	rxe_cq_resize_queue(cq, cqe, uresp, udata)
		rxe_queue_resize(cq->queue, (unsigned int *)&cqe, sizeof(struct rxe_cqe), udata, uresp ? &uresp->mi : NULL, NULL, &cq->cq_lock)
			err = do_mmap_info(new_q->rxe, outbuf, udata, new_q->buf, new_q->buf_size, &new_q->ip)
				ip = rxe_create_mmap_info(rxe, buf_size, udata, buf)
					ip = kmalloc(sizeof(*ip), GFP_KERNEL)
				copy_to_user(outbuf, &ip->info, sizeof(ip->info))
				list_add(&ip->pending_mmaps, &rxe->pending_mmaps)
			if (producer_lock)
				spin_lock_irqsave(producer_lock, producer_flags)
				err = resize_finish(q, new_q, num_elem)
					new_prod = queue_get_producer(new_q, type);
					prod = queue_get_producer(q, type);
					cons = queue_get_consumer(q, type);
					memcpy(queue_addr_from_index(new_q, new_prod), queue_addr_from_index(q, cons), new_q->elem_size)
					swap(*q, *new_q)
			else
				err = resize_finish(q, new_q, num_elem)
			rxe_queue_cleanup(new_q)
				kref_put(&q->ip->ref, rxe_mmap_release)
					list_del(&ip->pending_mmaps)
					vfree(ip->obj)
					kfree(ip)



mlx kernel resize cq:
.resize_cq = mlx5_ib_resize_cq, -> int mlx5_ib_resize_cq(struct ib_cq *ibcq, int entries, struct ib_udata *udata)
    if (!MLX5_CAP_GEN(dev->mdev, cq_resize))
    entries = roundup_pow_of_two(entries + 1)
    if (udata)
        resize_user(dev, cq, entries, udata, &cqe_size)
            umem = ib_umem_get(&dev->ib_dev, ucmd.buf_addr, (size_t)ucmd.cqe_size * entries, IB_ACCESS_LOCAL_WRITE)
            cq->resize_umem = umem -> retrive resize_cq_buf
			*cqe_size = ucmd.cqe_size
        page_size = mlx5_umem_find_best_cq_quantized_pgoff(
			cq->resize_umem, cqc, log_page_size,
			MLX5_ADAPTER_PAGE_SHIFT, page_offset, 64,
			&page_offset_quantized)
        npas = ib_umem_num_dma_blocks(cq->resize_umem, page_size)
        page_shift = order_base_2(page_size)
    else -> kernel
        cqe_size = 64
        err = resize_kernel(dev, cq, entries, cqe_size)
            alloc_cq_frag_buf(dev, cq->resize_buf, entries, cqe_size)
                mlx5_frag_buf_alloc_node(dev->mdev, nent * cqe_size, frag_buf, dev->mdev->priv.numa_node)
                    for (i = 0; i < buf->npages; i++)
                        frag->buf = mlx5_dma_zalloc_coherent_node(dev, frag_sz, &frag->map, node)
                mlx5_init_fbc(frag_buf->frags, log_wq_stride, log_wq_sz, &buf->fbc)
                    mlx5_init_fbc_offset(frags, log_stride, log_sz, 0, fbc)
                        fbc->frags      = frags;
                        fbc->log_stride = log_stride;
                        fbc->log_sz     = log_sz;
                        fbc->sz_m1	= (1 << fbc->log_sz) - 1;
                        fbc->log_frag_strides = PAGE_SHIFT - fbc->log_stride;
                        fbc->frag_sz_m1	= (1 << fbc->log_frag_strides) - 1;
                        fbc->strides_offset = strides_offset;
            init_cq_frag_buf(cq->resize_buf)
                for (i = 0; i < buf->nent; i++)
                    cqe = mlx5_frag_buf_get_wqe(&buf->fbc, i)
                    cqe64 = buf->cqe_size == 64 ? cqe : cqe + 64
                    cqe64->op_own = MLX5_CQE_INVALID << 4
    if (udata)
        mlx5_ib_populate_pas(cq->resize_umem, 1UL << page_shift, pas, 0) -> 填写物理地址列表。ib_umem_num_dma_blocks() 条目将填写在 pas 数组中
			rdma_umem_for_each_dma_block (umem, &biter, page_size)
    else -> kernel
        mlx5_fill_page_frag_array(&cq->resize_buf->frag_buf, pas)
    cqc = MLX5_ADDR_OF(modify_cq_in, in, cq_context)
    mlx5_core_modify_cq(dev->mdev, &cq->mcq, in, inlen)
		MLX5_SET(modify_cq_in, in, opcode, MLX5_CMD_OP_MODIFY_CQ)
    if (udata)
        q->ibcq.cqe = entries - 1
        ib_umem_release(cq->buf.umem)
    else -> kernel
        err = copy_resize_cqes(cq)
			while (get_cqe_opcode(scqe64) != MLX5_CQE_RESIZE_CQ)
				dcqe = mlx5_frag_buf_get_wqe(&cq->resize_buf->fbc, (i + 1) & cq->resize_buf->nent)
					ix  += fbc->strides_offset;
					frag = ix >> fbc->log_frag_strides;
					return fbc->frags[frag].buf + ((fbc->frag_sz_m1 & ix) << fbc->log_stride)
				memcpy(dcqe, scqe, dsize)
				++cq->mcq.cons_index




irdma_initialize_ilq -> create iwarp local queue for cm


irdma_initialize_ieq -> create iwarp exception queue
	irdma_puda_create_rsrc
		irdma_puda_cq_create
			irdma_cqp_cq_create_cmd
				cqp_info->cqp_cmd = IRDMA_OP_CQ_CREATE



IB_USER_VERBS_CMD_CREATE_CQ -> ib_uverbs_create_cq
    create_cq
        cq->comp_handler  = ib_uverbs_comp_handler
			list_add_tail(&entry->list, &ev_queue->event_list)
			list_add_tail(&entry->obj_list, &uobj->comp_list)
			wake_up_interruptible(&ev_queue->poll_wait)
			kill_fasync(&ev_queue->async_queue, SIGIO, POLL_IN)
        cq->event_handler = ib_uverbs_cq_event_handler
			uverbs_uobj_event(&event->element.cq->uobject->uevent, event)
				ib_uverbs_async_handler(eobj->event_file, eobj->uobject.user_handle, event->event, &eobj->event_list, &eobj->events_reported)
		cq->cq_context    = ev_file ? &ev_file->ev_queue : NULL;
        rdma_restrack_new(&cq->res, RDMA_RESTRACK_CQ)
        ret = ib_dev->ops.create_cq(cq, &attr, &attrs->driver_udata) -> irdma_create_cq
			if (udata)
				iwpbl = irdma_get_pbl((unsigned long)req.user_cq_buf, &ucontext->cq_reg_mem_list)
				iwcq->iwpbl = iwpbl
				iwpbl_shadow = irdma_get_pbl((unsigned long)req.user_shadow_area, &ucontext->cq_reg_mem_list)
				iwcq->iwpbl_shadow = iwpbl_shadow
			else -> kernel create_cq
				rsize = info.cq_uk_init_info.cq_size * sizeof(struct irdma_cqe);
				iwcq->kmem.size = ALIGN(round_up(rsize, 256), 256);
				iwcq->kmem.va = dma_alloc_coherent(dev->hw->device, iwcq->kmem.size, &iwcq->kmem.pa, GFP_KERNEL)
				iwcq->kmem_shadow.size = ALIGN(IRDMA_SHADOW_AREA_SIZE << 3, 64)
				iwcq->kmem_shadow.va = dma_alloc_coherent(dev->hw->device, iwcq->kmem_shadow.size, &iwcq->kmem_shadow.pa, GFP_KERNEL)
				info.shadow_area_pa = iwcq->kmem_shadow.pa;
				ukinfo->shadow_area = iwcq->kmem_shadow.va;
				ukinfo->cq_base = iwcq->kmem.va;
				info.cq_base_pa = iwcq->kmem.pa;
            irdma_sc_cq_init(cq, &info))
                cq->cq_pa = info->cq_base_pa
                cq->virtual_map = info->virtual_map
                cq->shadow_area_pa = info->shadow_area_pa
			cqp_info->cqp_cmd = IRDMA_OP_CQ_CREATE
				irdma_sc_cq_create
					set_64bit_val(wqe, 32, (cq->virtual_map ? 0 : cq->cq_pa))
					set_64bit_val(wqe, 40, cq->shadow_area_pa)
		obj->uevent.event_file = READ_ONCE(attrs->ufile->default_async_file)
		resp.base.cq_handle = obj->uevent.uobject.id




cq = rdma_zalloc_drv_obj(ib_dev, ib_cq)
cq->comp_handler  = ib_uverbs_comp_handler;
cq->event_handler = ib_uverbs_cq_event_handler;
ret = ib_dev->ops.create_cq(cq, &attr, &attrs->driver_udata)
uverbs_copy_to(attrs, UVERBS_ATTR_CREATE_CQ_RESP_CQE, &cq->cqe, sizeof(cq->cqe))



poll_cq
set_ib_wc_op_sq




mlx5_ib_create_cq
    check_cq_create_flags(attr->flags)
    entries = roundup_pow_of_two(entries + 1)
    if (udata)
        create_cq_user(dev, udata, cq, entries, &cqb, &cqe_size, &index, &inlen)
            *cqe_size = ucmd.cqe_size
            cq->buf.umem = ib_umem_get(&dev->ib_dev, ucmd.buf_addr, entries * ucmd.cqe_size, IB_ACCESS_LOCAL_WRITE)
            page_size = mlx5_umem_find_best_cq_quantized_pgoff(cq->buf.umem, cqc, log_page_size, MLX5_ADAPTER_PAGE_SHIFT, page_offset, 64, &page_offset_quantized)
            mlx5_ib_db_map_user(context, ucmd.db_addr, &cq->db)
            ncont = ib_umem_num_dma_blocks(cq->buf.umem, page_size)
            mlx5_ib_populate_pas(cq->buf.umem, page_size, pas, 0)
    else
        cqe_size = cache_line_size() == 128 ? 128 : 64
        err = create_cq_kernel(dev, cq, entries, cqe_size, &cqb, &index, &inlen)
        INIT_WORK(&cq->notify_work, notify_soft_wc_handler)
			cq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context)
    err = mlx5_comp_eqn_get(dev->mdev, vector, &eqn)
    cqc = MLX5_ADDR_OF(create_cq_in, cqb, cq_context)
    mlx5_core_create_cq(dev->mdev, &cq->mcq, cqb, inlen, out, sizeof(out))
    userspace: cq->mcq.tasklet_ctx.comp = mlx5_ib_cq_comp
    kernel: cq->mcq.comp  = mlx5_ib_cq_comp
		ibcq->comp_handler(ibcq, ibcq->cq_context) -> ib_uverbs_comp_handler

    cq->mcq.event = mlx5_ib_cq_event


gsi_wr->cqe.done = &handle_single_completion
	generate_completions
		mlx5_ib_generate_wc
			schedule_work(&cq->notify_work)


dma director:
enum dma_data_direction {
	DMA_BIDIRECTIONAL = 0,
	DMA_TO_DEVICE = 1,
	DMA_FROM_DEVICE = 2,
	DMA_NONE = 3,
};


内存类型:
enum memory_type {
	/* 0 is reserved to catch uninitialized type fields */
	MEMORY_DEVICE_PRIVATE = 1,
	MEMORY_DEVICE_COHERENT,
	MEMORY_DEVICE_FS_DAX,
	MEMORY_DEVICE_GENERIC,
	MEMORY_DEVICE_PCI_P2PDMA,
};
将 ZONE_DEVICE 内存专门化为多种类型，每种类型都有不同的用途。
MEMORY_DEVICE_PRIVATE：CPU 无法直接寻址的设备内存：CPU 既不能读取也不能写入私有内存。在这种情况下，我们仍然有支持设备内存的结构页。这样做可以简化实现，但重要的是要记住，在某些时候，必须将结构页视为不透明对象，而不是“普通”结构页。有关不可寻址内存的更完整讨论可以在 include/linux/hmm.h 和 Documentation/mm/hmm.rst 中找到。
MEMORY_DEVICE_COHERENT：从设备和 CPU 的角度来看缓存一致的设备内存。这用于具有高级系统总线（如 CAPI 或 CXL）的平台上。驱动程序可以使用 ZONE_DEVICE 和该内存类型热插拔设备内存。进程的任何页面都可以迁移到此类内存。但是，不应允许任何人固定此类内存，以便始终可以将其逐出。
MEMORY_DEVICE_FS_DAX：具有与系统 RAM 类似的访问语义（即 DMA 一致）并支持页面固定的主机内存。为了支持协调页面固定与其他操作，MEMORY_DEVICE_FS_DAX 会在页面取消固定并变为空闲时安排唤醒事件。此唤醒用于协调物理地址空间管理（例如：fs 截断/打孔）与固定页面（例如：设备 dma）。
MEMORY_DEVICE_GENERIC：具有与系统 RAM 类似的访问语义（即 DMA 一致）并支持页面固定的主机内存。例如，这由使用字符设备公开内存的 DAX 设备使用。MEMORY_DEVICE_PCI_P2PDMA：驻留在 PCI BAR 中的设备内存，旨在用于点对点事务





ib kernel rdma source code sturct:
include/rdma/ib_verbs.h		mlx,intel,sun,cisco... IB接口

struct ib_umem_odp; -> RDMA/umem：在与 ODP 连接的所有函数签名中使用 ib_umem_odp 所有这些函数都已经需要 ODP 版本的 umem 结构，通过签名要求它来明确这一点。这为使用 container_of() 模式将 umem_odp 和 umem 链接在一起铺平了道路




irdma_reg_user_mr
	switch (req.reg_type)
	case IRDMA_MEMREG_TYPE_CQ
		irdma_reg_user_mr_type_cq(req, udata, iwmr)
			lvl = req.cq_pages > 1 ? PBLE_LEVEL_1 : PBLE_LEVEL_0
			irdma_handle_q_mem(iwdev, &req, iwpbl, lvl)
				err = irdma_setup_pbles(iwdev->rf, iwmr, lvl)
                switch (iwmr->type)
                case IRDMA_MEMREG_TYPE_CQ
                    cqmr->shadow = (dma_addr_t)arr[req->cq_pages]
                    if (lvl)
                        ret = irdma_check_mem_contiguous(arr, req->cq_pages, pg_size)
			list_add_tail(&iwpbl->list, &ucontext->cq_reg_mem_list)
            wpbl->on_list = true




ib_uverbs_event_read





ib_uverbs_req_notify_cq
	cq = uobj_get_obj_read(cq, UVERBS_OBJECT_CQ, cmd.cq_handle, attrs)
	ib_req_notify_cq(cq, cmd.solicited_only ? IB_CQ_SOLICITED : IB_CQ_NEXT_COMP) -> ib_req_notify_cq - 请求 CQ 的完成通知。@cq：要为其生成事件的 CQ。@flags：必须包含 %IB_CQ_SOLICITED 或 %IB_CQ_NEXT_COMP 中的一个，以分别请求下一个请求事件或下一个任何类型的工作完成的事件。%IB_CQ_REPORT_MISSED_EVENTS 也可以被加入以请求有关错过事件的提示，如下所述。返回值：< 0 表示请求通知时发生错误 == 0 表示通知请求成功，并且如果传入了 IB_CQ_REPORT_MISSED_EVENTS，则没有错过任何事件，可以安全地等待另一个事件。在这种情况下，可以保证自上次 CQ 轮询以来添加到 CQ 的任何工作完成都将触发完成通知事件。 > 仅当传入 IB_CQ_REPORT_MISSED_EVENTS 时才返回 0。这意味着消费者必须再次轮询 CQ 以确保它是空的，以避免由于请求通知和添加到 CQ 的条目之间的竞争而错过事件。此返回值意味着自上次轮询以来，工作完成可能（但不保证）已添加到 CQ 而没有触发完成通知事件。
		return cq->device->ops.req_notify_cq(cq, flags)


irdma_create_qp
	if (udata)
		err_code = irdma_setup_umode_qp(udata, iwdev, iwqp, &init_info, init_attr)
	else
		INIT_DELAYED_WORK(&iwqp->dwork_flush, irdma_flush_worker)
			void irdma_generate_flush_completions(struct irdma_qp *iwqp)
				if (compl_generated)
					irdma_comp_handler(iwqp->iwscq)
						cq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context) -> 


cq_err_event_notifier
	if (cq->event)
			cq->event(cq, type) -> mlx5_ib_cq_event
                if (ibcq->event_handler)
                    ibcq->event_handler(&event, ibcq->cq_context)



mlx5_ib_stage_dev_notifier_init
	dev->mdev_events.notifier_call = mlx5_ib_event
		INIT_WORK(&work->work, mlx5_ib_handle_event)
			case MLX5_DEV_EVENT_SYS_ERROR
				mlx5_ib_handle_internal_error
					list_for_each_entry(mcq, &cq_armed_list, reset_notify)
						mcq->comp(mcq, NULL)


bitmap_find_next_zero_area - 查找连续对齐的零区域@map：搜索所基于的地址@size：位图大小（以位为单位）@start：开始搜索的位数@nr：我们要查找的零位数@align_mask：零区域的对齐掩码@align_mask 应该比 2 的幂小一；效果是此函数找到的所有零区域的位偏移量都是 2 的幂的倍数。@align_mask 为 0 表示不需要对齐。




gdr:

mlx5 dmabuf: de8f847a5114ff7cfcdfc114af8485c431dec703
RDMA/mlx5：添加对使用 Data-direct 的 DMABUF MR 注册的支持 添加对使用 Data-direct 设备的 DMABUF MR 注册的支持。在用户空间调用注册具有数据直接位设置的 DMABUF MR 时，将遵循以下算法。1) 使用用户输入参数（FD、偏移量、长度）和 DMA PF 设备从 IB 核心获取固定的 DMABUF umem。需要 DMA PF 设备以允许 IOMMU 启用 DMA PF 通过 PCI 访问用户缓冲区。2) 根据用户缓冲区 VA 到 IOVA 映射设置其条目来创建 KSM MKEY，其中 MKEY 是数据直接设备交叉 MKEY。此 KSM MKEY 不可更改，将用作 MR 缓存的一部分。创建它的 PD 是内部设备“数据直接”内核 PD。3) 使用交叉访问模式创建指向 KSM MKEY 的交叉 MKEY。 4) 通过将 KSM MKEY 添加到 mlx5_ib 设备上管理的“数据直接”MKEY 列表中来管理 KSM MKEY。5) 将使用其提供的 PD 创建的交叉 MKEY 返回给用户。在 DMA PF 解除绑定流程中，驱动程序将撤销 KSM 条目。一旦应用程序注销其 MKEY，最终的注销将在后台进行。注意：- 此版本仅支持 PINNED UMEM 模式，因此不依赖于 ODP。- 由于 KSM 的硬件转换，应用程序提供的 IOVA 必须是系统页面对齐的。- 交叉 MKEY 不会是 umrable 或 MR 缓存的一部分，因为我们无法通过 UMR 更改其交叉（即 KSM）MKEY。

mlx5_ib_data_direct_init


enum uverbs_methods_mr {
	UVERBS_METHOD_DM_MR_REG,
	UVERBS_METHOD_MR_DESTROY,
	UVERBS_METHOD_ADVISE_MR,
	UVERBS_METHOD_QUERY_MR,
	UVERBS_METHOD_REG_DMABUF_MR,
};

IOCTL:
static int UVERBS_HANDLER(UVERBS_METHOD_REG_DMABUF_MR)
    mr = pd->device->ops.reg_user_mr_dmabuf(pd, offset, length, iova, fd, access_flags, &attrs->driver_udata)
    mr->type = IB_MR_TYPE_USER
    ret = uverbs_copy_to(attrs, UVERBS_ATTR_REG_DMABUF_MR_RESP_LKEY, &mr->lkey, sizeof(mr->lkey))
    ret = uverbs_copy_to(attrs, UVERBS_ATTR_REG_DMABUF_MR_RESP_RKEY, &mr->rkey, sizeof(mr->rkey))


to str:
enum ib_cm_rej_reason {
	IB_CM_REJ_NO_QP				= 1,
	IB_CM_REJ_NO_EEC			= 2,
	IB_CM_REJ_NO_RESOURCES			= 3,
}
static const char * const ibcm_rej_reason_strs[] = {
	[IB_CM_REJ_NO_QP]			= "no QP",
	[IB_CM_REJ_NO_EEC]			= "no EEC",
}


____cacheline_internodealigned_in_smp:
该宏____cacheline_internodealigned_in_smp在 Linux 内核中用于对齐数据结构，以提高对称多处理 (SMP) 系统的性能。下面是详细的细分：  目的 宏____cacheline_internodealigned_in_smp对齐变量或数据结构，以便它驻留在其自己的缓存行上，并且在某些情况下，驻留在 NUMA（非统一内存访问）系统中其自己的内存节点上。 这有助于最大限度地减少错误共享- 这是一个性能问题，由于多个处理器共享经常更新的内存，因此不必要地使彼此的缓存行无效。 定义 在 Linux 内核源代码中，这个宏在头文件中定义如下include/linux/cache.h（实现可能因架构不同而不同）：  丙  复制代码 #define ____cacheline_internodealigned_in_smp \     __attribute__((__aligned__(1 << INTERNODE_CACHE_SHIFT))) 这里：  INTERNODE_CACHE_SHIFT通常是一个编译时常量，用于指定对齐时要移位的位数。它决定了对齐的大小，通常与缓存行的大小相对应，或针对 NUMA 节点对齐进行调整。 用例 它一般应用于SMP环境中经常访问的全局变量或者静态结构。例如：  丙  复制代码 static int ____cacheline_internodealigned_in_smp my_variable; 这确保my_variable了：  与缓存行边界对齐，以避免 CPU 之间的缓存行争用。 在某些情况下，进一步针对 NUMA 节点局部性进行调整。 SMP 和 NUMA 注意事项 在SMP 系统中，多个 CPU 访问共享内存，对齐变量可以防止缓存行争用。 在NUMA 系统中，内存被划分为节点，每个节点都更靠近特定的 CPU。将数据与内存节点的缓存行边界对齐可以提高访问局部性并减少延迟。 关键点 对齐属性：  该__aligned__属性确保变量的内存地址从指定对齐的倍数开始。 例如，如果缓存行大小为 64 字节，则变量将在 64 字节边界上对齐。 性能优化：  该宏是针对 Linux 内核性能关键部分的一种低级优化技术。 在内核代码中使用：  它通常用于 CPU 之间共享的数据结构，例如每个 CPU 的变量、锁或队列。 Linux 内核中的示例 您可能会在 Linux 内核的每个 CPU 或 NUMA 感知结构中看到这个宏，如下所示：  丙  复制代码 struct shared_data {     int counter;     char padding[64]; // manual cache line alignment (before macros like this existed) };  static struct shared_data ____cacheline_internodealigned_in_smp global_shared_data; 这里，____cacheline_internodealigned_in_smp确保global_shared_data正确对齐，以防止多 CPU 系统中的性能问题。



ceph, rbd, drivers/block/rbd.c
echo 42 > /sys/devices/my_device/add
add_store
	do_rbd_add
		rbd_add_parse_args(buf, &ceph_opts, &rbd_opts, &spec)
		rbdc = rbd_get_client(ceph_opts)
		rc = ceph_pg_poolid_by_name(rbdc->client->osdc.osdmap, spec->pool_name)
		rbd_dev = rbd_dev_create(rbdc, spec, rbd_opts)
		rc = rbd_dev_image_probe(rbd_dev, 0)
		rc = rbd_dev_device_setup(rbd_dev)
			ret = rbd_init_disk(rbd_dev)
				rbd_dev->tag_set.ops = &rbd_mq_ops
				...
			set_capacity(rbd_dev->disk, rbd_dev->mapping.size / SECTOR_SIZE)
			ret = dev_set_name(&rbd_dev->dev, "%d", rbd_dev->dev_id)
		rc = rbd_add_acquire_lock(rbd_dev)
		rc = device_add(&rbd_dev->dev)
		rc = device_add_disk(&rbd_dev->dev, rbd_dev->disk, NULL)
		list_add_tail(&rbd_dev->node, &rbd_dev_list)

static const struct blk_mq_ops rbd_mq_ops = {
	.queue_rq	= rbd_queue_rq,
		rbd_img_request_init(img_req, rbd_dev, op_type)
		INIT_WORK(&img_req->work, rbd_queue_workfn)
			result = rbd_img_fill_from_bio(img_request, offset, length, rq->bio)
				rbd_img_fill_request(img_req, img_extents, num_img_extents, &fctx) -> 将 image 范围列表映射到对象范围列表，创建相应的对象请求（通常每个请求对应不同的对象，但并非总是如此）并将它们添加到@img_req。对于每个对象请求，设置其数据描述符以指向@fctx->pos 数据缓冲区的相应块。由于 ceph_file_to_extents() 会将相邻的对象范围合并在一起，因此每个对象请求的数据描述符可能指向@fctx->pos 数据缓冲区的多个不同块。@fctx->pos 数据缓冲区被认为足够大。
					for (i = 0; i < num_img_extents; i++)
						ret = ceph_file_to_extents(&rbd_dev->layout, img_extents[i].fe_off, img_extents[i].fe_len, &img_req->object_extents, alloc_object_extent, img_req, fctx->count_fn, &fctx->iter)
					__rbd_img_fill_request(img_req)
			rbd_img_handle_request(img_request, 0)
};

...
rbd_obj_write_object
	osd_req = rbd_obj_add_osd_request(obj_req, num_ops)
	rbd_osd_setup_write_ops(osd_req, which)
	rbd_osd_format_write(osd_req)
	ret = ceph_osdc_alloc_messages(osd_req, GFP_NOIO)
	rbd_osd_submit(osd_req)
		ceph_osdc_start_request(osd_req->r_osdc, osd_req)
			submit_request(req, false) -> __submit_request
				ct_res = calc_target(osdc, &req->r_t, false)
				osd = lookup_create_osd(osdc, req->r_t.osd, wrlocked)
				maybe_request_map(osdc)
					if (ceph_monc_want_map(&osdc->client->monc, CEPH_SUB_OSDMAP, osdc->osdmap->epoch + 1, continuous))
		ceph_monc_renew_subs(&osdc->client->monc)
				send_request(req)
					ceph_con_send(&osd->o_con, ceph_msg_get(req->r_request))




ae code:
/* Async Events codes */


irdma_disconnect_worker
	irdma_cm_disconn_true
		last_ae == IRDMA_AE_RDMAP_ROE_BAD_LLP_CLOSE -> RDMA/irdma: Flush iWARP QP if modified to ERR from RTR state,  When connection establishment fails in iWARP mode, an app can drain the QPs and hang because flush isn't issued when the QP is modified from RTR state to error. Issue a flush in this case using function irdma_cm_disconn().  Update irdma_cm_disconn() to do flush when cm_id is NULL, which is the case when the QP is in RTR state and there is an error in the connection establishment


huwei:
static const struct ib_device_ops hns_roce_v2_dev_ops = {
	.destroy_qp = hns_roce_v2_destroy_qp,
		set_bit(HNS_ROCE_STOP_FLUSH_FLAG, &hr_qp->flush_flag)
		flush_work(&hr_qp->flush_work.work)
		if (hr_qp->cong_type == CONG_TYPE_DIP)
			put_dip_ctx_idx(hr_dev, hr_qp) -> RDMA/hns：修复不同 dgid 映射到相同 dip_idx 的问题，DIP 算法要求 dgid 和 dip_idx 之间一一映射。目前使用队列“spare_idx”存储使用 DIP 算法的 QP 的 QPN。对于新的 dgid，使用 spare_idx 中的 QPN 作为 dip_idx。此方法缺乏对 QPN 进行重复数据删除的机制，这可能导致不同的 dgid 共享相同的 dip_idx 并破坏一一映射要求。此补丁用 xarray 替换 spare_idx，并引入 dip_idx 的 refcnt 来指示使用此 dip_idx 的 QP 数量。dip_idx 管理的状态机实现为：* xarray 中索引处的条目为空 - 这表示尚未创建相应的 dip_idx。 * xarray 中索引处的条目不为空，但 refcnt 为 0 -- 这表示相应的 dip_idx 已创建但尚未用作 dip_idx。 * xarray 中索引处的条目不为空，且 refcnt 非 0 -- 这表示相应的 dip_idx 正在被 refcnt 个 DIP QP 使用
		ret = hns_roce_v2_destroy_qp_common(hr_dev, hr_qp, udata)
			if (modify_qp_is_ok(hr_qp))
				ret = hns_roce_v2_modify_qp(&hr_qp->ibqp, NULL, 0, hr_qp->state, IB_QPS_RESET, udata)
			hns_roce_lock_cqs(send_cq, recv_cq)
			if (!udata)
				__hns_roce_v2_cq_clean(recv_cq, hr_qp->qpn, (hr_qp->ibqp.srq ? to_hr_srq(hr_qp->ibqp.srq) : NULL))
					for (prod_index = hr_cq->cons_index; get_sw_cqe_v2(hr_cq, prod_index); ++prod_index) {
						if (prod_index > hr_cq->cons_index + hr_cq->ib_cq.cqe)
							break;
					}
					while ((int) --prod_index - (int) hr_cq->cons_index >= 0)
						cqe = get_cqe_v2(hr_cq, prod_index & hr_cq->ib_cq.cqe)
						memcpy(dest, cqe, hr_cq->cqe_size)
					if (nfreed)
						hr_cq->cons_index += nfreed
						update_cq_db(hr_dev, hr_cq)
				if (send_cq && send_cq != recv_cq)
					__hns_roce_v2_cq_clean(send_cq, hr_qp->qpn, NULL)
			hns_roce_qp_remove(hr_dev, hr_qp)
				list_del(&hr_qp->node)
				list_del(&hr_qp->sq_node)
				list_del(&hr_qp->rq_node)
				__xa_erase(xa, hr_qp->qpn)
		hns_roce_qp_destroy(hr_dev, hr_qp, udata)
			wait_for_completion(&hr_qp->free)
			free_qpc(hr_dev, hr_qp);
			free_qpn(hr_dev, hr_qp);
			free_qp_buf(hr_dev, hr_qp);
			free_kernel_wrid(hr_qp);
			free_qp_db(hr_dev, hr_qp, udata);
			mutex_destroy(&hr_qp->mutex);
	.modify_cq = hns_roce_v2_modify_cq,
	.poll_cq = hns_roce_v2_poll_cq,
	.post_recv = hns_roce_v2_post_recv,
	.post_send = hns_roce_v2_post_send,
	.query_qp = hns_roce_v2_query_qp,
	.req_notify_cq = hns_roce_v2_req_notify_cq,
};



mlx5 create_qp, qpc
create_kernel_qp


ib_uverbs_destroy_cq
    